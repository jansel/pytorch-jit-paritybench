import sys
_module = sys.modules[__name__]
del sys
conf = _module
md2ipynb = _module
mdinclude = _module
generate_evaluations = _module
show_results = _module
anomaly_detection = _module
benchmark_m4 = _module
evaluate_model = _module
example_binned_pareto = _module
gp_synthetic_example = _module
persist_model = _module
warm_start = _module
setup = _module
gluonts = _module
core = _module
_base = _module
component = _module
serde = _module
_dataclass = _module
_json = _module
_parse = _module
flat = _module
np = _module
pd = _module
settings = _module
dataset = _module
arrow = _module
dec = _module
enc = _module
file = _module
artificial = _module
ar_p = _module
recipe = _module
common = _module
field_names = _module
hierarchical = _module
jsonl = _module
loader = _module
multivariate_grouper = _module
pandas = _module
repository = _module
_airpassengers = _module
_artificial = _module
_gp_copula_2019 = _module
_lstnet = _module
_m3 = _module
_m4 = _module
_m5 = _module
_tsf_datasets = _module
_tsf_reader = _module
_uber_tlc = _module
_util = _module
datasets = _module
schema = _module
translate = _module
types = _module
split = _module
stat = _module
util = _module
env = _module
ev = _module
aggregations = _module
evaluator = _module
metrics = _module
stats = _module
ts_stats = _module
evaluation = _module
backtest = _module
exceptions = _module
ext = _module
naive_2 = _module
_predictor = _module
npts = _module
_model = _module
_weighted_sampler = _module
prophet = _module
r_forecast = _module
_hierarchical_predictor = _module
_univariate_predictor = _module
rotbaum = _module
_estimator = _module
_preprocess = _module
statsforecast = _module
gluonts_tqdm = _module
itertools = _module
json = _module
meta = _module
_version = _module
cli = _module
colors = _module
style = _module
model = _module
estimator = _module
forecast = _module
forecast_generator = _module
predictor = _module
seasonal_naive = _module
trivial = _module
constant = _module
identity = _module
mean = _module
mx = _module
activation = _module
batchify = _module
block = _module
cnn = _module
decoder = _module
dropout = _module
enc2dec = _module
encoder = _module
feature = _module
mlp = _module
quantile_output = _module
regularization = _module
rnn = _module
scaler = _module
sndense = _module
snmlp = _module
context = _module
distribution = _module
beta = _module
bijection = _module
bijection_output = _module
binned = _module
box_cox_transform = _module
categorical = _module
deterministic = _module
dirichlet = _module
dirichlet_multinomial = _module
distribution_output = _module
empirical_distribution = _module
gamma = _module
gaussian = _module
genpareto = _module
inflated_beta = _module
iresnet = _module
isqf = _module
laplace = _module
lds = _module
logit_normal = _module
lowrank_gp = _module
lowrank_multivariate_gaussian = _module
mixture = _module
multivariate_gaussian = _module
nan_mixture = _module
neg_binomial = _module
piecewise_linear = _module
poisson = _module
student_t = _module
transformed_distribution = _module
transformed_distribution_output = _module
uniform = _module
kernels = _module
_kernel = _module
_kernel_output = _module
_periodic_kernel = _module
_rbf_kernel = _module
linalg_util = _module
canonical = _module
_network = _module
RNNModel = _module
deep_factor = _module
deepar = _module
deepstate = _module
issm = _module
deepvar = _module
deepvar_hierarchical = _module
gp_forecaster = _module
gaussian_process = _module
gpvar = _module
lstnet = _module
n_beats = _module
_ensemble = _module
renewal = _module
_transform = _module
san = _module
_layers = _module
seq2seq = _module
_forking_estimator = _module
_forking_network = _module
_mq_dnn_estimator = _module
_seq2seq_estimator = _module
_seq2seq_network = _module
simple_feedforward = _module
tft = _module
tpp = _module
deeptpp = _module
base = _module
loglogistic = _module
weibull = _module
transformer = _module
layers = _module
trans_decoder = _module
trans_encoder = _module
wavenet = _module
prelude = _module
representation = _module
binning_helpers = _module
custom_binning = _module
dim_expansion = _module
discrete_pit = _module
embedding = _module
global_relative_binning = _module
hybrid_representation = _module
local_absolute_binning = _module
mean_scaling = _module
representation_chain = _module
trainer = _module
callback = _module
learning_rate_scheduler = _module
model_averaging = _module
model_iteration_averaging = _module
algo_clustering = _module
electricity = _module
exchange_rate = _module
group_raw_data = _module
preprocess_data = _module
synthetic = _module
traffic = _module
ar = _module
ar_estimator = _module
ar_network = _module
lstm = _module
lstm_estimator = _module
lstm_network = _module
preprocess_data = _module
run = _module
Adagrad = _module
Adam = _module
SAdagrad = _module
SAdam = _module
SCSG = _module
SCott = _module
SGD = _module
trainers = _module
timer = _module
nursery = _module
filters = _module
supervised_metrics = _module
_buffered_precision_recall = _module
_precision_recall_utils = _module
_segment_precision_recall = _module
bounded_pr_auc = _module
utils = _module
auto_ode = _module
autogluon_tabular = _module
example = _module
quantile_example = _module
glide = _module
_partition = _module
parallel = _module
pipeline = _module
sequential = _module
gmm_base = _module
simulation = _module
action = _module
sagemaker_sdk = _module
defaults = _module
run_entry_point = _module
train_entry_point = _module
spliced_binned_pareto = _module
data_functions = _module
distr_tcn = _module
gaussian_model = _module
genpareto = _module
spliced_binned_pareto = _module
tcn = _module
training_functions = _module
_main = _module
analysis = _module
ensemble = _module
ensemble_recommender = _module
recommender = _module
ensemble = _module
ensemble_recommender = _module
recommender = _module
surrogate = _module
compute_catch22 = _module
compute_stats = _module
download = _module
upload = _module
ensembles = _module
simulate = _module
evaluations = _module
archive = _module
schedule = _module
config = _module
subprocess = _module
evaluate = _module
tsbench = _module
analyzer = _module
tracking = _module
client = _module
experiment = _module
loocv = _module
misc = _module
mo_metrics = _module
multiprocessing = _module
ranks = _module
_factory = _module
preprocessing = _module
transform = _module
sources = _module
models = _module
constants = _module
aws = _module
analytics = _module
ecr = _module
framework = _module
s3 = _module
session = _module
metric = _module
performance = _module
sagemaker = _module
_evaluations = _module
_info = _module
job = _module
training = _module
fit = _module
logging = _module
forecasts = _module
ensembling = _module
owa = _module
prediction = _module
quantile = _module
callbacks = _module
count = _module
learning_rate = _module
save = _module
_recommendation = _module
generator = _module
replay = _module
greedy = _module
optimal = _module
pareto = _module
surrogate = _module
autogluon = _module
deepset = _module
mlp = _module
nonparametric = _module
random = _module
random_forest = _module
deepset = _module
deepset_lightning_module = _module
losses = _module
mlp_lightning_module = _module
transformers = _module
xgboost = _module
filesystem = _module
latex = _module
scatterplot = _module
shell = _module
dyn = _module
nested_params = _module
params = _module
serve = _module
train = _module
app = _module
testutil = _module
dummy_datasets = _module
equality = _module
time_feature = _module
holiday = _module
lag = _module
seasonality = _module
batchify = _module
component = _module
distributions = _module
affine_transformed = _module
binned_uniforms = _module
discrete_distribution = _module
distribution_output = _module
generalized_pareto = _module
implicit_quantile_network = _module
isqf = _module
mqf2 = _module
piecewise_linear = _module
spliced_binned_pareto = _module
studentT = _module
deep_npts = _module
_estimator = _module
_network = _module
scaling = _module
estimator = _module
lightning_module = _module
module = _module
estimator = _module
forecast = _module
forecast_generator = _module
estimator = _module
icnn_utils = _module
lightning_module = _module
module = _module
predictor = _module
estimator = _module
lightning_module = _module
module = _module
modules = _module
feature = _module
lambda_layer = _module
loss = _module
scaler = _module
util = _module
convert = _module
field = _module
sampler = _module
conftest = _module
test_component = _module
test_serde = _module
test_serde_dataclass = _module
test_serde_flat = _module
test_settings = _module
test_complex_seasonal = _module
test_recipe = _module
test_util = _module
test_translate = _module
test_types = _module
test_arrow = _module
test_common = _module
test_data_loader = _module
test_dataset_mutability = _module
test_dataset_types = _module
test_fieldnames = _module
test_hierarchical = _module
test_jsonl = _module
test_multivariate_grouper = _module
test_pandas = _module
test_split = _module
test_stat = _module
test_train_test_data_leakage = _module
test_tsf_reader = _module
test_writer = _module
test_aggregations = _module
test_metrics_compared_to_previous_approach = _module
test_stats = _module
test_backtest_tools = _module
test_evaluator = _module
test_metrics = _module
test_predictors = _module
test_r_code_compliance_of_naive_2 = _module
test_model = _module
test_npts = _module
test_prophet = _module
test_r_hierarchical_predictor = _module
test_r_univariate_predictor = _module
test_rotbaum_smoke = _module
test_statsforecast = _module
test_forecast = _module
test_predictor = _module
test_moving_average = _module
test_activations = _module
test_feature = _module
test_quantile_loss = _module
test_regularization = _module
test_scaler = _module
test_default_quantile_method = _module
test_distribution_methods = _module
test_distribution_output_serde = _module
test_distribution_output_shapes = _module
test_distribution_sampling = _module
test_distribution_shapes = _module
test_distribution_slice = _module
test_flows = _module
test_inflated_beta = _module
test_isqf = _module
test_issue_287 = _module
test_label_smoothing = _module
test_lds = _module
test_mixture = _module
test_mx_distribution_inference = _module
test_nan_mixture = _module
test_piecewise_linear = _module
test_transformed_distribution = _module
test_periodic_kernel = _module
test_rbf_kernel = _module
test_deepar_auxiliary_outputs = _module
test_deepar_lags = _module
test_deepar_smoke = _module
test_deepstate_smoke = _module
test_issm = _module
test_deepvar = _module
generate_hierarchical_dataset = _module
test_coherency_error = _module
test_deepvar_hierarchical = _module
test_reconcile_samples = _module
test_train_prediction_with_hts = _module
data = _module
test_inference = _module
test_gpvar = _module
test_lstnet = _module
test_cnn = _module
test_encoders = _module
test_forking_sequence_splitter = _module
test_mx_incremental_training = _module
test_deeptpp = _module
test_tpp_predictor = _module
test_bin = _module
test_grb = _module
test_hyb = _module
test_lab = _module
test_mean = _module
test_rep = _module
test_distribution_forecast = _module
test_jitter = _module
test_mx_item_id_info = _module
test_mx_serde = _module
test_mx_util = _module
test_no_batches = _module
test_transform_equals = _module
test_variable_length = _module
test_callbacks = _module
test_learning_rate_scheduler = _module
test_model_averaging = _module
test_model_iteration_averaging = _module
test_trainer = _module
test_filters = _module
test_precision_recall = _module
test_autogluon_tabular = _module
test_entry_point_scripts = _module
test_axiv_paper_examples = _module
test_nested_params = _module
test_shell = _module
test_itertools = _module
test_json = _module
test_sanity = _module
test_agg_lags = _module
test_features = _module
test_holiday = _module
test_lag = _module
test_seasonality = _module
test_affine_transformed = _module
test_discrete_distribution = _module
test_studentt = _module
test_torch_isqf = _module
test_torch_piecewise_linear = _module
test_deep_npts = _module
test_deepar_modules = _module
test_estimators = _module
test_modules = _module
test_mqf2_modules = _module
test_torch_forecast = _module
test_torch_incremental_training = _module
test_torch_predictor = _module
test_torch_distribution_inference = _module
test_torch_item_id_info = _module
test_torch_util = _module
test_add_time_features = _module
test_transform = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


import numpy as np


from matplotlib import pyplot as plt


import itertools


import logging


from typing import Dict


from typing import List


from typing import Optional


from typing import Tuple


import math


import pandas as pd


import random


import time


import torch.nn as nn


from torch.utils.tensorboard import SummaryWriter


import copy


from scipy import stats


import torch.nn


from torch.distributions.normal import Normal


import torch.nn.functional as F


from torch import nn


from numbers import Number


from torch.distributions import constraints


from torch.distributions.distribution import Distribution


from torch.distributions.utils import broadcast_all


import torch.optim


import matplotlib


import matplotlib.pyplot as plt


from typing import Any


from typing import cast


from typing import Literal


import numpy.typing as npt


from torch.utils.data import TensorDataset


from torch import optim


from torch.distributions import AffineTransform


from torch.distributions import Distribution


from torch.distributions import TransformedDistribution


from typing import Callable


from typing import Type


from torch.distributions import Beta


from torch.distributions import Gamma


from torch.distributions import NegativeBinomial


from torch.distributions import Normal


from torch.distributions import Poisson


from functools import partial


from typing import Union


from scipy.stats import t as ScipyStudentT


from torch.distributions import StudentT as TorchStudentT


from copy import deepcopy


from torch.distributions import Categorical


from torch.distributions import MixtureSameFamily


from typing import Iterable


from torch.utils.data import DataLoader


from torch.optim.lr_scheduler import ReduceLROnPlateau


from typing import NamedTuple


from typing import Iterator


import inspect


from torch.distributions import Uniform


from itertools import islice


from scipy.special import softmax


from torch.distributions import StudentT


from torch.nn.utils import clip_grad_norm_


from torch.optim import SGD


class ARNetworkBase(nn.Module):

    def __init__(self, prediction_length: int, context_length: int) ->None:
        super().__init__()
        self.prediction_length = prediction_length
        self.context_length = context_length
        self.criterion = nn.SmoothL1Loss(reduction='none')
        modules = []
        modules.append(nn.Linear(context_length, prediction_length))
        self.linear = nn.Sequential(*modules)


class ARTrainingNetwork(ARNetworkBase):

    def forward(self, past_target: torch.Tensor, future_target: torch.Tensor) ->torch.Tensor:
        nu = min(torch.mean(past_target).item(), torch.mean(future_target).item())
        past_target /= 1 + nu
        future_target /= 1 + nu
        prediction = self.linear(past_target)
        loss = self.criterion(prediction, future_target)
        return loss


class ARPredictionNetwork(ARNetworkBase):

    def __init__(self, num_parallel_samples: int=100, *args, **kwargs) ->None:
        super().__init__(*args, **kwargs)
        self.num_parallel_samples = num_parallel_samples

    def forward(self, past_target: torch.Tensor) ->torch.Tensor:
        pass


class LSTMNetworkBase(nn.Module):

    def __init__(self, prediction_length: int, context_length: int, input_size: int=1, hidden_layer_size: int=100, num_layers: int=2) ->None:
        super().__init__()
        self.prediction_length = prediction_length
        self.context_length = context_length
        self.criterion = nn.SmoothL1Loss(reduction='none')
        self.hidden_layer_size = hidden_layer_size
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers)
        self.linear = nn.Linear(hidden_layer_size, prediction_length)


class LSTMTrainingNetwork(LSTMNetworkBase):

    def forward(self, past_target: torch.Tensor, future_target: torch.Tensor) ->torch.Tensor:
        nu = min(torch.mean(past_target).item(), torch.mean(future_target).item())
        past_target /= 1 + nu
        future_target /= 1 + nu
        inputs = past_target.view(past_target.shape[1], past_target.shape[0], 1)
        lstm_out, _ = self.lstm(inputs)
        prediction = self.linear(lstm_out)
        loss = self.criterion(prediction[-1], future_target)
        return loss


class LSTMPredictionNetwork(LSTMNetworkBase):

    def __init__(self, num_parallel_samples: int=100, *args, **kwargs) ->None:
        super().__init__(*args, **kwargs)
        self.num_parallel_samples = num_parallel_samples

    def forward(self, past_target: torch.Tensor) ->torch.Tensor:
        pass


class Chomp1d(torch.nn.Module):
    """
    Removes leading or trailing elements of a time series.

    Takes as input a three-dimensional tensor (`B`, `C`, `L`) where `B` is the
    batch size, `C` is the number of input channels, and `L` is the length of
    the input. Outputs a three-dimensional tensor (`B`, `C`, `L - s`) where `s`
    is the number of elements to remove.

    Args:
        chomp_size : Number of elements to remove.
        last : If True, removes the last elements in the time dimension,
            If False, removes the fist elements.
    """

    def __init__(self, chomp_size: int, last: bool=True):
        super().__init__()
        self.chomp_size = chomp_size
        self.last = last

    def forward(self, x):
        if self.last:
            x_chomped = x[:, :, :-self.chomp_size]
        else:
            x_chomped = x[:, :, self.chomp_size:]
        return x_chomped


class TCNBlock(torch.nn.Module):
    """
    Temporal Convolutional Network block.

    Composed sequentially of two causal convolutions (with leaky ReLU activation functions),
    and a parallel residual connection.

    Takes as input a three-dimensional tensor (`B`, `C`, `L`) where `B` is the
    batch size, `C` is the number of input channels, and `L` is the length of
    the input. Outputs a three-dimensional tensor (`B`, `C`, `L`).

    Args:
        in_channels : Number of input channels.
        out_channels : Number of output channels.
        kernel_size : Kernel size of the applied non-residual convolutions.
        dilation : Dilation parameter of non-residual convolutions.
        bias : If True, adds a learnable bias to the convolutions.
        fwd_time : If True, the network "causal" direction is from past to future (forward),
            if False, the relation is from future to past (backward).
        final : If True, the last activation function is disabled.
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, dilation: int, bias: bool=True, fwd_time: bool=True, final: bool=False):
        super().__init__()
        in_channels = int(in_channels)
        kernel_size = int(kernel_size)
        out_channels = int(out_channels)
        dilation = int(dilation)
        padding = int((kernel_size - 1) * dilation)
        conv1_pre = torch.nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=bias)
        conv1 = torch.nn.utils.weight_norm(conv1_pre)
        chomp1 = Chomp1d(chomp_size=padding, last=fwd_time)
        relu1 = torch.nn.LeakyReLU()
        conv2_pre = torch.nn.Conv1d(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, dilation=dilation, bias=bias)
        conv2 = torch.nn.utils.weight_norm(conv2_pre)
        chomp2 = Chomp1d(padding)
        relu2 = torch.nn.LeakyReLU()
        self.causal = torch.nn.Sequential(conv1, chomp1, relu1, conv2, chomp2, relu2)
        self.upordownsample = torch.nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=1) if in_channels != out_channels else None
        self.activation = torch.nn.LeakyReLU() if final else None

    def forward(self, x):
        out_causal = self.causal(x)
        res = x if self.upordownsample is None else self.upordownsample(x)
        if self.activation is None:
            return out_causal + res
        else:
            return self.activation(out_causal + res)


class DistributionalTCN(torch.nn.Module):
    """
    Distributional Temporal Convolutional Network: a TCN to learn a time-
    varying distribution.

    Composed of a sequence of causal convolution blocks.

    Takes as input a three-dimensional tensor (`B`, `C`, `L`) where `B` is the
    batch size, `C` is the number of input channels, and `L` is the length of
    the input. Outputs a three-dimensional tensor (`B`, `C_out`, `L`).

    Args:
        in_channels : Number of input channels, typically the dimensionality of the time series
        out_channels : Number of output channels, typically the number of parameters in the time series distribution
        kernel_size : Kernel size of the applied non-residual convolutions.
        channels : Number of channels processed in the network and of output channels,
                typically equal to out_channels for simplicity, expand for better performance.
        layers : Depth of the network.
        bias : If True, adds a learnable bias to the convolutions.
        fwd_time : If True the network is the relation relation if from past to future (forward),
                if False, the relation from future to past (backward).
        output_distr: Distribution whose parameters will be specified by the network output
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, channels: int, layers: int, bias: bool=True, fwd_time: bool=True, output_distr=Normal(torch.tensor([0.0]), torch.tensor([1.0]))):
        super().__init__()
        self.out_channels = out_channels
        layers = int(layers)
        net_layers = []
        dilation_size = 1
        for i in range(layers):
            in_channels_block = in_channels if i == 0 else channels
            net_layers.append(TCNBlock(in_channels=in_channels_block, out_channels=channels, kernel_size=kernel_size, dilation=dilation_size, bias=bias, fwd_time=fwd_time, final=False))
            dilation_size *= 2
        net_layers.append(TCNBlock(in_channels=channels, out_channels=self.out_channels, kernel_size=kernel_size, dilation=dilation_size, bias=bias, fwd_time=fwd_time, final=True))
        self.network = torch.nn.Sequential(*net_layers)
        self.output_distr = output_distr

    def forward(self, x):
        net_out = self.network(x)
        net_out_final = net_out[..., -1].squeeze()
        self.output_distr(net_out_final)
        return self.output_distr


class GaussianModel(nn.Module):
    """
    Model to learn a univariate Gaussian distribution.

    Arguments
    ----------
    mu: Mean of the Gaussian distribution
    sigma: Standard deviation of the Gaussian distribution
    device: The torch.device to use, typically cpu or gpu id
    """

    def __init__(self, mu, sigma, device=None):
        super().__init__()
        if device is not None:
            self.device = device
            mu = mu
            sigma = sigma
        self.mu = mu
        self.sigma = sigma
        self.distr = Normal(self.mu, self.sigma)

    def to_device(self, device):
        """
        Moves members to a specified torch.device.
        """
        self.device = device

    def forward(self, x):
        """
        Takes input x as new distribution parameters.
        """
        if len(x.shape) > 1:
            self.mu_batch = x[:, 0]
            self.sigma_batch = F.softplus(x[:, 1])
        else:
            self.mu = x[0]
            self.distr = Normal(self.mu, self.sigma)
        return self.distr

    def log_prob(self, x):
        x = x.view(x.shape.numel())
        if x.shape[0] == 1:
            return self.distr.log_prob(x[0]).view(1)
        log_like_arr = torch.ones_like(x)
        for i in range(len(x)):
            self.mu = self.mu_batch[i]
            self.distr = Normal(self.mu, self.sigma)
            lpxx = self.distr.log_prob(x[i]).view(1)
            log_like_arr[i] = lpxx
        lpx = log_like_arr
        return lpx

    def icdf(self, value):
        return self.distr.icdf(value)


class Binned(torch.nn.Module):
    """
    Binned univariate distribution designed as an nn.Module

    Arguments
    ----------
    bins_lower_bound: The lower bound of the bin edges
    bins_upper_bound: The upper bound of the bin edges
    nbins: The number of equidistance bins to allocate between `bins_lower_bound` and `bins_upper_bound`. Default value is 100.
    smoothing_indicator: The method of smoothing to perform on the bin probabilities
    """

    def __init__(self, bins_lower_bound: float, bins_upper_bound: float, nbins: int=100, smoothing_indicator: Optional[str]=[None, 'cheap', 'kernel'][1], validate_args=None):
        super().__init__()
        assert bins_lower_bound.shape.numel() == 1, 'bins_lower_bound needs to have shape torch.Size([1])'
        assert bins_upper_bound.shape.numel() == 1, 'bins_upper_bound needs to have shape torch.Size([1])'
        assert bins_lower_bound < bins_upper_bound, f'bins_lower_bound {bins_lower_bound} needs to less than bins_upper_bound {bins_upper_bound}'
        self.nbins = nbins
        self.epsilon = np.finfo(np.float32).eps
        self.smooth_indicator = smoothing_indicator
        self.bin_min = bins_lower_bound - self.epsilon * 6
        self.bin_max = bins_upper_bound + self.epsilon * 6
        self.bin_edges = torch.linspace(self.bin_min, self.bin_max, nbins + 1)
        self.bin_widths = self.bin_edges[1:] - self.bin_edges[:-1]
        self.bin_centres = (self.bin_edges[1:] + self.bin_edges[:-1]) * 0.5
        logits = torch.ones(nbins)
        logits = logits / logits.sum() / (1 + self.epsilon) / self.bin_widths.mean()
        self.logits = torch.log(logits)
        self.idx = None
        self.device = None

    def to_device(self, device):
        """
        Moves members to a specified torch.device.
        """
        self.device = device
        self.bin_min = self.bin_min
        self.bin_max = self.bin_max
        self.bin_edges = self.bin_edges
        self.bin_widths = self.bin_widths
        self.bin_centres = self.bin_centres

    def forward(self, x):
        """
        Takes input x as new logits.
        """
        self.logits = x
        return self.logits

    def log_bins_prob(self):
        if self.idx is None:
            log_bins_prob = F.log_softmax(self.logits, dim=0).sub(torch.log(self.bin_widths))
        else:
            log_bins_prob = F.log_softmax(self.logits[self.idx, :], dim=0).sub(torch.log(self.bin_widths))
        return log_bins_prob.float()

    def bins_prob(self):
        bins_prob = self.log_bins_prob().exp()
        return bins_prob

    def bins_cdf(self):
        incomplete_cdf = self.bins_prob().mul(self.bin_widths).cumsum(dim=0)
        zero = 0 * incomplete_cdf[0].view(1)
        return torch.cat((zero, incomplete_cdf))

    def log_binned_p(self, xx):
        """
        Log probability for one datapoint.
        """
        assert xx.shape.numel() == 1, 'log_binned_p() expects univariate'
        vect_above = xx - self.bin_edges[1:]
        vect_below = self.bin_edges[:-1] - xx
        one_hot_bin_indicator = (vect_above * vect_below >= 0).float()
        if xx > self.bin_edges[-1]:
            one_hot_bin_indicator[-1] = 1.0
        elif xx < self.bin_edges[0]:
            one_hot_bin_indicator[0] = 1.0
        if not (one_hot_bin_indicator == 1).sum() == 1:
            None
        if self.smooth_indicator == 'kernel':
            idx_one_hot = torch.argmax(one_hot_bin_indicator)
            kernel = [0.006, 0.061, 0.242, 0.383, 0.242, 0.061, 0.006]
            len_kernel = len(kernel)
            for i in range(len_kernel):
                idx = i - len_kernel // 2 + idx_one_hot
                if idx in range(len(one_hot_bin_indicator)):
                    one_hot_bin_indicator[idx] = kernel[i]
        elif self.smooth_indicator == 'cheap':
            idx_one_hot = torch.argmax(one_hot_bin_indicator)
            if not idx_one_hot + 1 >= len(one_hot_bin_indicator):
                one_hot_bin_indicator[idx_one_hot + 1] = 0.5
            if not idx_one_hot - 1 < 0:
                one_hot_bin_indicator[idx_one_hot - 1] = 0.5
            if not idx_one_hot + 2 >= len(one_hot_bin_indicator):
                one_hot_bin_indicator[idx_one_hot + 2] = 0.25
            if not idx_one_hot - 2 < 0:
                one_hot_bin_indicator[idx_one_hot - 2] = 0.25
        logp = torch.dot(one_hot_bin_indicator, self.log_bins_prob())
        return logp

    def log_p(self, xx):
        """
        Log probability for one datapoint `xx`.
        """
        assert xx.shape.numel() == 1, 'log_p() expects univariate'
        return self.log_binned_p(xx)

    def log_prob(self, x):
        """
        Log probability for a tensor of datapoints `x`.
        """
        x = x.view(x.shape.numel())
        self.idx = 0
        if x.shape[0] == 1:
            self.idx = None
        lpx = self.log_p(x[0]).view(1)
        if x.shape.numel() == 1:
            return lpx
        for xx in x[1:]:
            self.idx += 1
            lpxx = self.log_p(xx).view(1)
            lpx = torch.cat((lpx, lpxx), 0)
        self.idx = None
        return lpx

    def cdf_binned_components(self, xx, idx=0, cum_density=torch.tensor([0.0])):
        """
        Cumulative density given bins for one datapoint `xx`, where
        `cum_density` is the cdf up to bin_edges `idx`  which must be lower
        than `xx`
        """
        assert xx.shape.numel() == 1, 'cdf_components() expects univariate'
        bins_range = self.bin_edges[-1] - self.bin_edges[0]
        bin_cdf_relative = torch.tensor([0.0])
        if idx == 0:
            cum_density = torch.tensor([0.0])
        while xx > self.bin_edges[idx] and idx < self.nbins:
            bin_width = self.bin_edges[idx + 1] - self.bin_edges[idx]
            if xx < self.bin_edges[idx + 1]:
                bin_cdf = torch.distributions.uniform.Uniform(self.bin_edges[idx], self.bin_edges[idx + 1]).cdf(xx)
                bin_cdf_relative = bin_cdf * bin_width / bins_range
                break
            else:
                cum_density += self.bins_prob()[idx] * bin_width
                idx += 1
        return idx, cum_density, bin_cdf_relative

    def cdf_components(self, xx, idx=0, cum_density=torch.tensor([0.0])):
        """
        Cumulative density for one datapoint `xx`, where `cum_density` is the
        cdf up to bin_edges `idx` which must be lower than `xx`
        """
        return self.cdf_binned_components(xx, idx, cum_density)

    def cdf(self, x):
        """
        Cumulative density tensor for a tensor of datapoints `x`.
        """
        x = x.view(x.shape.numel())
        sorted_x = x.sort()
        x, unsorted_index = sorted_x.values, sorted_x.indices
        idx, cum_density, bin_cdf_relative = self.cdf_components(x[0], idx=0, cum_density=torch.tensor([0.0]))
        cdf_tensor = (cum_density + bin_cdf_relative).view(1)
        if x.shape.numel() == 1:
            return cdf_tensor
        for xx in x[1:]:
            idx, cum_density, bin_cdf_relative = self.cdf_components(xx, idx, cum_density)
            cdfx = (cum_density + bin_cdf_relative).view(1)
            cdf_tensor = torch.cat((cdf_tensor, cdfx), 0)
        cdf_tensor = cdf_tensor[unsorted_index]
        return cdf_tensor

    def inverse_binned_cdf(self, value):
        """
        Inverse binned cdf of a single quantile `value`
        """
        assert value.shape.numel() == 1, 'inverse_binned_cdf() expects univariate'
        if value == 0.0:
            return self.bin_edges[0]
        if value == 1:
            return self.bin_edges[-1]
        vect_above = value - self.bins_cdf()[1:]
        vect_below = self.bins_cdf()[:-1] - value
        if (vect_above == 0).any():
            result = self.bin_edges[1:][vect_above == 0]
        elif (vect_below == 0).any():
            result = self.bin_edges[:-1][vect_below == 0]
        else:
            one_hot_edge_indicator = vect_above * vect_below >= 0
            low = self.bin_edges[:-1][one_hot_edge_indicator]
            high = self.bin_edges[1:][one_hot_edge_indicator]
            value_relative = value - self.bins_cdf()[:-1][one_hot_edge_indicator]
            result = torch.distributions.uniform.Uniform(low, high).icdf(value_relative)
        return result

    def inverse_cdf(self, value):
        """
        Inverse cdf of a single percentile `value`
        """
        return self.inverse_binned_cdf(value)

    def icdf(self, values):
        """
        Inverse cdf of a tensor of quantile `values`
        """
        if self.device is not None:
            values = values
        values = values.view(values.shape.numel())
        icdf_tensor = self.inverse_cdf(values[0])
        icdf_tensor = icdf_tensor.view(1)
        if values.shape.numel() == 1:
            return icdf_tensor
        for value in values[1:]:
            icdf_value = self.inverse_cdf(value).view(1)
            icdf_tensor = torch.cat((icdf_tensor, icdf_value), 0)
        return icdf_tensor


class BinnedUniforms(Distribution):
    """
    Binned uniforms distribution.

    Args:
        bins_lower_bound (float): The lower bound of the bin edges
        bins_upper_bound (float): The upper bound of the bin edges
        numb_bins (int): The number of equidistance bins to allocate between
            `bins_lower_bound` and `bins_upper_bound`. Default value is 100.
        logits (tensor): the logits defining the probability of each bins.
            These are softmaxed. The tensor is of shape (*batch_shape,)
        validate_args (bool) from the pytorch Distribution class
    """
    arg_constraints = {'logits': constraints.real}
    support = constraints.real
    has_rsample = False

    def __init__(self, bins_lower_bound: float, bins_upper_bound: float, logits: torch.tensor, numb_bins: int=100, validate_args: bool=None):
        assert bins_lower_bound < bins_upper_bound, f'bins_lower_bound {bins_lower_bound} needs to less than bins_upper_bound {bins_upper_bound}'
        assert logits.shape[-1] == numb_bins, 'The distribution requires one logit per bin.'
        self.logits = logits
        setattr(self, 'logits', self.logits)
        device = logits.device
        super(BinnedUniforms, self).__init__(batch_shape=logits.shape[:-1], event_shape=logits.shape[-1], validate_args=validate_args)
        self.numb_bins = numb_bins
        self.bin_min = bins_lower_bound
        self.bin_max = bins_upper_bound
        self.bin_edges = torch.linspace(self.bin_min, self.bin_max, self.numb_bins + 1)
        self.bin_widths = self.bin_edges[1:] - self.bin_edges[:-1]
        self.bin_centres = (self.bin_edges[1:] + self.bin_edges[:-1]) * 0.5
        self.bin_edges = self.bin_edges
        self.bin_widths = self.bin_widths
        self.bin_centres = self.bin_centres

    @property
    def mode(self):
        """
        Returns the mode of the distribution.

        mode.shape : (*batch_shape,)
        """
        bins_prob = self.bins_prob
        values_max, index_max = torch.max(bins_prob, dim=-1)
        indicator_max = values_max.unsqueeze(dim=-1) == bins_prob
        bin_centres = self.bin_centres.unsqueeze(dim=0)
        batch_shape_extended = self.bins_prob[..., 0:1].shape
        bin_centres = bin_centres.repeat(batch_shape_extended)
        mode = (bin_centres * indicator_max).sum(dim=-1)
        return mode

    @property
    def median(self):
        """
        Returns the median of the distribution.

        median.shape : (*batch_shape,)
        """
        return self.icdf(torch.tensor(0.5))

    @property
    def mean(self):
        """
        Returns the mean of the distribution.

        mean.shape : (*batch_shape,)
        """
        batch_shape_extended = self.bins_prob[..., 0:1].shape
        bin_centres = self.bin_centres.unsqueeze(dim=0)
        bin_centres = bin_centres.repeat(batch_shape_extended)
        return torch.mean(bin_centres * self.bins_prob, dim=-1)

    @property
    def bins_prob(self):
        """
        Returns the probability of the observed point to be in each of the bins
        bins_prob.shape: (*batch_shape, event_shape).
        event_shape is numb_bins
        """
        bins_prob = self.log_bins_prob.exp()
        return bins_prob

    @property
    def log_bins_prob(self):
        return F.log_softmax(self.logits, dim=-1)

    def log_prob(self, x):
        """
        Log probability for a tensor of datapoints `x`.
        'x' is to have shape (*batch_shape)
        """
        for i in range(0, len(x.shape)):
            assert x.shape[i] == self.batch_shape[i], 'We expect the input to be a tensor of size batch_shape'
        return self.log_binned_p(x)

    def log_binned_p(self, x):
        """
        Log probability for a tensor of datapoints `x`.
        'x' is to have shape (*batch_shape)

        """
        one_hot_bin_indicator = self.get_one_hot_bin_indicator(x, in_float=True)
        logp = (one_hot_bin_indicator * self.log_bins_prob).sum(dim=-1)
        return logp

    def pdf(self, x):
        """
        Probability for a tensor of data points `x`.
        'x' is to have shape (*batch_shape)
        """
        return torch.exp(self.log_prob(x))

    def get_one_hot_bin_indicator(self, x, in_float=False):
        """
        'x' is to have shape (*batch_shape) which can be for example () or
        (32, ) or (32, 168, )
        """
        for i in range(0, len(x.shape)):
            assert x.shape[i] == self.batch_shape[i], 'We expect the input to be a tensor of size batch_shape'
        numb_dim_batch_shape = len(x.shape)
        x_copy = x
        x = x.unsqueeze(dim=-1)
        upper_edges = self.bin_edges[1:]
        for i in range(0, numb_dim_batch_shape):
            upper_edges = upper_edges.unsqueeze(dim=0)
        lower_edge = self.bin_edges[:-1]
        for i in range(0, numb_dim_batch_shape):
            lower_edge = lower_edge.unsqueeze(dim=0)
        one_hot_bin_indicator = ((lower_edge <= x) * (x < upper_edges)).long()
        is_higher_than_last_edge = x_copy >= self.bin_edges[..., -1]
        is_lower_than_first_edge = x_copy <= self.bin_edges[..., 0]
        one_hot_bin_indicator[..., -1][is_higher_than_last_edge] = 1
        one_hot_bin_indicator[..., 0][is_lower_than_first_edge] = 1
        if not in_float:
            return one_hot_bin_indicator == 1
        else:
            return one_hot_bin_indicator.float()

    def icdf(self, quantiles):
        """
        Inverse cdf of a tensor of quantile `quantiles`
        'quantiles' is of shape (*batch_shape) with values between (0.0, 1.0)

        This is the function to be called from the outside.
        """
        assert (quantiles >= 0.0).all(), 'quantiles must be between (0.0, 1.0)'
        assert (quantiles <= 1.0).all(), 'quantiles must be between (0.0, 1.0)'
        if len(quantiles.shape) == 0 or len(quantiles.shape) == 1 and quantiles.shape[0] == 1:
            batch_shape = self.bins_prob[..., 0].shape
            quantiles = quantiles.repeat(batch_shape)
        for i in range(0, len(quantiles.shape)):
            assert quantiles.shape[i] == self.batch_shape[i], 'We expect the quantile to be either a single float or a tensor of size batch_shape'
        return self._inverse_cdf(quantiles)

    def _inverse_cdf(self, quantiles):
        """
        Inverse cdf of a tensor of quantile `quantiles`
        'quantiles' is of shape (*batch_shape) with values between (0.0, 1.0)
        """
        return self._icdf_binned(quantiles)

    def _icdf_binned(self, quantiles):
        """
        Inverse cdf of a tensor of quantile `quantiles`
        'quantiles' is of shape (*batch_shape) with values between (0.0, 1.0)
        """
        quantiles = quantiles.unsqueeze(dim=-1)
        batch_shape_extended = quantiles.shape
        bins_prob = self.bins_prob
        incomplete_cdf_upper = bins_prob.cumsum(dim=-1)
        incomplete_cdf_lower = torch.zeros_like(incomplete_cdf_upper)
        incomplete_cdf_lower[..., 1:] = incomplete_cdf_upper[..., :-1]
        one_hot_bin_indicator = (incomplete_cdf_lower <= quantiles) * (quantiles < incomplete_cdf_upper)
        higher_than_last = quantiles[..., 0] >= incomplete_cdf_upper[..., -1]
        one_hot_bin_indicator[..., -1][higher_than_last] = True
        upper_edges = self.bin_edges[1:].unsqueeze(dim=0)
        upper_edges = upper_edges.repeat(batch_shape_extended)
        lower_edges = self.bin_edges[:-1].unsqueeze(dim=0)
        lower_edges = lower_edges.repeat(batch_shape_extended)
        bin_width = upper_edges[one_hot_bin_indicator].view(batch_shape_extended) - lower_edges[one_hot_bin_indicator].view(batch_shape_extended)
        prob_bin = bins_prob[one_hot_bin_indicator].view(batch_shape_extended)
        prob_left = quantiles.view(batch_shape_extended) - incomplete_cdf_lower[one_hot_bin_indicator].view(batch_shape_extended)
        bin_lower_edge = lower_edges[one_hot_bin_indicator].view(batch_shape_extended)
        result_icdf = bin_width * prob_left / prob_bin + bin_lower_edge
        return result_icdf.squeeze(dim=-1)

    def cdf(self, x):
        """
        Cumulative density tensor for a tensor of data points `x`.
        'x' is expected to be of shape (*batch_shape)
        """
        for i in range(0, len(x.shape)):
            assert x.shape[i] == self.batch_shape[i], 'We expect the input to be a tensor of size batch_shape'
        return self._cdf_binned(x)

    def _cdf_binned(self, x):
        """
        Cumulative density tensor for a tensor of data points `x`.
        'x' is expected to be of shape (*batch_shape)

        The cdf is composed of 2 parts:
            the cdf up to the bin
            the cdf within the bin that the point falls into (modeled with a
            uniform distribution)
        """
        bins_prob = self.bins_prob
        batch_shape_extended = bins_prob[..., 0:1].shape
        one_hot_bin_indicator = self.get_one_hot_bin_indicator(x)
        incomplete_cdf = bins_prob.cumsum(dim=-1) - bins_prob
        cdf_up_to_bin = (incomplete_cdf * one_hot_bin_indicator).sum(dim=-1).unsqueeze(dim=-1)
        upper_edges = self.bin_edges[1:].unsqueeze(dim=0)
        upper_edges = upper_edges.repeat(batch_shape_extended)
        lower_edges = self.bin_edges[:-1].unsqueeze(dim=0)
        lower_edges = lower_edges.repeat(batch_shape_extended)
        bin_width = upper_edges[one_hot_bin_indicator].view(batch_shape_extended) - lower_edges[one_hot_bin_indicator].view(batch_shape_extended)
        dist_in_bin = x.unsqueeze(dim=-1) - lower_edges[one_hot_bin_indicator].view(batch_shape_extended)
        dist_in_bin = torch.max(torch.min(dist_in_bin, bin_width), torch.zeros_like(dist_in_bin))
        cdf_in_bin = bins_prob[one_hot_bin_indicator].view(batch_shape_extended) * dist_in_bin / bin_width
        return (cdf_in_bin + cdf_up_to_bin).reshape(batch_shape_extended).squeeze(dim=-1)

    def sample(self, sample_shape=torch.Size()):
        """
        Returns samples from the distribution.

        Returns:
            samples of shape (*sample_shape, *batch_shape)

        """
        if len(sample_shape) == 0:
            quantiles = torch.rand(self.batch_shape)
            samples = self.icdf(quantiles)
        else:
            samples = torch.zeros(list(sample_shape) + list(self.batch_shape))
            for i in range(sample_shape[0]):
                quantiles = torch.rand(self.batch_shape)
                samples_i = self.icdf(quantiles)
                samples[i, ...] = samples_i
        return samples

    def expand(self, batch_shape, _instance=None):
        raise NotImplementedError

    def variance(self):
        raise NotImplementedError

    def rsample(self, sample_shape=torch.Size()):
        """
        We do not have an implementation for the reparameterization trick yet.
        """
        raise NotImplementedError

    def entropy(self):
        """
        We do not have an implementation of the entropy yet.
        """
        raise NotImplementedError

    def enumerate_support(self, expand=True):
        """
        This is a real valued distribution.
        """
        raise NotImplementedError


class GeneralizedPareto(Distribution):
    """
    Generalised Pareto distribution.

    Parameters
    ----------
    xi
        Tensor containing the xi (heaviness) shape parameters. The tensor is
        of shape (*batch_shape, 1)
    beta
        Tensor containing the beta scale parameters. The tensor is of
        shape (*batch_shape, 1)
    """
    arg_constraints = {'xi': constraints.positive, 'beta': constraints.positive}
    support = constraints.positive
    has_rsample = False

    def __init__(self, xi, beta, validate_args=None):
        self.xi, self.beta = broadcast_all(xi.squeeze(dim=-1), beta.squeeze(dim=-1))
        setattr(self, 'xi', xi)
        setattr(self, 'beta', beta)
        super(GeneralizedPareto, self).__init__()
        if isinstance(xi, Number) and isinstance(beta, Number):
            batch_shape = torch.Size()
        else:
            batch_shape = self.xi.size()
        super(GeneralizedPareto, self).__init__(batch_shape, validate_args=validate_args)
        if self._validate_args and not torch.lt(-self.beta, torch.zeros_like(self.beta)).all():
            raise ValueError('GenPareto is not defined when scale beta<=0')

    @property
    def mean(self):
        """
        Returns the mean of the distribution, of shape (*batch_shape,)
        """
        mu = torch.where(self.xi < 1, torch.div(self.beta, 1 - self.xi), np.nan * torch.ones_like(self.xi))
        return mu

    @property
    def variance(self):
        """
        Returns the variance of the distribution, of shape (*batch_shape,)
        """
        xi, beta = self.xi, self.beta
        var = torch.where(xi < 1 / 2.0, torch.div(beta ** 2, torch.mul((1 - xi) ** 2, 1 - 2 * xi)), np.nan * torch.ones_like(xi))
        return var

    @property
    def stddev(self):
        return torch.sqrt(self.variance)

    def log_prob(self, x):
        """
        Log probability for a tensor x of shape (*batch_shape)
        """
        x = x.unsqueeze(dim=-1)
        logp = -self.beta.log().double()
        logp += torch.where(self.xi == torch.zeros_like(self.xi), -x / self.beta, -(1 + 1.0 / (self.xi + 1e-06)) * torch.log(1 + self.xi * x / self.beta))
        logp = torch.where(x < torch.zeros_like(x), (-np.inf * torch.ones_like(x)).double(), logp)
        return logp.squeeze(dim=-1)

    def cdf(self, x):
        """
        cdf values for a tensor x of shape (*batch_shape)
        """
        x = x.unsqueeze(dim=-1)
        x_shifted = torch.div(x, self.beta)
        u = 1 - torch.pow(1 + self.xi * x_shifted, -torch.reciprocal(self.xi))
        return u.squeeze(dim=-1)

    def icdf(self, value):
        """
        icdf values for a tensor quantile values of shape (*batch_shape)
        """
        value = value.unsqueeze(dim=-1)
        x_shifted = torch.div(torch.pow(1 - value, -self.xi) - 1, self.xi)
        x = torch.mul(x_shifted, self.beta)
        return x.squeeze(dim=-1)


class SplicedBinnedPareto(BinnedUniforms):
    """
    Spliced Binned-Pareto univariate distribution.

    Arguments
    ----------
        bins_lower_bound: The lower bound of the bin edges
        bins_upper_bound: The upper bound of the bin edges
        numb_bins: The number of equidistance bins to allocate between
            `bins_lower_bound` and `bins_upper_bound`. Default value is 100.
        tail_percentile_gen_pareto: The percentile of the distribution that is
            each tail. Default value is 0.05. NB: This symmetric percentile
            can still represent asymmetric upper and lower tails.
    """
    arg_constraints = {'logits': constraints.real, 'lower_gp_xi': constraints.positive, 'lower_gp_beta': constraints.positive, 'upper_gp_xi': constraints.positive, 'upper_gp_beta': constraints.positive}
    support = constraints.real
    has_rsample = False

    def __init__(self, bins_lower_bound: float, bins_upper_bound: float, logits: torch.tensor, upper_gp_xi: torch.tensor, upper_gp_beta: torch.tensor, lower_gp_xi: torch.tensor, lower_gp_beta: torch.tensor, numb_bins: int=100, tail_percentile_gen_pareto: float=0.05, validate_args=None):
        assert tail_percentile_gen_pareto > 0 and tail_percentile_gen_pareto < 0.5, 'tail_percentile_gen_pareto must be between (0,1)'
        self.tail_percentile_gen_pareto = torch.tensor(tail_percentile_gen_pareto)
        device = logits.device
        self.tail_percentile_gen_pareto = self.tail_percentile_gen_pareto
        self.lower_gp_xi = lower_gp_xi
        self.lower_gp_beta = lower_gp_beta
        self.lower_gen_pareto = GeneralizedPareto(self.lower_gp_xi, self.lower_gp_beta)
        self.upper_gp_xi = upper_gp_xi
        self.upper_gp_beta = upper_gp_beta
        self.upper_gen_pareto = GeneralizedPareto(self.upper_gp_xi, self.upper_gp_beta)
        setattr(self, 'lower_gp_xi', self.lower_gp_xi)
        setattr(self, 'lower_gp_beta', self.lower_gp_beta)
        setattr(self, 'upper_gp_xi', self.upper_gp_xi)
        setattr(self, 'upper_gp_beta', self.upper_gp_beta)
        super(SplicedBinnedPareto, self).__init__(bins_lower_bound, bins_upper_bound, logits, numb_bins, validate_args)

    def log_prob(self, x: torch.tensor, for_training=True):
        """
        Arguments
        ----------
        x: a tensor of size 'batch_size', 1
        for_training: boolean to indicate a return of the log-probability, or
            of the loss (which is an adjusted log-probability)
        """
        upper_percentile = self._icdf_binned(torch.ones_like(x) * (1 - self.tail_percentile_gen_pareto))
        lower_percentile = self._icdf_binned(torch.ones_like(x) * self.tail_percentile_gen_pareto)
        upper_percentile = upper_percentile.detach()
        lower_percentile = lower_percentile.detach()
        logp_bins = self.log_binned_p(x)
        logp = logp_bins.double()
        upper_gen_pareto_log_prob = self.upper_gen_pareto.log_prob(torch.abs(x.squeeze(dim=-1) - upper_percentile)) + torch.log(self.tail_percentile_gen_pareto)
        lower_gen_pareto_log_prob = self.lower_gen_pareto.log_prob(torch.abs(lower_percentile - x.squeeze(dim=-1))) + torch.log(self.tail_percentile_gen_pareto)
        if for_training:
            logp += torch.where(x > upper_percentile, upper_gen_pareto_log_prob, torch.zeros_like(logp))
            logp += torch.where(x < lower_percentile, lower_gen_pareto_log_prob, torch.zeros_like(logp))
        else:
            logp = torch.where(x > upper_percentile, upper_gen_pareto_log_prob, logp)
            logp = torch.where(x < lower_percentile, lower_gen_pareto_log_prob, logp)
        return logp

    def pdf(self, x):
        """
        Probability for a tensor of data points `x`.
        'x' is to have shape (*batch_shape)
        """
        return torch.exp(self.log_prob(x, for_training=False))

    def _inverse_cdf(self, quantiles: torch.tensor):
        """
        Inverse cdf of a tensor of quantile `quantiles`
        'quantiles' is of shape (*batch_shape) with values between (0.0, 1.0)
        """
        icdf_body = self._icdf_binned(quantiles)
        adjusted_percentile_for_lower = 1 - quantiles / self.tail_percentile_gen_pareto
        icdf_lower = self._icdf_binned(torch.ones_like(quantiles) * self.tail_percentile_gen_pareto) - self.lower_gen_pareto.icdf(adjusted_percentile_for_lower)
        adjusted_percentile_for_upper = (quantiles - (1.0 - self.tail_percentile_gen_pareto)) / self.tail_percentile_gen_pareto
        icdf_upper = self.upper_gen_pareto.icdf(adjusted_percentile_for_upper) + self._icdf_binned(torch.ones_like(quantiles) * (1 - self.tail_percentile_gen_pareto))
        icdf_value = icdf_body
        icdf_value = torch.where(quantiles < self.tail_percentile_gen_pareto, icdf_lower, icdf_value)
        icdf_value = torch.where(quantiles > 1 - self.tail_percentile_gen_pareto, icdf_upper, icdf_value)
        return icdf_value

    def cdf(self, x: torch.tensor):
        """
        Cumulative density tensor for a tensor of data points `x`.
        'x' is expected to be of shape (*batch_shape)
        """
        for i in range(0, len(x.shape)):
            assert x.shape[i] == self.batch_shape[i], 'We expect the input to be a tensor of size batch_shape'
        lower_percentile_value = self.icdf(self.tail_percentile_gen_pareto)
        upper_percentile_value = self.icdf(1 - self.tail_percentile_gen_pareto)
        cdf_body = self._cdf_binned(x)
        adjusted_x_for_lower = lower_percentile_value - x
        cdf_lower = (1.0 - self.lower_gen_pareto.cdf(adjusted_x_for_lower)) * self.tail_percentile_gen_pareto
        adjusted_x_for_upper = x - upper_percentile_value
        cdf_upper = self.upper_gen_pareto.cdf(adjusted_x_for_upper) * self.tail_percentile_gen_pareto + (1 - self.tail_percentile_gen_pareto)
        cdf_value = cdf_body
        cdf_value = torch.where(x < lower_percentile_value, cdf_lower, cdf_value)
        cdf_value = torch.where(x > upper_percentile_value, cdf_upper, cdf_value)
        return cdf_value


class TCN(torch.nn.Module):
    """
    Temporal Convolutional Network.

    Composed of a sequence of causal convolution blocks.

    Takes as input a three-dimensional tensor (`B`, `C`, `L`) where `B` is the
    batch size, `C` is the number of input channels, and `L` is the length of
    the input. Outputs a three-dimensional tensor (`B`, `C_out`, `L`).

    Args:
        in_channels : Number of input channels.
        out_channels : Number of output channels.
        kernel_size : Kernel size of the applied non-residual convolutions.
        channels : Number of channels processed in the network and of output
            channels.
        layers : Depth of the network.
        bias : If True, adds a learnable bias to the convolutions.
        fwd_time : If True the network is the relation relation if from past to future (forward),
            if False, the relation from future to past (backward).
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, channels: int, layers: int, bias: bool=True, fwd_time: bool=True):
        super().__init__()
        layers = int(layers)
        net_layers = []
        dilation_size = 1
        for i in range(layers):
            in_channels_block = in_channels if i == 0 else channels
            net_layers.append(TCNBlock(in_channels=in_channels_block, out_channels=channels, kernel_size=kernel_size, dilation=dilation_size, bias=bias, fwd_time=fwd_time, final=False))
            dilation_size *= 2
        net_layers.append(TCNBlock(in_channels=channels, out_channels=out_channels, kernel_size=kernel_size, dilation=dilation_size, bias=bias, fwd_time=fwd_time, final=True))
        self.network = torch.nn.Sequential(*net_layers)

    def forward(self, x):
        return self.network(x)


class _MLP(nn.Sequential):

    def __init__(self, layer_dims: List[int], dropout: float):
        layers = []
        for i, (in_size, out_size) in enumerate(zip(layer_dims, layer_dims[1:])):
            if i > 0:
                layers.append(nn.LeakyReLU())
                if dropout > 0:
                    layers.append(nn.Dropout(dropout))
            layers.append(nn.Linear(in_size, out_size))
        super().__init__(*layers)


class DeepSetModel(nn.Module):
    """
    A model built on the DeepSet architecture.
    """

    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, encoder_dims: List[int], decoder_dims: List[int], dropout: float):
        super().__init__()
        self.encoder = _MLP([input_dim] + encoder_dims + [hidden_dim], dropout)
        self.decoder = _MLP([hidden_dim + 1] + decoder_dims + [output_dim], dropout)

    def forward(self, x: torch.Tensor, lengths: torch.Tensor) ->torch.Tensor:
        """
        Computes the model output by mapping all inputs independently into the
        latent space and averaging contiguous blocks of latent representations
        as specified by the lengths.

        Then passes the averaged latent representations along with the number
        of members that have been averaged to the decoder.
        """
        z = self.encoder(x)
        indices = torch.zeros(z.size(0), dtype=torch.long, device=z.device)
        indices[lengths.cumsum(0)[:-1]] = 1
        groups = indices.cumsum(0)
        encodings = torch.zeros(lengths.size(0), z.size(1), dtype=z.dtype, device=z.device)
        encodings.index_add_(0, groups, z)
        return self.decoder(torch.cat([encodings / lengths, lengths.float().unsqueeze(1)], dim=1))


class ListMLELoss(nn.Module):
    """
    Loss that is used for ListMLE.

    For each feature, ranking is performed independently. A lower score
    indicates a lower rank (i.e. "better" value).
    """

    def __init__(self, discount: Optional[Literal['logarithmic', 'linear', 'quadratic']]=None):
        """
        Args:
            discount: The type of discount to apply. If discounting, higher-ranked values are more
                important.
        """
        super().__init__()
        self.discount = discount

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor, group_ids: torch.Tensor) ->torch.Tensor:
        """
        Computes the listwise loss of the predictions.

        Args:
            y_pred: Tensor of shape [N, D] with the predicted values (N: number of items, D:
                number of metrics).
            y_true: Tensor of shape [N, D] with the actual values.
            group_ids: Tensor of shape [N] with the group IDs for each item. All items within a
                group are ranked.

        Returns:
            Tensor of shape [1] containing the loss.
        """
        log_likelihoods = []
        for group_id in group_ids.unique():
            mask = group_ids == group_id
            group_pred = -y_pred[mask]
            group_true = y_true[mask]
            order = group_true.argsort(0)
            num = group_pred.gather(0, order)
            denom = num.flip(0).logcumsumexp(0).flip(0)
            log_likeli = num - denom
            n = log_likeli.size(0)
            if self.discount is not None:
                if self.discount == 'logarithmic':
                    denom = (torch.arange(n) + 2)[:, None].log()
                elif self.discount == 'linear':
                    denom = (torch.arange(n) + 1)[:, None]
                else:
                    denom = (torch.arange(n) + 1)[:, None] ** 2
                log_likeli = log_likeli / denom * (n / denom.reciprocal().sum())
            log_likelihoods.append(log_likeli.mean())
        loss = -torch.stack(log_likelihoods).mean()
        return loss


class PtArgProj(nn.Module):
    """
    A PyTorch module that can be used to project from a dense layer
    to PyTorch distribution arguments.

    Parameters
    ----------
    in_features
        Size of the incoming features.
    dim_args
        Dictionary with string key and int value
        dimension of each arguments that will be passed to the domain
        map, the names are not used.
    domain_map
        Function returning a tuple containing one tensor
        a function or a nn.Module. This will be called with num_args
        arguments and should return a tuple of outputs that will be
        used when calling the distribution constructor.
    """

    def __init__(self, in_features: int, args_dim: Dict[str, int], domain_map: Callable[..., Tuple[torch.Tensor]], **kwargs) ->None:
        super().__init__(**kwargs)
        self.args_dim = args_dim
        self.proj = nn.ModuleList([nn.Linear(in_features, dim) for dim in args_dim.values()])
        self.domain_map = domain_map

    def forward(self, x: torch.Tensor) ->Tuple[torch.Tensor]:
        params_unbounded = [proj(x) for proj in self.proj]
        return self.domain_map(*params_unbounded)


class QuantileLayer(nn.Module):
    """
    Implicit Quantile Layer from the paper ``IQN for Distributional
    Reinforcement Learning`` (https://arxiv.org/abs/1806.06923) by
    Dabney et al. 2018.
    """

    def __init__(self, num_output: int, cos_embedding_dim: int=128):
        super().__init__()
        self.output_layer = nn.Sequential(nn.Linear(cos_embedding_dim, cos_embedding_dim), nn.PReLU(), nn.Linear(cos_embedding_dim, num_output))
        self.register_buffer('integers', torch.arange(0, cos_embedding_dim))

    def forward(self, tau: torch.Tensor) ->torch.Tensor:
        cos_emb_tau = torch.cos(tau.unsqueeze(-1) * self.integers * torch.pi)
        return self.output_layer(cos_emb_tau)


class ImplicitQuantileModule(nn.Module):
    """
    Implicit Quantile Network from the paper ``IQN for Distributional
    Reinforcement Learning`` (https://arxiv.org/abs/1806.06923) by
    Dabney et al. 2018.
    """

    def __init__(self, in_features: int, args_dim: Dict[str, int], domain_map: Callable[..., Tuple[torch.Tensor]], concentration1: float=1.0, concentration0: float=1.0, output_domain_map=None, cos_embedding_dim: int=64):
        super().__init__()
        self.output_domain_map = output_domain_map
        self.domain_map = domain_map
        self.beta = Beta(concentration1=concentration1, concentration0=concentration0)
        self.quantile_layer = QuantileLayer(in_features, cos_embedding_dim=cos_embedding_dim)
        self.output_layer = nn.Sequential(nn.Linear(in_features, in_features), nn.PReLU())
        self.proj = nn.ModuleList([nn.Linear(in_features, dim) for dim in args_dim.values()])

    def forward(self, inputs: torch.Tensor):
        if self.training:
            taus = self.beta.sample(sample_shape=inputs.shape[:-1])
        else:
            taus = torch.rand(size=inputs.shape[:-1], device=inputs.device)
        emb_taus = self.quantile_layer(taus)
        emb_inputs = inputs * (1.0 + emb_taus)
        emb_outputs = self.output_layer(emb_inputs)
        outputs = [proj(emb_outputs).squeeze(-1) for proj in self.proj]
        if self.output_domain_map is not None:
            outputs = [self.output_domain_map(output) for output in outputs]
        return *self.domain_map(*outputs), taus


class FeatureEmbedder(nn.Module):

    def __init__(self, cardinalities: List[int], embedding_dims: List[int]) ->None:
        super().__init__()
        self._num_features = len(cardinalities)
        self._embedders = nn.ModuleList([nn.Embedding(c, d) for c, d in zip(cardinalities, embedding_dims)])

    def forward(self, features: torch.Tensor) ->torch.Tensor:
        if self._num_features > 1:
            cat_feature_slices = torch.chunk(features, self._num_features, dim=-1)
        else:
            cat_feature_slices = [features]
        return torch.cat([embed(cat_feature_slice.squeeze(-1)) for embed, cat_feature_slice in zip(self._embedders, cat_feature_slices)], dim=-1)


def _one_if_too_small(x: torch.Tensor, min_value) ->torch.Tensor:
    return torch.where(x >= min_value, x, torch.ones(tuple(1 for _ in x.shape), dtype=x.dtype))


def min_max_scaling(seq: torch.Tensor, dim=-1, keepdim=False, min_scale=1e-06) ->Tuple[torch.Tensor, torch.Tensor]:
    loc = torch.min(seq, dim=dim, keepdim=keepdim)[0]
    scale = torch.max(seq, dim=dim, keepdim=keepdim)[0] - loc
    return loc, _one_if_too_small(scale, min_value=min_scale)


def standard_normal_scaling(seq: torch.Tensor, dim=-1, keepdim=False, min_scale=1e-06) ->Tuple[torch.Tensor, torch.Tensor]:
    scale, loc = torch.std_mean(seq, dim=dim, keepdim=keepdim)
    return loc, _one_if_too_small(scale, min_value=min_scale)


INPUT_SCALING_MAP = {'min_max_scaling': partial(min_max_scaling, dim=1, keepdim=True), 'standard_normal_scaling': partial(standard_normal_scaling, dim=1, keepdim=True)}


def init_weights(module: nn.Module, scale: float=1.0):
    if type(module) == nn.Linear:
        nn.init.uniform_(module.weight, -scale, scale)
        nn.init.zeros_(module.bias)


def fqname_for(obj: Any) ->str:
    """
    Returns the fully qualified name of ``obj``.

    Parameters
    ----------
    obj
        The class we are interested in.

    Returns
    -------
    str
        The fully qualified name of ``obj``.
    """
    if '<locals>' in obj.__qualname__:
        raise RuntimeError(f"Can't get fully qualified name of locally defined object. {obj.__qualname__}")
    return f'{obj.__module__}.{obj.__qualname__}'


def validated(base_model=None):
    """
    Decorates an ``__init__`` method with typed parameters with validation and
    auto-conversion logic.

    >>> class ComplexNumber:
    ...     @validated()
    ...     def __init__(self, x: float = 0.0, y: float = 0.0) -> None:
    ...         self.x = x
    ...         self.y = y

    Classes with decorated initializers can be instantiated using arguments of
    another type (e.g. an ``y`` argument of type ``str`` ). The decorator
    handles the type conversion logic.

    >>> c = ComplexNumber(y='42')
    >>> (c.x, c.y)
    (0.0, 42.0)

    If the bound argument cannot be converted, the decorator throws an error.

    >>> c = ComplexNumber(y=None)
    Traceback (most recent call last):
        ...
    pydantic.error_wrappers.ValidationError: 1 validation error for
    ComplexNumberModel
    y
      none is not an allowed value (type=type_error.none.not_allowed)

    Internally, the decorator delegates all validation and conversion logic to
    `a Pydantic model <https://pydantic-docs.helpmanual.io/>`_, which can be
    accessed through the ``Model`` attribute of the decorated initializer.

    >>> ComplexNumber.__init__.Model
    <class 'pydantic.main.ComplexNumberModel'>

    The Pydantic model is synthesized automatically from on the parameter
    names and types of the decorated initializer. In the ``ComplexNumber``
    example, the synthesized Pydantic model corresponds to the following
    definition.

    >>> class ComplexNumberModel(BaseValidatedInitializerModel):
    ...     x: float = 0.0
    ...     y: float = 0.0


    Clients can optionally customize the base class of the synthesized
    Pydantic model using the ``base_model`` decorator parameter. The default
    behavior uses :class:`BaseValidatedInitializerModel` and its
    `model config <https://pydantic-docs.helpmanual.io/#config>`_.

    See Also
    --------
    BaseValidatedInitializerModel
        Default base class for all synthesized Pydantic models.
    """

    def validator(init):
        init_qualname = dict(inspect.getmembers(init))['__qualname__']
        init_clsnme = init_qualname.split('.')[0]
        init_params = inspect.signature(init).parameters
        init_fields = {param.name: (param.annotation if param.annotation != inspect.Parameter.empty else Any, param.default if param.default != inspect.Parameter.empty else ...) for param in init_params.values() if param.name != 'self' and param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD}
        if base_model is None:
            PydanticModel = create_model(f'{init_clsnme}Model', __config__=BaseValidatedInitializerModel.Config, **init_fields)
        else:
            PydanticModel = create_model(f'{init_clsnme}Model', __base__=base_model, **init_fields)

        def validated_repr(self) ->str:
            cname = fqname_for(self.__class__)
            kwargs = ', '.join(f'{key}={value!r}' for key, value in self.__init_args__.items())
            return f'{cname}({kwargs})'

        def validated_getnewargs_ex(self):
            return (), self.__init_args__

        @functools.wraps(init)
        def init_wrapper(*args, **kwargs):
            self, *args = args
            nmargs = {name: arg for (name, param), arg in zip(list(init_params.items()), [self] + args) if name != 'self'}
            model = PydanticModel(**{**nmargs, **kwargs})
            all_args = {**nmargs, **kwargs, **model.__dict__}
            if not getattr(self, '__init_args__', {}):
                self.__init_args__ = OrderedDict({name: arg for name, arg in sorted(all_args.items()) if not skip_encoding(arg)})
                self.__class__.__getnewargs_ex__ = validated_getnewargs_ex
                self.__class__.__repr__ = validated_repr
            return init(self, **all_args)
        setattr(init_wrapper, 'Model', PydanticModel)
        return init_wrapper
    return validator


class DiscreteDistribution(torch.distributions.Distribution):
    """
    Implements discrete distribution where the underlying random variable
    takes a value from the finite set `values` with the corresponding
    probabilities.

    Note: `values` can have duplicates in which case the probability mass
    of duplicates is added up.

    A natural loss function, especially given that the new observation does
    not have to be from the finite set `values`, is ranked probability score
    (RPS). For this reason and to be consitent with terminology of other
    models, `log_prob` is implemented as the negative RPS.
    """

    def __init__(self, values: torch.Tensor, probs: torch.Tensor, validate_args: Optional[bool]=None):
        if validate_args:
            total_prob = probs.sum(dim=1)
            assert torch.allclose(total_prob, torch.ones_like(total_prob))
        self.values = values
        self.probs = probs
        self.values_sorted, ix_sorted = torch.sort(values, dim=1)
        self.probs_sorted = probs[torch.arange(values.shape[0]).unsqueeze(-1), ix_sorted]
        self.CDF = torch.cumsum(self.probs_sorted, dim=1)
        if isinstance(values, Number) and isinstance(probs, Number):
            batch_shape = torch.Size()
        else:
            batch_shape = self.values_sorted[:, 0:1].size()
        super().__init__(batch_shape, validate_args=validate_args)

    @staticmethod
    def adjust_probs(values_sorted, probs_sorted):
        """
        Puts probability mass of all duplicate values into one position (last
        index of the duplicate).

        Assumption: `values_sorted` is sorted!

        :param values_sorted:
        :param probs_sorted:
        :return:
        """

        def _adjust_probs_per_element(values_sorted, probs_sorted):
            if torch.sum(torch.diff(values_sorted) == 0) == 0:
                probs_adjusted = probs_sorted
            else:
                _, counts = torch.unique_consecutive(values_sorted, return_counts=True)
                unique_splits = torch.split(probs_sorted, list(counts))
                probs_cumsum_per_split = torch.cat([torch.cumsum(s, dim=0) for s in unique_splits])
                mask_unique_prob = torch.diff(values_sorted, append=values_sorted[-1:] + 1.0) > 0
                probs_adjusted = probs_cumsum_per_split * mask_unique_prob
            return probs_adjusted
        probs_adjusted_it = map(_adjust_probs_per_element, torch.unbind(values_sorted, dim=0), torch.unbind(probs_sorted, dim=0))
        return torch.stack(list(probs_adjusted_it))

    def mean(self):
        return (self.probs_sorted * self.values_sorted).sum(dim=1)

    def log_prob(self, obs: torch.Tensor):
        return -self.rps(obs)

    def rps(self, obs: torch.Tensor, check_for_duplicates: bool=True):
        """
        Implements ranked probability score which is the sum of the qunatile
        losses for all possible quantiles.

        Here, the number of quantiles is finite and is equal to the number of
        unique values in (each batch element of) `obs`.

        Parameters
        ----------
        obs
        check_for_duplicates

        Returns
        -------

        """
        if self._validate_args:
            self._validate_sample(obs)
        probs_sorted, values_sorted = self.probs_sorted, self.values_sorted
        if check_for_duplicates and torch.sum(torch.diff(values_sorted) == 0) > 0:
            probs_sorted = self.adjust_probs(values_sorted=values_sorted, probs_sorted=probs_sorted)
        CDF = torch.cumsum(probs_sorted, dim=1)
        mask_non_zero_prob = (probs_sorted > 0).int()
        quantile_losses = mask_non_zero_prob * self.quantile_losses(obs, quantiles=values_sorted, levels=CDF)
        return torch.sum(quantile_losses, dim=-1) / torch.sum(mask_non_zero_prob, dim=-1)

    def quantile_losses(self, obs: torch.Tensor, quantiles: torch.Tensor, levels: torch.Tensor):
        assert obs.shape[:-1] == quantiles.shape[:-1]
        assert obs.shape[:-1] == levels.shape[:-1]
        assert obs.shape[-1] == 1
        return torch.where(obs >= quantiles, levels * (obs - quantiles), (1 - levels) * (quantiles - obs))

    def sample(self, sample_shape=torch.Size()):
        shape = self._extended_shape(sample_shape)
        with torch.no_grad():
            cat_distr = torch.distributions.Categorical(probs=self.probs_sorted)
            samples_ix = cat_distr.sample().reshape(shape=shape)
            samples = self.values_sorted[torch.arange(samples_ix.shape[0]).unsqueeze(-1), samples_ix]
            assert samples.shape == self.batch_shape
            return samples


class AffineTransformed(TransformedDistribution):
    """
    Represents the distribution of an affinely transformed random variable.

    This is the distribution of ``Y = scale * X + loc``, where ``X`` is a
    random variable distributed according to ``base_distribution``.

    Parameters
    ----------
    base_distribution
        Original distribution
    loc
        Translation parameter of the affine transformation.
    scale
        Scaling parameter of the affine transformation.
    """

    def __init__(self, base_distribution: Distribution, loc=None, scale=None):
        self.scale = 1.0 if scale is None else scale
        self.loc = 0.0 if loc is None else loc
        super().__init__(base_distribution, [AffineTransform(self.loc, self.scale)])

    @property
    def mean(self):
        """
        Returns the mean of the distribution.
        """
        return self.base_dist.mean * self.scale + self.loc

    @property
    def variance(self):
        """
        Returns the variance of the distribution.
        """
        return self.base_dist.variance * self.scale ** 2

    @property
    def stddev(self):
        """
        Returns the standard deviation of the distribution.
        """
        return self.variance.sqrt()


class LambdaLayer(nn.Module):

    def __init__(self, function):
        super().__init__()
        self._func = function

    def forward(self, x, *args):
        return self._func(x, *args)


class Output:
    """
    Class to connect a network to some output.
    """
    in_features: int
    args_dim: Dict[str, int]
    _dtype: Type = np.float32

    @property
    def dtype(self):
        return self._dtype

    @dtype.setter
    def dtype(self, dtype: Type):
        self._dtype = dtype

    def get_args_proj(self, in_features: int) ->nn.Module:
        return PtArgProj(in_features=in_features, args_dim=self.args_dim, domain_map=LambdaLayer(self.domain_map))

    def domain_map(self, *args: torch.Tensor):
        raise NotImplementedError()


def get_lags_for_frequency(freq_str: str, num_lags: Optional[int]=None) ->List[int]:
    offset = to_offset(freq_str)
    if offset.name == 'M':
        lags = [[1, 12]]
    elif offset.name == 'D':
        lags = [[1, 7, 14]]
    elif offset.name == 'B':
        lags = [[1, 2]]
    elif offset.name == 'H':
        lags = [[1, 24, 168]]
    elif offset.name in ('min', 'T'):
        lags = [[1, 4, 12, 24, 48]]
    else:
        lags = [[1]]
    output_lags = list(int(lag) for sub_list in lags for lag in sub_list)
    output_lags = sorted(list(set(output_lags)))
    return output_lags[:num_lags]


def slice_along_dim(a: torch.Tensor, dim: int, slice_: slice) ->torch.Tensor:
    """
    Slice a tensor along a given dimension.

    Parameters
    ----------
    a
        Original tensor to slice.
    dim
        Dimension to slice over.
    slice_
        Slice to take.

    Returns
    -------
    torch.Tensor
        A tensor with the same size as the input one, except dimension
        ``dim`` which has length equal to the slice length.
    """
    idx = [slice(None)] * len(a.shape)
    idx[dim] = slice_
    return a[idx]


def lagged_sequence_values(indices: List[int], prior_sequence: torch.Tensor, sequence: torch.Tensor, dim: int) ->torch.Tensor:
    """
    Constructs an array of lagged values from a given sequence.

    Parameters
    ----------
    indices
        Indices of the lagged observations. For example, ``[0]`` indicates
        that, at any time ``t``, the will have only the observation from
        time ``t`` itself; instead, ``[0, 24]`` indicates that the output
        will have observations from times ``t`` and ``t-24``.
    prior_sequence
        Tensor containing the input sequence prior to the time range for
        which the output is required.
    sequence
        Tensor containing the input sequence in the time range where the
        output is required.
    dim
        Time dimension.

    Returns
    -------
    Tensor
        A tensor of shape (*sequence.shape, len(indices)).
    """
    assert max(indices) <= prior_sequence.shape[dim], f'lags cannot go further than prior sequence length, found lag {max(indices)} while prior sequence is only{prior_sequence.shape[dim]}-long'
    full_sequence = torch.cat((prior_sequence, sequence), dim=dim)
    lags_values = []
    for lag_index in indices:
        begin_index = -lag_index - sequence.shape[dim]
        end_index = -lag_index if lag_index > 0 else None
        lags_values.append(slice_along_dim(full_sequence, dim=dim, slice_=slice(begin_index, end_index)))
    return torch.stack(lags_values, dim=-1)


def prod(xs):
    """
    Compute the product of the elements of an iterable object.
    """
    p = 1
    for x in xs:
        p *= x
    return p


def repeat_along_dim(a: torch.Tensor, dim: int, repeats: int) ->torch.Tensor:
    """
    Repeat a tensor along a given dimension, using ``torch.repeat`` internally.

    Parameters
    ----------
    a
        Original tensor to repeat.
    dim
        Dimension to repeat data over.
    repeats
        How many time to repeat the input tensor.

    Returns
    -------
    torch.Tensor
        A tensor with the same size as the input one, except dimension
        ``dim`` which is multiplied by ``repeats``.
    """
    if repeats == 1:
        return a
    r = [1] * len(a.shape)
    r[dim] = repeats
    return a.repeat(*r)


def unsqueeze_expand(a: torch.Tensor, dim: int, size: int) ->torch.Tensor:
    """
    Unsqueeze a dimension and expand over it in one go.

    Parameters
    ----------
    a
        Original tensor to unsqueeze.
    dim
        Dimension to unsqueeze.
    size
        Size for the new dimension.

    Returns
    -------
    torch.Tensor
        A tensor with an added dimension ``dim`` of size ``size``.
    """
    a = a.unsqueeze(dim)
    sizes = list(a.shape)
    sizes[dim] = size
    return a.expand(*sizes)


class MQF2Distribution(torch.distributions.Distribution):
    """
    Distribution class for the model MQF2 proposed in the paper
    ``Multivariate Quantile Function Forecaster``
    by Kan, Aubet, Januschowski, Park, Benidis, Ruthotto, Gasthaus

    Parameters
    ----------
    picnn
        A SequentialNet instance of a
        partially input convex neural network (picnn)
    hidden_state
        hidden_state obtained by unrolling the RNN encoder
        shape = (batch_size, context_length, hidden_size) in training
        shape = (batch_size, hidden_size) in inference
    prediction_length
        Length of the prediction horizon
    is_energy_score
        If True, use energy score as objective function
        otherwise use maximum likelihood as
        objective function (normalizing flows)
    es_num_samples
        Number of samples drawn to approximate the energy score
    beta
        Hyperparameter of the energy score (power of the two terms)
    threshold_input
        Clamping threshold of the (scaled) input when maximum
        likelihood is used as objective function
        this is used to make the forecaster more robust
        to outliers in training samples
    validate_args
        Sets whether validation is enabled or disabled
        For more details, refer to the descriptions in
        torch.distributions.distribution.Distribution
    """

    def __init__(self, picnn: torch.nn.Module, hidden_state: torch.Tensor, prediction_length: int, is_energy_score: bool=True, es_num_samples: int=50, beta: float=1.0, threshold_input: float=100.0, validate_args: bool=False) ->None:
        self.picnn = picnn
        self.hidden_state = hidden_state
        self.prediction_length = prediction_length
        self.is_energy_score = is_energy_score
        self.es_num_samples = es_num_samples
        self.beta = beta
        self.threshold_input = threshold_input
        super().__init__(batch_shape=self.batch_shape, validate_args=validate_args)
        self.context_length = self.hidden_state.shape[-2] if len(self.hidden_state.shape) > 2 else 1
        self.numel_batch = MQF2Distribution.get_numel(self.batch_shape)
        mu = torch.tensor(0, dtype=hidden_state.dtype, device=hidden_state.device)
        sigma = torch.ones_like(mu)
        self.standard_normal = Normal(mu, sigma)

    def stack_sliding_view(self, z: torch.Tensor) ->torch.Tensor:
        """
        Auxiliary function for loss computation.

        Unfolds the observations by sliding a window of size prediction_length
        over the observations z
        Then, reshapes the observations into a 2-dimensional tensor for
        further computation

        Parameters
        ----------
        z
            A batch of time series with shape
            (batch_size, context_length + prediction_length - 1)

        Returns
        -------
        Tensor
            Unfolded time series with shape
            (batch_size * context_length, prediction_length)
        """
        z = z.unfold(dimension=-1, size=self.prediction_length, step=1)
        z = z.reshape(-1, z.shape[-1])
        return z

    def loss(self, z: torch.Tensor) ->torch.Tensor:
        if self.is_energy_score:
            return self.energy_score(z)
        else:
            return -self.log_prob(z)

    def log_prob(self, z: torch.Tensor) ->torch.Tensor:
        """
        Computes the log likelihood  log(g(z)) + logdet(dg(z)/dz), where g is
        the gradient of the picnn.

        Parameters
        ----------
        z
            A batch of time series with shape
            (batch_size, context_length + prediciton_length - 1)

        Returns
        -------
        loss
            Tesnor of shape (batch_size * context_length,)
        """
        z = torch.clamp(z, min=-self.threshold_input, max=self.threshold_input)
        z = self.stack_sliding_view(z)
        loss = self.picnn.logp(z, self.hidden_state.reshape(-1, self.hidden_state.shape[-1]))
        return loss

    def energy_score(self, z: torch.Tensor) ->torch.Tensor:
        """
        Computes the (approximated) energy score sum_i ES(g,z_i), where
        ES(g,z_i) =

        -1/(2*es_num_samples^2) * sum_{w,w'} ||w-w'||_2^beta
        + 1/es_num_samples * sum_{w''} ||w''-z_i||_2^beta,
        w's are samples drawn from the
        quantile function g(., h_i) (gradient of picnn),
        h_i is the hidden state associated with z_i,
        and es_num_samples is the number of samples drawn
        for each of w, w', w'' in energy score approximation

        Parameters
        ----------
        z
            A batch of time series with shape
            (batch_size, context_length + prediction_length - 1)

        Returns
        -------
        loss
            Tensor of shape (batch_size * context_length,)
        """
        es_num_samples = self.es_num_samples
        beta = self.beta
        z = self.stack_sliding_view(z)
        reshaped_hidden_state = self.hidden_state.reshape(-1, self.hidden_state.shape[-1])
        loss = self.picnn.energy_score(z, reshaped_hidden_state, es_num_samples=es_num_samples, beta=beta)
        return loss

    def rsample(self, sample_shape: torch.Size=torch.Size()) ->torch.Tensor:
        """
        Generates the sample paths.

        Parameters
        ----------
        sample_shape
            Shape of the samples

        Returns
        -------
        sample_paths
            Tesnor of shape (batch_size, *sample_shape, prediction_length)
        """
        numel_batch = self.numel_batch
        prediction_length = self.prediction_length
        num_samples_per_batch = MQF2Distribution.get_numel(sample_shape)
        num_samples = num_samples_per_batch * numel_batch
        hidden_state_repeat = self.hidden_state.repeat_interleave(repeats=num_samples_per_batch, dim=0)
        alpha = torch.rand((num_samples, prediction_length), dtype=self.hidden_state.dtype, device=self.hidden_state.device, layout=self.hidden_state.layout)
        return self.quantile(alpha, hidden_state_repeat).reshape((numel_batch,) + sample_shape + (prediction_length,))

    def quantile(self, alpha: torch.Tensor, hidden_state: Optional[torch.Tensor]=None) ->torch.Tensor:
        """
        Generates the predicted paths associated with the quantile levels
        alpha.

        Parameters
        ----------
        alpha
            quantile levels,
            shape = (batch_shape, prediction_length)
        hidden_state
            hidden_state, shape = (batch_shape, hidden_size)

        Returns
        -------
        results
            predicted paths of shape = (batch_shape, prediction_length)
        """
        if hidden_state is None:
            hidden_state = self.hidden_state
        normal_quantile = self.standard_normal.icdf(alpha)
        if self.is_energy_score:
            result = self.picnn(normal_quantile, context=hidden_state)
        else:
            result = self.picnn.reverse(normal_quantile, context=hidden_state)
        return result

    @staticmethod
    def get_numel(tensor_shape: torch.Size) ->int:
        return torch.prod(torch.tensor(tensor_shape)).item()

    @property
    def batch_shape(self) ->torch.Size:
        return self.hidden_state.shape[:-1]

    @property
    def event_shape(self) ->Tuple:
        return ()

    @property
    def event_dim(self) ->int:
        return 0


def make_linear_layer(dim_in, dim_out):
    lin = nn.Linear(dim_in, dim_out)
    torch.nn.init.uniform_(lin.weight, -0.07, 0.07)
    torch.nn.init.zeros_(lin.bias)
    return lin


def mean_abs_scaling(seq, min_scale=1e-05):
    return seq.abs().mean(1).clamp(min_scale, None).unsqueeze(1)


class FeatureAssembler(nn.Module):

    def __init__(self, T: int, embed_static: Optional[FeatureEmbedder]=None, embed_dynamic: Optional[FeatureEmbedder]=None) ->None:
        super().__init__()
        self.T = T
        self.embeddings = nn.ModuleDict({'embed_static': embed_static, 'embed_dynamic': embed_dynamic})

    def forward(self, feat_static_cat: torch.Tensor, feat_static_real: torch.Tensor, feat_dynamic_cat: torch.Tensor, feat_dynamic_real: torch.Tensor) ->torch.Tensor:
        processed_features = [self.process_static_cat(feat_static_cat), self.process_static_real(feat_static_real), self.process_dynamic_cat(feat_dynamic_cat), self.process_dynamic_real(feat_dynamic_real)]
        return torch.cat(processed_features, dim=-1)

    def process_static_cat(self, feature: torch.Tensor) ->torch.Tensor:
        if self.embeddings['embed_static'] is not None:
            feature = self.embeddings['embed_static'](feature)
        return feature.unsqueeze(1).expand(-1, self.T, -1).float()

    def process_dynamic_cat(self, feature: torch.Tensor) ->torch.Tensor:
        if self.embeddings['embed_dynamic'] is None:
            return feature.float()
        else:
            return self.embeddings['embed_dynamic'](feature)

    def process_static_real(self, feature: torch.Tensor) ->torch.Tensor:
        return feature.unsqueeze(1).expand(-1, self.T, -1)

    def process_dynamic_real(self, feature: torch.Tensor) ->torch.Tensor:
        return feature


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ARPredictionNetwork,
     lambda: ([], {'prediction_length': 4, 'context_length': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ARTrainingNetwork,
     lambda: ([], {'prediction_length': 4, 'context_length': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Chomp1d,
     lambda: ([], {'chomp_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FeatureAssembler,
     lambda: ([], {'T': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4]), torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (GaussianModel,
     lambda: ([], {'mu': 4, 'sigma': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LSTMPredictionNetwork,
     lambda: ([], {'prediction_length': 4, 'context_length': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LSTMTrainingNetwork,
     lambda: ([], {'prediction_length': 4, 'context_length': 4}),
     lambda: ([torch.rand([4, 4, 1]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (LambdaLayer,
     lambda: ([], {'function': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ListMLELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (QuantileLayer,
     lambda: ([], {'num_output': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TCN,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4, 'channels': 4, 'layers': 1}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (TCNBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4, 'dilation': 1}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (_MLP,
     lambda: ([], {'layer_dims': [4, 4], 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_awslabs_gluonts(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

