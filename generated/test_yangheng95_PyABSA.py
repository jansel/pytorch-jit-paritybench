import sys
_module = sys.modules[__name__]
del sys
clean = _module
docs = _module
conf = _module
ensemble_inference = _module
inference = _module
train_apc = _module
train_apc_glove = _module
train_apc_large = _module
train_apc_plm = _module
extract_aspect_and_make_dataset = _module
extract_aspects = _module
train_atepc = _module
aspect_sentiment_classification_augmentation = _module
text_classification_augmentation = _module
configuration = _module
inference2 = _module
preprocess = _module
train_degrad = _module
train_degrad_bert = _module
train_lstm_rna_classification = _module
train_rna_classification = _module
train_lstm_rna_regressor = _module
train_mhsa_rna_regressor = _module
train_rna_regressor = _module
main_tad_bae = _module
main_tad_pwws = _module
main_tad_textfooler = _module
tad_inference = _module
train_tad = _module
glove_classification_inference = _module
train_text_classification = _module
train_text_classification_glove = _module
train_rna_tokenizers = _module
pyabsa = _module
augmentation = _module
apc_augment = _module
text_augment = _module
tc_augment = _module
framework = _module
checkpoint_class = _module
checkpoint_template = _module
checkpoint_utils = _module
configuration_class = _module
config_verification = _module
configuration_template = _module
dataset_class = _module
dataset_dict_class = _module
dataset_template = _module
inference_dataset_class = _module
flag_class = _module
flag_template = _module
instructor_class = _module
instructor_template = _module
prediction_class = _module
predictor_template = _module
tokenizer_class = _module
trainer_class = _module
trainer_template = _module
networks = _module
attention = _module
bert_mean_pooler = _module
dynamic_rnn = _module
lcf_pooler = _module
MAELoss = _module
R2Loss = _module
RMSELoss = _module
losses = _module
lsa = _module
point_wise_feed_forward = _module
sa_encoder = _module
squeeze_embedding = _module
AspectPolarityClassification = _module
apc_configuration = _module
__classic__ = _module
classic_glove_apc_utils = _module
data_utils_for_inference = _module
data_utils_for_training = _module
dependency_graph = _module
dataset_utils = _module
__lcf__ = _module
apc_utils = _module
apc_utils_for_dlcf_dca = _module
data_utils_for_inference = _module
__plm__ = _module
classic_bert_apc_utils = _module
data_utils_for_inference = _module
dataset_list = _module
instructor = _module
apc_instructor = _module
ensembler = _module
aoa = _module
asgcn = _module
atae_lstm = _module
cabasc = _module
ian = _module
lstm = _module
memnet = _module
mgan = _module
ram = _module
tc_lstm = _module
td_lstm = _module
tnet_lf = _module
models = _module
bert_base = _module
bert_spc = _module
bert_spc_v2 = _module
dlcf_dca_bert = _module
dlcfs_dca_bert = _module
fast_lcf_bert = _module
fast_lcf_bert_att = _module
fast_lcfs_bert = _module
fast_lsa_s = _module
fast_lsa_s_v2 = _module
fast_lsa_t = _module
fast_lsa_t_v2 = _module
lca_bert = _module
lcf_bert = _module
lcf_dual_bert = _module
lcf_template_apc = _module
lcfs_bert = _module
lcfs_dual_bert = _module
lsa_s = _module
lsa_t = _module
ssw_s = _module
ssw_t = _module
aoa_bert = _module
asgcn_bert = _module
atae_lstm_bert = _module
cabasc_bert = _module
ian_bert = _module
lstm_bert = _module
memnet_bert = _module
mgan_bert = _module
ram_bert = _module
tc_lstm_bert = _module
td_lstm_bert = _module
tnet_lf_bert = _module
prediction = _module
sentiment_classifier = _module
trainer = _module
apc_trainer = _module
AspectTermExtraction = _module
atepc_configuration = _module
atepc_utils = _module
atepc_instructor = _module
bert_base_atepc = _module
fast_lcf_atepc = _module
fast_lcfs_atepc = _module
lcf_atepc = _module
lcf_atepc_large = _module
lcf_template_atepc = _module
lcfs_atepc = _module
lcfs_atepc_large = _module
aspect_extractor = _module
atepc_trainer = _module
RNAClassification = _module
rnac_configuration = _module
data_utils_for_training = _module
data_utils_for_inference = _module
data_utils_for_inference = _module
data_utils_for_training = _module
rnac_instructor = _module
cnn = _module
lstm = _module
mhsa = _module
transformer = _module
bert = _module
rna_classifier = _module
rnac_trainer = _module
RNARegression = _module
rnar_configuration = _module
data_utils_for_inference = _module
data_utils_for_training = _module
data_utils_for_inference = _module
data_utils_for_training = _module
rnar_instructor = _module
cnn = _module
lstm = _module
mhsa = _module
transformer = _module
bert = _module
rna_regressor = _module
rnar_trainer = _module
TextAdversarialDefense = _module
tad_configuration = _module
data_utils_for_inference = _module
data_utils_for_training = _module
data_utils_for_inference = _module
data_utils_for_training = _module
tad_instructor = _module
tad_lstm = _module
tad_bert = _module
tad_classifier = _module
tad_trainer = _module
TextClassification = _module
tc_configuration = _module
data_utils_for_inference = _module
data_utils_for_inference = _module
classifier_instructor = _module
lstm = _module
bert = _module
text_classifier = _module
tc_trainer = _module
ProteinRegression = _module
proteinr_configuration = _module
data_utils_for_inference = _module
data_utils_for_training = _module
data_utils_for_inference = _module
data_utils_for_training = _module
proteinr_instructor = _module
cnn = _module
lstm = _module
mhsa = _module
transformer = _module
bert = _module
protein_regressor = _module
proteinr_trainer = _module
data_utils_for_inference = _module
data_utils_for_training = _module
rnac_instructor = _module
cnn = _module
lstm = _module
mhsa = _module
transformer = _module
bert = _module
rna_classifier = _module
_Archive = _module
tasks = _module
utils = _module
absa_utils = _module
make_absa_dataset = _module
cache_utils = _module
check_utils = _module
dataset_version_check = _module
package_version_check = _module
data_utils = _module
dataset_item = _module
dataset_manager = _module
exception_utils = _module
file_utils = _module
file_utils = _module
logger = _module
notification_utils = _module
proxy_utils = _module
pyabsa_utils = _module
text_utils = _module
bpe_tokenizer = _module
cosine_similarity = _module
mlm = _module
word2vec = _module
setup = _module
test_adversarial_defense = _module
test_apc_inference_set_generation = _module
test_apc_pretrain = _module
test_atepc_pretrain = _module
test_augmentation = _module
test_dataset_dict_test = _module
test_dataset_downloading = _module
test_tc_pretrain = _module
zip_datasets = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from torch.utils.data import Dataset


import math


import random


import re


import numpy


from torch.utils.data import DataLoader


from torch.utils.data import random_split


from torch.utils.data import ConcatDataset


from torch.utils.data import RandomSampler


from torch.utils.data import SequentialSampler


import time


from typing import Union


from torch import cuda


import warnings


import torch.nn as nn


import torch.nn.functional as F


from torch import nn


import numpy as np


from typing import List


import pandas


from sklearn import metrics


import copy


from torch.nn import ModuleList


import sklearn.metrics as metrics


from sklearn.metrics import f1_score


from torch.utils.data import TensorDataset


from torch.nn import CrossEntropyLoss


from collections import OrderedDict


from torch.nn import Conv1d


from torch.nn import MaxPool1d


from torch.nn import Linear


from torch.nn import Dropout


from torch.nn import functional as F


class Attention(nn.Module):

    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', dropout=0):
        """ Attention Mechanism
        :param embed_dim:
        :param hidden_dim:
        :param out_dim:
        :param n_head: num of head (Multi-Head Attention)
        :param score_function: scaled_dot_product / mlp (concat) / bi_linear (general dot)
        :return (?, q_len, out_dim,)
        """
        super(Attention, self).__init__()
        if hidden_dim is None:
            hidden_dim = embed_dim // n_head
        if out_dim is None:
            out_dim = embed_dim
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.n_head = n_head
        self.score_function = score_function
        self.w_k = nn.Linear(embed_dim, n_head * hidden_dim)
        self.w_q = nn.Linear(embed_dim, n_head * hidden_dim)
        self.proj = nn.Linear(n_head * hidden_dim, out_dim)
        self.dropout = nn.Dropout(dropout)
        if score_function == 'mlp':
            self.weight = nn.Parameter(torch.Tensor(hidden_dim * 2))
        elif self.score_function == 'bi_linear':
            self.weight = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))
        else:
            self.register_parameter('weight', None)
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.hidden_dim)
        if self.weight is not None:
            self.weight.data.uniform_(-stdv, stdv)

    def forward(self, k, q):
        if len(q.shape) == 2:
            q = torch.unsqueeze(q, dim=1)
        if len(k.shape) == 2:
            k = torch.unsqueeze(k, dim=1)
        mb_size = k.shape[0]
        k_len = k.shape[1]
        q_len = q.shape[1]
        kx = self.w_k(k).view(mb_size, k_len, self.n_head, self.hidden_dim)
        kx = kx.permute(2, 0, 1, 3).contiguous().view(-1, k_len, self.hidden_dim)
        qx = self.w_q(q).view(mb_size, q_len, self.n_head, self.hidden_dim)
        qx = qx.permute(2, 0, 1, 3).contiguous().view(-1, q_len, self.hidden_dim)
        if self.score_function == 'dot_product':
            kt = kx.permute(0, 2, 1)
            score = torch.bmm(qx, kt)
        elif self.score_function == 'scaled_dot_product':
            kt = kx.permute(0, 2, 1)
            qkt = torch.bmm(qx, kt)
            score = torch.div(qkt, math.sqrt(self.hidden_dim))
        elif self.score_function == 'mlp':
            kxx = torch.unsqueeze(kx, dim=1).expand(-1, q_len, -1, -1)
            qxx = torch.unsqueeze(qx, dim=2).expand(-1, -1, k_len, -1)
            kq = torch.cat((kxx, qxx), dim=-1)
            score = F.tanh(torch.matmul(kq, self.weight))
        elif self.score_function == 'bi_linear':
            qw = torch.matmul(qx, self.weight)
            kt = kx.permute(0, 2, 1)
            score = torch.bmm(qw, kt)
        else:
            raise RuntimeError('invalid score_function')
        score = F.softmax(score, dim=-1)
        output = torch.bmm(score, kx)
        output = torch.cat(torch.split(output, mb_size, dim=0), dim=-1)
        output = self.proj(output)
        output = self.dropout(output)
        return output, score


class NoQueryAttention(Attention):
    """q is a parameter"""

    def __init__(self, embed_dim, hidden_dim=None, out_dim=None, n_head=1, score_function='dot_product', q_len=1, dropout=0):
        super(NoQueryAttention, self).__init__(embed_dim, hidden_dim, out_dim, n_head, score_function, dropout)
        self.q_len = q_len
        self.q = nn.Parameter(torch.Tensor(q_len, embed_dim))
        self.reset_q()

    def reset_q(self):
        stdv = 1.0 / math.sqrt(self.embed_dim)
        self.q.data.uniform_(-stdv, stdv)

    def forward(self, k, **kwargs):
        mb_size = k.shape[0]
        q = self.q.expand(mb_size, -1, -1)
        return super(NoQueryAttention, self).forward(k, q)


class BERTMeanPooler(nn.Module):

    def __init__(self):
        super(BERTMeanPooler, self).__init__()

    def forward(self, model_output, attention_mask):
        token_embeddings = model_output
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-09)
        return sum_embeddings / sum_mask


class DynamicLSTM(nn.Module):

    def __init__(self, input_size, hidden_size, num_layers=1, bias=True, batch_first=True, dropout=0, bidirectional=False, only_use_last_hidden_state=False, rnn_type='LSTM'):
        """
        LSTM which can hold variable length sequence, use like TensorFlow's RNN(input, length...).

        :param input_size:The number of expected features in the input x
        :param hidden_size:The number of features in the hidden state h
        :param num_layers:Number of recurrent layers.
        :param bias:If False, then the layer does not use bias weights b_ih and b_hh. Default: True
        :param batch_first:If True, then the input and output tensors are provided as (batch, seq, feature)
        :param dropout:If non-zero, introduces a dropout layer on the outputs of each RNN layer except the last layer
        :param bidirectional:If True, becomes a bidirectional RNN. Default: False
        :param rnn_type: {LSTM, GRU, RNN}
        """
        super(DynamicLSTM, self).__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bias = bias
        self.batch_first = batch_first
        self.dropout = dropout
        self.bidirectional = bidirectional
        self.only_use_last_hidden_state = only_use_last_hidden_state
        self.rnn_type = rnn_type
        if self.rnn_type == 'LSTM':
            self.RNN = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)
        elif self.rnn_type == 'GRU':
            self.RNN = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)
        elif self.rnn_type == 'RNN':
            self.RNN = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)

    def forward(self, x, x_len):
        """
        sequence -> sort -> pad and pack ->process using RNN -> unpack ->unsort

        :param x: sequence embedding vectors
        :param x_len: numpy/tensor list
        :return:
        """
        """sort"""
        x_sort_idx = torch.sort(-x_len)[1].long()
        x_unsort_idx = torch.sort(x_sort_idx)[1].long()
        x_len = x_len[x_sort_idx]
        x = x[x_sort_idx]
        """pack"""
        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, lengths=x_len.cpu(), batch_first=self.batch_first)
        if self.rnn_type == 'LSTM':
            out_pack, (ht, ct) = self.RNN(x_emb_p, None)
        else:
            out_pack, ht = self.RNN(x_emb_p, None)
            ct = None
        """unsort: h"""
        ht = torch.transpose(ht, 0, 1)[x_unsort_idx]
        ht = torch.transpose(ht, 0, 1)
        if self.only_use_last_hidden_state:
            return ht
        else:
            """unpack: out"""
            out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first=self.batch_first)
            out = out[0]
            out = out[x_unsort_idx]
            """unsort: out c"""
            if self.rnn_type == 'LSTM':
                ct = torch.transpose(ct, 0, 1)[x_unsort_idx]
                ct = torch.transpose(ct, 0, 1)
            return out, (ht, ct)


class LCF_Pooler(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.config = config
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states, lcf_vec):
        device = hidden_states.device
        lcf_vec = lcf_vec.detach().cpu().numpy()
        pooled_output = numpy.zeros((hidden_states.shape[0], hidden_states.shape[2]), dtype=numpy.float32)
        hidden_states = hidden_states.detach().cpu().numpy()
        for i, vec in enumerate(lcf_vec):
            lcf_ids = [j for j in range(len(vec)) if sum(vec[j] - 1.0) == 0]
            pooled_output[i] = hidden_states[i][lcf_ids[len(lcf_ids) // 2]]
        pooled_output = torch.Tensor(pooled_output)
        pooled_output = self.dense(pooled_output)
        pooled_output = self.activation(pooled_output)
        return pooled_output


class MAELoss(nn.Module):

    def __init__(self):
        super(MAELoss, self).__init__()

    def forward(self, y_pred, y_true):
        return torch.mean(torch.abs(y_pred - y_true))


class R2Loss(nn.Module):

    def __init__(self):
        super(R2Loss, self).__init__()
        self.mse = nn.MSELoss()

    def forward(self, y_pred, y_true):
        y_true_mean = torch.mean(y_true, dim=0)
        ss_tot = torch.sum((y_true - y_true_mean) ** 2, dim=0)
        ss_res = torch.sum((y_true - y_pred) ** 2, dim=0)
        r2 = 1 - ss_res / ss_tot
        return 1 - torch.mean(r2)


class RMSELoss(nn.Module):

    def __init__(self):
        super(RMSELoss, self).__init__()

    def forward(self, y_pred, y_true):
        return torch.sqrt(nn.MSELoss()(y_pred, y_true))


class BertSelfAttention(nn.Module):

    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, 'embedding_size'):
            raise ValueError(f'The hidden size ({config.hidden_size}) is not a multiple of the number of attention heads ({config.num_attention_heads})')
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)
        self.dropout = nn.Dropout(config.attention_probs_dropout_prob if hasattr(config, 'attention_probs_dropout_prob') else 0)
        self.position_embedding_type = getattr(config, 'position_embedding_type', 'absolute')
        if self.position_embedding_type == 'relative_key' or self.position_embedding_type == 'relative_key_query':
            self.max_position_embeddings = config.max_position_embeddings
            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)
        self.is_decoder = config.is_decoder

    def transpose_for_scores(self, x):
        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)

    def forward(self, hidden_states, attention_mask=None, head_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, past_key_value=None, output_attentions=False):
        mixed_query_layer = self.query(hidden_states)
        is_cross_attention = encoder_hidden_states is not None
        if is_cross_attention and past_key_value is not None:
            key_layer = past_key_value[0]
            value_layer = past_key_value[1]
            attention_mask = encoder_attention_mask
        elif is_cross_attention:
            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
            attention_mask = encoder_attention_mask
        elif past_key_value is not None:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
        else:
            key_layer = self.transpose_for_scores(self.key(hidden_states))
            value_layer = self.transpose_for_scores(self.value(hidden_states))
        query_layer = self.transpose_for_scores(mixed_query_layer)
        if self.is_decoder:
            past_key_value = key_layer, value_layer
        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
        if self.position_embedding_type == 'relative_key' or self.position_embedding_type == 'relative_key_query':
            seq_length = hidden_states.size()[1]
            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)
            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)
            distance = position_ids_l - position_ids_r
            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)
            positional_embedding = positional_embedding
            if self.position_embedding_type == 'relative_key':
                relative_position_scores = torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores
            elif self.position_embedding_type == 'relative_key_query':
                relative_position_scores_query = torch.einsum('bhld,lrd->bhlr', query_layer, positional_embedding)
                relative_position_scores_key = torch.einsum('bhrd,lrd->bhlr', key_layer, positional_embedding)
                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key
        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask
        attention_probs = nn.Softmax(dim=-1)(attention_scores)
        attention_probs = self.dropout(attention_probs)
        if head_mask is not None:
            attention_probs = attention_probs * head_mask
        context_layer = torch.matmul(attention_probs, value_layer)
        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.view(*new_context_layer_shape)
        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)
        if self.is_decoder:
            outputs = outputs + (past_key_value,)
        return outputs


class SelfAttention(nn.Module):

    def __init__(self, bert_config, config):
        super(SelfAttention, self).__init__()
        self.bert_config = bert_config
        self.config = config
        self.SA = BertSelfAttention(bert_config)

    def forward(self, inputs):
        zero_vec = np.zeros((inputs.size(0), 1, 1, self.config.max_seq_len))
        zero_tensor = torch.tensor(zero_vec).float()
        SA_out = self.SA(inputs, zero_tensor)
        return SA_out


class Encoder(nn.Module):

    def __init__(self, bert_config, config, layer_num=1):
        super(Encoder, self).__init__()
        self.bert_config = bert_config
        self.config = config
        self.encoder = nn.ModuleList([SelfAttention(bert_config, config) for _ in range(layer_num)])
        self.tanh = torch.nn.Tanh()

    def forward(self, x):
        for i, enc in enumerate(self.encoder):
            x = self.tanh(enc(x)[0])
        return x


def fprint(*objects, sep=' ', end='\n', file=sys.stdout, flush=False):
    None


class LSA(nn.Module):

    def __init__(self, bert, config):
        super(LSA, self).__init__()
        self.config = config
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.eta1 = nn.Parameter(torch.tensor(self.config.eta, dtype=torch.float))
        self.eta2 = nn.Parameter(torch.tensor(self.config.eta, dtype=torch.float))

    def forward(self, global_context_features, spc_mask_vec, lcf_matrix, left_lcf_matrix, right_lcf_matrix):
        masked_global_context_features = torch.mul(spc_mask_vec, global_context_features)
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(masked_global_context_features, left_lcf_matrix)
        left_lcf_features = self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(masked_global_context_features, right_lcf_matrix)
        right_lcf_features = self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            if self.eta1 <= 0 and self.config.eta != -1:
                torch.nn.init.uniform_(self.eta1)
                fprint('reset eta1 to: {}'.format(self.eta1.item()))
            if self.eta2 <= 0 and self.config.eta != -1:
                torch.nn.init.uniform_(self.eta2)
                fprint('reset eta2 to: {}'.format(self.eta2.item()))
            if self.config.eta >= 0:
                cat_features = torch.cat((lcf_features, self.eta1 * left_lcf_features, self.eta2 * right_lcf_features), -1)
            else:
                cat_features = torch.cat((lcf_features, left_lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, self.eta1 * left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, self.eta2 * right_lcf_features), -1))
        else:
            raise KeyError('Invalid parameter:', self.config.window)
        return sent_out


class PositionwiseFeedForward(nn.Module):
    """ A two-feed-forward-layer module """

    def __init__(self, d_hid, d_inner_hid=None, dropout=0):
        super(PositionwiseFeedForward, self).__init__()
        if d_inner_hid is None:
            d_inner_hid = d_hid
        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)
        self.w_2 = nn.Conv1d(d_inner_hid, d_hid, 1)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x):
        output = self.relu(self.w_1(x.transpose(1, 2)))
        output = self.w_2(output).transpose(2, 1)
        output = self.dropout(output)
        return output


class SqueezeEmbedding(nn.Module):
    """
    Squeeze sequence embedding length to the longest one in the batch
    """

    def __init__(self, batch_first=True):
        super(SqueezeEmbedding, self).__init__()
        self.batch_first = batch_first

    def forward(self, x, x_len):
        """
        sequence -> sort -> pad and pack -> unpack ->unsort
        :param x: sequence embedding vectors
        :param x_len: numpy/tensor list
        :return:
        """
        """sort"""
        x_sort_idx = torch.sort(-x_len)[1].long()
        x_unsort_idx = torch.sort(x_sort_idx)[1].long()
        x_len = x_len[x_sort_idx]
        x = x[x_sort_idx]
        """pack"""
        x_emb_p = torch.nn.utils.rnn.pack_padded_sequence(x, x_len.cpu(), batch_first=self.batch_first)
        """unpack: out"""
        out = torch.nn.utils.rnn.pad_packed_sequence(x_emb_p, batch_first=self.batch_first)
        out = out[0]
        """unsort"""
        out = out[x_unsort_idx]
        return out


class PyABSADataset(Dataset):
    data = []

    def __init__(self, config, tokenizer, dataset_type, **kwargs):
        super(PyABSADataset, self).__init__()
        self.config = config
        self.tokenizer = tokenizer
        self.dataset_type = dataset_type
        if hasattr(self.config, 'dataset_dict') and dataset_type in self.config.dataset_dict and self.config.dataset_dict[dataset_type]:
            self.load_data_from_dict(config.dataset_dict, dataset_type=dataset_type, **kwargs)
            self.data = self.covert_to_tensor(self.data)
        elif hasattr(self.config, 'dataset_file') and dataset_type in self.config.dataset_file and self.config.dataset_file[dataset_type]:
            self.load_data_from_file(self.config.dataset_file, dataset_type=dataset_type, **kwargs)
            self.data = self.covert_to_tensor(self.data)
        self.config.logger.info('{} data examples:\n {}'.format(dataset_type, self.data[:2]))

    @staticmethod
    def covert_to_tensor(data):
        for d in data:
            if isinstance(d, dict):
                for key, value in d.items():
                    try:
                        d[key] = torch.tensor(value)
                    except Exception as e:
                        pass
            elif isinstance(d, list):
                for value in d:
                    PyABSADataset.covert_to_tensor(value)
        return data

    def load_data_from_dict(self, dataset_dict, **kwargs):
        raise NotImplementedError('Please implement load_data_from_dict() in your dataset class')

    def load_data_from_file(self, dataset_file, **kwargs):
        raise NotImplementedError('Please implement load_data_from_file() in your dataset class')

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        return self.data[index]

    def __str__(self):
        return f'PyABASDataset: {len(self.data)} samples'

    def __repr__(self):
        return self.__str__()


def copy_side_aspect(direct, target, source, examples, input_demands):
    if 'cluster_ids' not in target:
        target['cluster_ids'] = copy.deepcopy(set(target['aspect_position']))
        target['side_ex_ids'] = copy.deepcopy({target['ex_id']})
    if 'cluster_ids' not in source:
        source['cluster_ids'] = copy.deepcopy(set(source['aspect_position']))
        source['side_ex_ids'] = copy.deepcopy({source['ex_id']})
    if target['polarity'] == source['polarity']:
        try:
            target['side_ex_ids'] |= source['side_ex_ids']
            source['side_ex_ids'] |= target['side_ex_ids']
            target['cluster_ids'] |= source['cluster_ids']
            source['cluster_ids'] |= target['cluster_ids']
        except Exception as e:
            fprint(e)
            fprint(target)
            fprint(source)
            fprint(examples)
            fprint(input_demands)
            raise e
        for ex_id in target['side_ex_ids']:
            examples[ex_id]['cluster_ids'] |= source['cluster_ids']
            examples[ex_id]['side_ex_ids'] |= target['side_ex_ids']
    for data_item in input_demands:
        if 'right_right_' in data_item or 'left_left_' in data_item:
            data_item = data_item.replace('right_right_', 'right_', 1).replace('left_left_', 'left_', 1)
        elif data_item.startswith('right_') or data_item.startswith('left_'):
            continue
        target[direct + '_' + data_item] = source[data_item]
    try:
        target[direct + '_dist'] = int(abs(np.average(list(source['aspect_position'])) - np.average(list(target['aspect_position']))))
    except:
        target[direct + '_dist'] = np.inf


def is_similar(s1, s2, tokenizer, similarity_threshold):
    if isinstance(s1, int) or isinstance(s2, int):
        return False
    if abs(np.count_nonzero(s1) - np.count_nonzero(s2)) > 5:
        return False
    count = 0.0
    s1 = list(s1)
    s2 = list(s2)
    len1 = len(s1)
    len2 = len(s2)
    while s1 and s2:
        if s1[-1] in s2:
            count += 1
            s2.remove(s1[-1])
        s1.remove(s1[-1])
    if count / len1 >= similarity_threshold and count / len2 >= similarity_threshold:
        return True
    else:
        return False


def build_sentiment_window(examples, tokenizer, similarity_threshold, input_demands=None):
    copy_side_aspect('left', examples[0], examples[0], examples, input_demands)
    for idx in range(1, len(examples)):
        if is_similar(examples[idx - 1]['text_indices'], examples[idx]['text_indices'], tokenizer=None, similarity_threshold=similarity_threshold):
            copy_side_aspect('right', examples[idx - 1], examples[idx], examples, input_demands)
            copy_side_aspect('left', examples[idx], examples[idx - 1], examples, input_demands)
        else:
            copy_side_aspect('right', examples[idx - 1], examples[idx - 1], examples, input_demands)
            copy_side_aspect('left', examples[idx], examples[idx], examples, input_demands)
    copy_side_aspect('right', examples[-1], examples[-1], examples, input_demands)
    return examples


def build_spc_mask_vec(opt, text_ids):
    spc_mask_vec = np.zeros((opt.max_seq_len, opt.hidden_dim), dtype=np.float32)
    for i in range(len(text_ids)):
        spc_mask_vec[i] = np.ones(opt.hidden_dim, dtype=np.float32)
    return spc_mask_vec


def check_and_fix_labels(label_set: set, label_name, all_data, config):
    if '-100' in label_set:
        label_to_index = {origin_label: (int(idx) - 1 if origin_label != '-100' else -100) for origin_label, idx in zip(sorted(label_set), range(len(label_set)))}
        index_to_label = {(int(idx) - 1 if origin_label != '-100' else -100): origin_label for origin_label, idx in zip(sorted(label_set), range(len(label_set)))}
    else:
        label_to_index = {origin_label: int(idx) for origin_label, idx in zip(sorted(label_set), range(len(label_set)))}
        index_to_label = {int(idx): origin_label for origin_label, idx in zip(sorted(label_set), range(len(label_set)))}
    if 'index_to_label' not in config.args:
        config.index_to_label = index_to_label
        config.label_to_index = label_to_index
    if config.index_to_label != index_to_label:
        config.index_to_label.update(index_to_label)
        config.label_to_index.update(label_to_index)
    num_label = {l: (0) for l in label_set}
    num_label['Sum'] = len(all_data)
    for item in all_data:
        try:
            num_label[item[label_name]] += 1
            item[label_name] = label_to_index[item[label_name]]
        except Exception as e:
            num_label[item.polarity] += 1
            item.polarity = label_to_index[item.polarity]
    config.logger.info('Dataset Label Details: {}'.format(num_label))


def configure_dlcf_spacy_model(opt):
    if not hasattr(opt, 'spacy_model'):
        opt.spacy_model = 'en_core_web_sm'
    global nlp
    try:
        nlp = spacy.load(opt.spacy_model)
    except:
        fprint('Can not load {} from spacy, try to download it in order to parse syntax tree:'.format(opt.spacy_model), termcolor.colored('\npython -m spacy download {}'.format(opt.spacy_model), 'green'))
        try:
            os.system('python -m spacy download {}'.format(opt.spacy_model))
            nlp = spacy.load(opt.spacy_model)
        except:
            raise RuntimeError('Download failed, you can download {} manually.'.format(opt.spacy_model))
    return nlp


def configure_spacy_model(opt):
    if not hasattr(opt, 'spacy_model'):
        opt.spacy_model = 'en_core_web_sm'
    global nlp
    try:
        nlp = spacy.load(opt.spacy_model)
    except:
        fprint('Can not load {} from spacy, try to download it in order to parse syntax tree:'.format(opt.spacy_model), termcolor.colored('\npython -m spacy download {}'.format(opt.spacy_model), 'green'))
        try:
            os.system('python -m spacy download {}'.format(opt.spacy_model))
            nlp = spacy.load(opt.spacy_model)
        except:
            raise RuntimeError('Download failed, you can download {} manually.'.format(opt.spacy_model))
    return nlp


def load_dataset_from_file(fname, **kwargs):
    logger = kwargs.get('logger', None)
    lines = []
    if isinstance(fname, str):
        fname = [fname]
    for f in fname:
        if logger:
            logger.info('Load dataset from {}'.format(f))
        else:
            fprint('Load dataset from {}'.format(f))
        fin = open(f, 'r', encoding='utf-8')
        _lines_ = fin.readlines()
        for i, line in enumerate(_lines_):
            if not line.strip():
                raise ValueError('empty line: #{} in {}, previous line: {}'.format(i, f, _lines_[i - 1]))
            lines.append(line.strip())
        fin.close()
    return lines


def pad_and_truncate(sequence, max_seq_len, value, **kwargs):
    if isinstance(sequence, ndarray):
        sequence = list(sequence)
        if len(sequence) > max_seq_len:
            sequence = sequence[:max_seq_len]
        else:
            sequence = sequence + [value] * (max_seq_len - len(sequence))
        return np.array(sequence)
    else:
        if len(sequence) > max_seq_len:
            sequence = sequence[:max_seq_len]
        else:
            sequence = sequence + [value] * (max_seq_len - len(sequence))
        return sequence


def text_to_sequence(tokenizer, text, max_seq_len):
    return pad_and_truncate(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text)), max_seq_len)


def prepare_input_for_apc(opt, tokenizer, text_left, text_right, aspect):
    if hasattr(opt, 'dynamic_truncate') and opt.dynamic_truncate:
        _max_seq_len = opt.max_seq_len - len(aspect.split(' '))
        text_left = text_left.split(' ')
        text_right = text_right.split(' ')
        if _max_seq_len < len(text_left) + len(text_right):
            cut_len = len(text_left) + len(text_right) - _max_seq_len
            if len(text_left) > len(text_right):
                text_left = text_left[cut_len:]
            else:
                text_right = text_right[:len(text_right) - cut_len]
        text_left = ' '.join(text_left)
        text_right = ' '.join(text_right)
    bos_token = ''
    eos_token = ''
    text_raw = text_left + ' ' + aspect + ' ' + text_right
    text_spc = bos_token + ' ' + text_raw + ' ' + eos_token + ' ' + aspect + ' ' + eos_token
    text_indices = text_to_sequence(tokenizer, text_spc, opt.max_seq_len)
    text_raw_bert_indices = text_to_sequence(tokenizer, bos_token + ' ' + text_raw + ' ' + eos_token, opt.max_seq_len)
    aspect_bert_indices = text_to_sequence(tokenizer, aspect, opt.max_seq_len)
    aspect_begin = len(tokenizer.tokenize(bos_token + ' ' + text_left))
    aspect_position = set(range(aspect_begin, aspect_begin + np.count_nonzero(aspect_bert_indices)))
    inputs = {'text_raw': text_raw, 'text_spc': text_spc, 'aspect': aspect, 'aspect_position': aspect_position, 'text_indices': text_indices, 'text_raw_indices': text_raw_bert_indices, 'aspect_indices': aspect_bert_indices}
    return inputs


def calculate_cluster(sentence, aspect, opt):
    terms = [a.lower() for a in aspect.split()]
    doc_list = []
    doc = [a.lower() for a in sentence.split()]
    for i in range(len(doc)):
        doc_list.append(i)
    doc = nlp(sentence.strip())
    edges = []
    cnt = 0
    term_ids = [0] * len(terms)
    for token in doc:
        if cnt < len(terms) and token.lower_ == terms[cnt]:
            term_ids[cnt] = token.i
            cnt += 1
        for child in token.children:
            edges.append((token.i, child.i))
    graph = nx.DiGraph(edges)
    graph2 = nx.Graph(edges)
    no_connect = []
    for i, word in enumerate(doc):
        source = i
        for j in term_ids:
            target = j
            try:
                sum = nx.shortest_path_length(graph2, source=source, target=target)
            except:
                if i not in no_connect and i not in term_ids:
                    no_connect.append(i)
    depend_ids = []
    depended_ids = doc_list
    for k in range(len(terms)):
        temp_aspcet_ids = term_ids[k]
        try:
            temp_nodes = list(nx.dfs_preorder_nodes(graph, source=temp_aspcet_ids))
        except:
            temp_nodes = [temp_aspcet_ids]
        for i in range(len(temp_nodes)):
            flag = 1
            for j in range(len(depend_ids)):
                if depend_ids[j] == temp_nodes[i]:
                    flag = 0
            if flag == 1:
                depend_ids.append(temp_nodes[i])
    for i in range(len(depend_ids)):
        s = depend_ids[i]
        if s in depended_ids:
            depended_ids.remove(s)
    for i in range(len(terms)):
        temp_aspcet_ids = term_ids[i]
        if temp_aspcet_ids in depend_ids:
            depend_ids.remove(temp_aspcet_ids)
    for i in range(len(terms)):
        temp_aspcet_ids = term_ids[i]
        if temp_aspcet_ids in depended_ids:
            depended_ids.remove(temp_aspcet_ids)
    for i in range(len(no_connect)):
        if no_connect[i] in depended_ids:
            depended_ids.remove(no_connect[i])
    depend_vec = np.zeros(opt.max_seq_len, dtype=np.float32)
    depended_vec = np.zeros(opt.max_seq_len, dtype=np.float32)
    depended_vec[0] = 1
    depend_vec[0] = 1
    for i in range(len(depend_ids)):
        if depend_ids[i] < opt.max_seq_len - 1:
            depend_vec[depend_ids[i] + 1] = 1
    for i in range(len(depended_ids)):
        if depended_ids[i] < opt.max_seq_len - 1:
            depended_vec[depended_ids[i] + 1] = 1
    return depend_vec, depended_vec


def get_dynamic_cdm_vec(opt, max_dist, bert_spc_indices, aspect_indices, aspect_begin, syntactical_dist=None):
    a = opt.dlcf_a
    if max_dist > 0:
        dynamic_threshold = math.log(max_dist, a) + a - 1
    else:
        dynamic_threshold = 3
    cdm_vec = np.zeros(opt.max_seq_len, dtype=np.float32)
    aspect_len = np.count_nonzero(aspect_indices)
    text_len = np.count_nonzero(bert_spc_indices) - np.count_nonzero(aspect_indices) - 1
    if syntactical_dist is not None:
        for i in range(min(text_len, opt.max_seq_len)):
            if syntactical_dist[i] <= dynamic_threshold:
                cdm_vec[i] = 1
    else:
        local_context_begin = max(0, aspect_begin - dynamic_threshold)
        local_context_end = min(aspect_begin + aspect_len + dynamic_threshold - 1, opt.max_seq_len)
        for i in range(min(text_len, opt.max_seq_len)):
            if local_context_begin <= i <= local_context_end:
                cdm_vec[i] = 1
    return cdm_vec


def get_dynamic_cdw_vec(opt, max_dist, bert_spc_indices, aspect_indices, aspect_begin, syntactical_dist=None):
    a = opt.dlcf_a
    if max_dist > 0:
        dynamic_threshold = math.log(max_dist, a) + a - 1
    else:
        dynamic_threshold = 3
    cdw_vec = np.zeros(opt.max_seq_len, dtype=np.float32)
    aspect_len = np.count_nonzero(aspect_indices)
    text_len = np.count_nonzero(bert_spc_indices) - np.count_nonzero(aspect_indices) - 1
    if syntactical_dist is not None:
        for i in range(min(text_len, opt.max_seq_len)):
            if max_dist > 0:
                if syntactical_dist[i] > dynamic_threshold:
                    w = 1 - syntactical_dist[i] / max_dist
                    cdw_vec[i] = w
                else:
                    cdw_vec[i] = 1
            else:
                cdw_vec[i] = 1
    else:
        local_context_begin = max(0, aspect_begin - dynamic_threshold)
        local_context_end = min(aspect_begin + aspect_len + dynamic_threshold - 1, opt.max_seq_len)
        for i in range(min(text_len, opt.max_seq_len)):
            if i < local_context_begin:
                w = 1 - (local_context_begin - i) / text_len
            elif local_context_begin <= i <= local_context_end:
                w = 1
            else:
                w = 1 - (i - local_context_end) / text_len
            try:
                assert 0 <= w <= 1
            except:
                fprint('Warning! invalid CDW weight:', w)
            cdw_vec[i] = 1
    return cdw_vec


def calculate_dep_dist(sentence, aspect):
    terms = [a.lower() for a in aspect.split()]
    try:
        doc = nlp(sentence)
    except NameError as e:
        raise RuntimeError('Fail to load nlp model, maybe you forget to download en_core_web_sm')
    edges = []
    cnt = 0
    term_ids = [0] * len(terms)
    for token in doc:
        if cnt < len(terms) and token.lower_ == terms[cnt]:
            term_ids[cnt] = token.i
            cnt += 1
        for child in token.children:
            edges.append(('{}_{}'.format(token.lower_, token.i), '{}_{}'.format(child.lower_, child.i)))
    graph = nx.Graph(edges)
    dist = [0.0] * len(doc)
    text = [''] * len(doc)
    max_dist_temp = []
    for i, word in enumerate(doc):
        source = '{}_{}'.format(word.lower_, word.i)
        sum = 0
        flag = 1
        max_dist = 0
        for term_id, term in zip(term_ids, terms):
            target = '{}_{}'.format(term, term_id)
            try:
                sum += nx.shortest_path_length(graph, source=source, target=target)
            except:
                sum += len(doc)
                flag = 0
        dist[i] = sum / len(terms)
        text[i] = word.text
        if flag == 1:
            max_dist_temp.append(sum / len(terms))
        if dist[i] > max_dist:
            max_dist = dist[i]
    return text, dist, max_dist


def pad_syntax_based_srd(text, dep_dist, tokenizer, opt):
    sequence, distances = [], []
    for word, dist in zip(text, dep_dist):
        tokens = tokenizer.tokenize(word)
        for jx, token in enumerate(tokens):
            sequence.append(token)
            distances.append(dist)
    sequence = tokenizer.convert_tokens_to_ids(sequence)
    sequence = pad_and_truncate(sequence, opt.max_seq_len)
    dep_dist = pad_and_truncate(dep_dist, opt.max_seq_len, value=opt.max_seq_len)
    return sequence, dep_dist


def syntax_distance_alignment(tokens, dist, max_seq_len, tokenizer):
    text = tokens[:]
    dep_dist = dist[:]
    bert_tokens = tokenizer.tokenize(' '.join(text))
    _bert_tokens = bert_tokens[:]
    align_dist = []
    if bert_tokens != text:
        while text or bert_tokens:
            try:
                if text[0] == ' ' or text[0] == '\xa0':
                    text = text[1:]
                    dep_dist = dep_dist[1:]
                elif text[0] == bert_tokens[0]:
                    text = text[1:]
                    bert_tokens = bert_tokens[1:]
                    align_dist.append(dep_dist[0])
                    dep_dist = dep_dist[1:]
                elif len(text[0]) < len(bert_tokens[0]):
                    tmp_str = text[0]
                    while len(tmp_str) < len(bert_tokens[0]):
                        text = text[1:]
                        tmp_str += text[0]
                        dep_dist = dep_dist[1:]
                    align_dist.append(dep_dist[0])
                    dep_dist = dep_dist[1:]
                    text = text[1:]
                    bert_tokens = bert_tokens[1:]
                elif len(text[0]) > len(bert_tokens[0]):
                    tmp_tokens = tokenizer.tokenize(text[0])
                    for jx, tmp_token in enumerate(tmp_tokens):
                        align_dist.append(dep_dist[0])
                    text = text[1:]
                    dep_dist = dep_dist[1:]
                    bert_tokens = bert_tokens[len(tmp_tokens):]
                else:
                    text = text[1:]
                    bert_tokens = bert_tokens[1:]
                    align_dist.append(dep_dist[0])
                    dep_dist = dep_dist[1:]
            except:
                align_dist = pad_and_truncate(align_dist, max_seq_len, value=max_seq_len)
                return align_dist
    else:
        align_dist = dep_dist
    align_dist = pad_and_truncate(align_dist, max_seq_len, value=max_seq_len)
    return align_dist


def get_syntax_distance(text_raw, aspect, tokenizer, opt):
    if isinstance(text_raw, list):
        text_raw = ' '.join(text_raw)
    if isinstance(aspect, list):
        aspect = ' '.join(aspect)
    try:
        raw_tokens, dist, max_dist = calculate_dep_dist(text_raw, aspect)
    except Exception as e:
        fprint('Text: {} Aspect: {}'.format(text_raw, aspect))
        raise RuntimeError('Ignore failure in calculate the syntax based SRD: {}, maybe the aspect is None'.format(e))
    if opt.model_name == 'dlcf_dca_bert':
        dist.insert(0, 0)
        dist.append(0)
    else:
        dist.insert(0, max(dist))
        dist.append(max(dist))
    raw_tokens.insert(0, tokenizer.bos_token)
    raw_tokens.append(tokenizer.eos_token)
    if opt.srd_alignment:
        syntactical_dist = syntax_distance_alignment(raw_tokens, dist, opt.max_seq_len, tokenizer)
    else:
        syntactical_dist = pad_syntax_based_srd(raw_tokens, dist, tokenizer, opt)[1]
    return syntactical_dist, max_dist


def prepare_input_for_dlcf_dca(opt, tokenizer, text_left, text_right, aspect):
    if hasattr(opt, 'dynamic_truncate') and opt.dynamic_truncate:
        _max_seq_len = opt.max_seq_len - len(aspect.split(' '))
        text_left = text_left.split(' ')
        text_right = text_right.split(' ')
        if _max_seq_len < len(text_left) + len(text_right):
            cut_len = len(text_left) + len(text_right) - _max_seq_len
            if len(text_left) > len(text_right):
                text_left = text_left[cut_len:]
            else:
                text_right = text_right[:len(text_right) - cut_len]
        text_left = ' '.join(text_left)
        text_right = ' '.join(text_right)
        text_left = ' '.join(text_left.split(' ')[int(-(opt.max_seq_len - len(aspect.split())) / 2) - 1:])
        text_right = ' '.join(text_right.split(' ')[:int((opt.max_seq_len - len(aspect.split())) / 2) + 1])
        bos_token = tokenizer.bos_token if tokenizer.bos_token else '[CLS]'
        eos_token = tokenizer.eos_token if tokenizer.eos_token else '[SEP]'
        text_raw = text_left + ' ' + aspect + ' ' + text_right
        text_spc = bos_token + ' ' + text_raw + ' ' + eos_token + ' ' + aspect + ' ' + eos_token
        text_indices = text_to_sequence(tokenizer, text_spc, opt.max_seq_len)
        aspect_bert_indices = text_to_sequence(tokenizer, aspect, opt.max_seq_len)
        aspect_begin = len(tokenizer.tokenize(bos_token + ' ' + text_left))
        syntactical_dist, max_dist = get_syntax_distance(text_raw, aspect, tokenizer, opt)
        dlcf_cdm_vec = get_dynamic_cdm_vec(opt, max_dist, text_indices, aspect_bert_indices, aspect_begin, syntactical_dist=None)
        dlcf_cdw_vec = get_dynamic_cdw_vec(opt, max_dist, text_indices, aspect_bert_indices, aspect_begin, syntactical_dist=None)
        dlcfs_cdm_vec = get_dynamic_cdm_vec(opt, max_dist, text_indices, aspect_bert_indices, aspect_begin, syntactical_dist)
        dlcfs_cdw_vec = get_dynamic_cdw_vec(opt, max_dist, text_indices, aspect_bert_indices, aspect_begin, syntactical_dist)
        depend_vec, depended_vec = calculate_cluster(text_raw, aspect, opt)
        inputs = {'dlcf_cdm_vec': dlcf_cdm_vec, 'dlcf_cdw_vec': dlcf_cdw_vec, 'dlcfs_cdm_vec': dlcfs_cdm_vec, 'dlcfs_cdw_vec': dlcfs_cdw_vec, 'depend_vec': depend_vec, 'depended_vec': depended_vec}
        return inputs


def validate_example(text: str, aspect: str, polarity: str, config):
    if len(text) < len(aspect):
        raise ValueError('AspectLengthExceedTextError -> <aspect: {}> is longer than <text: {}>, <polarity: {}>'.format(aspect, text, polarity))
    if aspect.strip().lower() not in text.strip().lower():
        raise ValueError('AspectNotInTextError -> <aspect: {}> is not in <text: {}>>'.format(aspect, text))
    warning = False
    if len(aspect.split(' ')) > 10:
        config.logger.warning('AspectTooLongWarning -> <aspect: {}> is too long, <text: {}>, <polarity: {}>'.format(aspect, text, polarity))
        warning = True
    if not aspect.strip():
        raise ValueError('AspectIsNullError -> <text: {}>, <aspect: {}>, <polarity: {}>'.format(aspect, text, polarity))
    if len(polarity.split(' ')) > 3:
        config.logger.warning('LabelTooLongWarning -> <polarity: {}> is too long, <text: {}>, <aspect: {}>'.format(polarity, text, aspect))
        warning = True
    if not polarity.strip():
        raise ValueError('PolarityIsNullError -> <text: {}>, <aspect: {}>, <polarity: {}>'.format(aspect, text, polarity))
    if text.strip() == aspect.strip():
        config.logger.warning('AspectEqualsTextWarning -> <aspect: {}> equals <text: {}>, <polarity: {}>'.format(aspect, text, polarity))
        warning = True
    if not text.strip():
        raise ValueError('TextIsNullError -> <text: {}>, <aspect: {}>, <polarity: {}>'.format(aspect, text, polarity))
    return warning


class ABSADataset(PyABSADataset):

    def load_data_from_dict(self, data_dict, **kwargs):
        pass

    def load_data_from_file(self, file_path, **kwargs):
        configure_spacy_model(self.config)
        lines = load_dataset_from_file(self.config.dataset_file[self.dataset_type])
        if len(lines) % 3 != 0:
            fprint(colored('ERROR: one or more datasets are corrupted, make sure the number of lines in a dataset should be multiples of 3.', 'red'))
        all_data = []
        label_set = set()
        ex_id = 0
        for i in tqdm.tqdm(range(0, len(lines), 3), postfix='preparing dataloader...'):
            if lines[i].count('$T$') > 1:
                continue
            text_left, _, text_right = [s.strip() for s in lines[i].partition('$T$')]
            aspect = lines[i + 1].strip()
            polarity = lines[i + 2].strip()
            prepared_inputs = prepare_input_for_apc(self.config, self.tokenizer, text_left, text_right, aspect, input_demands=self.config.inputs_cols)
            text_raw = prepared_inputs['text_raw']
            text_spc = prepared_inputs['text_spc']
            aspect = prepared_inputs['aspect']
            aspect_position = prepared_inputs['aspect_position']
            text_indices = prepared_inputs['text_indices']
            text_raw_bert_indices = prepared_inputs['text_raw_bert_indices']
            aspect_bert_indices = prepared_inputs['aspect_bert_indices']
            lcf_cdw_vec = prepared_inputs['lcf_cdw_vec']
            lcf_cdm_vec = prepared_inputs['lcf_cdm_vec']
            lcf_vec = prepared_inputs['lcf_vec']
            lcfs_cdw_vec = prepared_inputs['lcfs_cdw_vec']
            lcfs_cdm_vec = prepared_inputs['lcfs_cdm_vec']
            lcfs_vec = prepared_inputs['lcfs_vec']
            if validate_example(text_raw, aspect, polarity, self.config):
                continue
            if self.config.model_name == 'dlcf_dca_bert' or self.config.model_name == 'dlcfs_dca_bert':
                configure_dlcf_spacy_model(self.config)
                prepared_inputs = prepare_input_for_dlcf_dca(self.config, self.tokenizer, text_left, text_right, aspect)
                dlcf_vec = prepared_inputs['dlcf_cdm_vec'] if self.config.lcf == 'cdm' else prepared_inputs['dlcf_cdw_vec']
                dlcfs_vec = prepared_inputs['dlcfs_cdm_vec'] if self.config.lcf == 'cdm' else prepared_inputs['dlcfs_cdw_vec']
                depend_vec = prepared_inputs['depend_vec']
                depended_vec = prepared_inputs['depended_vec']
            data = {'ex_id': ex_id, 'text_raw': text_raw, 'text_spc': text_spc, 'aspect': aspect, 'aspect_position': aspect_position, 'lca_ids': lcf_vec, 'lcf_vec': lcf_vec if 'lcf_vec' in self.config.inputs_cols else 0, 'lcf_cdw_vec': lcf_cdw_vec if 'lcf_cdw_vec' in self.config.inputs_cols else 0, 'lcf_cdm_vec': lcf_cdm_vec if 'lcf_cdm_vec' in self.config.inputs_cols else 0, 'lcfs_vec': lcfs_vec if 'lcfs_vec' in self.config.inputs_cols else 0, 'lcfs_cdw_vec': lcfs_cdw_vec if 'lcfs_cdw_vec' in self.config.inputs_cols else 0, 'lcfs_cdm_vec': lcfs_cdm_vec if 'lcfs_cdm_vec' in self.config.inputs_cols else 0, 'dlcf_vec': dlcf_vec if 'dlcf_vec' in self.config.inputs_cols else 0, 'dlcfs_vec': dlcfs_vec if 'dlcfs_vec' in self.config.inputs_cols else 0, 'depend_vec': depend_vec if 'depend_vec' in self.config.inputs_cols else 0, 'depended_vec': depended_vec if 'depended_vec' in self.config.inputs_cols else 0, 'spc_mask_vec': build_spc_mask_vec(self.config, text_raw_bert_indices) if 'spc_mask_vec' in self.config.inputs_cols else 0, 'text_indices': text_indices if 'text_indices' in self.config.inputs_cols else 0, 'aspect_bert_indices': aspect_bert_indices if 'aspect_bert_indices' in self.config.inputs_cols else 0, 'text_raw_bert_indices': text_raw_bert_indices if 'text_raw_bert_indices' in self.config.inputs_cols else 0, 'polarity': polarity}
            ex_id += 1
            label_set.add(polarity)
            all_data.append(data)
        check_and_fix_labels(label_set, 'polarity', all_data, self.config)
        self.config.output_dim = len(label_set)
        all_data = build_sentiment_window(all_data, self.tokenizer, self.config.similarity_threshold, input_demands=self.config.inputs_cols)
        for data in all_data:
            cluster_ids = []
            for pad_idx in range(self.config.max_seq_len):
                if pad_idx in data['cluster_ids']:
                    cluster_ids.append(data['polarity'])
                else:
                    cluster_ids.append(-100)
            data['cluster_ids'] = np.asarray(cluster_ids, dtype=np.int64)
            data['side_ex_ids'] = np.array(0)
            data['aspect_position'] = np.array(0)
        self.data = all_data

    def __init__(self, config, tokenizer, dataset_type='train'):
        super().__init__(config=config, tokenizer=tokenizer, dataset_type=dataset_type)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return len(self.data)


def dependency_adj_matrix(text):
    tokens = nlp(text)
    words = text.split()
    matrix = np.zeros((len(words), len(words))).astype('float32')
    assert len(words) == len(list(tokens))
    for token in tokens:
        matrix[token.i][token.i] = 1
        for child in token.children:
            matrix[token.i][child.i] = 1
            matrix[child.i][token.i] = 1
    return matrix


def prepare_dependency_graph(dataset_list, graph_path, max_seq_len, opt):
    if 'train' in dataset_list[0].lower():
        append_name = 'train_set_{}x{}.graph'.format(max_seq_len, max_seq_len)
    elif 'test' in dataset_list[0].lower():
        append_name = 'test_set_{}x{}.graph'.format(max_seq_len, max_seq_len)
    elif 'val' in dataset_list[0].lower():
        append_name = 'val_set_{}x{}.graph'.format(max_seq_len, max_seq_len)
    else:
        append_name = 'unrecognized_set_{}x{}.graph'.format(max_seq_len, max_seq_len)
    graph_path = os.path.join(graph_path, append_name)
    if os.path.isfile(graph_path):
        return graph_path
    idx2graph = {}
    if os.path.isdir(graph_path):
        fout = open(os.path.join(graph_path, append_name), 'wb')
        graph_path = os.path.join(graph_path, append_name)
    elif os.path.isfile(graph_path):
        return graph_path
    else:
        fout = open(graph_path, 'wb')
    for filename in dataset_list:
        try:
            fprint('parsing dependency matrix:', filename)
            fin = open(filename, 'r', encoding='utf-8', newline='\n', errors='ignore')
            lines = fin.readlines()
            fin.close()
            for i in tqdm.tqdm(range(0, len(lines), 3), postfix='Construct graph for {}'.format(filename)):
                text_left, _, text_right = [s.strip() for s in lines[i].partition('$T$')]
                aspect = lines[i + 1].strip()
                adj_matrix = dependency_adj_matrix(text_left + ' ' + aspect + ' ' + text_right)
                text = text_left + ' ' + aspect + ' ' + text_right
                idx2graph[text.lower()] = adj_matrix
        except Exception as e:
            fprint(e)
            fprint('unprocessed:', filename)
    pickle.dump(idx2graph, fout)
    fout.close()
    return graph_path


class BERTBaselineABSADataset(PyABSADataset):

    def load_data_from_dict(self, dataset_dict, **kwargs):
        pass

    def load_data_from_file(self, dataset_file, **kwargs):
        configure_spacy_model(self.config)
        lines = load_dataset_from_file(self.config.dataset_file[self.dataset_type])
        all_data = []
        label_set = set()
        dep_cache_path = os.path.join(os.getcwd(), 'run/{}/dependency_cache/'.format(self.config.dataset_name))
        if not os.path.exists(dep_cache_path):
            os.makedirs(dep_cache_path)
        graph_path = prepare_dependency_graph(self.config.dataset_file[self.dataset_type], dep_cache_path, self.config.max_seq_len, self.config)
        fin = open(graph_path, 'rb')
        idx2graph = pickle.load(fin)
        ex_id = 0
        if len(lines) % 3 != 0:
            fprint(colored('ERROR: one or more datasets are corrupted, make sure the number of lines in a dataset should be multiples of 3.', 'red'))
        for i in tqdm.tqdm(range(0, len(lines), 3), postfix='preparing dataloader...'):
            if lines[i].count('$T$') > 1:
                continue
            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition('$T$')]
            aspect = lines[i + 1].lower().strip()
            text_raw = text_left + ' ' + aspect + ' ' + text_right
            polarity = lines[i + 2].strip()
            if validate_example(text_raw, aspect, polarity, self.config):
                continue
            prepared_inputs = prepare_input_for_apc(self.config, self.tokenizer, text_left, text_right, aspect)
            aspect_position = prepared_inputs['aspect_position']
            text_indices = self.tokenizer.text_to_sequence(text_left + ' ' + aspect + ' ' + text_right)
            context_indices = self.tokenizer.text_to_sequence(text_left + text_right)
            left_indices = self.tokenizer.text_to_sequence(text_left)
            left_with_aspect_indices = self.tokenizer.text_to_sequence(text_left + ' ' + aspect)
            right_indices = self.tokenizer.text_to_sequence(text_right)
            right_with_aspect_indices = self.tokenizer.text_to_sequence(aspect + ' ' + text_right)
            aspect_indices = self.tokenizer.text_to_sequence(aspect)
            aspect_len = np.count_nonzero(aspect_indices)
            left_len = min(self.config.max_seq_len - aspect_len, np.count_nonzero(left_indices))
            left_indices = np.concatenate((left_indices[:left_len], np.asarray([0] * (self.config.max_seq_len - left_len))))
            aspect_boundary = np.asarray([left_len, min(left_len + aspect_len - 1, self.config.max_seq_len)])
            dependency_graph = np.pad(idx2graph[text_raw], ((0, max(0, self.config.max_seq_len - idx2graph[text_raw].shape[0])), (0, max(0, self.config.max_seq_len - idx2graph[text_raw].shape[0]))), 'constant')
            dependency_graph = dependency_graph[:, range(0, self.config.max_seq_len)]
            dependency_graph = dependency_graph[range(0, self.config.max_seq_len), :]
            data = {'ex_id': ex_id, 'text_indices': text_indices if 'text_indices' in self.config.inputs_cols else 0, 'context_indices': context_indices if 'context_indices' in self.config.inputs_cols else 0, 'left_indices': left_indices if 'left_indices' in self.config.inputs_cols else 0, 'left_with_aspect_indices': left_with_aspect_indices if 'left_with_aspect_indices' in self.config.inputs_cols else 0, 'right_indices': right_indices if 'right_indices' in self.config.inputs_cols else 0, 'right_with_aspect_indices': right_with_aspect_indices if 'right_with_aspect_indices' in self.config.inputs_cols else 0, 'aspect_indices': aspect_indices if 'aspect_indices' in self.config.inputs_cols else 0, 'aspect_boundary': aspect_boundary if 'aspect_boundary' in self.config.inputs_cols else 0, 'aspect_position': aspect_position, 'dependency_graph': dependency_graph if 'dependency_graph' in self.config.inputs_cols else 0, 'polarity': polarity}
            ex_id += 1
            label_set.add(polarity)
            all_data.append(data)
        check_and_fix_labels(label_set, 'polarity', all_data, self.config)
        self.config.output_dim = len(label_set)
        all_data = build_sentiment_window(all_data, self.tokenizer, self.config.similarity_threshold, input_demands=self.config.inputs_cols)
        for data in all_data:
            cluster_ids = []
            for pad_idx in range(self.config.max_seq_len):
                if pad_idx in data['cluster_ids']:
                    cluster_ids.append(data['polarity'])
                else:
                    cluster_ids.append(-100)
            data['cluster_ids'] = np.asarray(cluster_ids, dtype=np.int64)
            data['side_ex_ids'] = np.array(0)
            data['aspect_position'] = np.array(0)
        self.data = all_data

    def __init__(self, config, tokenizer, dataset_type='train', **kwargs):
        super().__init__(config, tokenizer, dataset_type=dataset_type, **kwargs)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return len(self.data)


class GloVeABSADataset(PyABSADataset):

    def load_data_from_dict(self, dataset_dict, **kwargs):
        pass

    def load_data_from_file(self, dataset_file, **kwargs):
        configure_spacy_model(self.config)
        lines = load_dataset_from_file(self.config.dataset_file[self.dataset_type])
        all_data = []
        label_set = set()
        dep_cache_path = os.path.join(os.getcwd(), 'run/{}/dependency_cache/'.format(self.config.dataset_name))
        if not os.path.exists(dep_cache_path):
            os.makedirs(dep_cache_path)
        graph_path = prepare_dependency_graph(self.config.dataset_file[self.dataset_type], dep_cache_path, self.config.max_seq_len, self.config)
        fin = open(graph_path, 'rb')
        idx2graph = pickle.load(fin)
        ex_id = 0
        if len(lines) % 3 != 0:
            fprint(colored('ERROR: one or more datasets are corrupted, make sure the number of lines in a dataset should be multiples of 3.', 'red'))
        for i in tqdm.tqdm(range(0, len(lines), 3), postfix='preparing dataloader...'):
            if lines[i].count('$T$') > 1:
                continue
            text_left, _, text_right = [s.lower().strip() for s in lines[i].partition('$T$')]
            text_left = text_left.lower().strip()
            text_right = text_right.lower().strip()
            aspect = lines[i + 1].lower().strip()
            text_raw = text_left + ' ' + aspect + ' ' + text_right
            polarity = lines[i + 2].strip()
            if validate_example(text_raw, aspect, polarity, self.config):
                continue
            text_indices = self.tokenizer.text_to_sequence(text_left + ' ' + aspect + ' ' + text_right)
            context_indices = self.tokenizer.text_to_sequence(text_left + ' ' + text_right)
            left_indices = self.tokenizer.text_to_sequence(text_left)
            left_with_aspect_indices = self.tokenizer.text_to_sequence(text_left + ' ' + aspect)
            right_indices = self.tokenizer.text_to_sequence(text_right)
            right_with_aspect_indices = self.tokenizer.text_to_sequence(aspect + ' ' + text_right)
            aspect_indices = self.tokenizer.text_to_sequence(aspect)
            left_len = np.count_nonzero(left_indices)
            aspect_len = np.count_nonzero(aspect_indices)
            aspect_boundary = np.asarray([left_len, min(left_len + aspect_len - 1, self.config.max_seq_len)])
            dependency_graph = np.pad(idx2graph[text_raw], ((0, max(0, self.config.max_seq_len - idx2graph[text_raw].shape[0])), (0, max(0, self.config.max_seq_len - idx2graph[text_raw].shape[0]))), 'constant')
            dependency_graph = dependency_graph[:, range(0, self.config.max_seq_len)]
            dependency_graph = dependency_graph[range(0, self.config.max_seq_len), :]
            aspect_begin = np.count_nonzero(self.tokenizer.text_to_sequence(text_left))
            aspect_position = set(range(aspect_begin, aspect_begin + np.count_nonzero(aspect_indices)))
            if len(aspect_position) < 1:
                raise RuntimeError('Invalid Input: {}'.format(text_raw))
            data = {'ex_id': ex_id, 'text_indices': text_indices if 'text_indices' in self.config.inputs_cols else 0, 'context_indices': context_indices if 'context_indices' in self.config.inputs_cols else 0, 'left_indices': left_indices if 'left_indices' in self.config.inputs_cols else 0, 'left_with_aspect_indices': left_with_aspect_indices if 'left_with_aspect_indices' in self.config.inputs_cols else 0, 'right_indices': right_indices if 'right_indices' in self.config.inputs_cols else 0, 'right_with_aspect_indices': right_with_aspect_indices if 'right_with_aspect_indices' in self.config.inputs_cols else 0, 'aspect_indices': aspect_indices if 'aspect_indices' in self.config.inputs_cols else 0, 'aspect_boundary': aspect_boundary if 'aspect_boundary' in self.config.inputs_cols else 0, 'aspect_position': aspect_position, 'dependency_graph': dependency_graph if 'dependency_graph' in self.config.inputs_cols else 0, 'polarity': polarity}
            ex_id += 1
            label_set.add(polarity)
            all_data.append(data)
        check_and_fix_labels(label_set, 'polarity', all_data, self.config)
        self.config.output_dim = len(label_set)
        all_data = build_sentiment_window(all_data, self.tokenizer, self.config.similarity_threshold, input_demands=self.config.inputs_cols)
        for data in all_data:
            cluster_ids = []
            for pad_idx in range(self.config.max_seq_len):
                if pad_idx in data['cluster_ids']:
                    cluster_ids.append(data['polarity'])
                else:
                    cluster_ids.append(-100)
            data['cluster_ids'] = np.asarray(cluster_ids, dtype=np.int64)
            data['side_ex_ids'] = np.array(0)
            data['aspect_position'] = np.array(0)
        self.data = all_data

    def __init__(self, config, tokenizer, dataset_type='train'):
        super().__init__(config=config, tokenizer=tokenizer, dataset_type=dataset_type)

    def __getitem__(self, index):
        return self.data[index]

    def __len__(self):
        return len(self.data)


class PretrainedTokenizer:

    def __init__(self, config, **kwargs):
        self.config = config
        self.tokenizer = AutoTokenizer.from_pretrained(config.pretrained_bert, **kwargs)
        self.max_seq_len = self.config.max_seq_len
        self.pad_token_id = self.tokenizer.pad_token_id
        self.unk_token_id = self.tokenizer.unk_token_id
        self.cls_token_id = self.tokenizer.cls_token_id
        self.sep_token_id = self.tokenizer.sep_token_id
        self.mask_token_id = self.tokenizer.mask_token_id

    def text_to_sequence(self, text, padding='max_length', return_tensors=None, **kwargs):
        return self.tokenizer.encode(text, truncation=True, padding=padding, max_length=self.max_seq_len, return_tensors=return_tensors, **kwargs)

    def sequence_to_text(self, sequence, **kwargs):
        return self.tokenizer.decode(sequence, **kwargs)

    def tokenize(self, text, **kwargs):
        return self.tokenizer.tokenize(text, **kwargs)

    def convert_tokens_to_ids(self, return_tensors=None, **kwargs):
        return self.tokenizer.convert_tokens_to_ids(return_tensors, **kwargs)

    def convert_ids_to_tokens(self, ids, **kwargs):
        return self.tokenizer.convert_ids_to_tokens(ids, **kwargs)


class Tokenizer(object):

    def __init__(self, config):
        self.config = config
        self.max_seq_len = self.config.max_seq_len
        self.word2idx = {}
        self.idx2word = {}
        self.idx = 1
        self.pre_tokenizer = None
        self.pad_token_id = 0
        self.unk_token_id = 0
        self.cls_token_id = 0
        self.sep_token_id = 0
        self.mask_token_id = 0

    @staticmethod
    def build_tokenizer(config, cache_path=None, pre_tokenizer=None, **kwargs):
        Tokenizer.pre_tokenizer = pre_tokenizer
        dataset_name = os.path.basename(config.dataset_name)
        if not os.path.exists('run/{}'.format(dataset_name)):
            os.makedirs('run/{}'.format(dataset_name))
        tokenizer_path = 'run/{}/{}'.format(dataset_name, cache_path)
        if cache_path and os.path.exists(tokenizer_path) and not config.overwrite_cache:
            config.logger.info('Loading tokenizer on {}'.format(tokenizer_path))
            tokenizer = pickle.load(open(tokenizer_path, 'rb'))
        else:
            words = set()
            if hasattr(config, 'dataset_file'):
                config.logger.info('Building tokenizer for {} on {}'.format(config.dataset_file, tokenizer_path))
                for dataset_type in config.dataset_file:
                    for file in config.dataset_file[dataset_type]:
                        fin = open(file, 'r', encoding='utf-8', newline='\n', errors='ignore')
                        lines = fin.readlines()
                        fin.close()
                        for i in range(0, len(lines)):
                            if pre_tokenizer:
                                words.update(pre_tokenizer.tokenize(lines[i].strip()))
                            else:
                                words.update(lines[i].strip().split())
            elif hasattr(config, 'dataset_dict'):
                config.logger.info('Building tokenizer for {} on {}'.format(config.dataset_name, tokenizer_path))
                for dataset_type in ['train', 'test', 'valid']:
                    for i, data in enumerate(config.dataset_dict[dataset_type]):
                        if pre_tokenizer:
                            words.update(pre_tokenizer.tokenize(data['data']))
                        else:
                            words.update(data['data'].split())
            tokenizer = Tokenizer(config)
            tokenizer.pre_tokenizer = pre_tokenizer
            tokenizer.fit_on_text(list(words))
            if config.cache_dataset:
                pickle.dump(tokenizer, open(tokenizer_path, 'wb'))
        return tokenizer

    def fit_on_text(self, text: Union[str, List[str]], **kwargs):
        if isinstance(text, str):
            if self.pre_tokenizer:
                words = self.pre_tokenizer.tokenize(text)
            else:
                words = text.split()
        else:
            words = text
        for word in words:
            if self.config.do_lower_case:
                word = word.lower()
            if word not in self.word2idx:
                self.word2idx[word] = self.idx
                self.idx2word[self.idx] = word
                self.idx += 1

    def text_to_sequence(self, text: Union[str, List[str]], padding='max_length', **kwargs):
        if isinstance(text, str):
            if self.config.do_lower_case:
                text = text.lower()
            if self.pre_tokenizer:
                words = self.pre_tokenizer.tokenize(text)
            else:
                words = text.split()
            sequence = [(self.word2idx[w] if w in self.word2idx else 0) for w in words]
            if len(sequence) == 0:
                sequence = [0]
            if kwargs.get('reverse', False):
                sequence = sequence[::-1]
            if padding == 'max_length':
                return pad_and_truncate(sequence, self.max_seq_len, self.pad_token_id)
            else:
                return sequence
        elif isinstance(text, list):
            sequences = []
            for t in text:
                sequences.append(self.text_to_sequence(t, **kwargs))
            return sequences
        else:
            raise ValueError('text_to_sequence only support str or list of str')

    def sequence_to_text(self, sequence):
        words = [(self.idx2word[idx] if idx in self.idx2word else '<unk>') for idx in sequence]
        return ' '.join(words)


def _load_word_vec(path, word2idx=None, embed_dim=300):
    fin = open(path, 'r', encoding='utf-8', newline='\n', errors='ignore')
    word_vec = {}
    for line in tqdm.tqdm(fin.readlines(), postfix='Loading embedding file...'):
        tokens = line.rstrip().split()
        word, vec = ' '.join(tokens[:-embed_dim]), tokens[-embed_dim:]
        if word in word2idx.keys():
            word_vec[word] = np.asarray(vec, dtype='float32')
    return word_vec


def prepare_glove840_embedding(glove_path, embedding_dim, config):
    if config.get('glove_or_word2vec_path', None):
        glove_path = config.glove_or_word2vec_path
        return glove_path
    logger = config.logger
    glove840_id = '1G-vd6W1oF9ByyJ-pzp9dcqKnr_plh4Em'
    if os.path.exists(glove_path) and os.path.isfile(glove_path):
        return glove_path
    else:
        embedding_files = []
        dir_path = os.getenv('$HOME') if os.getenv('$HOME') else os.getcwd()
        if find_files(dir_path, ['glove', 'B', 'd', '.txt', str(embedding_dim)], exclude_key='.zip'):
            embedding_files += find_files(dir_path, ['glove', 'B', '.txt', str(embedding_dim)], exclude_key='.zip')
        elif find_files(dir_path, ['word2vec', 'd', '.txt'], exclude_key='.zip'):
            embedding_files += find_files(dir_path, ['word2vec', 'd', '.txt', str(embedding_dim)], exclude_key='.zip')
        else:
            embedding_files += find_files(dir_path, ['d', '.txt', str(embedding_dim)], exclude_key='.zip')
        if embedding_files:
            logger.info('Find embedding file: {}, use: {}'.format(embedding_files, embedding_files[0]))
            return embedding_files[0]
        else:
            if config.embed_dim != 300:
                raise ValueError('Please provide embedding file for embedding dim: {} in current wording dir '.format(config.embed_dim))
            zip_glove_path = os.path.join(os.path.dirname(glove_path), 'glove.840B.300d.zip')
            logger.info('No GloVe embedding found at {}, downloading glove.840B.300d.txt (2GB will be downloaded / 5.5GB after unzip)...'.format(glove_path))
            try:
                response = requests.get('https://huggingface.co/spaces/yangheng/PyABSA-ATEPC/resolve/main/open-access/glove.840B.300d.zip', stream=True)
                with open(zip_glove_path, 'wb') as f:
                    for chunk in tqdm.tqdm(response.iter_content(chunk_size=1024 * 1024), unit='MB', total=int(response.headers['content-length']) // 1024 // 1024, postfix=colored('Downloading GloVe-840B embedding...', 'yellow')):
                        f.write(chunk)
            except Exception as e:
                raise ValueError('Download failed, please download glove.840B.300d.zip from https://nlp.stanford.edu/projects/glove/, unzip it and put it in {}.'.format(glove_path))
        if find_cwd_file('glove.840B.300d.zip'):
            logger.info('unzip glove.840B.300d.zip...')
            with zipfile.ZipFile(find_cwd_file('glove.840B.300d.zip'), 'r') as z:
                z.extractall()
            logger.info('Zip file extraction Done.')
        return prepare_glove840_embedding(glove_path, embedding_dim, config)


def build_embedding_matrix(config, tokenizer, cache_path=None):
    if not os.path.exists('run/{}'.format(config.dataset_name)):
        os.makedirs('run/{}'.format(config.dataset_name))
    embed_matrix_path = 'run/{}'.format(os.path.join(config.dataset_name, cache_path))
    if cache_path and os.path.exists(embed_matrix_path) and not config.overwrite_cache:
        fprint(colored('Loading cached embedding_matrix from {} (Please remove all cached files if there is any problem!)'.format(embed_matrix_path), 'green'))
        embedding_matrix = pickle.load(open(embed_matrix_path, 'rb'))
    else:
        glove_path = prepare_glove840_embedding(embed_matrix_path, config.embed_dim, config=config)
        embedding_matrix = np.zeros((len(tokenizer.word2idx) + 1, config.embed_dim))
        word_vec = _load_word_vec(glove_path, word2idx=tokenizer.word2idx, embed_dim=config.embed_dim)
        for word, i in tqdm.tqdm(tokenizer.word2idx.items(), postfix=colored('Building embedding_matrix {}'.format(cache_path), 'yellow')):
            vec = word_vec.get(word)
            if vec is not None:
                embedding_matrix[i] = vec
        if config.cache_dataset:
            pickle.dump(embedding_matrix, open(embed_matrix_path, 'wb'))
    return embedding_matrix


def model_pool_check(models):
    set1 = set([model for model in models if hasattr(APCModelList, model.__name__)])
    set2 = set([model for model in models if hasattr(BERTBaselineAPCModelList, model.__name__)])
    set3 = set([model for model in models if hasattr(GloVeAPCModelList, model.__name__)])
    if set1 and set2 or set1 and set3 or set2 and set3:
        raise RuntimeError('The APCEnsembler only support the models in same type. ')


class APCEnsembler(nn.Module):

    def __init__(self, config, load_dataset=True, **kwargs):
        super(APCEnsembler, self).__init__()
        self.config = config
        models = [config.model] if not isinstance(config.model, list) else config.model
        model_pool_check(models)
        self.config.inputs_cols = set()
        for model in models:
            self.config.inputs_cols |= set(model.inputs)
        self.config.inputs_cols = sorted(self.config.inputs_cols)
        self.inputs_cols = self.config.inputs_cols
        self.models = ModuleList()
        self.tokenizer = None
        self.bert = None
        self.embedding_matrix = None
        self.train_set = None
        self.test_set = None
        self.valid_set = None
        self.test_dataloader = None
        self.valid_dataloader = None
        for i in range(len(models)):
            config_str = re.sub('<.*?>', '', str(sorted([str(self.config.args[k]) for k in self.config.args if k != 'seed'])))
            hash_tag = sha256(config_str.encode()).hexdigest()
            cache_path = '{}.{}.dataset.{}.cache'.format(self.config.model_name, self.config.dataset_name, hash_tag)
            if load_dataset and os.path.exists(cache_path) and not self.config.overwrite_cache:
                fprint(colored('Loading dataset cache: {}'.format(cache_path), 'green'))
                with open(cache_path, mode='rb') as f_cache:
                    self.train_set, self.valid_set, self.test_set, self.config = pickle.load(f_cache)
                    config.update(self.config)
                    config.args_call_count.update(self.config.args_call_count)
            if hasattr(APCModelList, models[i].__name__):
                try:
                    if kwargs.get('offline', False):
                        self.tokenizer = AutoTokenizer.from_pretrained(find_cwd_dir(self.config.pretrained_bert.split('/')[-1]), do_lower_case='uncased' in self.config.pretrained_bert)
                        self.bert = AutoModel.from_pretrained(find_cwd_dir(self.config.pretrained_bert.split('/')[-1])) if not self.bert else self.bert
                    else:
                        self.tokenizer = AutoTokenizer.from_pretrained(self.config.pretrained_bert, do_lower_case='uncased' in self.config.pretrained_bert)
                        self.bert = AutoModel.from_pretrained(self.config.pretrained_bert) if not self.bert else self.bert
                except ValueError as e:
                    fprint('Init pretrained model failed, exception: {}'.format(e))
                    exit(-1)
                if load_dataset and not os.path.exists(cache_path) or self.config.overwrite_cache:
                    self.train_set = ABSADataset(self.config, self.tokenizer, dataset_type='train') if not self.train_set else self.train_set
                    self.test_set = ABSADataset(self.config, self.tokenizer, dataset_type='test') if not self.test_set else self.test_set
                    self.valid_set = ABSADataset(self.config, self.tokenizer, dataset_type='valid') if not self.valid_set else self.valid_set
                self.models.append(models[i](self.bert, self.config))
            elif hasattr(BERTBaselineAPCModelList, models[i].__name__):
                self.tokenizer = PretrainedTokenizer(self.config) if not self.tokenizer else self.tokenizer
                self.bert = AutoModel.from_pretrained(self.config.pretrained_bert) if not self.bert else self.bert
                if load_dataset and not os.path.exists(cache_path) or self.config.overwrite_cache:
                    self.train_set = BERTBaselineABSADataset(self.config, self.tokenizer, dataset_type='train') if not self.train_set else self.train_set
                    self.test_set = BERTBaselineABSADataset(self.config, self.tokenizer, dataset_type='test') if not self.test_set else self.test_set
                    self.valid_set = BERTBaselineABSADataset(self.config, self.tokenizer, dataset_type='valid') if not self.valid_set else self.valid_set
                self.models.append(models[i](copy.deepcopy(self.bert) if self.config.deep_ensemble else self.bert, self.config))
            elif hasattr(GloVeAPCModelList, models[i].__name__):
                self.tokenizer = Tokenizer.build_tokenizer(config=self.config, cache_path='{0}_tokenizer.dat'.format(os.path.basename(config.dataset_name))) if not self.tokenizer else self.tokenizer
                self.embedding_matrix = build_embedding_matrix(config=self.config, tokenizer=self.tokenizer, cache_path='{0}_{1}_embedding_matrix.dat'.format(str(config.embed_dim), os.path.basename(config.dataset_name))) if not self.embedding_matrix else self.embedding_matrix
                if load_dataset and not os.path.exists(cache_path) or self.config.overwrite_cache:
                    self.train_set = GloVeABSADataset(self.config, self.tokenizer, dataset_type='train') if not self.train_set else self.train_set
                    self.test_set = GloVeABSADataset(self.config, self.tokenizer, dataset_type='test') if not self.test_set else self.test_set
                    self.valid_set = GloVeABSADataset(self.config, self.tokenizer, dataset_type='valid') if not self.valid_set else self.valid_set
                self.models.append(models[i](copy.deepcopy(self.embedding_matrix) if self.config.deep_ensemble else self.embedding_matrix, self.config))
                self.config.embedding_matrix = self.embedding_matrix
            if self.config.cache_dataset and not os.path.exists(cache_path) and not self.config.overwrite_cache:
                fprint(colored('Caching dataset... please remove cached dataset if any problem happens.', 'red'))
                with open(cache_path, mode='wb') as f_cache:
                    pickle.dump((self.train_set, self.valid_set, self.test_set, self.config), f_cache)
            if load_dataset:
                train_sampler = RandomSampler(self.train_set if not self.train_set else self.train_set)
                self.train_dataloader = DataLoader(self.train_set, batch_size=self.config.batch_size, pin_memory=True, sampler=train_sampler)
                if self.test_set:
                    test_sampler = SequentialSampler(self.test_set if not self.test_set else self.test_set)
                    self.test_dataloader = DataLoader(self.test_set, batch_size=self.config.batch_size, pin_memory=True, sampler=test_sampler)
                if self.valid_set:
                    valid_sampler = SequentialSampler(self.valid_set if not self.valid_set else self.valid_set)
                    self.valid_dataloader = DataLoader(self.valid_set, batch_size=self.config.batch_size, pin_memory=True, sampler=valid_sampler)
            self.config.tokenizer = self.tokenizer
        self.dense = nn.Linear(config.output_dim * len(models), config.output_dim)

    def forward(self, inputs):
        outputs = [self.models[i](inputs) for i in range(len(self.models))]
        loss = torch.tensor(0.0, requires_grad=True)
        if 'ensemble_mode' not in self.config:
            self.config.ensemble_mode = 'cat'
        logits = None
        if len(outputs) > 1:
            for i, out in enumerate(outputs):
                if self.config.ensemble_mode == 'cat':
                    logits = torch.cat((logits, out['logits']), dim=-1) if i != 0 else out['logits']
                elif self.config.ensemble_mode == 'mean':
                    logits = logits + out['logits'] if i != 0 else out['logits']
                else:
                    raise KeyError('Invalid ensemble_mode!')
                if 'loss' in out:
                    loss = loss + out['loss'] if i != 0 else out['loss']
            if 'ensemble_mode' not in self.config or self.config.ensemble_mode == 'cat':
                logits = self.dense(logits)
            elif self.config.ensemble_mode == 'mean':
                logits = logits / len(self.models)
        else:
            logits = outputs[0]['logits']
            loss = outputs[0]['loss'] if 'loss' in outputs[0] else loss
        return {'logits': logits, 'loss': loss}


class AOA(nn.Module):
    inputs = ['text_indices', 'aspect_indices']

    def __init__(self, embedding_matrix, config):
        super(AOA, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.ctx_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.asp_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.dense = nn.Linear(2 * config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        aspect_indices = inputs['aspect_indices']
        ctx_len = torch.sum(text_indices != 0, dim=1)
        asp_len = torch.sum(aspect_indices != 0, dim=1)
        ctx = self.embed(text_indices)
        asp = self.embed(aspect_indices)
        ctx_out, (_, _) = self.ctx_lstm(ctx, ctx_len)
        asp_out, (_, _) = self.asp_lstm(asp, asp_len)
        interaction_mat = torch.matmul(ctx_out, torch.transpose(asp_out, 1, 2))
        alpha = F.softmax(interaction_mat, dim=1)
        beta = F.softmax(interaction_mat, dim=2)
        beta_avg = beta.mean(dim=1, keepdim=True)
        gamma = torch.matmul(alpha, beta_avg.transpose(1, 2))
        weighted_sum = torch.matmul(torch.transpose(ctx_out, 1, 2), gamma).squeeze(-1)
        out = self.dense(weighted_sum)
        return {'logits': out}


class GraphConvolution(nn.Module):
    """
    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907
    """

    def __init__(self, in_features, out_features, bias=True):
        super(GraphConvolution, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))
        if bias:
            self.bias = nn.Parameter(torch.FloatTensor(out_features))
        else:
            self.register_parameter('bias', None)

    def forward(self, text, adj):
        hidden = torch.matmul(text, self.weight)
        denom = torch.sum(adj, dim=2, keepdim=True) + 1
        output = torch.matmul(adj, hidden) / denom
        if self.bias is not None:
            return output + self.bias
        else:
            return output


class ASGCN_Unit(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices', 'dependency_graph']

    def __init__(self, embedding_matrix, config):
        super(ASGCN_Unit, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.text_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.gc1 = GraphConvolution(2 * config.hidden_dim, 2 * config.hidden_dim)
        self.gc2 = GraphConvolution(2 * config.hidden_dim, 2 * config.hidden_dim)
        self.text_embed_dropout = nn.Dropout()

    def position_weight(self, x, aspect_double_idx, text_len, aspect_len):
        batch_size = x.shape[0]
        seq_len = x.shape[1]
        aspect_double_idx = aspect_double_idx.cpu().numpy()
        text_len = text_len.cpu().numpy()
        aspect_len = aspect_len.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        for i in range(batch_size):
            context_len = text_len[i] - aspect_len[i]
            for j in range(aspect_double_idx[i, 0]):
                weight[i].append(1 - (aspect_double_idx[i, 0] - j) / context_len)
            for j in range(aspect_double_idx[i, 0], aspect_double_idx[i, 1] + 1):
                weight[i].append(0)
            for j in range(aspect_double_idx[i, 1] + 1, text_len[i]):
                weight[i].append(1 - (j - aspect_double_idx[i, 1]) / context_len)
            for j in range(text_len[i], seq_len):
                weight[i].append(0)
        weight = torch.tensor(weight, dtype=torch.float).unsqueeze(2)
        return weight * x

    def mask(self, x, aspect_double_idx):
        batch_size, seq_len = x.shape[0], x.shape[1]
        aspect_double_idx = aspect_double_idx.cpu().numpy()
        mask = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for j in range(aspect_double_idx[i, 0]):
                mask[i].append(0)
            for j in range(aspect_double_idx[i, 0], aspect_double_idx[i, 1] + 1):
                mask[i].append(1)
            for j in range(aspect_double_idx[i, 1] + 1, seq_len):
                mask[i].append(0)
        mask = torch.tensor(mask, dtype=torch.float).unsqueeze(2)
        return mask * x

    def forward(self, inputs):
        text_indices, aspect_indices, left_indices, adj = inputs[0], inputs[1], inputs[2], inputs[3]
        text_len = torch.sum(text_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        left_len = torch.sum(left_indices != 0, dim=-1)
        aspect_double_idx = torch.cat([left_len.unsqueeze(1), torch.where(left_len + aspect_len - 1 < self.config.max_seq_len, left_len + aspect_len - 1, self.config.max_seq_len).unsqueeze(1)], dim=1)
        text = self.embed(text_indices)
        text = self.text_embed_dropout(text)
        text_out, (_, _) = self.text_lstm(text, text_len)
        seq_len = text_out.shape[1]
        adj = adj[:, :seq_len, :seq_len]
        x = F.relu(self.gc1(self.position_weight(text_out, aspect_double_idx, text_len, aspect_len), adj))
        x = F.relu(self.gc2(self.position_weight(x, aspect_double_idx, text_len, aspect_len), adj))
        x = self.mask(x, aspect_double_idx)
        alpha_mat = torch.matmul(x, text_out.transpose(1, 2))
        alpha = F.softmax(alpha_mat.sum(1, keepdim=True), dim=2)
        x = torch.matmul(alpha, text_out).squeeze(1)
        return x


class ASGCN(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices', 'dependency_graph', 'left_aspect_indices', 'left_left_indices', 'left_dependency_graph', 'right_aspect_indices', 'right_left_indices', 'right_dependency_graph']

    def __init__(self, bert, config):
        super(ASGCN, self).__init__()
        self.config = config
        self.asgcn_left = ASGCN_Unit(bert, config) if self.config.lsa else None
        self.asgcn_central = ASGCN_Unit(bert, config)
        self.asgcn_right = ASGCN_Unit(bert, config) if self.config.lsa else None
        self.dense = nn.Linear(self.config.hidden_dim * 6, self.config.output_dim) if self.config.lsa else nn.Linear(self.config.hidden_dim * 2, self.config.output_dim)

    def forward(self, inputs):
        res = {'logits': None}
        if self.config.lsa:
            cat_feat = torch.cat((self.asgcn_left([inputs['text_indices'], inputs['left_aspect_indices'], inputs['left_left_indices'], inputs['left_dependency_graph']]), self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['left_indices'], inputs['dependency_graph']]), self.asgcn_right([inputs['text_indices'], inputs['right_aspect_indices'], inputs['right_left_indices'], inputs['right_dependency_graph']])), -1)
            res['logits'] = self.dense(cat_feat)
        else:
            res['logits'] = self.dense(self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['left_indices'], inputs['dependency_graph']]))
        return res


class ATAE_LSTM(nn.Module):
    inputs = ['text_indices', 'aspect_indices']

    def __init__(self, embedding_matrix, config):
        super(ATAE_LSTM, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.squeeze_embedding = SqueezeEmbedding()
        self.lstm = DynamicLSTM(config.embed_dim * 2, config.hidden_dim, num_layers=1, batch_first=True)
        self.attention = NoQueryAttention(config.hidden_dim + config.embed_dim, score_function='bi_linear')
        self.dense = nn.Linear(config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_indices, aspect_indices = inputs['text_indices'], inputs['text_indices']
        x_len = torch.sum(text_indices != 0, dim=-1)
        x_len_max = torch.max(x_len)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()
        x = self.embed(text_indices)
        x = self.squeeze_embedding(x, x_len)
        aspect = self.embed(aspect_indices)
        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))
        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)
        x = torch.cat((aspect, x), dim=-1)
        h, (_, _) = self.lstm(x, x_len)
        ha = torch.cat((h, aspect), dim=-1)
        _, score = self.attention(ha)
        output = torch.squeeze(torch.bmm(score, h), dim=1)
        out = self.dense(output)
        return {'logits': out}


class Cabasc(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']

    def __init__(self, embedding_matrix, config, _type='c'):
        super(Cabasc, self).__init__()
        self.config = config
        self.type = _type
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.squeeze_embedding = SqueezeEmbedding(batch_first=True)
        self.linear1 = nn.Linear(3 * config.embed_dim, config.embed_dim)
        self.linear2 = nn.Linear(config.embed_dim, 1, bias=False)
        self.mlp = nn.Linear(config.embed_dim, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)
        self.rnn_l = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, rnn_type='GRU')
        self.rnn_r = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, rnn_type='GRU')
        self.mlp_l = nn.Linear(config.hidden_dim, 1)
        self.mlp_r = nn.Linear(config.hidden_dim, 1)

    def context_attention(self, x_l, x_r, memory, memory_len, aspect_len):
        left_len, right_len = torch.sum(x_l != 0, dim=-1), torch.sum(x_r != 0, dim=-1)
        x_l, x_r = self.embed(x_l), self.embed(x_r)
        context_l, (_, _) = self.rnn_l(x_l, left_len)
        context_r, (_, _) = self.rnn_r(x_r, right_len)
        attn_l = torch.sigmoid(self.mlp_l(context_l)) + 0.5
        attn_r = torch.sigmoid(self.mlp_r(context_r)) + 0.5
        for i in range(memory.size(0)):
            aspect_start = (left_len[i] - aspect_len[i]).item()
            aspect_end = left_len[i]
            for idx in range(memory_len[i]):
                if idx < aspect_start:
                    memory[i][idx] *= attn_l[i][idx]
                elif idx < aspect_end:
                    memory[i][idx] *= (attn_l[i][idx] + attn_r[i][idx - aspect_start]) / 2
                else:
                    memory[i][idx] *= attn_r[i][idx - aspect_start]
        return memory

    def locationed_memory(self, memory, memory_len):
        """
        # differ from description in paper here, but may be better
        for i in range(memory.size(0)):
            for idx in range(memory_len[i]):
                aspect_start = left_len[i] - aspect_len[i]
                aspect_end = left_len[i] 
                if idx < aspect_start: l = aspect_start.item() - idx                   
                elif idx <= aspect_end: l = 0 
                else: l = idx - aspect_end.item()
                memory[i][idx] *= (1-float(l)/int(memory_len[i]))
        """
        for i in range(memory.size(0)):
            for idx in range(memory_len[i]):
                memory[i][idx] *= 1 - float(idx) / int(memory_len[i])
        return memory

    def forward(self, inputs):
        text_raw_indices, aspect_indices, x_l, x_r = inputs['text_indices'], inputs['aspect_indices'], inputs['left_with_aspect_indices'], inputs['right_with_aspect_indices']
        memory_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        nonzeros_aspect = aspect_len.float()
        aspect = self.embed(aspect_indices)
        aspect = torch.sum(aspect, dim=1)
        v_a = torch.div(aspect, nonzeros_aspect.unsqueeze(1)).unsqueeze(1)
        memory = self.embed(text_raw_indices)
        memory = self.squeeze_embedding(memory, memory_len)
        nonzeros_memory = memory_len.float()
        v_s = torch.sum(memory, dim=1)
        v_s = torch.div(v_s, nonzeros_memory.unsqueeze(1)).unsqueeze(1)
        if self.type == 'c':
            memory = self.locationed_memory(memory, memory_len)
        elif self.type == 'cabasc':
            memory = self.context_attention(x_l, x_r, memory, memory_len, aspect_len)
            v_s = torch.sum(memory, dim=1)
            v_s = torch.div(v_s, nonzeros_memory.unsqueeze(1))
            v_s = v_s.unsqueeze(dim=1)
        """
        # no multi-hop, but may be better. 
        # however, here is something totally different from what paper depits
        for _ in range(self.config.hops):  
            #x = self.x_linear(x)
            v_ts, _ = self.attention(memory, v_a)
        """
        memory_chunks = memory.chunk(memory.size(1), dim=1)
        c = []
        for memory_chunk in memory_chunks:
            c_i = self.linear1(torch.cat([memory_chunk, v_a, v_s], dim=1).view(memory_chunk.size(0), -1))
            c_i = self.linear2(torch.tanh(c_i))
            c.append(c_i)
        alpha = F.softmax(torch.cat(c, dim=1), dim=1)
        v_ts = torch.matmul(memory.transpose(1, 2), alpha.unsqueeze(-1)).transpose(1, 2)
        v_ns = v_ts + v_s
        v_ns = v_ns.view(v_ns.size(0), -1)
        v_ms = torch.tanh(self.mlp(v_ns))
        out = self.dense(v_ms)
        return {'logits': out}


class IAN(nn.Module):
    inputs = ['text_indices', 'aspect_indices']

    def __init__(self, embedding_matrix, config):
        super(IAN, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.lstm_context = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.lstm_aspect = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.attention_aspect = Attention(config.hidden_dim, score_function='bi_linear')
        self.attention_context = Attention(config.hidden_dim, score_function='bi_linear')
        self.dense = nn.Linear(config.hidden_dim * 2, config.output_dim)

    def forward(self, inputs):
        text_raw_indices, aspect_indices = inputs['text_indices'], inputs['aspect_indices']
        text_raw_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        context = self.embed(text_raw_indices)
        aspect = self.embed(aspect_indices)
        context, (_, _) = self.lstm_context(context, text_raw_len)
        aspect, (_, _) = self.lstm_aspect(aspect, aspect_len)
        aspect_len = torch.tensor(aspect_len, dtype=torch.float)
        aspect_pool = torch.sum(aspect, dim=1)
        aspect_pool = torch.div(aspect_pool, aspect_len.view(aspect_len.size(0), 1))
        text_raw_len = text_raw_len.clone().detach()
        context_pool = torch.sum(context, dim=1)
        context_pool = torch.div(context_pool, text_raw_len.view(text_raw_len.size(0), 1))
        aspect_final, _ = self.attention_aspect(aspect, context_pool)
        aspect_final = aspect_final.squeeze(dim=1)
        context_final, _ = self.attention_context(context, aspect_pool)
        context_final = context_final.squeeze(dim=1)
        x = torch.cat((aspect_final, context_final), dim=-1)
        out = self.dense(x)
        return {'logits': out}


class LSTMLayer(nn.Module):

    def __init__(self, config):
        super(LSTMLayer, self).__init__()
        self.lstms = nn.ModuleList()
        self.config = config
        for i in range(self.config.num_lstm_layer):
            self.lstms.append(DynamicLSTM(self.config.embed_dim, self.config.hidden_dim, num_layers=self.config.num_lstm_layer, batch_first=True, bidirectional=True))

    def forward(self, x, x_len):
        h, c = None, None
        for i in range(len(self.lstms)):
            x, (h, c) = self.lstms[i](x, x_len)
        return x, (h, c)


class LSTM(nn.Module):
    inputs = ['text_indices', 'rna_type']

    def __init__(self, embedding_matrix, config):
        super(LSTM, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.lstm = LSTMLayer(config)
        self.type_embed = nn.Embedding(4, self.config.embed_dim, padding_idx=0)
        self.linear = nn.Linear(self.config.hidden_dim * 2, self.config.hidden_dim)
        self.dense = nn.Linear(config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_indices = inputs[0]
        rna_type = inputs[1]
        x = self.embed(text_raw_indices)
        x_len = torch.count_nonzero(text_raw_indices, dim=-1)
        _, (h_n, _) = self.lstm(x, x_len)
        out = self.dense(h_n[0])
        return out


class MemNet(nn.Module):
    inputs = ['context_indices', 'aspect_indices']

    def locationed_memory(self, memory, memory_len):
        batch_size = memory.shape[0]
        seq_len = memory.shape[1]
        memory_len = memory_len.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for idx in range(memory_len[i]):
                weight[i].append(1 - float(idx + 1) / memory_len[i])
            for idx in range(memory_len[i], seq_len):
                weight[i].append(1)
        weight = torch.tensor(weight)
        memory = weight.unsqueeze(2) * memory
        return memory

    def __init__(self, embedding_matrix, config):
        super(MemNet, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.squeeze_embedding = SqueezeEmbedding(batch_first=True)
        self.attention = Attention(config.embed_dim, score_function='mlp')
        self.x_linear = nn.Linear(config.embed_dim, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_without_aspect_indices, aspect_indices = inputs['context_indices'], inputs['aspect_indices']
        memory_len = torch.sum(text_raw_without_aspect_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float)
        memory = self.embed(text_raw_without_aspect_indices)
        memory = self.squeeze_embedding(memory, memory_len)
        aspect = self.embed(aspect_indices)
        aspect = torch.sum(aspect, dim=1)
        aspect = torch.div(aspect, nonzeros_aspect.view(nonzeros_aspect.size(0), 1))
        x = aspect.unsqueeze(dim=1)
        if 'hops' not in self.config.args:
            self.config.hops = 3
        for _ in range(self.config.hops):
            x = self.x_linear(x)
            out_at, _ = self.attention(memory, x)
            x = out_at + x
        x = x.view(x.size(0), -1)
        out = self.dense(x)
        return {'logits': out}


class LocationEncoding(nn.Module):

    def __init__(self, config):
        super(LocationEncoding, self).__init__()
        self.config = config

    def forward(self, x, pos_inx):
        batch_size, seq_len = x.size()[0], x.size()[1]
        weight = self.weight_matrix(pos_inx, batch_size, seq_len)
        x = weight.unsqueeze(2) * x
        return x

    def weight_matrix(self, pos_inx, batch_size, seq_len):
        pos_inx = pos_inx.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for j in range(pos_inx[i][0]):
                relative_pos = pos_inx[i][0] - j
                aspect_len = pos_inx[i][1] - pos_inx[i][0] + 1
                sentence_len = seq_len - aspect_len
                weight[i].append(1 - relative_pos / sentence_len)
            for j in range(pos_inx[i][0], pos_inx[i][1] + 1):
                weight[i].append(0)
            for j in range(pos_inx[i][1] + 1, seq_len):
                relative_pos = j - pos_inx[i][1]
                aspect_len = pos_inx[i][1] - pos_inx[i][0] + 1
                sentence_len = seq_len - aspect_len
                weight[i].append(1 - relative_pos / sentence_len)
        weight = torch.tensor(weight)
        return weight


class AlignmentMatrix(nn.Module):

    def __init__(self, config):
        super(AlignmentMatrix, self).__init__()
        self.config = config
        self.w_u = nn.Parameter(torch.Tensor(6 * config.hidden_dim, 1))

    def forward(self, batch_size, ctx, asp):
        ctx_len = ctx.size(1)
        asp_len = asp.size(1)
        alignment_mat = torch.zeros(batch_size, ctx_len, asp_len)
        ctx_chunks = ctx.chunk(ctx_len, dim=1)
        asp_chunks = asp.chunk(asp_len, dim=1)
        for i, ctx_chunk in enumerate(ctx_chunks):
            for j, asp_chunk in enumerate(asp_chunks):
                feat = torch.cat([ctx_chunk, asp_chunk, ctx_chunk * asp_chunk], dim=2)
                alignment_mat[:, i, j] = feat.matmul(self.w_u.expand(batch_size, -1, -1)).squeeze(-1).squeeze(-1)
        return alignment_mat


class MGAN(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices']

    def __init__(self, embedding_matrix, config):
        super(MGAN, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.ctx_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.asp_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.location = LocationEncoding(config)
        self.w_a2c = nn.Parameter(torch.Tensor(2 * config.hidden_dim, 2 * config.hidden_dim))
        self.w_c2a = nn.Parameter(torch.Tensor(2 * config.hidden_dim, 2 * config.hidden_dim))
        self.alignment = AlignmentMatrix(config)
        self.dense = nn.Linear(8 * config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_indices = inputs['text_indices']
        aspect_indices = inputs['aspect_indices']
        text_left_indices = inputs['left_indices']
        batch_size = text_raw_indices.size(0)
        ctx_len = torch.sum(text_raw_indices != 0, dim=1)
        asp_len = torch.sum(aspect_indices != 0, dim=1)
        left_len = torch.sum(text_left_indices != 0, dim=-1)
        aspect_in_text = torch.cat([left_len.unsqueeze(-1), (left_len + asp_len - 1).unsqueeze(-1)], dim=-1)
        ctx = self.embed(text_raw_indices)
        asp = self.embed(aspect_indices)
        ctx_out, (_, _) = self.ctx_lstm(ctx, ctx_len)
        ctx_out = self.location(ctx_out, aspect_in_text).float()
        ctx_pool = torch.sum(ctx_out, dim=1)
        ctx_pool = torch.div(ctx_pool, ctx_len.float().unsqueeze(-1)).unsqueeze(-1)
        asp_out, (_, _) = self.asp_lstm(asp, asp_len)
        asp_pool = torch.sum(asp_out, dim=1)
        asp_pool = torch.div(asp_pool, asp_len.float().unsqueeze(-1)).unsqueeze(-1)
        alignment_mat = self.alignment(batch_size, ctx_out, asp_out.float())
        f_asp2ctx = torch.matmul(ctx_out.transpose(1, 2), F.softmax(alignment_mat.max(2, keepdim=True)[0], dim=1)).squeeze(-1)
        f_ctx2asp = torch.matmul(F.softmax(alignment_mat.max(1, keepdim=True)[0], dim=2), asp_out).transpose(1, 2).squeeze(-1)
        c_asp2ctx_alpha = F.softmax(ctx_out.matmul(self.w_a2c.expand(batch_size, -1, -1)).matmul(asp_pool), dim=1)
        c_asp2ctx = torch.matmul(ctx_out.transpose(1, 2), c_asp2ctx_alpha).squeeze(-1)
        c_ctx2asp_alpha = F.softmax(asp_out.matmul(self.w_c2a.expand(batch_size, -1, -1)).matmul(ctx_pool), dim=1)
        c_ctx2asp = torch.matmul(asp_out.transpose(1, 2), c_ctx2asp_alpha).squeeze(-1)
        feat = torch.cat([c_asp2ctx, f_asp2ctx, f_ctx2asp, c_ctx2asp], dim=1)
        out = self.dense(feat)
        return {'logits': out}


class RAM(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices']

    def locationed_memory(self, memory, memory_len, left_len, aspect_len):
        batch_size = memory.shape[0]
        seq_len = memory.shape[1]
        memory_len = memory_len.cpu().numpy()
        left_len = left_len.cpu().numpy()
        aspect_len = aspect_len.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        u = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for idx in range(left_len[i]):
                weight[i].append(1 - (left_len[i] - idx) / memory_len[i])
                u[i].append(idx - left_len[i])
            for idx in range(left_len[i], left_len[i] + aspect_len[i]):
                weight[i].append(1)
                u[i].append(0)
            for idx in range(left_len[i] + aspect_len[i], memory_len[i]):
                weight[i].append(1 - (idx - left_len[i] - aspect_len[i] + 1) / memory_len[i])
                u[i].append(idx - left_len[i] - aspect_len[i] + 1)
            for idx in range(memory_len[i], seq_len):
                weight[i].append(1)
                u[i].append(0)
        u = torch.tensor(u, dtype=memory.dtype).unsqueeze(2)
        weight = torch.tensor(weight).unsqueeze(2)
        v = memory * weight
        memory = torch.cat([v, u], dim=2)
        return memory

    def __init__(self, embedding_matrix, config):
        super(RAM, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.bi_lstm_context = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.att_linear = nn.Linear(config.hidden_dim * 2 + 1 + config.embed_dim * 2, 1)
        self.gru_cell = nn.GRUCell(config.hidden_dim * 2 + 1, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_indices, aspect_indices, text_left_indices = inputs['text_indices'], inputs['aspect_indices'], inputs['left_indices']
        left_len = torch.sum(text_left_indices != 0, dim=-1)
        memory_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        nonzeros_aspect = aspect_len.float()
        memory = self.embed(text_raw_indices)
        memory, (_, _) = self.bi_lstm_context(memory, memory_len)
        memory = self.locationed_memory(memory, memory_len, left_len, aspect_len)
        memory = memory.float()
        aspect = self.embed(aspect_indices)
        aspect = torch.sum(aspect, dim=1)
        aspect = torch.div(aspect, nonzeros_aspect.unsqueeze(-1))
        et = torch.zeros_like(aspect)
        batch_size = memory.size(0)
        seq_len = memory.size(1)
        if 'hops' not in self.config.args:
            self.config.hops = 3
        for _ in range(self.config.hops):
            g = self.att_linear(torch.cat([memory, torch.zeros(batch_size, seq_len, self.config.embed_dim) + et.unsqueeze(1), torch.zeros(batch_size, seq_len, self.config.embed_dim) + aspect.unsqueeze(1)], dim=-1))
            alpha = F.softmax(g, dim=1)
            i = torch.bmm(alpha.transpose(1, 2), memory).squeeze(1)
            et = self.gru_cell(i, et)
        out = self.dense(et)
        return {'logits': out}


class TC_LSTM(nn.Module):
    inputs = ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']

    def __init__(self, embedding_matrix, config):
        super(TC_LSTM, self).__init__()
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.lstm_l = DynamicLSTM(config.embed_dim * 2, config.hidden_dim, num_layers=1, batch_first=True)
        self.lstm_r = DynamicLSTM(config.embed_dim * 2, config.hidden_dim, num_layers=1, batch_first=True)
        self.dense = nn.Linear(config.hidden_dim * 2, config.output_dim)

    def forward(self, inputs):
        x_l, x_r, target = inputs['left_with_aspect_indices'], inputs['right_with_aspect_indices'], inputs['aspect_indices']
        x_l_len, x_r_len = torch.sum(x_l != 0, dim=-1), torch.sum(x_r != 0, dim=-1)
        target_len = torch.sum(target != 0, dim=-1, dtype=torch.float)[:, None, None]
        x_l, x_r, target = self.embed(x_l), self.embed(x_r), self.embed(target)
        v_target = torch.div(target.sum(dim=1, keepdim=True), target_len)
        x_l = torch.cat((x_l, torch.cat([v_target] * x_l.shape[1], 1)), 2)
        x_r = torch.cat((x_r, torch.cat([v_target] * x_r.shape[1], 1)), 2)
        _, (h_n_l, _) = self.lstm_l(x_l, x_l_len)
        _, (h_n_r, _) = self.lstm_r(x_r, x_r_len)
        h_n = torch.cat((h_n_l[0], h_n_r[0]), dim=-1)
        out = self.dense(h_n)
        return {'logits': out}


class TD_LSTM(nn.Module):
    inputs = ['left_with_aspect_indices', 'right_with_aspect_indices']

    def __init__(self, embedding_matrix, config):
        super(TD_LSTM, self).__init__()
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.lstm_l = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.lstm_r = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.dense = nn.Linear(config.hidden_dim * 2, config.output_dim)

    def forward(self, inputs):
        x_l, x_r = inputs['left_with_aspect_indices'], inputs['right_with_aspect_indices']
        x_l_len, x_r_len = torch.sum(x_l != 0, dim=-1), torch.sum(x_r != 0, dim=-1)
        x_l, x_r = self.embed(x_l), self.embed(x_r)
        _, (h_n_l, _) = self.lstm_l(x_l, x_l_len)
        _, (h_n_r, _) = self.lstm_r(x_r, x_r_len)
        h_n = torch.cat((h_n_l[0], h_n_r[0]), dim=-1)
        out = self.dense(h_n)
        return {'logits': out}


class Absolute_Position_Embedding(nn.Module):

    def __init__(self, config, size=None, mode='sum'):
        self.config = config
        self.size = size
        self.mode = mode
        super(Absolute_Position_Embedding, self).__init__()

    def forward(self, x, pos_inx):
        if self.size is None or self.mode == 'sum':
            self.size = int(x.size(-1))
        batch_size, seq_len = x.size()[0], x.size()[1]
        weight = self.weight_matrix(pos_inx, batch_size, seq_len)
        x = weight.unsqueeze(2) * x
        return x

    def weight_matrix(self, pos_inx, batch_size, seq_len):
        pos_inx = pos_inx.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for j in range(pos_inx[i][1]):
                relative_pos = pos_inx[i][1] - j
                weight[i].append(1 - relative_pos / 40)
            for j in range(pos_inx[i][1], seq_len):
                relative_pos = j - pos_inx[i][0]
                weight[i].append(1 - relative_pos / 40)
        weight = torch.tensor(weight)
        return weight


class TNet_LF_Unit(nn.Module):

    def __init__(self, embedding_matrix, config):
        super(TNet_LF_Unit, self).__init__()
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.position = Absolute_Position_Embedding(config)
        self.config = config
        D = config.embed_dim
        C = config.output_dim
        L = config.max_seq_len
        HD = config.hidden_dim
        self.lstm1 = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.lstm2 = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.convs3 = nn.Conv1d(2 * HD, 50, 3, padding=1)
        self.fc1 = nn.Linear(4 * HD, 2 * HD)
        self.fc = nn.Linear(50, C)

    def forward(self, inputs):
        text_raw_indices, aspect_indices, aspect_in_text = inputs[0], inputs[1], inputs[2]
        feature_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        feature = self.embed(text_raw_indices)
        aspect = self.embed(aspect_indices)
        v, (_, _) = self.lstm1(feature, feature_len)
        e, (_, _) = self.lstm2(aspect, aspect_len)
        v = v.transpose(1, 2)
        e = e.transpose(1, 2)
        for i in range(2):
            a = torch.bmm(e.transpose(1, 2), v)
            a = F.softmax(a, 1)
            aspect_mid = torch.bmm(e, a)
            aspect_mid = torch.cat((aspect_mid, v), dim=1).transpose(1, 2)
            aspect_mid = F.relu(self.fc1(aspect_mid).transpose(1, 2))
            v = aspect_mid + v
            v = self.position(v.transpose(1, 2), aspect_in_text).transpose(1, 2)
            e = e.float()
            v = v.float()
        z = F.relu(self.convs3(v))
        z = F.max_pool1d(z, z.size(2)).squeeze(2)
        return z


class TNet_LF(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'aspect_boundary', 'left_aspect_indices', 'left_aspect_boundary', 'right_aspect_indices', 'right_aspect_boundary']

    def __init__(self, bert, config):
        super(TNet_LF, self).__init__()
        self.config = config
        self.asgcn_left = TNet_LF_Unit(bert, config) if self.config.lsa else None
        self.asgcn_central = TNet_LF_Unit(bert, config)
        self.asgcn_right = TNet_LF_Unit(bert, config) if self.config.lsa else None
        self.dense = nn.Linear(50 * 3, self.config.output_dim) if self.config.lsa else nn.Linear(50, self.config.output_dim)

    def forward(self, inputs):
        res = {'logits': None}
        if self.config.lsa:
            cat_feat = torch.cat((self.asgcn_left([inputs['text_indices'], inputs['left_aspect_indices'], inputs['left_aspect_boundary']]), self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['aspect_boundary'], inputs['aspect_len']]), self.asgcn_right([inputs['text_indices'], inputs['right_aspect_indices'], inputs['right_aspect_boundary']])), -1)
            res['logits'] = self.dense(cat_feat)
        else:
            res['logits'] = self.dense(self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['aspect_boundary']]))
        return res


class BERT_MLP(nn.Module):
    inputs = ['text_indices', 'rna_type']

    def __init__(self, bert, config):
        super(BERT_MLP, self).__init__()
        self.config = config
        self.bert = bert
        self.pooler = BertPooler(bert.config)
        self.dense = nn.Linear(self.config.hidden_dim, self.config.output_dim)
        self.dropout = nn.Dropout(self.config.dropout)
        self.type_embed = nn.Embedding(4, self.config.embed_dim, padding_idx=0)
        self.linear = nn.Linear(self.config.hidden_dim * 2, self.config.hidden_dim)
        if self.config.sigmoid_regression:
            self.sigmoid = nn.Sigmoid()
        else:
            self.sigmoid = None

    def forward(self, inputs):
        text_raw_indices = inputs[0]
        rna_type = inputs[1]
        last_hidden_state = self.bert(text_raw_indices)['last_hidden_state']
        pooled_out = self.pooler(last_hidden_state)
        out = self.dense(pooled_out)
        if self.sigmoid:
            out = self.sigmoid(out)
        return out


class BERT_SPC(nn.Module):
    inputs = ['text_indices', 'left_text_indices', 'right_text_indices']

    def __init__(self, bert, config):
        super(BERT_SPC, self).__init__()
        self.bert = bert
        self.config = config
        self.linear = nn.Linear(config.embed_dim, config.embed_dim)
        self.linear_window_2h = nn.Linear(2 * config.embed_dim, config.embed_dim) if self.config.lsa else nn.Linear(config.embed_dim, config.embed_dim)
        self.linear_window_3h = nn.Linear(3 * config.embed_dim, config.embed_dim) if self.config.lsa else nn.Linear(config.embed_dim, config.embed_dim)
        self.encoder = Encoder(bert.config, config)
        self.dropout = nn.Dropout(config.dropout)
        self.pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        res = {'logits': None}
        if self.config.lsa:
            feat = self.bert(inputs['text_indices'])['last_hidden_state']
            left_feat = self.bert(inputs['left_text_indices'])['last_hidden_state']
            right_feat = self.bert(inputs['right_text_indices'])['last_hidden_state']
            if 'lr' == self.config.window or 'rl' == self.config.window:
                if self.config.eta >= 0:
                    cat_features = torch.cat((feat, self.config.eta * left_feat, (1 - self.config.eta) * right_feat), -1)
                else:
                    cat_features = torch.cat((feat, left_feat, right_feat), -1)
                sent_out = self.linear_window_3h(cat_features)
            elif 'l' == self.config.window:
                sent_out = self.linear_window_2h(torch.cat((feat, left_feat), -1))
            elif 'r' == self.config.window:
                sent_out = self.linear_window_2h(torch.cat((feat, right_feat), -1))
            else:
                raise KeyError('Invalid parameter:', self.config.window)
            cat_feat = self.linear(sent_out)
            cat_feat = self.dropout(cat_feat)
            cat_feat = self.encoder(cat_feat)
            cat_feat = self.pooler(cat_feat)
            res['logits'] = self.dense(cat_feat)
        else:
            cat_feat = self.bert(inputs['text_indices'])['last_hidden_state']
            cat_feat = self.linear(cat_feat)
            cat_feat = self.dropout(cat_feat)
            cat_feat = self.encoder(cat_feat)
            cat_feat = self.pooler(cat_feat)
            res['logits'] = self.dense(cat_feat)
        return res


class BERT_SPC_V2(nn.Module):
    inputs = ['text_indices', 'left_text_indices', 'right_text_indices']

    def __init__(self, bert, config):
        super(BERT_SPC_V2, self).__init__()
        self.bert = bert
        self.config = config
        self.linear = nn.Linear(config.embed_dim, config.embed_dim)
        self.linear_window_2h = nn.Linear(2 * config.embed_dim, config.embed_dim) if self.config.lsa else nn.Linear(config.embed_dim, config.embed_dim)
        self.linear_window_3h = nn.Linear(3 * config.embed_dim, config.embed_dim) if self.config.lsa else nn.Linear(config.embed_dim, config.embed_dim)
        self.encoder = Encoder(bert.config, config)
        self.dropout = nn.Dropout(config.dropout)
        self.pooler = BertPooler(bert.config)
        self.eta1 = nn.Parameter(torch.tensor(self.config.eta, dtype=torch.float))
        self.eta2 = nn.Parameter(torch.tensor(self.config.eta, dtype=torch.float))
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        res = {'logits': None}
        if self.config.lsa:
            feat = self.bert(inputs['text_indices'])['last_hidden_state']
            left_feat = self.bert(inputs['left_text_indices'])['last_hidden_state']
            right_feat = self.bert(inputs['right_text_indices'])['last_hidden_state']
            if 'lr' == self.config.window or 'rl' == self.config.window:
                if self.eta1 <= 0 and self.config.eta != -1:
                    torch.nn.init.uniform_(self.eta1)
                    fprint('reset eta1 to: {}'.format(self.eta1.item()))
                if self.eta2 <= 0 and self.config.eta != -1:
                    torch.nn.init.uniform_(self.eta2)
                    fprint('reset eta2 to: {}'.format(self.eta2.item()))
                if self.config.eta >= 0:
                    cat_features = torch.cat((feat, self.eta1 * left_feat, self.eta2 * right_feat), -1)
                else:
                    cat_features = torch.cat((feat, left_feat, right_feat), -1)
                sent_out = self.linear_window_3h(cat_features)
            elif 'l' == self.config.window:
                sent_out = self.linear_window_2h(torch.cat((feat, self.eta1 * left_feat), -1))
            elif 'r' == self.config.window:
                sent_out = self.linear_window_2h(torch.cat((feat, self.eta2 * right_feat), -1))
            else:
                raise KeyError('Invalid parameter:', self.config.window)
            cat_feat = self.linear(sent_out)
            cat_feat = self.dropout(cat_feat)
            cat_feat = self.encoder(cat_feat)
            cat_feat = self.pooler(cat_feat)
            res['logits'] = self.dense(cat_feat)
        else:
            cat_feat = self.bert(inputs['text_indices'])['last_hidden_state']
            cat_feat = self.linear(cat_feat)
            cat_feat = self.dropout(cat_feat)
            cat_feat = self.encoder(cat_feat)
            cat_feat = self.pooler(cat_feat)
            res['logits'] = self.dense(cat_feat)
        return res


class PointwiseFeedForward(nn.Module):
    """ A two-feed-forward-layer module """

    def __init__(self, d_hid, d_inner_hid=None, d_out=None, dropout=0):
        super(PointwiseFeedForward, self).__init__()
        if d_inner_hid is None:
            d_inner_hid = d_hid
        if d_out is None:
            d_out = d_inner_hid
        self.w_1 = nn.Conv1d(d_hid, d_inner_hid, 1)
        self.w_2 = nn.Conv1d(d_inner_hid, d_out, 1)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()

    def forward(self, x):
        output = self.relu(self.w_1(x.transpose(1, 2)))
        output = self.w_2(output).transpose(2, 1)
        output = self.dropout(output)
        return output


def weight_distrubute_local(bert_local_out, depend_weight, depended_weight, depend_vec, depended_vec, config):
    bert_local_out2 = torch.zeros_like(bert_local_out)
    depend_vec2 = torch.mul(depend_vec, depend_weight.unsqueeze(2))
    depended_vec2 = torch.mul(depended_vec, depended_weight.unsqueeze(2))
    bert_local_out2 = bert_local_out2 + torch.mul(bert_local_out, depend_vec2) + torch.mul(bert_local_out, depended_vec2)
    for j in range(depend_weight.size()[0]):
        bert_local_out2[j][0] = bert_local_out[j][0]
    return bert_local_out2


class DLCF_DCA_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'dlcf_vec', 'depend_vec', 'depended_vec']

    def __init__(self, bert, config):
        super(DLCF_DCA_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.hidden = config.embed_dim
        self.config = config
        self.config.bert_dim = config.embed_dim
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA_ = Encoder(bert.config, config)
        self.mean_pooling_double = PointwiseFeedForward(self.hidden * 2, self.hidden, self.hidden)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(self.hidden, config.output_dim)
        self.dca_sa = nn.ModuleList()
        self.dca_pool = nn.ModuleList()
        self.dca_lin = nn.ModuleList()
        for i in range(config.dca_layer):
            self.dca_sa.append(Encoder(bert.config, config))
            self.dca_pool.append(BertPooler(bert.config))
            self.dca_lin.append(nn.Sequential(nn.Linear(config.bert_dim, config.bert_dim * 2), nn.GELU(), nn.Linear(config.bert_dim * 2, 1), nn.Sigmoid()))

    def weight_calculate(self, sa, pool, lin, d_w, ded_w, depend_out, depended_out):
        depend_sa_out = sa(depend_out)
        depend_sa_out = self.dropout(depend_sa_out)
        depended_sa_out = sa(depended_out)
        depended_sa_out = self.dropout(depended_sa_out)
        depend_pool_out = pool(depend_sa_out)
        depend_pool_out = self.dropout(depend_pool_out)
        depended_pool_out = pool(depended_sa_out)
        depended_pool_out = self.dropout(depended_pool_out)
        depend_weight = lin(depend_pool_out)
        depend_weight = self.dropout(depend_weight)
        depended_weight = lin(depended_pool_out)
        depended_weight = self.dropout(depended_weight)
        for i in range(depend_weight.size()[0]):
            depend_weight[i] = depend_weight[i].item() * d_w[i].item()
            depended_weight[i] = depended_weight[i].item() * ded_w[i].item()
            weight_sum = depend_weight[i].item() + depended_weight[i].item()
            if weight_sum != 0:
                depend_weight[i] = (2 * depend_weight[i] / weight_sum) ** self.config.dca_p
                if depend_weight[i] > 2:
                    depend_weight[i] = 2
                depended_weight[i] = (2 * depended_weight[i] / weight_sum) ** self.config.dca_p
                if depended_weight[i] > 2:
                    depended_weight[i] = 2
            else:
                depend_weight[i] = 1
                depended_weight[i] = 1
        return depend_weight, depended_weight

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['dlcf_vec'].unsqueeze(2)
        depend_vec = inputs['depend_vec'].unsqueeze(2)
        depended_vec = inputs['depended_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        local_context_features = self.bert4local(text_local_indices)['last_hidden_state']
        bert_local_out = torch.mul(local_context_features, lcf_matrix)
        depend_weight = torch.ones(bert_local_out.size()[0])
        depended_weight = torch.ones(bert_local_out.size()[0])
        for i in range(self.config.dca_layer):
            depend_out = torch.mul(bert_local_out, depend_vec)
            depended_out = torch.mul(bert_local_out, depended_vec)
            depend_weight, depended_weight = self.weight_calculate(self.dca_sa[i], self.dca_pool[i], self.dca_lin[i], depend_weight, depended_weight, depend_out, depended_out)
            bert_local_out = weight_distrubute_local(bert_local_out, depend_weight, depended_weight, depend_vec, depended_vec, self.config)
        out_cat = torch.cat((bert_local_out, global_context_features), dim=-1)
        out_cat = self.mean_pooling_double(out_cat)
        out_cat = self.bert_SA_(out_cat)
        out_cat = self.bert_pooler(out_cat)
        dense_out = self.dense(out_cat)
        return {'logits': dense_out, 'hidden_state': out_cat}


class DLCFS_DCA_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'dlcfs_vec', 'depend_vec', 'depended_vec']

    def __init__(self, bert, config):
        super(DLCFS_DCA_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.hidden = config.embed_dim
        self.config = config
        self.config.bert_dim = config.embed_dim
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA_ = Encoder(bert.config, config)
        self.mean_pooling_double = PointwiseFeedForward(self.hidden * 2, self.hidden, self.hidden)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(self.hidden, config.output_dim)
        self.dca_sa = nn.ModuleList()
        self.dca_pool = nn.ModuleList()
        self.dca_lin = nn.ModuleList()
        for i in range(config.dca_layer):
            self.dca_sa.append(Encoder(bert.config, config))
            self.dca_pool.append(BertPooler(bert.config))
            self.dca_lin.append(nn.Sequential(nn.Linear(config.bert_dim, config.bert_dim * 2), nn.GELU(), nn.Linear(config.bert_dim * 2, 1), nn.Sigmoid()))

    def weight_calculate(self, sa, pool, lin, d_w, ded_w, depend_out, depended_out):
        depend_sa_out = sa(depend_out)
        depend_sa_out = self.dropout(depend_sa_out)
        depended_sa_out = sa(depended_out)
        depended_sa_out = self.dropout(depended_sa_out)
        depend_pool_out = pool(depend_sa_out)
        depend_pool_out = self.dropout(depend_pool_out)
        depended_pool_out = pool(depended_sa_out)
        depended_pool_out = self.dropout(depended_pool_out)
        depend_weight = lin(depend_pool_out)
        depend_weight = self.dropout(depend_weight)
        depended_weight = lin(depended_pool_out)
        depended_weight = self.dropout(depended_weight)
        for i in range(depend_weight.size()[0]):
            depend_weight[i] = depend_weight[i].item() * d_w[i].item()
            depended_weight[i] = depended_weight[i].item() * ded_w[i].item()
            weight_sum = depend_weight[i].item() + depended_weight[i].item()
            if weight_sum != 0:
                depend_weight[i] = (2 * depend_weight[i] / weight_sum) ** self.config.dca_p
                if depend_weight[i] > 2:
                    depend_weight[i] = 2
                depended_weight[i] = (2 * depended_weight[i] / weight_sum) ** self.config.dca_p
                if depended_weight[i] > 2:
                    depended_weight[i] = 2
            else:
                depend_weight[i] = 1
                depended_weight[i] = 1
        return depend_weight, depended_weight

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['dlcfs_vec'].unsqueeze(2)
        depend_vec = inputs['depend_vec'].unsqueeze(2)
        depended_vec = inputs['depended_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        local_context_features = self.bert4local(text_local_indices)['last_hidden_state']
        bert_local_out = torch.mul(local_context_features, lcf_matrix)
        depend_weight = torch.ones(bert_local_out.size()[0])
        depended_weight = torch.ones(bert_local_out.size()[0])
        for i in range(self.config.dca_layer):
            depend_out = torch.mul(bert_local_out, depend_vec)
            depended_out = torch.mul(bert_local_out, depended_vec)
            depend_weight, depended_weight = self.weight_calculate(self.dca_sa[i], self.dca_pool[i], self.dca_lin[i], depend_weight, depended_weight, depend_out, depended_out)
            bert_local_out = weight_distrubute_local(bert_local_out, depend_weight, depended_weight, depend_vec, depended_vec, self.config)
        out_cat = torch.cat((bert_local_out, global_context_features), dim=-1)
        out_cat = self.mean_pooling_double(out_cat)
        out_cat = self.bert_SA_(out_cat)
        out_cat = self.bert_pooler(out_cat)
        dense_out = self.dense(out_cat)
        return {'logits': dense_out, 'hidden_state': out_cat}


class FAST_LCF_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcf_vec']

    def __init__(self, bert, config):
        super(FAST_LCF_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear2 = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        cat_features = torch.cat((lcf_features, global_context_features), dim=-1)
        cat_features = self.linear2(cat_features)
        cat_features = self.dropout(cat_features)
        cat_features = self.bert_SA_(cat_features)
        pooled_out = self.bert_pooler(cat_features)
        dense_out = self.dense(pooled_out)
        return {'logits': dense_out, 'hidden_state': pooled_out}


class FAST_LCF_BERT_ATT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcf_vec']

    def __init__(self, bert, config):
        super(FAST_LCF_BERT_ATT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear3 = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)
        fprint('{} is a test model!'.format(self.__class__.__name__))

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        alpha_mat = torch.matmul(lcf_features, global_context_features.transpose(1, 2))
        alpha = F.softmax(alpha_mat.sum(1, keepdim=True), dim=2)
        lcf_att_features = torch.matmul(alpha, global_context_features).squeeze(1)
        global_features = self.bert_pooler(global_context_features)
        lcf_features = self.bert_pooler(lcf_features)
        out = self.linear3(torch.cat((global_features, lcf_att_features, lcf_features), dim=-1))
        dense_out = self.dense(out)
        return {'logits': dense_out, 'hidden_state': out}


class FAST_LCFS_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcfs_vec']

    def __init__(self, bert, config):
        super(FAST_LCFS_BERT, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear2 = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcfs_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        cat_features = torch.cat((lcf_features, global_context_features), dim=-1)
        cat_features = self.linear2(cat_features)
        cat_features = self.dropout(cat_features)
        cat_features = self.bert_SA_(cat_features)
        pooled_out = self.bert_pooler(cat_features)
        dense_out = self.dense(pooled_out)
        return {'logits': dense_out, 'hidden_state': pooled_out}


class FAST_LSA_S(nn.Module):
    inputs = ['text_indices', 'spc_mask_vec', 'lcfs_vec', 'left_lcfs_vec', 'right_lcfs_vec']

    def __init__(self, bert, config):
        super(FAST_LSA_S, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        spc_mask_vec = inputs['spc_mask_vec'].unsqueeze(2)
        lcf_matrix = inputs['lcfs_vec'].unsqueeze(2)
        left_lcf_matrix = inputs['left_lcfs_vec'].unsqueeze(2)
        right_lcf_matrix = inputs['right_lcfs_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        masked_global_context_features = torch.mul(spc_mask_vec, global_context_features)
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(masked_global_context_features, left_lcf_matrix)
        left_lcf_features = self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(masked_global_context_features, right_lcf_matrix)
        right_lcf_features = self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            cat_features = torch.cat((lcf_features, left_lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, self.eta1 * left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, self.eta2 * right_lcf_features), -1))
        else:
            raise KeyError('Invalid parameter:', self.config.window)
        sent_out = torch.cat((global_context_features, sent_out), -1)
        sent_out = self.post_linear(sent_out)
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        dense_out = self.dense(sent_out)
        return {'logits': dense_out, 'hidden_state': sent_out}


class FAST_LSA_S_V2(nn.Module):
    inputs = ['text_indices', 'spc_mask_vec', 'lcfs_cdw_vec', 'left_lcfs_cdw_vec', 'right_lcfs_cdw_vec', 'lcfs_cdm_vec', 'left_lcfs_cdm_vec', 'right_lcfs_cdm_vec']

    def __init__(self, bert, config):
        super(FAST_LSA_S_V2, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        if self.config.lcf == 'cdw':
            self.CDW_LSA = LSA(self.bert4global, self.config)
        if self.config.lcf == 'cdm':
            self.CDM_LSA = LSA(self.bert4global, self.config)
        elif self.config.lcf == 'fusion':
            self.CDW_LSA = LSA(self.bert4global, self.config)
            self.CDM_LSA = LSA(self.bert4global, self.config)
            self.fusion_linear = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        spc_mask_vec = inputs['spc_mask_vec'].unsqueeze(2)
        lcfs_cdw_matrix = inputs['lcfs_cdw_vec'].unsqueeze(2)
        left_lcfs_cdw_matrix = inputs['left_lcfs_cdw_vec'].unsqueeze(2)
        right_lcfs_cdw_matrix = inputs['right_lcfs_cdw_vec'].unsqueeze(2)
        lcfs_cdm_matrix = inputs['lcfs_cdm_vec'].unsqueeze(2)
        left_lcfs_cdm_matrix = inputs['left_lcfs_cdm_vec'].unsqueeze(2)
        right_lcfs_cdm_matrix = inputs['right_lcfs_cdm_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        if self.config.lcf == 'cdw':
            sent_out = self.CDW_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcfs_cdw_matrix, left_lcf_matrix=left_lcfs_cdw_matrix, right_lcf_matrix=right_lcfs_cdw_matrix)
            sent_out = torch.cat((global_context_features, sent_out), -1)
            sent_out = self.post_linear(sent_out)
        elif self.config.lcf == 'cdm':
            sent_out = self.CDM_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcfs_cdm_matrix, left_lcf_matrix=left_lcfs_cdm_matrix, right_lcf_matrix=right_lcfs_cdm_matrix)
            sent_out = torch.cat((global_context_features, sent_out), -1)
            sent_out = self.post_linear(sent_out)
        elif self.config.lcf == 'fusion':
            cdw_sent_out = self.CDW_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcfs_cdw_matrix, left_lcf_matrix=left_lcfs_cdw_matrix, right_lcf_matrix=right_lcfs_cdw_matrix)
            cdm_sent_out = self.CDM_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcfs_matrix=lcfs_cdm_matrix, left_lcf_matrix=left_lcfs_cdm_matrix, right_lcf_matrix=right_lcfs_cdm_matrix)
            sent_out = self.fusion_linear(torch.cat((global_context_features, cdw_sent_out, cdm_sent_out), -1))
        else:
            fprint('Invalid LCF mode: {}'.format(self.config.lcf))
            sent_out = global_context_features
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        dense_out = self.dense(sent_out)
        return {'logits': dense_out, 'hidden_state': sent_out}


class FAST_LSA_T(nn.Module):
    inputs = ['text_indices', 'spc_mask_vec', 'lcf_vec', 'left_lcf_vec', 'right_lcf_vec']

    def __init__(self, bert, config):
        super(FAST_LSA_T, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        spc_mask_vec = inputs['spc_mask_vec'].unsqueeze(2)
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        left_lcf_matrix = inputs['left_lcf_vec'].unsqueeze(2)
        right_lcf_matrix = inputs['right_lcf_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        masked_global_context_features = torch.mul(spc_mask_vec, global_context_features)
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(masked_global_context_features, left_lcf_matrix)
        left_lcf_features = self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(masked_global_context_features, right_lcf_matrix)
        right_lcf_features = self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            cat_features = torch.cat((lcf_features, left_lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, self.eta1 * left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, self.eta2 * right_lcf_features), -1))
        else:
            raise KeyError('Invalid parameter:', self.config.window)
        sent_out = torch.cat((global_context_features, sent_out), -1)
        sent_out = self.post_linear(sent_out)
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        dense_out = self.dense(sent_out)
        return {'logits': dense_out, 'hidden_state': sent_out}


class FAST_LSA_T_V2(nn.Module):
    inputs = ['text_indices', 'spc_mask_vec', 'lcf_cdw_vec', 'left_lcf_cdw_vec', 'right_lcf_cdw_vec', 'lcf_cdm_vec', 'left_lcf_cdm_vec', 'right_lcf_cdm_vec']

    def __init__(self, bert, config):
        super(FAST_LSA_T_V2, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        if self.config.lcf == 'cdw':
            self.CDW_LSA = LSA(self.bert4global, self.config)
        if self.config.lcf == 'cdm':
            self.CDM_LSA = LSA(self.bert4global, self.config)
        elif self.config.lcf == 'fusion':
            self.CDW_LSA = LSA(self.bert4global, self.config)
            self.CDM_LSA = LSA(self.bert4global, self.config)
            self.fusion_linear = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        spc_mask_vec = inputs['spc_mask_vec'].unsqueeze(2)
        lcf_cdw_matrix = inputs['lcf_cdw_vec'].unsqueeze(2)
        left_lcf_cdw_matrix = inputs['left_lcf_cdw_vec'].unsqueeze(2)
        right_lcf_cdw_matrix = inputs['right_lcf_cdw_vec'].unsqueeze(2)
        lcf_cdm_matrix = inputs['lcf_cdm_vec'].unsqueeze(2)
        left_lcf_cdm_matrix = inputs['left_lcf_cdm_vec'].unsqueeze(2)
        right_lcf_cdm_matrix = inputs['right_lcf_cdm_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        if self.config.lcf == 'cdw':
            sent_out = self.CDW_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcf_cdw_matrix, left_lcf_matrix=left_lcf_cdw_matrix, right_lcf_matrix=right_lcf_cdw_matrix)
            sent_out = torch.cat((global_context_features, sent_out), -1)
            sent_out = self.post_linear(sent_out)
        elif self.config.lcf == 'cdm':
            sent_out = self.CDM_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcf_cdm_matrix, left_lcf_matrix=left_lcf_cdm_matrix, right_lcf_matrix=right_lcf_cdm_matrix)
            sent_out = torch.cat((global_context_features, sent_out), -1)
            sent_out = self.post_linear(sent_out)
        elif self.config.lcf == 'fusion':
            cdw_sent_out = self.CDW_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcf_cdw_matrix, left_lcf_matrix=left_lcf_cdw_matrix, right_lcf_matrix=right_lcf_cdw_matrix)
            cdm_sent_out = self.CDM_LSA(global_context_features, spc_mask_vec=spc_mask_vec, lcf_matrix=lcf_cdm_matrix, left_lcf_matrix=left_lcf_cdm_matrix, right_lcf_matrix=right_lcf_cdm_matrix)
            sent_out = self.fusion_linear(torch.cat((global_context_features, cdw_sent_out, cdm_sent_out), -1))
        else:
            fprint('Invalid LCF mode: {}'.format(self.config.lcf))
            sent_out = global_context_features
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        dense_out = self.dense(sent_out)
        return {'logits': dense_out, 'hidden_state': sent_out}


class LCA_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcf_cdm_vec', 'polarity']

    def __init__(self, bert, config):
        super(LCA_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = copy.deepcopy(bert)
        self.lc_embed = nn.Embedding(2, config.embed_dim)
        self.lc_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA_L = Encoder(bert.config, config)
        self.bert_SA_G = Encoder(bert.config, config)
        self.cat_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.pool = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)
        self.classifier = nn.Linear(config.embed_dim, 2)
        self.lca_criterion = nn.CrossEntropyLoss()
        self.classification_criterion = nn.CrossEntropyLoss()

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_global_indices = inputs['text_indices']
        else:
            text_global_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lca_ids = inputs['lcf_cdm_vec'].long()
        lcf_matrix = lca_ids.unsqueeze(2)
        polarity = inputs['polarity'] if 'polarity' in inputs else None
        bert_global_out = self.bert4global(text_global_indices)['last_hidden_state']
        bert_local_out = self.bert4local(text_local_indices)['last_hidden_state']
        lc_embedding = self.lc_embed(lca_ids)
        bert_global_out = self.lc_linear(torch.cat((bert_global_out, lc_embedding), -1))
        bert_local_out = torch.mul(bert_local_out, lcf_matrix)
        bert_local_out = self.bert_SA_L(bert_local_out)
        cat_features = torch.cat((bert_local_out, bert_global_out), dim=-1)
        cat_features = self.cat_linear(cat_features)
        lca_logits = self.classifier(cat_features)
        lca_logits = lca_logits.view(-1, 2)
        lca_ids = lca_ids.view(-1)
        cat_features = self.dropout(cat_features)
        pooled_out = self.pool(cat_features)
        sent_logits = self.dense(pooled_out)
        if polarity is not None:
            lcp_loss = self.lca_criterion(lca_logits, lca_ids)
            sent_loss = self.classification_criterion(sent_logits, polarity)
            return {'logits': sent_logits, 'hidden_state': pooled_out, 'loss': (1 - self.config.sigma) * sent_loss + self.config.sigma * lcp_loss}
        else:
            return {'logits': sent_logits, 'hidden_state': pooled_out}


class LCF_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcf_vec']

    def __init__(self, bert, config):
        super(LCF_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear2 = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        local_context_features = self.bert4local(text_local_indices)['last_hidden_state']
        lcf_features = torch.mul(local_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        cat_features = torch.cat((lcf_features, global_context_features), dim=-1)
        cat_features = self.linear2(cat_features)
        cat_features = self.dropout(cat_features)
        cat_features = self.bert_SA_(cat_features)
        pooled_out = self.bert_pooler(cat_features)
        dense_out = self.dense(pooled_out)
        return {'logits': dense_out, 'hidden_state': pooled_out}


class LCF_DUAL_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcf_vec']

    def __init__(self, bert, config):
        super(LCF_DUAL_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = copy.deepcopy(bert)
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear2 = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        local_context_features = self.bert4local(text_local_indices)['last_hidden_state']
        lcf_features = torch.mul(local_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        cat_features = torch.cat((lcf_features, global_context_features), dim=-1)
        cat_features = self.linear2(cat_features)
        cat_features = self.dropout(cat_features)
        cat_features = self.bert_SA_(cat_features)
        pooled_out = self.bert_pooler(cat_features)
        dense_out = self.dense(pooled_out)
        return {'logits': dense_out, 'hidden_state': pooled_out}


class LCF_TEMPLATE_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcf_vec']

    def __init__(self, bert, config):
        super(LCF_TEMPLATE_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.config = config
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, inputs):
        raise NotImplementedError('This is a template ATEPC model based on LCF, please implement your model use this template.')


class LCFS_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcfs_vec']

    def __init__(self, bert, config):
        super(LCFS_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = self.bert4global
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear2 = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcfs_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        local_context_features = self.bert4local(text_local_indices)['last_hidden_state']
        lcf_features = torch.mul(local_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        cat_features = torch.cat((lcf_features, global_context_features), dim=-1)
        cat_features = self.linear2(cat_features)
        cat_features = self.dropout(cat_features)
        cat_features = self.bert_SA_(cat_features)
        pooled_out = self.bert_pooler(cat_features)
        dense_out = self.dense(pooled_out)
        return {'logits': dense_out, 'hidden_state': pooled_out}


class LCFS_DUAL_BERT(nn.Module):
    inputs = ['text_indices', 'text_raw_bert_indices', 'lcfs_vec']

    def __init__(self, bert, config):
        super(LCFS_DUAL_BERT, self).__init__()
        self.bert4global = bert
        self.bert4local = copy.deepcopy(bert)
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.bert_SA = Encoder(bert.config, config)
        self.linear2 = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.bert_SA_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        if self.config.use_bert_spc:
            text_indices = inputs['text_indices']
        else:
            text_indices = inputs['text_raw_bert_indices']
        text_local_indices = inputs['text_raw_bert_indices']
        lcf_matrix = inputs['lcfs_vec'].unsqueeze(2)
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        local_context_features = self.bert4local(text_local_indices)['last_hidden_state']
        lcf_features = torch.mul(local_context_features, lcf_matrix)
        lcf_features = self.bert_SA(lcf_features)
        cat_features = torch.cat((lcf_features, global_context_features), dim=-1)
        cat_features = self.linear2(cat_features)
        cat_features = self.dropout(cat_features)
        cat_features = self.bert_SA_(cat_features)
        pooled_out = self.bert_pooler(cat_features)
        dense_out = self.dense(pooled_out)
        return {'logits': dense_out, 'hidden_state': pooled_out}


class LSA_S(nn.Module):
    inputs = ['text_indices', 'left_text_indices', 'right_text_indices', 'spc_mask_vec', 'lcfs_vec', 'left_lcfs_vec', 'right_lcfs_vec']

    def __init__(self, bert, config):
        super(LSA_S, self).__init__()
        self.bert4central = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        left_text_indices = inputs['left_text_indices']
        right_text_indices = inputs['right_text_indices']
        spc_mask_vec = inputs['spc_mask_vec'].unsqueeze(2)
        lcf_matrix = inputs['lcfs_vec'].unsqueeze(2)
        left_lcf_matrix = inputs['left_lcfs_vec'].unsqueeze(2)
        right_lcf_matrix = inputs['right_lcfs_vec'].unsqueeze(2)
        global_context_features = self.bert4central(text_indices)['last_hidden_state']
        left_global_context_features = self.bert4central(left_text_indices)['last_hidden_state']
        right_global_context_features = self.bert4central(right_text_indices)['last_hidden_state']
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(left_global_context_features, left_lcf_matrix)
        left_lcf_features = self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(right_global_context_features, right_lcf_matrix)
        right_lcf_features = self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            if self.config.eta >= 0:
                cat_features = torch.cat((lcf_features, self.config.eta * left_lcf_features, (1 - self.config.eta) * right_lcf_features), -1)
            else:
                cat_features = torch.cat((lcf_features, left_lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, right_lcf_features), -1))
        else:
            raise KeyError('Invalid parameter:', self.config.window)
        sent_out = torch.cat((global_context_features, sent_out), -1)
        sent_out = self.post_linear(sent_out)
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        dense_out = self.dense(sent_out)
        return {'logits': dense_out, 'hidden_state': sent_out}


class LSA_T(nn.Module):
    inputs = ['text_indices', 'left_text_indices', 'right_text_indices', 'spc_mask_vec', 'lcf_vec', 'left_lcf_vec', 'right_lcf_vec']

    def __init__(self, bert, config):
        super(LSA_T, self).__init__()
        self.bert4central = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        left_text_indices = inputs['left_text_indices']
        right_text_indices = inputs['right_text_indices']
        spc_mask_vec = inputs['spc_mask_vec'].unsqueeze(2)
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        left_lcf_matrix = inputs['left_lcf_vec'].unsqueeze(2)
        right_lcf_matrix = inputs['right_lcf_vec'].unsqueeze(2)
        global_context_features = self.bert4central(text_indices)['last_hidden_state']
        left_global_context_features = self.bert4central(left_text_indices)['last_hidden_state']
        right_global_context_features = self.bert4central(right_text_indices)['last_hidden_state']
        lcf_features = torch.mul(global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(left_global_context_features, left_lcf_matrix)
        left_lcf_features = self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(right_global_context_features, right_lcf_matrix)
        right_lcf_features = self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            if self.config.eta >= 0:
                cat_features = torch.cat((lcf_features, self.config.eta * left_lcf_features, (1 - self.config.eta) * right_lcf_features), -1)
            else:
                cat_features = torch.cat((lcf_features, left_lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, right_lcf_features), -1))
        else:
            raise KeyError('Invalid parameter:', self.config.window)
        sent_out = torch.cat((global_context_features, sent_out), -1)
        sent_out = self.post_linear(sent_out)
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        dense_out = self.dense(sent_out)
        return {'logits': dense_out, 'hidden_state': sent_out}


class SSW_S(nn.Module):
    inputs = ['text_indices', 'spc_mask_vec', 'lcfs_vec', 'left_lcfs_vec', 'right_lcfs_vec', 'polarity', 'left_dist', 'right_dist']

    def __init__(self, bert, config):
        super(SSW_S, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.dist_embed = nn.Embedding(config.max_seq_len, config.embed_dim)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.linear_left_ = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_right_ = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.classification_criterion = nn.CrossEntropyLoss()
        self.sent_dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        spc_mask_vec = inputs['spc_mask_vec']
        lcf_matrix = inputs['lcfs_vec'].unsqueeze(2)
        left_lcf_matrix = inputs['left_lcfs_vec'].unsqueeze(2)
        right_lcf_matrix = inputs['right_lcfs_vec'].unsqueeze(2)
        polarity = inputs['polarity'] if 'polarity' in inputs else None
        left_dist = self.dist_embed(inputs['left_dist'].unsqueeze(1))
        right_dist = self.dist_embed(inputs['right_dist'].unsqueeze(1))
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        masked_global_context_features = torch.mul(spc_mask_vec, global_context_features)
        lcf_features = torch.mul(masked_global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(masked_global_context_features, left_lcf_matrix)
        left_lcf_features = left_dist * self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(masked_global_context_features, right_lcf_matrix)
        right_lcf_features = right_dist * self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            if self.config.eta >= 0:
                cat_features = torch.cat((lcf_features, self.config.eta * left_lcf_features, (1 - self.config.eta) * right_lcf_features), -1)
            else:
                cat_features = torch.cat((left_lcf_features, lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, right_lcf_features), -1))
        else:
            sent_out = lcf_features
        sent_out = torch.cat((global_context_features, sent_out), -1)
        sent_out = self.post_linear(sent_out)
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        sent_logits = self.sent_dense(sent_out)
        if polarity is not None:
            sent_loss = self.classification_criterion(sent_logits, polarity)
            return {'logits': sent_logits, 'hidden_state': sent_out, 'loss': sent_loss}
        else:
            return {'logits': sent_logits, 'hidden_state': sent_out}


class SSW_T(nn.Module):
    inputs = ['text_indices', 'spc_mask_vec', 'lcf_vec', 'left_lcf_vec', 'right_lcf_vec', 'polarity', 'left_dist', 'right_dist']

    def __init__(self, bert, config):
        super(SSW_T, self).__init__()
        self.bert4global = bert
        self.config = config
        self.dropout = nn.Dropout(config.dropout)
        self.encoder = Encoder(bert.config, config)
        self.encoder_left = Encoder(bert.config, config)
        self.encoder_right = Encoder(bert.config, config)
        self.post_linear = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_window_3h = nn.Linear(config.embed_dim * 3, config.embed_dim)
        self.linear_window_2h = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.dist_embed = nn.Embedding(config.max_seq_len, config.embed_dim)
        self.post_encoder = Encoder(bert.config, config)
        self.post_encoder_ = Encoder(bert.config, config)
        self.bert_pooler = BertPooler(bert.config)
        self.linear_left_ = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.linear_right_ = nn.Linear(config.embed_dim * 2, config.embed_dim)
        self.classification_criterion = nn.CrossEntropyLoss()
        self.sent_dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        spc_mask_vec = inputs['spc_mask_vec']
        lcf_matrix = inputs['lcf_vec'].unsqueeze(2)
        left_lcf_matrix = inputs['left_lcf_vec'].unsqueeze(2)
        right_lcf_matrix = inputs['right_lcf_vec'].unsqueeze(2)
        polarity = inputs['polarity'] if 'polarity' in inputs else None
        left_dist = self.dist_embed(inputs['left_dist'].unsqueeze(1))
        right_dist = self.dist_embed(inputs['right_dist'].unsqueeze(1))
        global_context_features = self.bert4global(text_indices)['last_hidden_state']
        masked_global_context_features = torch.mul(spc_mask_vec, global_context_features)
        lcf_features = torch.mul(masked_global_context_features, lcf_matrix)
        lcf_features = self.encoder(lcf_features)
        left_lcf_features = torch.mul(masked_global_context_features, left_lcf_matrix)
        left_lcf_features = left_dist * self.encoder_left(left_lcf_features)
        right_lcf_features = torch.mul(masked_global_context_features, right_lcf_matrix)
        right_lcf_features = right_dist * self.encoder_right(right_lcf_features)
        if 'lr' == self.config.window or 'rl' == self.config.window:
            if self.config.eta >= 0:
                cat_features = torch.cat((lcf_features, self.config.eta * left_lcf_features, (1 - self.config.eta) * right_lcf_features), -1)
            else:
                cat_features = torch.cat((left_lcf_features, lcf_features, right_lcf_features), -1)
            sent_out = self.linear_window_3h(cat_features)
        elif 'l' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, left_lcf_features), -1))
        elif 'r' == self.config.window:
            sent_out = self.linear_window_2h(torch.cat((lcf_features, right_lcf_features), -1))
        else:
            sent_out = lcf_features
        sent_out = torch.cat((global_context_features, sent_out), -1)
        sent_out = self.post_linear(sent_out)
        sent_out = self.dropout(sent_out)
        sent_out = self.post_encoder_(sent_out)
        sent_out = self.bert_pooler(sent_out)
        sent_logits = self.sent_dense(sent_out)
        if polarity is not None:
            sent_loss = self.classification_criterion(sent_logits, polarity)
            return {'logits': sent_logits, 'hidden_state': sent_out, 'loss': sent_loss}
        else:
            return {'logits': sent_logits, 'hidden_state': sent_out}


class AOA_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_text_indices', 'left_aspect_indices', 'right_text_indices', 'right_aspect_indices']

    def __init__(self, bert, config):
        super(AOA_BERT, self).__init__()
        self.config = config
        self.embed = bert
        self.ctx_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.asp_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.dense = nn.Linear(2 * config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_indices = inputs['text_indices']
        aspect_indices = inputs['aspect_indices']
        ctx_len = torch.sum(text_indices != 0, dim=1)
        asp_len = torch.sum(aspect_indices != 0, dim=1)
        ctx = self.embed(text_indices)['last_hidden_state']
        asp = self.embed(aspect_indices)['last_hidden_state']
        ctx_out, (_, _) = self.ctx_lstm(ctx, ctx_len)
        asp_out, (_, _) = self.asp_lstm(asp, asp_len)
        interaction_mat = torch.matmul(ctx_out, torch.transpose(asp_out, 1, 2))
        alpha = F.softmax(interaction_mat, dim=1)
        beta = F.softmax(interaction_mat, dim=2)
        beta_avg = beta.mean(dim=1, keepdim=True)
        gamma = torch.matmul(alpha, beta_avg.transpose(1, 2))
        weighted_sum = torch.matmul(torch.transpose(ctx_out, 1, 2), gamma).squeeze(-1)
        out = self.dense(weighted_sum)
        return {'logits': out}


class ASGCN_BERT_Unit(nn.Module):

    def __init__(self, bert, config):
        super(ASGCN_BERT_Unit, self).__init__()
        self.config = config
        self.embed = bert
        self.text_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.gc1 = GraphConvolution(2 * config.hidden_dim, 2 * config.hidden_dim)
        self.gc2 = GraphConvolution(2 * config.hidden_dim, 2 * config.hidden_dim)
        self.text_embed_dropout = nn.Dropout()

    def position_weight(self, x, aspect_double_idx, text_len, aspect_len):
        batch_size = x.shape[0]
        seq_len = x.shape[1]
        aspect_double_idx = aspect_double_idx.cpu().numpy()
        text_len = text_len.cpu().numpy()
        aspect_len = aspect_len.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        for i in range(batch_size):
            context_len = text_len[i] - aspect_len[i]
            for j in range(aspect_double_idx[i, 0]):
                weight[i].append(1 - (aspect_double_idx[i, 0] - j) / context_len)
            for j in range(aspect_double_idx[i, 0], aspect_double_idx[i, 1] + 1):
                weight[i].append(0)
            for j in range(aspect_double_idx[i, 1] + 1, text_len[i]):
                weight[i].append(1 - (j - aspect_double_idx[i, 1]) / context_len)
            for j in range(text_len[i], seq_len):
                weight[i].append(0)
        weight = torch.tensor(weight, dtype=torch.float).unsqueeze(2)
        return weight * x

    def mask(self, x, aspect_double_idx):
        batch_size, seq_len = x.shape[0], x.shape[1]
        aspect_double_idx = aspect_double_idx.cpu().numpy()
        mask = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for j in range(aspect_double_idx[i, 0]):
                mask[i].append(0)
            for j in range(aspect_double_idx[i, 0], aspect_double_idx[i, 1] + 1):
                mask[i].append(1)
            for j in range(aspect_double_idx[i, 1] + 1, seq_len):
                mask[i].append(0)
        mask = torch.tensor(mask, dtype=torch.float).unsqueeze(2)
        return mask * x

    def forward(self, inputs):
        text_indices, aspect_indices, left_indices, adj = inputs[0], inputs[1], inputs[2], inputs[3]
        text_len = torch.sum(text_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        left_len = torch.sum(left_indices != 0, dim=-1)
        aspect_double_idx = torch.cat([left_len.unsqueeze(1), (left_len + aspect_len - 1).unsqueeze(1)], dim=1)
        text = self.embed(text_indices)['last_hidden_state']
        text = self.text_embed_dropout(text)
        text_out, (_, _) = self.text_lstm(text, text_len)
        seq_len = text_out.shape[1]
        adj = adj[:, :seq_len, :seq_len]
        x = F.relu(self.gc1(self.position_weight(text_out, aspect_double_idx, text_len, aspect_len), adj))
        x = F.relu(self.gc2(self.position_weight(x, aspect_double_idx, text_len, aspect_len), adj))
        x = self.mask(x, aspect_double_idx)
        alpha_mat = torch.matmul(x, text_out.transpose(1, 2))
        alpha = F.softmax(alpha_mat.sum(1, keepdim=True), dim=2)
        x = torch.matmul(alpha, text_out).squeeze(1)
        return x


class ASGCN_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices', 'dependency_graph', 'left_aspect_indices', 'left_left_indices', 'left_dependency_graph', 'right_aspect_indices', 'right_left_indices', 'right_dependency_graph']

    def __init__(self, bert, config):
        super(ASGCN_BERT, self).__init__()
        self.config = config
        self.asgcn_left = ASGCN_BERT_Unit(bert, config) if self.config.lsa else None
        self.asgcn_central = ASGCN_BERT_Unit(bert, config)
        self.encoder = Encoder(bert.config, config)
        self.dropout = nn.Dropout(config.dropout)
        self.pooler = BertPooler(bert.config)
        self.asgcn_right = ASGCN_BERT_Unit(bert, config) if self.config.lsa else None
        self.linear = nn.Linear(self.config.hidden_dim * 6, self.config.output_dim)
        self.dense = nn.Linear(self.config.hidden_dim * 2, self.config.output_dim)

    def forward(self, inputs):
        res = {'logits': None}
        if self.config.lsa:
            cat_feat = torch.cat((self.asgcn_left([inputs['text_indices'], inputs['left_aspect_indices'], inputs['left_left_indices'], inputs['left_dependency_graph']]), self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['left_indices'], inputs['dependency_graph']]), self.asgcn_right([inputs['text_indices'], inputs['right_aspect_indices'], inputs['right_left_indices'], inputs['right_dependency_graph']])), -1)
            cat_feat = self.dropout(cat_feat)
            res['logits'] = self.linear(cat_feat)
        else:
            cat_feat = self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['left_indices'], inputs['dependency_graph']])
            cat_feat = self.dropout(cat_feat)
            res['logits'] = self.dense(cat_feat)
        return res


class ATAE_LSTM_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices']

    def __init__(self, bert, config):
        super(ATAE_LSTM_BERT, self).__init__()
        self.config = config
        self.embed = bert
        self.squeeze_embedding = SqueezeEmbedding()
        self.lstm = DynamicLSTM(config.embed_dim * 2, config.hidden_dim, num_layers=1, batch_first=True)
        self.attention = NoQueryAttention(config.hidden_dim + config.embed_dim, score_function='bi_linear')
        self.dense = nn.Linear(config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_indices, aspect_indices = inputs['text_indices'], inputs['text_indices']
        x_len = torch.sum(text_indices != 0, dim=-1)
        x_len_max = torch.max(x_len)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1).float()
        x = self.embed(text_indices)['last_hidden_state']
        x = self.squeeze_embedding(x, x_len)
        aspect = self.embed(aspect_indices)['last_hidden_state']
        aspect_pool = torch.div(torch.sum(aspect, dim=1), aspect_len.unsqueeze(1))
        aspect = aspect_pool.unsqueeze(1).expand(-1, x_len_max, -1)
        x = torch.cat((aspect, x), dim=-1)
        h, (_, _) = self.lstm(x, x_len)
        ha = torch.cat((h, aspect), dim=-1)
        _, score = self.attention(ha)
        output = torch.squeeze(torch.bmm(score, h), dim=1)
        out = self.dense(output)
        return {'logits': out}


class Cabasc_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_with_aspect_indices', 'right_with_aspect_indices']

    def __init__(self, bert, config, _type='c'):
        super(Cabasc_BERT, self).__init__()
        self.config = config
        self.type = _type
        self.embed = bert
        self.squeeze_embedding = SqueezeEmbedding(batch_first=True)
        self.linear1 = nn.Linear(3 * config.embed_dim, config.embed_dim)
        self.linear2 = nn.Linear(config.embed_dim, 1, bias=False)
        self.mlp = nn.Linear(config.embed_dim, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)
        self.rnn_l = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, rnn_type='GRU')
        self.rnn_r = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, rnn_type='GRU')
        self.mlp_l = nn.Linear(config.hidden_dim, 1)
        self.mlp_r = nn.Linear(config.hidden_dim, 1)

    def context_attention(self, x_l, x_r, memory, memory_len, aspect_len):
        left_len, right_len = torch.sum(x_l != 0, dim=-1), torch.sum(x_r != 0, dim=-1)
        x_l, x_r = self.embed(x_l)['last_hidden_state'], self.embed(x_r)['last_hidden_state']
        context_l, (_, _) = self.rnn_l(x_l, left_len)
        context_r, (_, _) = self.rnn_r(x_r, right_len)
        attn_l = torch.sigmoid(self.mlp_l(context_l)) + 0.5
        attn_r = torch.sigmoid(self.mlp_r(context_r)) + 0.5
        for i in range(memory.size(0)):
            aspect_start = (left_len[i] - aspect_len[i]).item()
            aspect_end = left_len[i]
            for idx in range(memory_len[i]):
                if idx < aspect_start:
                    memory[i][idx] *= attn_l[i][idx]
                elif idx < aspect_end:
                    memory[i][idx] *= (attn_l[i][idx] + attn_r[i][idx - aspect_start]) / 2
                else:
                    memory[i][idx] *= attn_r[i][idx - aspect_start]
        return memory

    def locationed_memory(self, memory, memory_len):
        """
        # differ from description in paper here, but may be better
        for i in range(memory.size(0)):
            for idx in range(memory_len[i]):
                aspect_start = left_len[i] - aspect_len[i]
                aspect_end = left_len[i] 
                if idx < aspect_start: l = aspect_start.item() - idx                   
                elif idx <= aspect_end: l = 0 
                else: l = idx - aspect_end.item()
                memory[i][idx] *= (1-float(l)/int(memory_len[i]))
        """
        for i in range(memory.size(0)):
            for idx in range(memory_len[i]):
                memory[i][idx] *= 1 - float(idx) / int(memory_len[i])
        return memory

    def forward(self, inputs):
        text_raw_indices, aspect_indices, x_l, x_r = inputs['text_indices'], inputs['aspect_indices'], inputs['left_with_aspect_indices'], inputs['right_with_aspect_indices']
        memory_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        nonzeros_aspect = aspect_len.float()
        aspect = self.embed(aspect_indices)['last_hidden_state']
        aspect = torch.sum(aspect, dim=1)
        v_a = torch.div(aspect, nonzeros_aspect.unsqueeze(1)).unsqueeze(1)
        memory = self.embed(text_raw_indices)['last_hidden_state']
        memory = self.squeeze_embedding(memory, memory_len)
        nonzeros_memory = memory_len.float()
        v_s = torch.sum(memory, dim=1)
        v_s = torch.div(v_s, nonzeros_memory.unsqueeze(1)).unsqueeze(1)
        if self.type == 'c':
            memory = self.locationed_memory(memory, memory_len)
        elif self.type == 'cabasc':
            memory = self.context_attention(x_l, x_r, memory, memory_len, aspect_len)
            v_s = torch.sum(memory, dim=1)
            v_s = torch.div(v_s, nonzeros_memory.unsqueeze(1))
            v_s = v_s.unsqueeze(dim=1)
        """
        # no multi-hop, but may be better. 
        # however, here is something totally different from what paper depits
        for _ in range(self.config.hops):  
            #x = self.x_linear(x)
            v_ts, _ = self.attention(memory, v_a)
        """
        memory_chunks = memory.chunk(memory.size(1), dim=1)
        c = []
        for memory_chunk in memory_chunks:
            c_i = self.linear1(torch.cat([memory_chunk, v_a, v_s], dim=1).view(memory_chunk.size(0), -1))
            c_i = self.linear2(torch.tanh(c_i))
            c.append(c_i)
        alpha = F.softmax(torch.cat(c, dim=1), dim=1)
        v_ts = torch.matmul(memory.transpose(1, 2), alpha.unsqueeze(-1)).transpose(1, 2)
        v_ns = v_ts + v_s
        v_ns = v_ns.view(v_ns.size(0), -1)
        v_ms = torch.tanh(self.mlp(v_ns))
        out = self.dense(v_ms)
        return {'logits': out}


class IAN_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices']

    def __init__(self, bert, config):
        super(IAN_BERT, self).__init__()
        self.config = config
        self.embed = self.embed = bert
        self.lstm_context = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.lstm_aspect = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.attention_aspect = Attention(config.hidden_dim, score_function='bi_linear')
        self.attention_context = Attention(config.hidden_dim, score_function='bi_linear')
        self.dense = nn.Linear(config.hidden_dim * 2, config.output_dim)

    def forward(self, inputs):
        text_raw_indices, aspect_indices = inputs['text_indices'], inputs['aspect_indices']
        text_raw_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        context = self.embed(text_raw_indices)['last_hidden_state']
        aspect = self.embed(aspect_indices)['last_hidden_state']
        context, (_, _) = self.lstm_context(context, text_raw_len)
        aspect, (_, _) = self.lstm_aspect(aspect, aspect_len)
        aspect_len = torch.tensor(aspect_len, dtype=torch.float)
        aspect_pool = torch.sum(aspect, dim=1)
        aspect_pool = torch.div(aspect_pool, aspect_len.view(aspect_len.size(0), 1))
        text_raw_len = text_raw_len.clone().detach()
        context_pool = torch.sum(context, dim=1)
        context_pool = torch.div(context_pool, text_raw_len.view(text_raw_len.size(0), 1))
        aspect_final, _ = self.attention_aspect(aspect, context_pool)
        aspect_final = aspect_final.squeeze(dim=1)
        context_final, _ = self.attention_context(context, aspect_pool)
        context_final = context_final.squeeze(dim=1)
        x = torch.cat((aspect_final, context_final), dim=-1)
        out = self.dense(x)
        return {'logits': out}


class LSTM_BERT(nn.Module):
    inputs = ['text_indices']

    def __init__(self, bert, config):
        super(LSTM_BERT, self).__init__()
        self.embed = bert
        self.lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.dense = nn.Linear(config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_indices = inputs['text_indices']
        x = self.embed(text_raw_indices)['last_hidden_state']
        x_len = torch.sum(text_raw_indices != 0, dim=-1)
        _, (h_n, _) = self.lstm(x, x_len)
        out = self.dense(h_n[0])
        return {'logits': out}


class MemNet_BERT(nn.Module):
    inputs = ['context_indices', 'aspect_indices']

    def locationed_memory(self, memory, memory_len):
        batch_size = memory.shape[0]
        seq_len = memory.shape[1]
        memory_len = memory_len.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for idx in range(memory_len[i]):
                weight[i].append(1 - float(idx + 1) / memory_len[i])
            for idx in range(memory_len[i], seq_len):
                weight[i].append(1)
        weight = torch.tensor(weight)
        memory = weight.unsqueeze(2) * memory
        return memory

    def __init__(self, bert, config):
        super(MemNet_BERT, self).__init__()
        self.config = config
        self.embed = bert
        self.squeeze_embedding = SqueezeEmbedding(batch_first=True)
        self.attention = Attention(config.embed_dim, score_function='mlp')
        self.x_linear = nn.Linear(config.embed_dim, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_without_aspect_indices, aspect_indices = inputs['context_indices'], inputs['aspect_indices']
        memory_len = torch.sum(text_raw_without_aspect_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        nonzeros_aspect = torch.tensor(aspect_len, dtype=torch.float)
        memory = self.embed(text_raw_without_aspect_indices)['last_hidden_state']
        memory = self.squeeze_embedding(memory, memory_len)
        aspect = self.embed(aspect_indices)['last_hidden_state']
        aspect = torch.sum(aspect, dim=1)
        aspect = torch.div(aspect, nonzeros_aspect.view(nonzeros_aspect.size(0), 1))
        x = aspect.unsqueeze(dim=1)
        if 'hops' not in self.config.args:
            self.config.hops = 3
        for _ in range(self.config.hops):
            x = self.x_linear(x)
            out_at, _ = self.attention(memory, x)
            x = out_at + x
        x = x.view(x.size(0), -1)
        out = self.dense(x)
        return {'logits': out}


class MGAN_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices']

    def __init__(self, bert, config):
        super(MGAN_BERT, self).__init__()
        self.config = config
        self.embed = bert
        self.ctx_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.asp_lstm = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.location = LocationEncoding(config)
        self.w_a2c = nn.Parameter(torch.Tensor(2 * config.hidden_dim, 2 * config.hidden_dim))
        self.w_c2a = nn.Parameter(torch.Tensor(2 * config.hidden_dim, 2 * config.hidden_dim))
        self.alignment = AlignmentMatrix(config)
        self.dense = nn.Linear(8 * config.hidden_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_indices = inputs['text_indices']
        aspect_indices = inputs['aspect_indices']
        text_left_indices = inputs['left_indices']
        batch_size = text_raw_indices.size(0)
        ctx_len = torch.sum(text_raw_indices != 0, dim=1)
        asp_len = torch.sum(aspect_indices != 0, dim=1)
        left_len = torch.sum(text_left_indices != 0, dim=-1)
        aspect_in_text = torch.cat([left_len.unsqueeze(-1), (left_len + asp_len - 1).unsqueeze(-1)], dim=-1)
        ctx = self.embed(text_raw_indices)['last_hidden_state']
        asp = self.embed(aspect_indices)['last_hidden_state']
        ctx_out, (_, _) = self.ctx_lstm(ctx, ctx_len)
        ctx_out = self.location(ctx_out, aspect_in_text).float()
        ctx_pool = torch.sum(ctx_out, dim=1)
        ctx_pool = torch.div(ctx_pool, ctx_len.float().unsqueeze(-1)).unsqueeze(-1)
        asp_out, (_, _) = self.asp_lstm(asp, asp_len)
        asp_pool = torch.sum(asp_out, dim=1)
        asp_pool = torch.div(asp_pool, asp_len.float().unsqueeze(-1)).unsqueeze(-1)
        alignment_mat = self.alignment(batch_size, ctx_out, asp_out.float())
        f_asp2ctx = torch.matmul(ctx_out.transpose(1, 2), F.softmax(alignment_mat.max(2, keepdim=True)[0], dim=1)).squeeze(-1)
        f_ctx2asp = torch.matmul(F.softmax(alignment_mat.max(1, keepdim=True)[0], dim=2), asp_out).transpose(1, 2).squeeze(-1)
        c_asp2ctx_alpha = F.softmax(ctx_out.matmul(self.w_a2c.expand(batch_size, -1, -1)).matmul(asp_pool), dim=1)
        c_asp2ctx = torch.matmul(ctx_out.transpose(1, 2), c_asp2ctx_alpha).squeeze(-1)
        c_ctx2asp_alpha = F.softmax(asp_out.matmul(self.w_c2a.expand(batch_size, -1, -1)).matmul(ctx_pool), dim=1)
        c_ctx2asp = torch.matmul(asp_out.transpose(1, 2), c_ctx2asp_alpha).squeeze(-1)
        feat = torch.cat([c_asp2ctx, f_asp2ctx, f_ctx2asp, c_ctx2asp], dim=1)
        out = self.dense(feat)
        return {'logits': out}


class RAM_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'left_indices']

    def locationed_memory(self, memory, memory_len, left_len, aspect_len):
        batch_size = memory.shape[0]
        seq_len = memory.shape[1]
        memory_len = memory_len.cpu().numpy()
        left_len = left_len.cpu().numpy()
        aspect_len = aspect_len.cpu().numpy()
        weight = [[] for i in range(batch_size)]
        u = [[] for i in range(batch_size)]
        for i in range(batch_size):
            for idx in range(left_len[i]):
                weight[i].append(1 - (left_len[i] - idx) / memory_len[i])
                u[i].append(idx - left_len[i])
            for idx in range(left_len[i], left_len[i] + aspect_len[i]):
                weight[i].append(1)
                u[i].append(0)
            for idx in range(left_len[i] + aspect_len[i], memory_len[i]):
                weight[i].append(1 - (idx - left_len[i] - aspect_len[i] + 1) / memory_len[i])
                u[i].append(idx - left_len[i] - aspect_len[i] + 1)
            for idx in range(memory_len[i], seq_len):
                weight[i].append(1)
                u[i].append(0)
        u = torch.tensor(u, dtype=memory.dtype).unsqueeze(2)
        weight = torch.tensor(weight).unsqueeze(2)
        v = memory * weight
        memory = torch.cat([v, u], dim=2)
        return memory

    def __init__(self, bert, config):
        super(RAM_BERT, self).__init__()
        self.config = config
        self.embed = bert
        self.bi_lstm_context = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.att_linear = nn.Linear(config.hidden_dim * 2 + 1 + config.embed_dim * 2, 1)
        self.gru_cell = nn.GRUCell(config.hidden_dim * 2 + 1, config.embed_dim)
        self.dense = nn.Linear(config.embed_dim, config.output_dim)

    def forward(self, inputs):
        text_raw_indices, aspect_indices, text_left_indices = inputs['text_indices'], inputs['aspect_indices'], inputs['left_indices']
        left_len = torch.sum(text_left_indices != 0, dim=-1)
        memory_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        nonzeros_aspect = aspect_len.float()
        memory = self.embed(text_raw_indices)['last_hidden_state']
        memory, (_, _) = self.bi_lstm_context(memory, memory_len)
        memory = self.locationed_memory(memory, memory_len, left_len, aspect_len)
        memory = memory.float()
        aspect = self.embed(aspect_indices)['last_hidden_state']
        aspect = torch.sum(aspect, dim=1)
        aspect = torch.div(aspect, nonzeros_aspect.unsqueeze(-1))
        et = torch.zeros_like(aspect)
        batch_size = memory.size(0)
        seq_len = memory.size(1)
        if 'hops' not in self.config.args:
            self.config.hops = 3
        for _ in range(self.config.hops):
            g = self.att_linear(torch.cat([memory, torch.zeros(batch_size, seq_len, self.config.embed_dim) + et.unsqueeze(1), torch.zeros(batch_size, seq_len, self.config.embed_dim) + aspect.unsqueeze(1)], dim=-1))
            alpha = F.softmax(g, dim=1)
            i = torch.bmm(alpha.transpose(1, 2), memory).squeeze(1)
            et = self.gru_cell(i, et)
        out = self.dense(et)
        return {'logits': out}


class TC_LSTM_BERT(nn.Module):
    inputs = ['left_with_aspect_indices', 'right_with_aspect_indices', 'aspect_indices']

    def __init__(self, bert, config):
        super(TC_LSTM_BERT, self).__init__()
        self.embed = bert
        self.lstm_l = DynamicLSTM(config.embed_dim * 2, config.hidden_dim, num_layers=1, batch_first=True)
        self.lstm_r = DynamicLSTM(config.embed_dim * 2, config.hidden_dim, num_layers=1, batch_first=True)
        self.dense = nn.Linear(config.hidden_dim * 2, config.output_dim)

    def forward(self, inputs):
        x_l, x_r, target = inputs['left_with_aspect_indices'], inputs['right_with_aspect_indices'], inputs['aspect_indices']
        x_l_len, x_r_len = torch.sum(x_l != 0, dim=-1), torch.sum(x_r != 0, dim=-1)
        target_len = torch.sum(target != 0, dim=-1, dtype=torch.float)[:, None, None]
        x_l, x_r, target = self.embed(x_l)['last_hidden_state'], self.embed(x_r)['last_hidden_state'], self.embed(target)['last_hidden_state']
        v_target = torch.div(target.sum(dim=1, keepdim=True), target_len)
        x_l = torch.cat((x_l, torch.cat([v_target] * x_l.shape[1], 1)), 2)
        x_r = torch.cat((x_r, torch.cat([v_target] * x_r.shape[1], 1)), 2)
        _, (h_n_l, _) = self.lstm_l(x_l, x_l_len)
        _, (h_n_r, _) = self.lstm_r(x_r, x_r_len)
        h_n = torch.cat((h_n_l[0], h_n_r[0]), dim=-1)
        out = self.dense(h_n)
        return {'logits': out}


class TD_LSTM_BERT(nn.Module):
    inputs = ['left_with_aspect_indices', 'right_with_aspect_indices']

    def __init__(self, bert, config):
        super(TD_LSTM_BERT, self).__init__()
        self.embed = bert
        self.lstm_l = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.lstm_r = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True)
        self.dense = nn.Linear(config.hidden_dim * 2, config.output_dim)

    def forward(self, inputs):
        x_l, x_r = inputs['left_with_aspect_indices'], inputs['right_with_aspect_indices']
        x_l_len, x_r_len = torch.sum(x_l != 0, dim=-1), torch.sum(x_r != 0, dim=-1)
        x_l, x_r = self.embed(x_l)['last_hidden_state'], self.embed(x_r)['last_hidden_state']
        _, (h_n_l, _) = self.lstm_l(x_l, x_l_len)
        _, (h_n_r, _) = self.lstm_r(x_r, x_r_len)
        h_n = torch.cat((h_n_l[0], h_n_r[0]), dim=-1)
        out = self.dense(h_n)
        return {'logits': out}


class TNet_LF_BERT_Unit(nn.Module):

    def __init__(self, bert, config):
        super(TNet_LF_BERT_Unit, self).__init__()
        self.embed = bert
        self.position = Absolute_Position_Embedding(config)
        self.config = config
        D = config.embed_dim
        C = config.output_dim
        L = config.max_seq_len
        HD = config.hidden_dim
        self.lstm1 = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.lstm2 = DynamicLSTM(config.embed_dim, config.hidden_dim, num_layers=1, batch_first=True, bidirectional=True)
        self.convs3 = nn.Conv1d(2 * HD, 50, 3, padding=1)
        self.fc1 = nn.Linear(4 * HD, 2 * HD)
        self.fc = nn.Linear(50, C)

    def forward(self, inputs):
        text_raw_indices, aspect_indices, aspect_in_text = inputs[0], inputs[1], inputs[2]
        feature_len = torch.sum(text_raw_indices != 0, dim=-1)
        aspect_len = torch.sum(aspect_indices != 0, dim=-1)
        feature = self.embed(text_raw_indices)['last_hidden_state']
        aspect = self.embed(aspect_indices)['last_hidden_state']
        v, (_, _) = self.lstm1(feature, feature_len)
        e, (_, _) = self.lstm2(aspect, aspect_len)
        v = v.transpose(1, 2)
        e = e.transpose(1, 2)
        for i in range(2):
            a = torch.bmm(e.transpose(1, 2), v)
            a = F.softmax(a, 1)
            aspect_mid = torch.bmm(e, a)
            aspect_mid = torch.cat((aspect_mid, v), dim=1).transpose(1, 2)
            aspect_mid = F.relu(self.fc1(aspect_mid).transpose(1, 2))
            v = aspect_mid + v
            v = self.position(v.transpose(1, 2), aspect_in_text).transpose(1, 2)
            e = e.float()
            v = v.float()
        z = F.relu(self.convs3(v))
        z = F.max_pool1d(z, z.size(2)).squeeze(2)
        return z


class TNet_LF_BERT(nn.Module):
    inputs = ['text_indices', 'aspect_indices', 'aspect_boundary', 'left_aspect_indices', 'left_aspect_boundary', 'right_aspect_indices', 'right_aspect_boundary']

    def __init__(self, bert, config):
        super(TNet_LF_BERT, self).__init__()
        self.config = config
        self.asgcn_left = TNet_LF_BERT_Unit(bert, config) if self.config.lsa else None
        self.asgcn_central = TNet_LF_BERT_Unit(bert, config)
        self.asgcn_right = TNet_LF_BERT_Unit(bert, config) if self.config.lsa else None
        self.dropout = nn.Dropout(config.dropout)
        self.pooler = BertPooler(bert.config)
        self.linear = nn.Linear(50 * 3, self.config.output_dim)
        self.dense = nn.Linear(50, self.config.output_dim)

    def forward(self, inputs):
        res = {'logits': None}
        if self.config.lsa:
            cat_feat = torch.cat((self.asgcn_left([inputs['text_indices'], inputs['left_aspect_indices'], inputs['left_aspect_boundary']]), self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['aspect_boundary']]), self.asgcn_right([inputs['text_indices'], inputs['right_aspect_indices'], inputs['right_aspect_boundary']])), -1)
            cat_feat = self.dropout(cat_feat)
            res['logits'] = self.linear(cat_feat)
        else:
            cat_feat = self.asgcn_central([inputs['text_indices'], inputs['aspect_indices'], inputs['aspect_boundary']])
            cat_feat = self.dropout(cat_feat)
            res['logits'] = self.dense(cat_feat)
        return res


class LabelPaddingOption:
    SENTIMENT_PADDING = -100
    LABEL_PADDING = -100


class BERT_BASE_ATEPC(nn.Module):

    def __init__(self, bert_base_model, config):
        super(BERT_BASE_ATEPC, self).__init__()
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.config = config
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_config, config)
        self.SA2 = Encoder(bert_config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        if self.config.use_bert_spc:
            input_ids_spc = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        pooled_out = self.pooler(global_context_out)
        pooled_out = self.dropout(pooled_out)
        apc_logits = self.dense(pooled_out)
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class FAST_LCF_ATEPC(nn.Module):

    def __init__(self, bert_base_model, config):
        super(FAST_LCF_ATEPC, self).__init__()
        self.bert4global = bert_base_model
        self.config = config
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_base_model.config, config)
        self.SA2 = Encoder(bert_base_model.config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_base_model.config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        if self.config.use_bert_spc:
            input_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        if lcf_cdm_vec is not None or lcf_cdw_vec is not None:
            if 'cdm' in self.config.lcf:
                cdm_context_out = torch.mul(global_context_out, lcf_cdm_vec)
                cdm_context_out = self.SA1(cdm_context_out)
                cat_out = torch.cat((global_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'cdw' in self.config.lcf:
                cdw_context_out = torch.mul(global_context_out, lcf_cdw_vec)
                cdw_context_out = self.SA1(cdw_context_out)
                cat_out = torch.cat((global_context_out, cdw_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'fusion' in self.config.lcf:
                cdm_context_out = torch.mul(global_context_out, lcf_cdm_vec)
                cdw_context_out = torch.mul(global_context_out, lcf_cdw_vec)
                cat_out = torch.cat((global_context_out, cdw_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_triple(cat_out)
            sa_out = self.SA2(cat_out)
            pooled_out = self.pooler(sa_out)
            pooled_out = self.dropout(pooled_out)
            apc_logits = self.dense(pooled_out)
        else:
            apc_logits = None
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class FAST_LCFS_ATEPC(nn.Module):

    def __init__(self, bert_base_model, config):
        super(FAST_LCFS_ATEPC, self).__init__()
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.config = config
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_config, config)
        self.SA2 = Encoder(bert_config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        if self.config.use_bert_spc:
            input_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        if lcf_cdm_vec is not None or lcf_cdw_vec is not None:
            if 'cdm' in self.config.lcf:
                cdm_context_out = torch.mul(global_context_out, lcf_cdm_vec)
                cdm_context_out = self.SA1(cdm_context_out)
                cat_out = torch.cat((global_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'cdw' in self.config.lcf:
                cdw_context_out = torch.mul(global_context_out, lcf_cdw_vec)
                cdw_context_out = self.SA1(cdw_context_out)
                cat_out = torch.cat((global_context_out, cdw_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'fusion' in self.config.lcf:
                cdm_context_out = torch.mul(global_context_out, lcf_cdm_vec)
                cdw_context_out = torch.mul(global_context_out, lcf_cdw_vec)
                cat_out = torch.cat((global_context_out, cdw_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_triple(cat_out)
            sa_out = self.SA2(cat_out)
            pooled_out = self.pooler(sa_out)
            pooled_out = self.dropout(pooled_out)
            apc_logits = self.dense(pooled_out)
        else:
            apc_logits = None
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class LCF_ATEPC(nn.Module):

    def __init__(self, bert_base_model, config):
        super(LCF_ATEPC, self).__init__()
        self.config = config
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.bert4local = self.bert4global
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_config, config)
        self.SA2 = Encoder(bert_config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        if self.config.use_bert_spc:
            input_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        if lcf_cdm_vec is not None or lcf_cdw_vec is not None:
            local_context_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            local_context_out = self.bert4local(input_ids=local_context_ids)['last_hidden_state']
            batch_size, max_len, feat_dim = local_context_out.shape
            local_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
            for i in range(batch_size):
                jj = -1
                for j in range(max_len):
                    if valid_ids[i][j].item() == 1:
                        jj += 1
                        local_valid_output[i][jj] = local_context_out[i][j]
            local_context_out = self.dropout(local_valid_output)
            if 'cdm' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdm_context_out = self.SA1(cdm_context_out)
                cat_out = torch.cat((global_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'cdw' in self.config.lcf:
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cdw_context_out = self.SA1(cdw_context_out)
                cat_out = torch.cat((global_context_out, cdw_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'fusion' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cat_out = torch.cat((global_context_out, cdw_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_triple(cat_out)
            sa_out = self.SA2(cat_out)
            pooled_out = self.pooler(sa_out)
            pooled_out = self.dropout(pooled_out)
            apc_logits = self.dense(pooled_out)
        else:
            apc_logits = None
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class LCF_ATEPC_LARGE(nn.Module):

    def __init__(self, bert_base_model, config):
        super(LCF_ATEPC_LARGE, self).__init__()
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.config = config
        self.bert4local = copy.deepcopy(self.bert4global)
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_config, config)
        self.SA2 = Encoder(bert_config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        if self.config.use_bert_spc:
            input_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        if lcf_cdm_vec is not None or lcf_cdw_vec is not None:
            local_context_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            local_context_out = self.bert4local(input_ids=local_context_ids)['last_hidden_state']
            batch_size, max_len, feat_dim = local_context_out.shape
            local_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
            for i in range(batch_size):
                jj = -1
                for j in range(max_len):
                    if valid_ids[i][j].item() == 1:
                        jj += 1
                        local_valid_output[i][jj] = local_context_out[i][j]
            local_context_out = self.dropout(local_valid_output)
            if 'cdm' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdm_context_out = self.SA1(cdm_context_out)
                cat_out = torch.cat((global_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'cdw' in self.config.lcf:
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cdw_context_out = self.SA1(cdw_context_out)
                cat_out = torch.cat((global_context_out, cdw_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'fusion' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cat_out = torch.cat((global_context_out, cdw_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_triple(cat_out)
            sa_out = self.SA2(cat_out)
            pooled_out = self.pooler(sa_out)
            pooled_out = self.dropout(pooled_out)
            apc_logits = self.dense(pooled_out)
        else:
            apc_logits = None
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class LCF_TEMPLATE_ATEPC(nn.Module):

    def __init__(self, bert_base_model, config):
        super(LCF_TEMPLATE_ATEPC, self).__init__()
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.config = config
        self.bert4local = self.bert4global
        self.dropout = nn.Dropout(self.config.dropout)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        raise NotImplementedError('This is a template ATEPC model based on LCF, please implement your model use this template.')


class LCFS_ATEPC(nn.Module):

    def __init__(self, bert_base_model, config):
        super(LCFS_ATEPC, self).__init__()
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.config = config
        self.bert4local = self.bert4global
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_config, config)
        self.SA2 = Encoder(bert_config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        if self.config.use_bert_spc:
            input_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        if lcf_cdm_vec is not None or lcf_cdw_vec is not None:
            local_context_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            local_context_out = self.bert4local(input_ids=local_context_ids)['last_hidden_state']
            batch_size, max_len, feat_dim = local_context_out.shape
            local_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
            for i in range(batch_size):
                jj = -1
                for j in range(max_len):
                    if valid_ids[i][j].item() == 1:
                        jj += 1
                        local_valid_output[i][jj] = local_context_out[i][j]
            local_context_out = self.dropout(local_valid_output)
            if 'cdm' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdm_context_out = self.SA1(cdm_context_out)
                cat_out = torch.cat((global_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'cdw' in self.config.lcf:
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cdw_context_out = self.SA1(cdw_context_out)
                cat_out = torch.cat((global_context_out, cdw_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'fusion' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cat_out = torch.cat((global_context_out, cdw_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_triple(cat_out)
            sa_out = self.SA2(cat_out)
            pooled_out = self.pooler(sa_out)
            pooled_out = self.dropout(pooled_out)
            apc_logits = self.dense(pooled_out)
        else:
            apc_logits = None
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class LCFS_ATEPC_LARGE(nn.Module):

    def __init__(self, bert_base_model, config):
        super(LCFS_ATEPC_LARGE, self).__init__()
        bert_config = bert_base_model.config
        self.bert4global = bert_base_model
        self.config = config
        self.bert4local = copy.deepcopy(self.bert4global)
        self.dropout = nn.Dropout(self.config.dropout)
        self.SA1 = Encoder(bert_config, config)
        self.SA2 = Encoder(bert_config, config)
        self.linear_double = nn.Linear(config.hidden_dim * 2, config.hidden_dim)
        self.linear_triple = nn.Linear(config.hidden_dim * 3, config.hidden_dim)
        self.pooler = BertPooler(bert_config)
        self.dense = torch.nn.Linear(config.hidden_dim, config.output_dim)
        self.num_labels = config.get('num_labels', 0)
        self.classifier = nn.Linear(config.hidden_dim, self.num_labels)

    def get_batch_token_labels_bert_base_indices(self, labels):
        if labels is None:
            return
        labels = labels.detach().cpu().numpy()
        for text_i in range(len(labels)):
            sep_index = np.argmax(labels[text_i] == self.num_labels - 1)
            labels[text_i][sep_index + 1:] = 0
        return torch.tensor(labels)

    def get_ids_for_local_context_extractor(self, text_indices):
        text_ids = text_indices.detach().cpu().numpy()
        for text_i in range(len(text_ids)):
            sep_index = np.argmax(text_ids[text_i] == self.config.sep_indices)
            text_ids[text_i][sep_index + 1:] = 0
        return torch.tensor(text_ids)

    def forward(self, input_ids_spc, token_type_ids=None, attention_mask=None, labels=None, polarity=None, valid_ids=None, attention_mask_label=None, lcf_cdm_vec=None, lcf_cdw_vec=None):
        lcf_cdm_vec = lcf_cdm_vec.unsqueeze(2) if lcf_cdm_vec is not None else None
        lcf_cdw_vec = lcf_cdw_vec.unsqueeze(2) if lcf_cdw_vec is not None else None
        if self.config.use_bert_spc:
            input_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            labels = self.get_batch_token_labels_bert_base_indices(labels)
            global_context_out = self.bert4global(input_ids=input_ids, attention_mask=attention_mask)['last_hidden_state']
        else:
            global_context_out = self.bert4global(input_ids=input_ids_spc, attention_mask=attention_mask)['last_hidden_state']
        batch_size, max_len, feat_dim = global_context_out.shape
        global_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
        for i in range(batch_size):
            jj = -1
            for j in range(max_len):
                if valid_ids[i][j].item() == 1:
                    jj += 1
                    global_valid_output[i][jj] = global_context_out[i][j]
        global_context_out = self.dropout(global_valid_output)
        ate_logits = self.classifier(global_context_out)
        if lcf_cdm_vec is not None or lcf_cdw_vec is not None:
            local_context_ids = self.get_ids_for_local_context_extractor(input_ids_spc)
            local_context_out = self.bert4local(input_ids=local_context_ids)['last_hidden_state']
            batch_size, max_len, feat_dim = local_context_out.shape
            local_valid_output = torch.zeros(batch_size, max_len, feat_dim, dtype=torch.float32)
            for i in range(batch_size):
                jj = -1
                for j in range(max_len):
                    if valid_ids[i][j].item() == 1:
                        jj += 1
                        local_valid_output[i][jj] = local_context_out[i][j]
            local_context_out = self.dropout(local_valid_output)
            if 'cdm' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdm_context_out = self.SA1(cdm_context_out)
                cat_out = torch.cat((global_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'cdw' in self.config.lcf:
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cdw_context_out = self.SA1(cdw_context_out)
                cat_out = torch.cat((global_context_out, cdw_context_out), dim=-1)
                cat_out = self.linear_double(cat_out)
            elif 'fusion' in self.config.lcf:
                cdm_context_out = torch.mul(local_context_out, lcf_cdm_vec)
                cdw_context_out = torch.mul(local_context_out, lcf_cdw_vec)
                cat_out = torch.cat((global_context_out, cdw_context_out, cdm_context_out), dim=-1)
                cat_out = self.linear_triple(cat_out)
            sa_out = self.SA2(cat_out)
            pooled_out = self.pooler(sa_out)
            pooled_out = self.dropout(pooled_out)
            apc_logits = self.dense(pooled_out)
        else:
            apc_logits = None
        if labels is not None:
            criterion_ate = CrossEntropyLoss(ignore_index=0)
            criterion_apc = CrossEntropyLoss(ignore_index=LabelPaddingOption.SENTIMENT_PADDING)
            loss_ate = criterion_ate(ate_logits.view(-1, self.num_labels), labels.view(-1))
            loss_apc = criterion_apc(apc_logits, polarity)
            return loss_ate, loss_apc
        else:
            return ate_logits, apc_logits


class CNN(nn.Module):
    inputs = ['text_indices']

    def __init__(self, embedding_matrix, config):
        super(CNN, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.cnn = Conv1d(self.config.embed_dim, self.config.hidden_dim, kernel_size=self.config.kernel_size, padding=self.config.padding)
        self.pooling = MaxPool1d(self.config.max_seq_len - self.config.kernel_size + 1)
        self.dense = nn.Linear(self.config.hidden_dim, self.config.output_dim)

    def forward(self, inputs):
        text_raw_indices = inputs[0]
        x = self.embed(text_raw_indices)
        hidden_states = self.cnn(x.transpose(1, 2))
        pooled_states = self.pooling(hidden_states)
        transposed_states = pooled_states.transpose(1, 2)
        out = self.dense(transposed_states).sum(dim=1, keepdim=False)
        return out


class MultiHeadSelfAttention(nn.Module):

    def __init__(self, bert_config, config):
        super(MultiHeadSelfAttention, self).__init__()
        self.config = config
        self.config.hidden_size = self.config.hidden_dim
        self.mhsa = Encoder(bert_config=bert_config, config=self.config, layer_num=self.config.num_mhsa_layer)
        self.config = config

    def forward(self, x):
        return self.mhsa(x)


class MHSA(nn.Module):
    inputs = ['text_indices']

    def __init__(self, embedding_matrix, config):
        super(MHSA, self).__init__()
        self.config = config
        self.bert_config = AutoConfig.from_pretrained('bert-base-uncased')
        self.bert_config.hidden_size = self.config.hidden_dim
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.mhsa = MultiHeadSelfAttention(self.bert_config, self.config)
        self.pooler = BertPooler(self.bert_config)
        self.dense = nn.Linear(self.config.hidden_dim, self.config.output_dim)

    def forward(self, inputs):
        text_raw_indices = inputs[0]
        x = self.embed(text_raw_indices)
        out = self.mhsa(x)
        out = self.pooler(out)
        out = self.dense(out)
        return out


class Transformer(nn.Module):

    def __init__(self, embedding_matrix, config):
        super(Transformer, self).__init__()
        self.config = self.config
        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)
        self.dropout = nn.Dropout(self.config.dropout)
        self.transformer = nn.Transformer(d_model=self.config.hidden_dim, dropout=self.config.dropout, activation=self.config.hidden_act, custom_encoder=None, custom_decoder=None)
        self.classifier = nn.Linear(self.config.hidden_dim, self.config.output_dim)

    def forward(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):
        transformer_outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)
        sequence_output = transformer_outputs[0]
        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)
        logits = logits.squeeze(-1)
        return logits


class TADLSTM(nn.Module):
    inputs = ['text_indices']

    def __init__(self, embedding_matrix, config):
        super(TADLSTM, self).__init__()
        self.config = config
        self.embed = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float))
        self.lstm = DynamicLSTM(self.config.embed_dim, self.config.hidden_dim, num_layers=1, batch_first=True)
        self.dense1 = nn.Linear(self.config.hidden_dim, self.config.class_dim)
        self.dense2 = nn.Linear(self.config.hidden_dim, self.config.adv_det_dim)
        self.dense2 = nn.Linear(self.config.hidden_dim, self.config.class_dim)

    def forward(self, inputs):
        text_raw_indices = inputs[0]
        x = self.embed(text_raw_indices)
        x_len = torch.sum(text_raw_indices != 0, dim=-1)
        _, (h_n, _) = self.lstm(x, x_len)
        sent_logits = self.dense1(h_n[0])
        advdet_logits = self.dense2(h_n[0])
        adv_tr_logits = self.dense2(h_n[0])
        return sent_logits, advdet_logits, adv_tr_logits


class TADBERT(nn.Module):
    inputs = ['text_indices']

    def __init__(self, bert, config):
        super(TADBERT, self).__init__()
        self.config = config
        self.bert = bert
        self.pooler = BertPooler(bert.config)
        self.dense1 = nn.Linear(self.config.hidden_dim, self.config.class_dim)
        self.dense2 = nn.Linear(self.config.hidden_dim, self.config.adv_det_dim)
        self.dense3 = nn.Linear(self.config.hidden_dim, self.config.class_dim)
        self.encoder1 = Encoder(self.bert.config, config=self.config)
        self.encoder2 = Encoder(self.bert.config, config=self.config)
        self.encoder3 = Encoder(self.bert.config, config=self.config)

    def forward(self, inputs):
        text_raw_indices = inputs[0]
        last_hidden_state = self.bert(text_raw_indices)['last_hidden_state']
        sent_logits = self.dense1(self.pooler(last_hidden_state))
        advdet_logits = self.dense2(self.pooler(last_hidden_state))
        adv_tr_logits = self.dense3(self.pooler(last_hidden_state))
        att_score = torch.nn.functional.normalize(last_hidden_state.abs().sum(dim=1, keepdim=False) - last_hidden_state.abs().min(dim=1, keepdim=True)[0], p=1, dim=1)
        outputs = {'sent_logits': sent_logits, 'advdet_logits': advdet_logits, 'adv_tr_logits': adv_tr_logits, 'last_hidden_state': last_hidden_state, 'att_score': att_score}
        return outputs


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (Absolute_Position_Embedding,
     lambda: ([], {'config': _mock_config()}),
     lambda: ([torch.rand([4, 4]), torch.ones([4, 4], dtype=torch.int64)], {}),
     False),
    (Attention,
     lambda: ([], {'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 1, 4]), torch.rand([4, 4, 1, 4])], {}),
     False),
    (BERTMeanPooler,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4])], {}),
     True),
    (BertSelfAttention,
     lambda: ([], {'config': _mock_config(hidden_size=4, num_attention_heads=4, attention_probs_dropout_prob=0.5, position_embedding_type=4, is_decoder=4)}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (DynamicLSTM,
     lambda: ([], {'input_size': 4, 'hidden_size': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.ones([4], dtype=torch.int64)], {}),
     False),
    (Encoder,
     lambda: ([], {'bert_config': _mock_config(hidden_size=4, num_attention_heads=4, attention_probs_dropout_prob=0.5, position_embedding_type=4, is_decoder=4), 'config': _mock_config(max_seq_len=4)}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (GraphConvolution,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (LSTMLayer,
     lambda: ([], {'config': _mock_config(num_lstm_layer=1, embed_dim=4, hidden_dim=4)}),
     lambda: ([torch.rand([4, 4, 4]), torch.ones([4], dtype=torch.int64)], {}),
     False),
    (LocationEncoding,
     lambda: ([], {'config': _mock_config()}),
     lambda: ([torch.rand([4, 4]), torch.ones([4, 4], dtype=torch.int64)], {}),
     False),
    (MAELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (MultiHeadSelfAttention,
     lambda: ([], {'bert_config': _mock_config(hidden_size=4, num_attention_heads=4, attention_probs_dropout_prob=0.5, position_embedding_type=4, is_decoder=4), 'config': _mock_config(hidden_dim=4, num_mhsa_layer=1, max_seq_len=4)}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (NoQueryAttention,
     lambda: ([], {'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 1, 4])], {}),
     False),
    (PointwiseFeedForward,
     lambda: ([], {'d_hid': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (PositionwiseFeedForward,
     lambda: ([], {'d_hid': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (R2Loss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (RMSELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (SelfAttention,
     lambda: ([], {'bert_config': _mock_config(hidden_size=4, num_attention_heads=4, attention_probs_dropout_prob=0.5, position_embedding_type=4, is_decoder=4), 'config': _mock_config(max_seq_len=4)}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (SqueezeEmbedding,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.ones([4], dtype=torch.int64)], {}),
     True),
]

class Test_yangheng95_PyABSA(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

