import sys
_module = sys.modules[__name__]
del sys
common = _module
cvnets = _module
anchor_generator = _module
base_anchor_generator = _module
ssd_anchor_generator = _module
image_projection_layers = _module
attention_pool_2d = _module
base_image_projection = _module
global_pool_2d = _module
simple_projection_head = _module
layers = _module
activation = _module
gelu = _module
hard_sigmoid = _module
hard_swish = _module
leaky_relu = _module
prelu = _module
relu = _module
relu6 = _module
sigmoid = _module
swish = _module
tanh = _module
adaptive_pool = _module
base_layer = _module
conv_layer = _module
dropout = _module
embedding = _module
flatten = _module
global_pool = _module
identity = _module
linear_attention = _module
linear_layer = _module
multi_head_attention = _module
non_linear_layers = _module
normalization = _module
batch_norm = _module
group_norm = _module
instance_norm = _module
layer_norm = _module
sync_batch_norm = _module
normalization_layers = _module
pixel_shuffle = _module
pooling = _module
positional_embedding = _module
positional_encoding = _module
random_layers = _module
single_head_attention = _module
softmax = _module
stocastic_depth = _module
upsample = _module
matcher_det = _module
base_matcher = _module
ssd_matcher = _module
misc = _module
averaging_utils = _module
box_utils = _module
common = _module
init_utils = _module
profiler = _module
third_party = _module
ssd_utils = _module
models = _module
classification = _module
base_cls = _module
config = _module
efficientnet = _module
mobilenetv1 = _module
mobilenetv2 = _module
mobilenetv3 = _module
mobilevit = _module
mobilevit_v2 = _module
resnet = _module
swin_transformer = _module
vit = _module
efficientnet = _module
mobilenetv1 = _module
mobilenetv2 = _module
mobilenetv3 = _module
mobilevit = _module
mobilevit_v2 = _module
resnet = _module
swin_transformer = _module
vit = _module
detection = _module
base_detection = _module
mask_rcnn = _module
ssd = _module
utils = _module
rcnn_utils = _module
multi_modal_img_text = _module
base_multi_modal_img_text = _module
clip = _module
segmentation = _module
base_seg = _module
enc_dec = _module
heads = _module
base_seg_head = _module
deeplabv3 = _module
pspnet = _module
video_classification = _module
base_cls = _module
mobilevit_st = _module
modules = _module
aspp_block = _module
base_module = _module
efficientnet = _module
feature_pyramid = _module
mobilenetv2 = _module
mobilevit_block = _module
pspnet_module = _module
resnet_modules = _module
squeeze_excitation = _module
ssd_heads = _module
swin_transformer_block = _module
transformer = _module
neural_augmentor = _module
neural_aug = _module
neural_aug_utils = _module
text_encoders = _module
base_text_encoder = _module
transformer = _module
data = _module
collate_fns = _module
collate_functions = _module
data_loaders = _module
datasets = _module
imagenet = _module
imagenet_opencv = _module
imagenet_v2 = _module
dataset_base = _module
coco_base = _module
coco_mask_rcnn = _module
coco_ssd = _module
base_multi_modal_img_text = _module
img_text_tar_dataset = _module
zero_shot = _module
base_zero_shot = _module
imagenet = _module
ade20k = _module
coco_segmentation = _module
pascal_voc = _module
kinetics = _module
loader = _module
dataloader = _module
sampler = _module
base_sampler = _module
batch_sampler = _module
multi_scale_sampler = _module
variable_batch_sampler = _module
video_batch_sampler = _module
video_variable_seq_sampler = _module
text_tokenizer = _module
base_tokenizer = _module
clip_tokenizer = _module
transforms = _module
base_transforms = _module
image_opencv = _module
image_pil = _module
image_torch = _module
video = _module
video_reader = _module
base_video_reader = _module
default_video_reader = _module
key_frame_reader = _module
docs = _module
conf = _module
engine = _module
detection_utils = _module
coco_map = _module
eval_detection = _module
eval_segmentation = _module
evaluation_engine = _module
segmentation_utils = _module
cityscapes_iou = _module
training_engine = _module
utils = _module
loss_fn = _module
base_criteria = _module
base_neural_aug = _module
classification = _module
classification_loss_fns = _module
binary_cross_entropy = _module
cross_entropy = _module
cross_entropy_with_neural_aug = _module
detection = _module
detection_loss_fns = _module
mask_rcnn_loss = _module
mask_rcnn_loss_with_neural_aug = _module
ssd_multibox_loss = _module
utils = _module
distillation = _module
distillation_loss_fns = _module
cls_kl_div_loss = _module
cls_kl_div_loss_neural_aug = _module
utils = _module
multi_modal_img_text = _module
multi_modal_img_text_loss_fns = _module
contrastive_loss_clip = _module
contrastive_loss_clip_with_neural_aug = _module
segmentation = _module
segmentation_loss_fns = _module
cross_entropy = _module
seg_cross_entropy_with_neural_aug = _module
loss_landscape = _module
landscape_utils = _module
main_benchmark = _module
main_conversion = _module
main_eval = _module
main_loss_landscape = _module
main_train = _module
metrics = _module
coco_map = _module
confusion_mat = _module
intersection_over_union = _module
metric_monitor = _module
psnr = _module
stats = _module
topk_accuracy = _module
optim = _module
adam = _module
adamw = _module
base_optim = _module
scheduler = _module
base_scheduler = _module
cosine = _module
cyclic = _module
fixed = _module
multi_step = _module
polynomial = _module
sgd = _module
options = _module
opts = _module
parse_args = _module
setup = _module
tests = _module
configs = _module
dummy_datasets = _module
classification = _module
multi_modal_img_text = _module
segmentation = _module
ssd_detection = _module
video_classification = _module
dummy_loader = _module
test_coco_map = _module
test_image_pil = _module
test_metrics = _module
test_model = _module
test_multi_head_attn = _module
test_na_utils = _module
test_pos_embeddings = _module
test_sampler = _module
test_scheduler = _module
test_tokenizer = _module
test_training_engine = _module
checkpoint_utils = _module
color_map = _module
common_utils = _module
ddp_utils = _module
download_utils = _module
download_utils_base = _module
logger = _module
math_utils = _module
pytorch_to_coreml = _module
tensor_utils = _module
ddp_functional_utils = _module
visualization_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


from torch import Tensor


from typing import Optional


from typing import Tuple


from typing import Union


import numpy as np


from itertools import product


from typing import List


from torch import nn


from torch.nn import functional as F


from typing import Dict


import torch.nn


from typing import Any


from torch import Size


import math


import random


from torchvision.ops import StochasticDepth as StochasticDepthTorch


from copy import deepcopy


import re


from torch.utils.checkpoint import checkpoint as gradient_checkpoint_fn


from torch.utils.checkpoint import checkpoint_sequential as gradient_checkpoint_fn


from collections import namedtuple


from torchvision.models.detection.anchor_utils import AnchorGenerator


from torchvision.models.detection.mask_rcnn import MaskRCNN


from torchvision.ops import MultiScaleRoIAlign


import copy


from torchvision.ops import batched_nms


import torch.nn.functional as F


from typing import Sequence


from torchvision.ops.roi_align import RoIAlign


from functools import partial


from torchvision.datasets import ImageFolder


import warnings


from torch.utils import data


import time


from torchvision.io import read_image


from torchvision.io import read_file


from torchvision.io import decode_jpeg


from torchvision.io import ImageReadMode


from torchvision.io import decode_image


from torch.utils.data import DataLoader


from torch.utils.data.sampler import Sampler


import torch.distributed as dist


from torchtext.transforms import CLIPTokenizer


from torchvision import transforms as T


from torchvision.transforms import functional as F


from torch.nn import functional as F_torch


from torchvision.transforms import functional as F_vision


from torch.cuda.amp import autocast


from torchvision.models.detection.faster_rcnn import FasterRCNN


import matplotlib.pyplot as plt


from matplotlib import cm


from matplotlib import animation


from torch.cuda.amp import GradScaler


from torch.distributed.elastic.multiprocessing import errors


from numbers import Number


from torch.optim import Adam


from torch.optim import AdamW


from torch.optim import SGD


import torch.utils.data as data


from torch.utils.mobile_optimizer import optimize_for_mobile


from torch import distributed as dist


from torch.autograd import Function


from matplotlib.colors import hsv_to_rgb


class BaseAnchorGenerator(torch.nn.Module):
    """
    Base class for anchor generators for the task of object detection.
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()
        self.anchors_dict = dict()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """
        Add anchor generator-specific arguments to the parser
        """
        return parser

    def num_anchors_per_os(self):
        """Returns anchors per output stride. Child classes must implement this function."""
        raise NotImplementedError

    @torch.no_grad()
    def _generate_anchors(self, height: int, width: int, output_stride: int, device: Optional[str]='cpu', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, ...]]:
        raise NotImplementedError

    @torch.no_grad()
    def _get_anchors(self, fm_height: int, fm_width: int, fm_output_stride: int, device: Optional[str]='cpu', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, ...]]:
        key = 'h_{}_w_{}_os_{}'.format(fm_height, fm_width, fm_output_stride)
        if key not in self.anchors_dict:
            default_anchors_ctr = self._generate_anchors(*args, height=fm_height, width=fm_width, output_stride=fm_output_stride, device=device, **kwargs)
            self.anchors_dict[key] = default_anchors_ctr
            return default_anchors_ctr
        else:
            return self.anchors_dict[key]

    @torch.no_grad()
    def forward(self, fm_height: int, fm_width: int, fm_output_stride: int, device: Optional[str]='cpu', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, ...]]:
        """
        Returns anchors for the feature map

        Args:
            fm_height (int): Height of the feature map
            fm_width (int): Width of the feature map
            fm_output_stride (int): Output stride of the feature map
            device (Optional, str): Device (cpu or cuda). Defaults to cpu

        Returns:
            Tensor or Tuple of Tensors
        """
        return self._get_anchors(*args, fm_height=fm_height, fm_width=fm_width, fm_output_stride=fm_output_stride, device=device, **kwargs)


ANCHOR_GEN_REGISTRY = {}


def register_anchor_generator(name):
    """Register anchor generators for object detection"""

    def register_class(cls):
        if name in ANCHOR_GEN_REGISTRY:
            raise ValueError('Cannot register duplicate anchor generator ({})'.format(name))
        if not issubclass(cls, BaseAnchorGenerator):
            raise ValueError('Anchor generator ({}: {}) must extend BaseAnchorGenerator'.format(name, cls.__name__))
        ANCHOR_GEN_REGISTRY[name] = cls
        return cls
    return register_class


class SSDAnchorGenerator(BaseAnchorGenerator):
    """
    This class generates anchors (or priors) ``on-the-fly`` for the
    `single shot object detector (SSD) <https://arxiv.org/abs/1512.02325>`_.
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        output_strides = getattr(opts, 'anchor_generator.ssd.output_strides', [32, 64, 128, 256, -1])
        aspect_ratios = getattr(opts, 'anchor_generator.ssd.aspect_ratios', [[2, 3]] * len(output_strides))
        min_ratio = getattr(opts, 'anchor_generator.ssd.min_scale_ratio', 0.1)
        max_ratio = getattr(opts, 'anchor_generator.ssd.max_scale_ratio', 1.05)
        no_clipping = getattr(opts, 'anchor_generator.ssd.no_clipping', False)
        step = getattr(opts, 'anchor_generator.ssd.step', [1])
        if isinstance(step, int):
            step = [step] * len(output_strides)
        elif isinstance(step, List) and len(step) <= len(output_strides):
            step = step + [1] * (len(output_strides) - len(step))
        else:
            logger.error('--anchor-generator.ssd.step should be either a list of ints with the same length as the output strides OR an integer')
        super().__init__()
        aspect_ratios = [list(set(ar)) for ar in aspect_ratios]
        output_strides_aspect_ratio = dict()
        for k, v in zip(output_strides, aspect_ratios):
            output_strides_aspect_ratio[k] = v
        self.output_strides_aspect_ratio = output_strides_aspect_ratio
        self.output_strides = output_strides
        self.anchors_dict = dict()
        self.num_output_strides = len(output_strides)
        self.num_aspect_ratios = len(aspect_ratios)
        scales = np.linspace(min_ratio, max_ratio, len(output_strides) + 1)
        self.sizes = dict()
        for i, s in enumerate(output_strides):
            self.sizes[s] = {'min': scales[i], 'max': (scales[i] * scales[i + 1]) ** 0.5, 'step': step[i]}
        self.clip = not no_clipping
        self.min_scale_ratio = min_ratio
        self.max_scale_ratio = max_ratio
        self.step = step

    def __repr__(self):
        return '{}(min_scale_ratio={}, max_scale_ratio={}, n_output_strides={}, n_aspect_ratios={}, clipping={})'.format(self.__class__.__name__, self.min_scale_ratio, self.max_scale_ratio, self.num_output_strides, self.num_aspect_ratios, self.clip)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """
        Adds SSD anchor generator-specific arguments to the parser
        """
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--anchor-generator.ssd.output-strides', nargs='+', type=int, help='Output strides of the feature maps for which we want to generate anchors')
        group.add_argument('--anchor-generator.ssd.aspect-ratios', nargs='+', type=float, action='append', help='Aspect ratios at each output stride')
        group.add_argument('--anchor-generator.ssd.min-scale-ratio', type=float, help='Min. scale ratio')
        group.add_argument('--anchor-generator.ssd.max-scale-ratio', type=float, help='Max. scale ratio')
        group.add_argument('--anchor-generator.ssd.no-clipping', action='store_true', help="Don't clip the anchors")
        group.add_argument('--anchor-generator.ssd.step', type=int, default=[1], nargs='+', help='Step between pixels')
        return parser

    def num_anchors_per_os(self) ->List:
        """
        Returns anchors per output stride for SSD
        """
        return [(2 + 2 * len(ar)) for os, ar in self.output_strides_aspect_ratio.items()]

    @torch.no_grad()
    def _generate_anchors(self, height: int, width: int, output_stride: int, device: Optional[str]='cpu', *args, **kwargs) ->Tensor:
        min_size_h = self.sizes[output_stride]['min']
        min_size_w = self.sizes[output_stride]['min']
        max_size_h = self.sizes[output_stride]['max']
        max_size_w = self.sizes[output_stride]['max']
        aspect_ratio = self.output_strides_aspect_ratio[output_stride]
        step = max(1, self.sizes[output_stride]['step'])
        default_anchors_ctr = []
        start_step = max(0, step // 2)
        for y, x in product(range(start_step, height, step), range(start_step, width, step)):
            cx = (x + 0.5) / width
            cy = (y + 0.5) / height
            default_anchors_ctr.append([cx, cy, min_size_w, min_size_h])
            default_anchors_ctr.append([cx, cy, max_size_w, max_size_h])
            for ratio in aspect_ratio:
                ratio = ratio ** 0.5
                default_anchors_ctr.extend([[cx, cy, min_size_w * ratio, min_size_h / ratio], [cx, cy, min_size_w / ratio, min_size_h * ratio]])
        default_anchors_ctr = torch.tensor(default_anchors_ctr, dtype=torch.float, device=device)
        if self.clip:
            default_anchors_ctr = torch.clamp(default_anchors_ctr, min=0.0, max=1.0)
        return default_anchors_ctr


def parameter_list(named_parameters, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
    module_name = kwargs.get('module_name', '')
    with_decay = []
    without_decay = []
    with_decay_param_names = []
    without_decay_param_names = []
    if isinstance(named_parameters, list):
        for n_parameter in named_parameters:
            for p_name, param in n_parameter():
                if param.requires_grad and len(param.shape) == 1 and no_decay_bn_filter_bias:
                    without_decay.append(param)
                    without_decay_param_names.append(module_name + p_name)
                elif param.requires_grad:
                    with_decay.append(param)
                    with_decay_param_names.append(module_name + p_name)
    else:
        for p_name, param in named_parameters():
            if param.requires_grad and len(param.shape) == 1 and no_decay_bn_filter_bias:
                without_decay.append(param)
                without_decay_param_names.append(module_name + p_name)
            elif param.requires_grad:
                with_decay.append(param)
                with_decay_param_names.append(module_name + p_name)
    param_list = [{'params': with_decay, 'weight_decay': weight_decay, 'param_names': with_decay_param_names}]
    if len(without_decay) > 0:
        param_list.append({'params': without_decay, 'weight_decay': 0.0, 'param_names': without_decay_param_names})
    return param_list


class BaseImageProjectionHead(nn.Module):
    """Base class that projects image representations to the same space as text representations"""

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__()
        self.lr_mult = getattr(opts, 'model.image_projection_head.lr_multiplier', 1.0)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        """Add model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.image-projection-head.name', type=str, default=None, help='Name of the image projection head')
        group.add_argument('--model.image-projection-head.lr-multiplier', type=float, default=1.0, help='LR multiplier for image projection head')
        return parser

    def reset_parameters(self) ->None:
        """Reset weights of a given layer"""
        raise NotImplementedError

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        param_list = parameter_list(named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias)
        return param_list, [self.lr_mult] * len(param_list)

    def forward(self, input: Dict, *args, **kwargs) ->Dict:
        raise NotImplementedError


class BaseLayer(nn.Module):
    """
    Base class for neural network layers
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """Add layer specific arguments"""
        return parser

    def forward(self, *args, **kwargs) ->Any:
        pass

    def profile_module(self, *args, **kwargs) ->Tuple[Tensor, float, float]:
        raise NotImplementedError

    def __repr__(self):
        return '{}'.format(self.__class__.__name__)


class GlobalPool(BaseLayer):
    """
    This layers applies global pooling over a 4D or 5D input tensor

    Args:
        pool_type (Optional[str]): Pooling type. It can be mean, rms, or abs. Default: `mean`
        keep_dim (Optional[bool]): Do not squeeze the dimensions of a tensor. Default: `False`

    Shape:
        - Input: :math:`(N, C, H, W)` or :math:`(N, C, D, H, W)`
        - Output: :math:`(N, C, 1, 1)` or :math:`(N, C, 1, 1, 1)` if keep_dim else :math:`(N, C)`
    """
    pool_types = ['mean', 'rms', 'abs']

    def __init__(self, pool_type: Optional[str]='mean', keep_dim: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__()
        if pool_type not in self.pool_types:
            logger.error('Supported pool types are: {}. Got {}'.format(self.pool_types, pool_type))
        self.pool_type = pool_type
        self.keep_dim = keep_dim

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        cls_name = '{} arguments'.format(cls.__name__)
        group = parser.add_argument_group(title=cls_name, description=cls_name)
        group.add_argument('--model.layer.global-pool', type=str, default='mean', help='Which global pooling?')
        return parser

    def _global_pool(self, x: Tensor, dims: List):
        if self.pool_type == 'rms':
            x = x ** 2
            x = torch.mean(x, dim=dims, keepdim=self.keep_dim)
            x = x ** -0.5
        elif self.pool_type == 'abs':
            x = torch.mean(torch.abs(x), dim=dims, keepdim=self.keep_dim)
        else:
            x = torch.mean(x, dim=dims, keepdim=self.keep_dim)
        return x

    def forward(self, x: Tensor) ->Tensor:
        if x.dim() == 4:
            dims = [-2, -1]
        elif x.dim() == 5:
            dims = [-3, -2, -1]
        else:
            raise NotImplementedError('Currently 2D and 3D global pooling supported')
        return self._global_pool(x, dims=dims)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        input = self.forward(input)
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(type={})'.format(self.__class__.__name__, self.pool_type)


IMAGE_PROJECTION_HEAD_REGISTRY = {}


def register_image_projection_head(name):

    def register_image_projection_head_class(cls):
        if name in IMAGE_PROJECTION_HEAD_REGISTRY:
            raise ValueError('Cannot register duplicate image projection layer class ({})'.format(name))
        if not issubclass(cls, BaseImageProjectionHead):
            raise ValueError('Image projection layer class ({}: {}) must extend BaseImageProjection'.format(name, cls.__name__))
        IMAGE_PROJECTION_HEAD_REGISTRY[name] = cls
        return cls
    return register_image_projection_head_class


class GlobalPool2D(BaseImageProjectionHead):
    """This class implements global pooling with linear projection"""

    def __init__(self, opts, in_dim: int, out_dim: int, *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        scale = in_dim ** -0.5
        self.pool = GlobalPool(pool_type='mean', keep_dim=False)
        self.proj = nn.Parameter(scale * torch.randn(size=(in_dim, out_dim)))
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.feature_normalization = not getattr(opts, 'model.image_projection_head.global_pool_nchw2nc.no_feature_normalization', False)
        self.reset_parameters()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.image-projection-head.global-pool-nchw2nc.no-feature-normalization', action='store_true', help="Don't normalize image features")
        return parser

    def reset_parameters(self):
        pass

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        assert x.dim() == 4, 'Input should be 4-dimensional (Batch x in_dim x in_height x in_width). Got: {}'.format(x.shape)
        x = self.pool(x)
        x = x @ self.proj
        if self.feature_normalization:
            x = F.normalize(x, dim=-1)
        return x


class SimpleImageProjectionHead(BaseImageProjectionHead):
    """This class implements simple projection head"""

    def __init__(self, opts, in_dim: int, out_dim: int, *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        scale = in_dim ** -0.5
        self.proj = nn.Parameter(scale * torch.randn(size=(in_dim, out_dim)))
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.feature_normalizaiton = not getattr(opts, 'model.image_projection_head.simple_projection_nc2nc.no_feature_normalization', False)
        self.reset_parameters()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.image-projection-head.simple-projection-nc2nc.no-feature-normalization', action='store_true', help="Don't normalize image features")
        return parser

    def reset_parameters(self):
        pass

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        assert x.dim() == 2, 'Input should be 2-dimensional (Batch x in_dim). Got: {}'.format(x.shape)
        x = x @ self.proj
        if self.feature_normalizaiton:
            x = F.normalize(x, dim=-1)
        return x


ACT_FN_REGISTRY = {}


SUPPORTED_ACT_FNS = []


def register_act_fn(name):

    def register_fn(cls):
        if name in SUPPORTED_ACT_FNS:
            raise ValueError('Cannot register duplicate activation function ({})'.format(name))
        SUPPORTED_ACT_FNS.append(name)
        ACT_FN_REGISTRY[name] = cls
        return cls
    return register_fn


class GELU(nn.GELU):
    """
    Applies the `Gaussian Error Linear Units <https://arxiv.org/abs/1606.08415>`_ function
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class Hardsigmoid(nn.Hardsigmoid):
    """
    Applies the `Hard Sigmoid <https://arxiv.org/abs/1511.00363v3>`_ function
    """

    def __init__(self, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def forward(self, input: Tensor, *args, **kwargs) ->Tensor:
        if hasattr(F, 'hardsigmoid'):
            return F.hardsigmoid(input, self.inplace)
        else:
            return F.relu(input + 3) / 6

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class Hardswish(nn.Hardswish):
    """
    Applies the HardSwish function, as described in the paper
    `Searching for MobileNetv3 <https://arxiv.org/abs/1905.02244>`_
    """

    def __init__(self, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def forward(self, input: Tensor, *args, **kwargs) ->Tensor:
        if hasattr(F, 'hardswish'):
            return F.hardswish(input, self.inplace)
        else:
            x_hard_sig = F.relu(input + 3) / 6
            return input * x_hard_sig

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class LeakyReLU(nn.LeakyReLU):
    """
    Applies a leaky relu function. See `Rectifier Nonlinearities Improve Neural Network Acoustic Models`
    for more details.
    """

    def __init__(self, negative_slope: Optional[float]=0.01, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(negative_slope=negative_slope, inplace=inplace)

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class PReLU(nn.PReLU):
    """
    Applies the `Parametric Rectified Linear Unit <https://arxiv.org/abs/1502.01852>`_ function
    """

    def __init__(self, num_parameters: Optional[int]=1, init: Optional[float]=0.25, *args, **kwargs) ->None:
        super().__init__(num_parameters=num_parameters, init=init)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class ReLU(nn.ReLU):
    """
    Applies Rectified Linear Unit function
    """

    def __init__(self, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class ReLU6(nn.ReLU6):
    """
    Applies the ReLU6 function
    """

    def __init__(self, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class Sigmoid(nn.Sigmoid):
    """
    Applies the sigmoid function
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class Swish(nn.SiLU):
    """
    Applies the `Swish (also known as SiLU) <https://arxiv.org/abs/1702.03118>`_ function.
    """

    def __init__(self, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class Tanh(nn.Tanh):
    """
    Applies Tanh function
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class AdaptiveAvgPool2d(nn.AdaptiveAvgPool2d):
    """
    Applies a 2D adaptive average pooling over an input tensor.

    Args:
        output_size (Optional, int or Tuple[int, int]): The target output size. If a single int :math:`h` is passed,
        then a square output of size :math:`hxh` is produced. If a tuple of size :math:`hxw` is passed, then an
        output of size `hxw` is produced. Default is 1.
    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: :math:`(N, C, h, h)` or :math:`(N, C, h, w)`
    """

    def __init__(self, output_size: Union[int, Tuple[int, int]]=1, *args, **kwargs) ->None:
        super().__init__(output_size=output_size)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        input = self.forward(input)
        return input, 0.0, 0.0


class Conv2d(nn.Conv2d):
    """
    Applies a 2D convolution over an input

    Args:
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        kernel_size (Union[int, Tuple[int, int]]): Kernel size for convolution.
        stride (Union[int, Tuple[int, int]]): Stride for convolution. Defaults to 1
        padding (Union[int, Tuple[int, int]]): Padding for convolution. Defaults to 0
        dilation (Union[int, Tuple[int, int]]): Dilation rate for convolution. Default: 1
        groups (Optional[int]): Number of groups in convolution. Default: 1
        bias (bool): Use bias. Default: ``False``
        padding_mode (Optional[str]): Padding mode. Default: ``zeros``

        use_norm (Optional[bool]): Use normalization layer after convolution. Default: ``True``
        use_act (Optional[bool]): Use activation layer after convolution (or convolution and normalization).
                                Default: ``True``
        act_name (Optional[str]): Use specific activation function. Overrides the one specified in command line args.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`
    """

    def __init__(self, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Optional[Union[int, Tuple[int, int]]]=1, padding: Optional[Union[int, Tuple[int, int]]]=0, dilation: Optional[Union[int, Tuple[int, int]]]=1, groups: Optional[int]=1, bias: Optional[bool]=False, padding_mode: Optional[str]='zeros', *args, **kwargs) ->None:
        super().__init__(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)


def build_activation_layer(act_type: Optional[str]='relu', num_parameters: Optional[int]=-1, inplace: Optional[bool]=True, negative_slope: Optional[float]=0.1, *args, **kwargs) ->torch.nn.Module:
    """
    Helper function to build the activation function
    """
    if act_type is None:
        act_type = 'none'
    act_type = act_type.lower()
    act_layer = None
    if act_type in ACT_FN_REGISTRY:
        act_layer = ACT_FN_REGISTRY[act_type](*args, num_parameters=num_parameters, inplace=inplace, negative_slope=negative_slope, **kwargs)
    else:
        logger.error('Supported activation layers are: {}. Supplied argument is: {}'.format(SUPPORTED_ACT_FNS, act_type))
    return act_layer


def get_activation_fn(act_type: Optional[str]='relu', num_parameters: Optional[int]=-1, inplace: Optional[bool]=True, negative_slope: Optional[float]=0.1, *args, **kwargs) ->nn.Module:
    """
    Helper function to get activation (or non-linear) function
    """
    return build_activation_layer(*args, act_type=act_type, num_parameters=num_parameters, negative_slope=negative_slope, inplace=inplace, **kwargs)


class Identity(nn.Module):

    def __init__(self, *args, **kwargs):
        super().__init__()

    def forward(self, x: Any) ->Any:
        return x


NORM_LAYER_REGISTRY = {}


SUPPORTED_NORM_FNS = []


def build_normalization_layer(opts, num_features: int, norm_type: Optional[str]=None, num_groups: Optional[int]=None, *args, **kwargs) ->torch.nn.Module:
    """
    Helper function to build the normalization layer.
    The function can be used in either of below mentioned ways:
    Scenario 1: Set the default normalization layers using command line arguments. This is useful when the same normalization
    layer is used for the entire network (e.g., ResNet).
    Scenario 2: Network uses different normalization layers. In that case, we can override the default normalization
    layer by specifying the name using `norm_type` argument
    """
    norm_type = getattr(opts, 'model.normalization.name', 'batch_norm') if norm_type is None else norm_type
    num_groups = getattr(opts, 'model.normalization.groups', 1) if num_groups is None else num_groups
    momentum = getattr(opts, 'model.normalization.momentum', 0.1)
    norm_layer = None
    norm_type = norm_type.lower() if norm_type is not None else None
    if norm_type in NORM_LAYER_REGISTRY:
        if torch.cuda.device_count() < 1 and norm_type.find('sync_batch') > -1:
            norm_type = norm_type.replace('sync_', '')
        norm_layer = NORM_LAYER_REGISTRY[norm_type](normalized_shape=num_features, num_features=num_features, momentum=momentum, num_groups=num_groups)
    elif norm_type == 'identity':
        norm_layer = Identity()
    else:
        logger.error('Supported normalization layer arguments are: {}. Got: {}'.format(SUPPORTED_NORM_FNS, norm_type))
    return norm_layer


def get_normalization_layer(opts, num_features: int, norm_type: Optional[str]=None, num_groups: Optional[int]=None, *args, **kwargs) ->nn.Module:
    """
    Helper function to get normalization layers
    """
    return build_normalization_layer(opts, num_features, norm_type, num_groups)


class ConvLayer(BaseLayer):
    """
    Applies a 2D convolution over an input

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        kernel_size (Union[int, Tuple[int, int]]): Kernel size for convolution.
        stride (Union[int, Tuple[int, int]]): Stride for convolution. Default: 1
        dilation (Union[int, Tuple[int, int]]): Dilation rate for convolution. Default: 1
        padding (Union[int, Tuple[int, int]]): Padding for convolution. When not specified, 
                                               padding is automatically computed based on kernel size 
                                               and dilation rage. Default is ``None``
        groups (Optional[int]): Number of groups in convolution. Default: ``1``
        bias (Optional[bool]): Use bias. Default: ``False``
        padding_mode (Optional[str]): Padding mode. Default: ``zeros``
        use_norm (Optional[bool]): Use normalization layer after convolution. Default: ``True``
        use_act (Optional[bool]): Use activation layer after convolution (or convolution and normalization).
                                Default: ``True``
        act_name (Optional[str]): Use specific activation function. Overrides the one specified in command line args.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    .. note::
        For depth-wise convolution, `groups=C_{in}=C_{out}`.
    """

    def __init__(self, opts, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple[int, int]], stride: Optional[Union[int, Tuple[int, int]]]=1, dilation: Optional[Union[int, Tuple[int, int]]]=1, padding: Optional[Union[int, Tuple[int, int]]]=None, groups: Optional[int]=1, bias: Optional[bool]=False, padding_mode: Optional[str]='zeros', use_norm: Optional[bool]=True, use_act: Optional[bool]=True, act_name: Optional[str]=None, *args, **kwargs) ->None:
        super().__init__()
        if use_norm:
            norm_type = getattr(opts, 'model.normalization.name', 'batch_norm')
            if norm_type is not None and norm_type.find('batch') > -1:
                assert not bias, 'Do not use bias when using normalization layers.'
            elif norm_type is not None and norm_type.find('layer') > -1:
                bias = True
        if isinstance(kernel_size, int):
            kernel_size = kernel_size, kernel_size
        if isinstance(stride, int):
            stride = stride, stride
        if isinstance(dilation, int):
            dilation = dilation, dilation
        assert isinstance(kernel_size, Tuple)
        assert isinstance(stride, Tuple)
        assert isinstance(dilation, Tuple)
        if padding is None:
            padding = int((kernel_size[0] - 1) / 2) * dilation[0], int((kernel_size[1] - 1) / 2) * dilation[1]
        if in_channels % groups != 0:
            logger.error('Input channels are not divisible by groups. {}%{} != 0 '.format(in_channels, groups))
        if out_channels % groups != 0:
            logger.error('Output channels are not divisible by groups. {}%{} != 0 '.format(out_channels, groups))
        block = nn.Sequential()
        conv_layer = Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)
        block.add_module(name='conv', module=conv_layer)
        self.norm_name = None
        if use_norm:
            norm_layer = get_normalization_layer(opts=opts, num_features=out_channels)
            block.add_module(name='norm', module=norm_layer)
            self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        act_type = getattr(opts, 'model.activation.name', 'prelu') if act_name is None else act_name
        if act_type is not None and use_act:
            neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
            inplace = getattr(opts, 'model.activation.inplace', False)
            act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
            block.add_module(name='act', module=act_layer)
            self.act_name = act_layer.__class__.__name__
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.groups = groups
        self.kernel_size = conv_layer.kernel_size
        self.bias = bias
        self.dilation = dilation

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        cls_name = '{} arguments'.format(cls.__name__)
        group = parser.add_argument_group(title=cls_name, description=cls_name)
        group.add_argument('--model.layer.conv-init', type=str, default='kaiming_normal', help='Init type for conv layers')
        parser.add_argument('--model.layer.conv-init-std-dev', type=float, default=None, help='Std deviation for conv layers')
        return parser

    def forward(self, x: Tensor) ->Tensor:
        return self.block(x)

    def __repr__(self):
        repr_str = self.block[0].__repr__()
        repr_str = repr_str[:-1]
        if self.norm_name is not None:
            repr_str += ', normalization={}'.format(self.norm_name)
        if self.act_name is not None:
            repr_str += ', activation={}'.format(self.act_name)
        repr_str += ')'
        return repr_str

    def profile_module(self, input: Tensor) ->(Tensor, float, float):
        if input.dim() != 4:
            logger.error('Conv2d requires 4-dimensional input (BxCxHxW). Provided input has shape: {}'.format(input.size()))
        b, in_c, in_h, in_w = input.size()
        assert in_c == self.in_channels, '{}!={}'.format(in_c, self.in_channels)
        stride_h, stride_w = self.stride
        groups = self.groups
        out_h = in_h // stride_h
        out_w = in_w // stride_w
        k_h, k_w = self.kernel_size
        macs = k_h * k_w * (in_c * self.out_channels) * (out_h * out_w) * 1.0
        macs /= groups
        if self.bias:
            macs += self.out_channels * out_h * out_w
        params = sum([p.numel() for p in self.parameters()])
        output = torch.zeros(size=(b, self.out_channels, out_h, out_w), dtype=input.dtype, device=input.device)
        return output, params, macs


class TransposeConvLayer(BaseLayer):
    """
    Applies a 2D Transpose convolution (aka as Deconvolution) over an input

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        kernel_size (Union[int, Tuple[int, int]]): Kernel size for convolution.
        stride (Union[int, Tuple[int, int]]): Stride for convolution. Default: 1
        dilation (Union[int, Tuple[int, int]]): Dilation rate for convolution. Default: 1
        groups (Optional[int]): Number of groups in convolution. Default: 1
        bias (Optional[bool]): Use bias. Default: ``False``
        padding_mode (Optional[str]): Padding mode. Default: ``zeros``
        use_norm (Optional[bool]): Use normalization layer after convolution. Default: ``True``
        use_act (Optional[bool]): Use activation layer after convolution (or convolution and normalization).
                                Default: ``True``
        padding (Optional[Union[int, Tuple]]): Padding will be done on both sides of each dimension in the input
        output_padding (Optional[Union[int, Tuple]]): Additional padding on the output tensor
        auto_padding (Optional[bool]): Compute padding automatically. Default: ``True``

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`
    """

    def __init__(self, opts, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple], stride: Optional[Union[int, Tuple]]=1, dilation: Optional[Union[int, Tuple]]=1, groups: Optional[int]=1, bias: Optional[bool]=False, padding_mode: Optional[str]='zeros', use_norm: Optional[bool]=True, use_act: Optional[bool]=True, padding: Optional[Union[int, Tuple]]=(0, 0), output_padding: Optional[Union[int, Tuple]]=None, auto_padding: Optional[bool]=True, *args, **kwargs):
        super().__init__()
        if use_norm:
            assert not bias, 'Do not use bias when using normalization layers.'
        if isinstance(kernel_size, int):
            kernel_size = kernel_size, kernel_size
        if isinstance(stride, int):
            stride = stride, stride
        if isinstance(dilation, int):
            dilation = dilation, dilation
        if output_padding is None:
            output_padding = stride[0] - 1, stride[1] - 1
        assert isinstance(kernel_size, (tuple, list))
        assert isinstance(stride, (tuple, list))
        assert isinstance(dilation, (tuple, list))
        if auto_padding:
            padding = int((kernel_size[0] - 1) / 2) * dilation[0], int((kernel_size[1] - 1) / 2) * dilation[1]
        if in_channels % groups != 0:
            logger.error('Input channels are not divisible by groups. {}%{} != 0 '.format(in_channels, groups))
        if out_channels % groups != 0:
            logger.error('Output channels are not divisible by groups. {}%{} != 0 '.format(out_channels, groups))
        block = nn.Sequential()
        conv_layer = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode, output_padding=output_padding)
        block.add_module(name='conv', module=conv_layer)
        self.norm_name = None
        if use_norm:
            norm_layer = get_normalization_layer(opts=opts, num_features=out_channels)
            block.add_module(name='norm', module=norm_layer)
            self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        act_type = getattr(opts, 'model.activation.name', 'relu')
        if act_type is not None and use_act:
            neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
            inplace = getattr(opts, 'model.activation.inplace', False)
            act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
            block.add_module(name='act', module=act_layer)
            self.act_name = act_layer.__class__.__name__
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.groups = groups
        self.kernel_size = conv_layer.kernel_size
        self.bias = bias

    def forward(self, x: Tensor) ->Tensor:
        return self.block(x)

    def __repr__(self):
        repr_str = self.block[0].__repr__()
        repr_str = repr_str[:-1]
        if self.norm_name is not None:
            repr_str += ', normalization={}'.format(self.norm_name)
        if self.act_name is not None:
            repr_str += ', activation={}'.format(self.act_name)
        repr_str += ')'
        return repr_str

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        if input.dim() != 4:
            logger.error('Conv2d requires 4-dimensional input (BxCxHxW). Provided input has shape: {}'.format(input.size()))
        b, in_c, in_h, in_w = input.size()
        assert in_c == self.in_channels, '{}!={}'.format(in_c, self.in_channels)
        stride_h, stride_w = self.stride
        groups = self.groups
        out_h = in_h * stride_h
        out_w = in_w * stride_w
        k_h, k_w = self.kernel_size
        macs = k_h * k_w * (in_c * self.out_channels) * (out_h * out_w) * 1.0
        macs /= groups
        if self.bias:
            macs += self.out_channels * out_h * out_w
        params = sum([p.numel() for p in self.parameters()])
        output = torch.zeros(size=(b, self.out_channels, out_h, out_w), dtype=input.dtype, device=input.device)
        return output, params, macs


class NormActLayer(BaseLayer):
    """
    Applies a normalization layer followed by an activation layer

    Args:
        opts: command-line arguments
        num_features: :math:`C` from an expected input of size :math:`(N, C, H, W)`

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)`
    """

    def __init__(self, opts, num_features, *args, **kwargs):
        super().__init__()
        block = nn.Sequential()
        self.norm_name = None
        norm_layer = get_normalization_layer(opts=opts, num_features=num_features)
        block.add_module(name='norm', module=norm_layer)
        self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        act_type = getattr(opts, 'model.activation.name', 'prelu')
        neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        inplace = getattr(opts, 'model.activation.inplace', False)
        act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=num_features)
        block.add_module(name='act', module=act_layer)
        self.act_name = act_layer.__class__.__name__
        self.block = block

    def forward(self, x: Tensor) ->Tensor:
        return self.block(x)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        macs = 0.0
        return input, params, macs

    def __repr__(self):
        repr_str = '{}(normalization={}, activation={})'.format(self.__class__.__name__, self.norm_type, self.act_type)
        return repr_str


class ConvLayer3d(BaseLayer):
    """
    Applies a 3D convolution over an input

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`
        kernel_size (Union[int, Tuple[int, int]]): Kernel size for convolution.
        stride (Union[int, Tuple[int, int]]): Stride for convolution. Default: 1
        dilation (Union[int, Tuple[int, int]]): Dilation rate for convolution. Default: 1
        groups (Optional[int]): Number of groups in convolution. Default: 1
        bias (Optional[bool]): Use bias. Default: ``False``
        padding_mode (Optional[str]): Padding mode. Default: ``zeros``
        use_norm (Optional[bool]): Use normalization layer after convolution. Default: ``True``
        use_act (Optional[bool]): Use activation layer after convolution (or convolution and normalization).
                                Default: ``True``

    Shape:
        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})`

    .. note::
        For depth-wise convolution, `groups=C_{in}=C_{out}`.
    """

    def __init__(self, opts, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple], stride: Optional[Union[int, Tuple]]=1, dilation: Optional[Union[int, Tuple]]=1, groups: Optional[int]=1, bias: Optional[bool]=False, padding_mode: Optional[str]='zeros', use_norm: Optional[bool]=True, use_act: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__()
        if use_norm:
            assert not bias, 'Do not use bias when using normalization layers.'
        if isinstance(kernel_size, int):
            kernel_size = kernel_size, kernel_size, kernel_size
        if isinstance(stride, int):
            stride = stride, stride, stride
        if isinstance(dilation, int):
            dilation = dilation, dilation, dilation
        assert isinstance(kernel_size, (tuple, list))
        assert isinstance(stride, (tuple, list))
        assert isinstance(dilation, (tuple, list))
        padding = tuple([(int((kernel_size[i] - 1) / 2) * dilation[i]) for i in range(3)])
        if in_channels % groups != 0:
            logger.error('Input channels are not divisible by groups. {}%{} != 0 '.format(in_channels, groups))
        if out_channels % groups != 0:
            logger.error('Output channels are not divisible by groups. {}%{} != 0 '.format(out_channels, groups))
        block = nn.Sequential()
        conv_layer = nn.Conv3d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)
        block.add_module(name='conv', module=conv_layer)
        self.norm_name = None
        norm_type = getattr(opts, 'model.normalization.name', 'batch_norm')
        if use_norm and norm_type is not None:
            if norm_type.find('batch') > -1:
                norm_type = 'batch_norm_3d'
            norm_layer = get_normalization_layer(opts=opts, num_features=out_channels, norm_type=norm_type)
            block.add_module(name='norm', module=norm_layer)
            self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        act_type = getattr(opts, 'model.activation.name', 'prelu')
        if act_type is not None and use_act:
            neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
            inplace = getattr(opts, 'model.activation.inplace', False)
            act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
            block.add_module(name='act', module=act_layer)
            self.act_name = act_layer.__class__.__name__
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.groups = groups
        self.kernel_size = conv_layer.kernel_size
        self.bias = bias
        self.dilation = dilation

    def forward(self, x: Tensor) ->Tensor:
        return self.block(x)

    def __repr__(self):
        repr_str = self.block[0].__repr__()
        repr_str = repr_str[:-1]
        if self.norm_name is not None:
            repr_str += ', normalization={}'.format(self.norm_name)
        if self.act_name is not None:
            repr_str += ', activation={}'.format(self.act_name)
        repr_str += ')'
        return repr_str

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        if input.dim() != 4:
            logger.error('Conv2d requires 4-dimensional input (BxCxHxW). Provided input has shape: {}'.format(input.size()))
        b, in_c, in_d, in_h, in_w = input.size()
        assert in_c == self.in_channels, '{}!={}'.format(in_c, self.in_channels)
        stride_d, stride_h, stride_w = self.stride
        groups = self.groups
        out_h = in_h // stride_h
        out_w = in_w // stride_w
        out_d = in_d // stride_d
        k_d, k_h, k_w = self.kernel_size
        macs = k_d * k_h * k_w * (in_c * self.out_channels) * (out_h * out_w * out_d) * 1.0
        macs /= groups
        if self.bias:
            macs += self.out_channels * out_h * out_w * out_d
        params = sum([p.numel() for p in self.parameters()])
        output = torch.zeros(size=(b, self.out_channels, out_d, out_h, out_w), dtype=input.dtype, device=input.device)
        return output, params, macs


class SeparableConv(BaseLayer):
    """
    Applies a `2D depth-wise separable convolution <https://arxiv.org/abs/1610.02357>`_ over a 4D input tensor

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        kernel_size (Union[int, Tuple[int, int]]): Kernel size for convolution.
        stride (Union[int, Tuple[int, int]]): Stride for convolution. Default: 1
        dilation (Union[int, Tuple[int, int]]): Dilation rate for convolution. Default: 1
        use_norm (Optional[bool]): Use normalization layer after convolution. Default: ``True``
        use_act (Optional[bool]): Use activation layer after convolution (or convolution and normalization). Default: ``True``
        bias (Optional[bool]): Use bias. Default: ``False``
        padding_mode (Optional[str]): Padding mode. Default: ``zeros``

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    .. note::
        For depth-wise convolution, `groups=C_{in}=C_{out}`.
    """

    def __init__(self, opts, in_channels: int, out_channels: int, kernel_size: Union[int, Tuple], stride: Optional[Union[int, Tuple]]=1, dilation: Optional[Union[int, Tuple]]=1, use_norm: Optional[bool]=True, use_act: Optional[bool]=True, bias: Optional[bool]=False, padding_mode: Optional[str]='zeros', *args, **kwargs) ->None:
        super().__init__()
        self.dw_conv = ConvLayer(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, groups=in_channels, bias=False, padding_mode=padding_mode, use_norm=True, use_act=False)
        self.pw_conv = ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, dilation=1, groups=1, bias=bias, padding_mode=padding_mode, use_norm=use_norm, use_act=use_act)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.kernel_size = kernel_size
        self.dilation = dilation

    def __repr__(self):
        repr_str = '{}(in_channels={}, out_channels={}, kernel_size={}, stride={}, dilation={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.kernel_size, self.stride, self.dilation)
        return repr_str

    def forward(self, x: Tensor) ->Tensor:
        x = self.dw_conv(x)
        x = self.pw_conv(x)
        return x

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params, macs = 0.0, 0.0
        input, p, m = self.dw_conv.profile_module(input)
        params += p
        macs += m
        input, p, m = self.pw_conv.profile_module(input)
        params += p
        macs += m
        return input, params, macs


class Dropout(nn.Dropout):
    """
    This layer, during training, randomly zeroes some of the elements of the input tensor with probability `p`
    using samples from a Bernoulli distribution.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where :math:`N` is the batch size
        - Output: same as the input

    """

    def __init__(self, p: Optional[float]=0.5, inplace: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__(p=p, inplace=inplace)

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class Dropout2d(nn.Dropout2d):
    """
    This layer, during training, randomly zeroes some of the elements of the 4D input tensor with probability `p`
    using samples from a Bernoulli distribution.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the input channels,
            :math:`H` is the input tensor height, and :math:`W` is the input tensor width
        - Output: same as the input

    """

    def __init__(self, p: float=0.5, inplace: bool=False):
        super().__init__(p=p, inplace=inplace)

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        return input, 0.0, 0.0


class Embedding(nn.Embedding):
    """A lookup table that stores embeddings of a fixed dictionary and size.

    Args:
        num_embeddings (int): size of the dictionary of embeddings
        embedding_dim (int): the size of each embedding vector
        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                     i.e. it remains as a fixed "pad". For a newly constructed Embedding,
                                     the embedding vector at :attr:`padding_idx` will default to all zeros,
                                     but can be updated to another value to be used as the padding vector.

    Shape:
        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract
        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=	ext{embedding\\_dim}`
    """

    def __init__(self, opts, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int]=None, *args, **kwargs):
        super().__init__(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=padding_idx)

    def reset_parameters(self) ->None:
        nn.init.normal_(self.weight, mean=0, std=self.embedding_dim ** -0.5)
        if self.padding_idx is not None:
            nn.init.constant_(self.weight[self.padding_idx], 0)

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        return input, 0.0, 0.0


class Flatten(nn.Flatten):
    """
    This layer flattens a contiguous range of dimensions into a tensor.

    Args:
        start_dim (Optional[int]): first dim to flatten. Default: 1
        end_dim (Optional[int]): last dim to flatten. Default: -1

    Shape:
        - Input: :math:`(*, S_{	ext{start}},..., S_{i}, ..., S_{	ext{end}}, *)`,'
          where :math:`S_{i}` is the size at dimension :math:`i` and :math:`*` means any
          number of dimensions including none.
        - Output: :math:`(*, \\prod_{i=	ext{start}}^{	ext{end}} S_{i}, *)`.
    """

    def __init__(self, start_dim: Optional[int]=1, end_dim: Optional[int]=-1):
        super(Flatten, self).__init__(start_dim=start_dim, end_dim=end_dim)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        input = self.forward(input)
        return input, 0.0, 0.0


def module_profile(module, x: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
    """
    Helper function to profile a module.

    .. note::
        Module profiling is for reference only and may contain errors as it solely relies on user implementation to
        compute theoretical FLOPs
    """
    if isinstance(module, nn.Sequential):
        n_macs = n_params = 0.0
        for l in module:
            try:
                x, l_p, l_macs = l.profile_module(x)
                n_macs += l_macs
                n_params += l_p
            except Exception as e:
                None
                pass
    else:
        x, n_params, n_macs = module.profile_module(x)
    return x, n_params, n_macs


class LinearSelfAttention(BaseLayer):
    """
    This layer applies a self-attention with linear complexity, as described in `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ paper.
    This layer can be used for self- as well as cross-attention.

    Args:
        opts: command line arguments
        embed_dim (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        attn_dropout (Optional[float]): Dropout value for context scores. Default: 0.0
        bias (Optional[bool]): Use bias in learnable layers. Default: True

    Shape:
        - Input: :math:`(N, C, P, N)` where :math:`N` is the batch size, :math:`C` is the input channels,
        :math:`P` is the number of pixels in the patch, and :math:`N` is the number of patches
        - Output: same as the input

    .. note::
        For MobileViTv2, we unfold the feature map [B, C, H, W] into [B, C, P, N] where P is the number of pixels
        in a patch and N is the number of patches. Because channel is the first dimension in this unfolded tensor,
        we use point-wise convolution (instead of a linear layer). This avoids a transpose operation (which may be
        expensive on resource-constrained devices) that may be required to convert the unfolded tensor from
        channel-first to channel-last format in case of a linear layer.
    """

    def __init__(self, opts, embed_dim: int, attn_dropout: Optional[float]=0.0, bias: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__()
        self.qkv_proj = ConvLayer(opts=opts, in_channels=embed_dim, out_channels=1 + 2 * embed_dim, bias=bias, kernel_size=1, use_norm=False, use_act=False)
        self.attn_dropout = Dropout(p=attn_dropout)
        self.out_proj = ConvLayer(opts=opts, in_channels=embed_dim, out_channels=embed_dim, bias=bias, kernel_size=1, use_norm=False, use_act=False)
        self.embed_dim = embed_dim

    def __repr__(self):
        return '{}(embed_dim={}, attn_dropout={})'.format(self.__class__.__name__, self.embed_dim, self.attn_dropout.p)

    @staticmethod
    def visualize_context_scores(context_scores):
        batch_size, channels, num_pixels, num_patches = context_scores.shape
        assert batch_size == 1, 'For visualization purposes, use batch size of 1'
        assert channels == 1, 'The inner-product between input and latent node (query) is a scalar'
        up_scale_factor = int(num_pixels ** 0.5)
        patch_h = patch_w = int(context_scores.shape[-1] ** 0.5)
        context_scores = context_scores.reshape(1, num_pixels, patch_h, patch_w)
        context_map = F.pixel_shuffle(context_scores, upscale_factor=up_scale_factor)
        context_map = context_map.squeeze()
        min_val = torch.min(context_map)
        max_val = torch.max(context_map)
        context_map = (context_map - min_val) / (max_val - min_val)
        try:
            context_map = (context_map * 255).byte().cpu().numpy()
            context_map = cv2.resize(context_map, (80, 80), interpolation=cv2.INTER_NEAREST)
            colored_context_map = cv2.applyColorMap(context_map, cv2.COLORMAP_JET)
            res_dir_name = 'attn_res'
            if not os.path.isdir(res_dir_name):
                os.makedirs(res_dir_name)
            f_name = '{}/h_{}_w_{}_index_'.format(res_dir_name, patch_h, patch_w)
            files_cmap = glob('{}/h_{}_w_{}_index_*.png'.format(res_dir_name, patch_h, patch_w))
            idx = len(files_cmap)
            f_name += str(idx)
            cv2.imwrite('{}.png'.format(f_name), colored_context_map)
            return colored_context_map
        except ModuleNotFoundError as mnfe:
            None
            return context_map

    def _forward_self_attn(self, x: Tensor, *args, **kwargs) ->Tensor:
        qkv = self.qkv_proj(x)
        query, key, value = torch.split(qkv, split_size_or_sections=[1, self.embed_dim, self.embed_dim], dim=1)
        context_scores = F.softmax(query, dim=-1)
        context_scores = self.attn_dropout(context_scores)
        context_vector = key * context_scores
        context_vector = torch.sum(context_vector, dim=-1, keepdim=True)
        out = F.relu(value) * context_vector.expand_as(value)
        out = self.out_proj(out)
        return out

    def _forward_cross_attn(self, x: Tensor, x_prev: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        batch_size, in_dim, kv_patch_area, kv_num_patches = x.shape
        q_patch_area, q_num_patches = x.shape[-2:]
        assert kv_patch_area == q_patch_area, 'The number of pixels in a patch for query and key_value should be the same'
        qk = F.conv2d(x_prev, weight=self.qkv_proj.block.conv.weight[:self.embed_dim + 1, ...], bias=self.qkv_proj.block.conv.bias[:self.embed_dim + 1, ...])
        query, key = torch.split(qk, split_size_or_sections=[1, self.embed_dim], dim=1)
        value = F.conv2d(x, weight=self.qkv_proj.block.conv.weight[self.embed_dim + 1:, ...], bias=self.qkv_proj.block.conv.bias[self.embed_dim + 1:, ...])
        context_scores = F.softmax(query, dim=-1)
        context_scores = self.attn_dropout(context_scores)
        context_vector = key * context_scores
        context_vector = torch.sum(context_vector, dim=-1, keepdim=True)
        out = F.relu(value) * context_vector.expand_as(value)
        out = self.out_proj(out)
        return out

    def forward(self, x: Tensor, x_prev: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if x_prev is None:
            return self._forward_self_attn(x, *args, **kwargs)
        else:
            return self._forward_cross_attn(x, *args, x_prev=x_prev, **kwargs)

    def profile_module(self, input) ->Tuple[Tensor, float, float]:
        params = macs = 0.0
        qkv, p, m = module_profile(module=self.qkv_proj, x=input)
        params += p
        macs += m
        query, key, value = torch.split(qkv, split_size_or_sections=[1, self.embed_dim, self.embed_dim], dim=1)
        if self.out_proj is not None:
            out_p, p, m = module_profile(module=self.out_proj, x=value)
            params += p
            macs += m
        return input, params, macs


class LinearLayer(BaseLayer):
    """
    Applies a linear transformation to the input data

    Args:
        in_features (int): number of features in the input tensor
        out_features (int): number of features in the output tensor
        bias  (Optional[bool]): use bias or not
        channel_first (Optional[bool]): Channels are first or last dimension. If first, then use Conv2d

    Shape:
        - Input: :math:`(N, *, C_{in})` if not channel_first else :math:`(N, C_{in}, *)` where :math:`*` means any number of dimensions.
        - Output: :math:`(N, *, C_{out})` if not channel_first else :math:`(N, C_{out}, *)`

    """

    def __init__(self, in_features: int, out_features: int, bias: Optional[bool]=True, channel_first: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__()
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features)) if bias else None
        self.in_features = in_features
        self.out_features = out_features
        self.channel_first = channel_first
        self.reset_params()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        parser.add_argument('--model.layer.linear-init', type=str, default='xavier_uniform', help='Init type for linear layers')
        parser.add_argument('--model.layer.linear-init-std-dev', type=float, default=0.01, help='Std deviation for Linear layers')
        return parser

    def reset_params(self):
        if self.weight is not None:
            torch.nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            torch.nn.init.constant_(self.bias, 0)

    def forward(self, x: Tensor) ->Tensor:
        if self.channel_first:
            if not self.training:
                logger.error('Channel-first mode is only supported during inference')
            if x.dim() != 4:
                logger.error('Input should be 4D, i.e., (B, C, H, W) format')
            with torch.no_grad():
                return F.conv2d(input=x, weight=self.weight.clone().detach().reshape(self.out_features, self.in_features, 1, 1), bias=self.bias)
        else:
            x = F.linear(x, weight=self.weight, bias=self.bias)
        return x

    def __repr__(self):
        repr_str = '{}(in_features={}, out_features={}, bias={}, channel_first={})'.format(self.__class__.__name__, self.in_features, self.out_features, True if self.bias is not None else False, self.channel_first)
        return repr_str

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        out_size = list(input.shape)
        out_size[-1] = self.out_features
        params = sum([p.numel() for p in self.parameters()])
        macs = params
        output = torch.zeros(size=out_size, dtype=input.dtype, device=input.device)
        return output, params, macs


class GroupLinear(BaseLayer):
    """
    Applies a GroupLinear transformation layer, as defined `here <https://arxiv.org/abs/1808.09029>`_,
    `here <https://arxiv.org/abs/1911.12385>`_ and `here <https://arxiv.org/abs/2008.00623>`_

    Args:
        in_features (int): number of features in the input tensor
        out_features (int): number of features in the output tensor
        n_groups (int): number of groups
        bias (Optional[bool]): use bias or not
        feature_shuffle (Optional[bool]): Shuffle features between groups

    Shape:
        - Input: :math:`(N, *, C_{in})`
        - Output: :math:`(N, *, C_{out})`

    """

    def __init__(self, in_features: int, out_features: int, n_groups: int, bias: Optional[bool]=True, feature_shuffle: Optional[bool]=False, *args, **kwargs) ->None:
        if in_features % n_groups != 0:
            logger.error('Input dimensions ({}) must be divisible by n_groups ({})'.format(in_features, n_groups))
        if out_features % n_groups != 0:
            logger.error('Output dimensions ({}) must be divisible by n_groups ({})'.format(out_features, n_groups))
        in_groups = in_features // n_groups
        out_groups = out_features // n_groups
        super().__init__()
        self.weight = nn.Parameter(torch.Tensor(n_groups, in_groups, out_groups))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(n_groups, 1, out_groups))
        else:
            self.bias = None
        self.out_features = out_features
        self.in_features = in_features
        self.n_groups = n_groups
        self.feature_shuffle = feature_shuffle
        self.reset_params()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        parser.add_argument('--model.layer.group-linear-init', type=str, default='xavier_uniform', help='Init type for group linear layers')
        parser.add_argument('--model.layer.group-linear-init-std-dev', type=float, default=0.01, help='Std deviation for group linear layers')
        return parser

    def reset_params(self):
        if self.weight is not None:
            torch.nn.init.xavier_uniform_(self.weight.data)
        if self.bias is not None:
            torch.nn.init.constant_(self.bias.data, 0)

    def _forward(self, x: Tensor) ->Tensor:
        bsz = x.shape[0]
        x = x.reshape(bsz, self.n_groups, -1)
        x = x.transpose(0, 1)
        x = torch.bmm(x, self.weight)
        if self.bias is not None:
            x = torch.add(x, self.bias)
        if self.feature_shuffle:
            x = x.permute(1, 2, 0)
            x = x.reshape(bsz, self.n_groups, -1)
        else:
            x = x.transpose(0, 1)
        return x.reshape(bsz, -1)

    def forward(self, x: Tensor) ->Tensor:
        if x.dim() == 2:
            x = self._forward(x)
            return x
        else:
            in_dims = x.shape[:-1]
            n_elements = x.numel() // self.in_features
            x = x.reshape(n_elements, -1)
            x = self._forward(x)
            x = x.reshape(*in_dims, -1)
            return x

    def __repr__(self):
        repr_str = '{}(in_features={}, out_features={}, groups={}, bias={}, shuffle={})'.format(self.__class__.__name__, self.in_features, self.out_features, self.n_groups, True if self.bias is not None else False, self.feature_shuffle)
        return repr_str

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        macs = params
        out_size = list(input.shape)
        out_size[-1] = self.out_features
        output = torch.zeros(size=out_size, dtype=input.dtype, device=input.device)
        return output, params, macs


class MultiHeadAttention(BaseLayer):
    """
    This layer applies a multi-head self- or cross-attention as described in
    `Attention is all you need <https://arxiv.org/abs/1706.03762>`_ paper

    Args:
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, S, C_{in})`
        num_heads (int): Number of heads in multi-head attention
        attn_dropout (Optional[float]): Attention dropout. Default: 0.0
        bias (Optional[bool]): Use bias or not. Default: ``True``

    Shape:
        - Input:
           - Query tensor (x_q) :math:`(N, S, C_{in})` where :math:`N` is batch size, :math:`S` is number of source tokens,
        and :math:`C_{in}` is input embedding dim
           - Optional Key-Value tensor (x_kv) :math:`(N, T, C_{in})` where :math:`T` is number of target tokens
        - Output: same shape as the input

    """

    def __init__(self, embed_dim: int, num_heads: int, attn_dropout: Optional[float]=0.0, bias: Optional[bool]=True, output_dim: Optional[int]=None, coreml_compatible: Optional[bool]=False, *args, **kwargs) ->None:
        if output_dim is None:
            output_dim = embed_dim
        super().__init__()
        if embed_dim % num_heads != 0:
            logger.error('Embedding dim must be divisible by number of heads in {}. Got: embed_dim={} and num_heads={}'.format(self.__class__.__name__, embed_dim, num_heads))
        self.qkv_proj = LinearLayer(in_features=embed_dim, out_features=3 * embed_dim, bias=bias)
        self.attn_dropout = Dropout(p=attn_dropout)
        self.out_proj = LinearLayer(in_features=embed_dim, out_features=output_dim, bias=bias)
        self.head_dim = embed_dim // num_heads
        self.scaling = self.head_dim ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.coreml_compatible = coreml_compatible
        self.use_separate_proj_weight = embed_dim != output_dim

    def __repr__(self):
        return '{}(head_dim={}, num_heads={}, attn_dropout={})'.format(self.__class__.__name__, self.head_dim, self.num_heads, self.attn_dropout.p)

    def forward_tracing(self, x_q: Tensor, x_kv: Optional[Tensor]=None, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None) ->Tensor:
        if x_kv is None:
            qkv = self.qkv_proj(x_q)
            query, key, value = torch.chunk(qkv, chunks=3, dim=-1)
        else:
            query = F.linear(x_q, weight=self.qkv_proj.weight[:self.embed_dim, ...], bias=self.qkv_proj.bias[:self.embed_dim] if self.qkv_proj.bias is not None else None)
            kv = F.linear(x_kv, weight=self.qkv_proj.weight[self.embed_dim:, ...], bias=self.qkv_proj.bias[self.embed_dim:] if self.qkv_proj.bias is not None else None)
            key, value = torch.chunk(kv, chunks=2, dim=-1)
        query = query * self.scaling
        query = torch.chunk(query, chunks=self.num_heads, dim=-1)
        value = torch.chunk(value, chunks=self.num_heads, dim=-1)
        key = torch.chunk(key, chunks=self.num_heads, dim=-1)
        wt_out = []
        for h in range(self.num_heads):
            attn_h = torch.matmul(query[h], key[h].transpose(-1, -2))
            attn_h = self.softmax(attn_h)
            attn_h = self.attn_dropout(attn_h)
            out_h = torch.matmul(attn_h, value[h])
            wt_out.append(out_h)
        wt_out = torch.cat(wt_out, dim=-1)
        wt_out = self.out_proj(wt_out)
        return wt_out

    def forward_default(self, x_q: Tensor, x_kv: Optional[Tensor]=None, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None) ->Tensor:
        b_sz, S_len, in_channels = x_q.shape
        if x_kv is None:
            qkv = self.qkv_proj(x_q).reshape(b_sz, S_len, 3, self.num_heads, -1)
            qkv = qkv.transpose(1, 3).contiguous()
            query, key, value = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]
        else:
            T_len = x_kv.shape[1]
            query = F.linear(x_q, weight=self.qkv_proj.weight[:self.embed_dim, ...], bias=self.qkv_proj.bias[:self.embed_dim] if self.qkv_proj.bias is not None else None)
            query = query.reshape(b_sz, S_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
            kv = F.linear(x_kv, weight=self.qkv_proj.weight[self.embed_dim:, ...], bias=self.qkv_proj.bias[self.embed_dim:] if self.qkv_proj.bias is not None else None)
            kv = kv.reshape(b_sz, T_len, 2, self.num_heads, self.head_dim)
            kv = kv.transpose(1, 3).contiguous()
            key, value = kv[:, :, 0], kv[:, :, 1]
        query = query * self.scaling
        key = key.transpose(-1, -2)
        attn = torch.matmul(query, key)
        batch_size, num_heads, num_src_tokens, num_tgt_tokens = attn.shape
        if attn_mask is not None:
            assert list(attn_mask.shape) == [batch_size, num_src_tokens, num_tgt_tokens], 'Shape of attention mask should be [{}, {}, {}]. Got: {}'.format(batch_size, num_src_tokens, num_tgt_tokens, attn_mask.shape)
            attn_mask = attn_mask.unsqueeze(1)
            attn = attn + attn_mask
        if key_padding_mask is not None:
            assert key_padding_mask.dim() == 2 and list(key_padding_mask.shape) == [batch_size, num_tgt_tokens], 'Key_padding_mask should be 2-dimension with shape [{}, {}]. Got: {}'.format(batch_size, num_tgt_tokens, key_padding_mask.shape)
            attn = attn.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
        attn_dtype = attn.dtype
        attn_as_float = self.softmax(attn.float())
        attn = attn_as_float
        attn = self.attn_dropout(attn)
        out = torch.matmul(attn, value)
        out = out.transpose(1, 2).reshape(b_sz, S_len, -1)
        out = self.out_proj(out)
        return out

    def forward_pytorch(self, x_q: Tensor, x_kv: Optional[Tensor]=None, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None) ->Tensor:
        out, _ = F.multi_head_attention_forward(query=x_q, key=x_kv if x_kv is not None else x_q, value=x_kv if x_kv is not None else x_q, embed_dim_to_check=self.embed_dim, num_heads=self.num_heads, in_proj_weight=torch.empty([0]), in_proj_bias=self.qkv_proj.bias, bias_k=None, bias_v=None, add_zero_attn=False, dropout_p=self.attn_dropout.p, out_proj_weight=self.out_proj.weight, out_proj_bias=self.out_proj.bias, training=self.training, key_padding_mask=key_padding_mask, need_weights=False, attn_mask=attn_mask, use_separate_proj_weight=True, q_proj_weight=self.qkv_proj.weight[:self.embed_dim, ...], k_proj_weight=self.qkv_proj.weight[self.embed_dim:2 * self.embed_dim, ...], v_proj_weight=self.qkv_proj.weight[2 * self.embed_dim:, ...])
        return out

    def forward(self, x_q: Tensor, x_kv: Optional[Tensor]=None, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if self.coreml_compatible:
            return self.forward_tracing(x_q=x_q, x_kv=x_kv, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        elif kwargs.get('use_pytorch_mha', False):
            return self.forward_pytorch(x_q=x_q, x_kv=x_kv, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        else:
            return self.forward_default(x_q=x_q, x_kv=x_kv, key_padding_mask=key_padding_mask, attn_mask=attn_mask)

    def profile_module(self, input) ->Tuple[Tensor, float, float]:
        b_sz, seq_len, in_channels = input.shape
        params = macs = 0.0
        qkv, p, m = module_profile(module=self.qkv_proj, x=input)
        params += p
        macs += m * seq_len * b_sz
        m_qk = seq_len * seq_len * in_channels * b_sz
        macs += m_qk
        m_wt = seq_len * seq_len * in_channels * b_sz
        macs += m_wt
        out_p, p, m = module_profile(module=self.out_proj, x=input)
        params += p
        macs += m * seq_len * b_sz
        return input, params, macs


NORM_LAYER_CLS = []


def register_norm_fn(name):

    def register_fn(cls):
        if name in SUPPORTED_NORM_FNS:
            raise ValueError('Cannot register duplicate normalization function ({})'.format(name))
        SUPPORTED_NORM_FNS.append(name)
        NORM_LAYER_REGISTRY[name] = cls
        NORM_LAYER_CLS.append(cls)
        return cls
    return register_fn


class BatchNorm2d(nn.BatchNorm2d):
    """
    Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 4D input tensor

    Args:
        num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: same shape as the input
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class BatchNorm2dFP32(BatchNorm2d):
    """
    Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 4D input tensor in FP32
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(*args, num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats, **kwargs)

    def forward(self, input: Tensor) ->Tensor:
        inp_dtype = input.dtype
        return super().forward(input.to(torch.float32))

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class BatchNorm1d(nn.BatchNorm1d):
    """
    Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 2D or 3D input tensor

    Args:
        num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C)` or :math:`(N, C, L)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C)` or :math:`(N, C, L)` where :math:`N` is the batch size,
        :math:`C` is the number of input channels,  and :math:`L` is the sequence length
        - Output: same shape as the input
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class BatchNorm3d(nn.BatchNorm3d):

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        """
        Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 5D input tensor

        Args:
            num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C, D, H, W)`
            eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
            momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
            affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
            track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

        Shape:
            - Input: :math:`(N, C, D, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input
            channels, :math:`D` is the input depth, :math:`H` is the input height, and :math:`W` is the input width
            - Output: same shape as the input
        """
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class GroupNorm(nn.GroupNorm):
    """
    Applies a `Group Normalization <https://arxiv.org/abs/1803.08494>`_ over an input tensor

    Args:
        num_groups (int): number of groups to separate the input channels into
        num_features (int): :math:`C` from an expected input of size :math:`(N, C, *)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``

    Shape:
        - Input: :math:`(N, C, *)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        and :math:`*` is the remaining dimensions of the input tensor
        - Output: same shape as the input

    .. note::
        GroupNorm is the same as LayerNorm when `num_groups=1` and it is the same as InstanceNorm when
        `num_groups=C`.
    """

    def __init__(self, num_groups: int, num_features: int, eps: Optional[float]=1e-05, affine: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_groups=num_groups, num_channels=num_features, eps=eps, affine=affine)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class InstanceNorm2d(nn.InstanceNorm2d):
    """
    Applies a `Instance Normalization <https://arxiv.org/abs/1607.08022>`_ over a 4D input tensor

    Args:
        num_features (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: same shape as the input
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class InstanceNorm1d(nn.InstanceNorm1d):
    """
    Applies a `Instance Normalization <https://arxiv.org/abs/1607.08022>`_ over a 2D or 3D input tensor

    Args:
        num_features (int): :math:`C` from an expected input of size :math:`(N, C)` or :math:`(N, C, L)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C)` or :math:`(N, C, L)` where :math:`N` is the batch size, :math:`C` is the number
        of input channels,  and :math:`L` is the sequence length
    - Output: same shape as the input
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class LayerNorm(nn.LayerNorm):
    """
    Applies `Layer Normalization <https://arxiv.org/abs/1607.06450>`_ over a input tensor

    Args:
        normalized_shape (int or list or torch.Size): input shape from an expected input
            of size

            .. math::
                [* 	imes 	ext{normalized\\_shape}[0] 	imes 	ext{normalized\\_shape}[1]
                    	imes \\ldots 	imes 	ext{normalized\\_shape}[-1]]

            If a single integer is used, it is treated as a singleton list, and this module will
            normalize over the last dimension which is expected to be of that specific size.
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        elementwise_affine (bool): If ``True``, use learnable affine parameters. Default: ``True``

    Shape:
        - Input: :math:`(N, *)` where :math:`N` is the batch size
        - Output: same shape as the input
    """

    def __init__(self, normalized_shape: Union[int, List[int], Size], eps: Optional[float]=1e-05, elementwise_affine: Optional[bool]=True, *args, **kwargs):
        super().__init__(normalized_shape=normalized_shape, eps=eps, elementwise_affine=elementwise_affine)

    def forward(self, x: Tensor) ->Tensor:
        n_dim = x.ndim
        if x.shape[1] == self.normalized_shape[0] and n_dim > 2:
            s, u = torch.std_mean(x, dim=1, keepdim=True, unbiased=False)
            x = (x - u) / (s + self.eps)
            if self.weight is not None:
                n_dim = x.ndim - 2
                new_shape = [1, self.normalized_shape[0]] + [1] * n_dim
                x = torch.addcmul(input=self.bias.reshape(*[new_shape]), value=1.0, tensor1=x, tensor2=self.weight.reshape(*[new_shape]))
            return x
        elif x.shape[-1] == self.normalized_shape[0]:
            return super().forward(x)
        else:
            raise NotImplementedError('LayerNorm is supported for channel-first and channel-last format only')

    def profile_module(self, input: Tensor) ->(Tensor, float, float):
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class LayerNorm2D_NCHW(nn.GroupNorm):
    """
    Applies `Layer Normalization <https://arxiv.org/abs/1607.06450>`_ over a 4D input tensor

    Args:
        num_features (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        elementwise_affine (bool): If ``True``, use learnable affine parameters. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: same shape as the input
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, elementwise_affine: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_channels=num_features, eps=eps, affine=elementwise_affine, num_groups=1)
        self.num_channels = num_features

    def __repr__(self):
        return '{}(num_channels={}, eps={}, affine={})'.format(self.__class__.__name__, self.num_channels, self.eps, self.affine)

    def profile_module(self, input: Tensor) ->(Tensor, float, float):
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class LayerNormFP32(LayerNorm):
    """
    Applies `Layer Normalization <https://arxiv.org/abs/1607.06450>`_ over a input tensor with FP32 precision
    """

    def __init__(self, normalized_shape: Union[int, List[int], Size], eps: Optional[float]=1e-05, elementwise_affine: Optional[bool]=True, *args, **kwargs):
        super().__init__(*args, normalized_shape=normalized_shape, eps=eps, elementwise_affine=elementwise_affine, **kwargs)

    def forward(self, x: Tensor) ->Tensor:
        inp_dtype = x.dtype
        return super().forward(x.to(torch.float32))


class SyncBatchNorm(nn.SyncBatchNorm):
    """
    Applies a `Syncronized Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over the input tensor

    Args:
        num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C, *)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C, *)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`*` is the remaining input dimensions
        - Output: same shape as the input

    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class SyncBatchNormFP32(SyncBatchNorm):
    """
    Synchronized BN in FP32
    """

    def __init__(self, num_features: int, eps: Optional[float]=1e-05, momentum: Optional[float]=0.1, affine: Optional[bool]=True, track_running_stats: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        in_dtype = x.dtype
        return super().forward(x.to(dtype=torch.float))

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        params = sum([p.numel() for p in self.parameters()])
        return input, params, 0.0


class PixelShuffle(nn.PixelShuffle):
    """
    Rearranges elements in a tensor of shape :math:`(*, C 	imes r^2, H, W)`
    to a tensor of shape :math:`(*, C, H 	imes r, W 	imes r)`, where r is an upscale factor.

    Args:
        upscale_factor (int): factor to increase spatial resolution by

    Shape:
        - Input: :math:`(*, C 	imes r^2, H, W)`, where * is zero or more dimensions
        - Output: :math:`(*, C, H 	imes r, W 	imes r)`
    """

    def __init__(self, upscale_factor: int, *args, **kwargs) ->None:
        super(PixelShuffle, self).__init__(upscale_factor=upscale_factor)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        input = self.forward(input)
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(upscale_factor={})'.format(self.__class__.__name__, self.upscale_factor)


class MaxPool2d(nn.MaxPool2d):
    """
    Applies a 2D max pooling over a 4D input tensor.

    Args:
        kernel_size (Optional[int]): the size of the window to take a max over
        stride (Optional[int]): The stride of the window. Default: 2
        padding (Optional[int]): Padding to be added on both sides of the tensor. Default: 1

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})` where :math:`N` is the batch size, :math:`C` is the input channels,
            :math:`H_{in}` is the input height, and :math:`W_{in}` is the input width
        - Output: :math:`(N, C, H_{out}, W_{out})` where :math:`H_{out}` is the output height, and :math:`W_{in}` is
            the output width
    """

    def __init__(self, kernel_size: Optional[int]=3, stride: Optional[int]=2, padding: Optional[int]=1, *args, **kwargs) ->None:
        super().__init__(kernel_size=kernel_size, stride=stride, padding=padding)

    def profile_module(self, input: Tensor) ->(Tensor, float, float):
        input = self.forward(input)
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(kernel_size={}, stride={})'.format(self.__class__.__name__, self.kernel_size, self.stride)


class AvgPool2d(nn.AvgPool2d):
    """
    Applies a 2D average pooling over a 4D input tensor.

    Args:
        kernel_size (Optional[int]): the size of the window to take a max over
        stride (Optional[int]): The stride of the window. Default: 2
        padding (Optional[int]): Padding to be added on both sides of the tensor. Default: 1
        ceil_mode (Optional[bool]): When True, will use `ceil` instead of `floor` to compute the output shape. Default: False
        count_include_pad (Optional[bool]): When True, will include the zero-padding in the averaging calculation. Default: True
        divisor_override: if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})` where :math:`N` is the batch size, :math:`C` is the input channels,
            :math:`H_{in}` is the input height, and :math:`W_{in}` is the input width
        - Output: :math:`(N, C, H_{out}, W_{out})` where :math:`H_{out}` is the output height, and :math:`W_{in}` is
            the output width
    """

    def __init__(self, kernel_size: tuple, stride: Optional[tuple]=None, padding: Optional[tuple]=(0, 0), ceil_mode: Optional[bool]=False, count_include_pad: Optional[bool]=True, divisor_override: Optional[bool]=None):
        super(AvgPool2d, self).__init__(kernel_size=kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode, count_include_pad=count_include_pad, divisor_override=divisor_override)

    def profile_module(self, input: Tensor) ->(Tensor, float, float):
        input = self.forward(input)
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(upscale_factor={})'.format(self.__class__.__name__, self.upscale_factor)


class LearnablePositionalEmbedding(nn.Module):
    """Learnable Positional embedding"""

    def __init__(self, opts, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int]=None, sequence_first: Optional[bool]=False, interpolation_mode: Optional[str]='bilinear', *args, **kwargs):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.empty(1, 1, num_embeddings, embedding_dim))
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.padding_idx = padding_idx
        self.sequence_first = sequence_first
        self.interpolation_mode = interpolation_mode
        self.reset_parameters()

    def reset_parameters(self) ->None:
        nn.init.trunc_normal_(self.pos_embed, mean=0, std=self.embedding_dim ** -0.5)
        if self.padding_idx is not None:
            with torch.no_grad():
                self.pos_embed[:, :, self.padding_idx, ...] = 0.0

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        return input, 0.0, 0.0

    def forward(self, seq_len: int, *args, **kwargs) ->Tensor:
        pos_embed = self.pos_embed
        if self.padding_idx is not None:
            with torch.no_grad():
                pos_embed[:, :, self.padding_idx, ...] = 0.0
        if seq_len != self.num_embeddings:
            pos_embed = F.interpolate(pos_embed, size=(seq_len, self.embedding_dim), mode=self.interpolation_mode)
        if self.sequence_first:
            return pos_embed.reshape(seq_len, 1, self.embedding_dim)
        else:
            return pos_embed.reshape(1, seq_len, self.embedding_dim)

    def __repr__(self):
        return '{}(num_embeddings={}, embedding_dim={}, padding_idx={}, sequence_first={})'.format(self.__class__.__name__, self.num_embeddings, self.embedding_dim, self.padding_idx, self.sequence_first)


class SinusoidalPositionalEmbedding(nn.Module):

    def __init__(self, opts, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int]=None, sequence_first: Optional[bool]=False, interpolation_mode: Optional[str]='bilinear', *args, **kwargs):
        super().__init__()
        self.padding_idx = padding_idx
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.sequence_first = sequence_first
        self.interpolation_mode = interpolation_mode
        self.register_buffer('pos_embed', self.get_weights())

    def get_weights(self) ->Tensor:
        """Build sinusoidal embeddings. Adapted from Fairseq."""
        half_dim = self.embedding_dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
        emb = torch.arange(self.num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).reshape(self.num_embeddings, -1)
        if self.embedding_dim % 2 == 1:
            emb = torch.cat([emb, torch.zeros(self.num_embeddings, 1)], dim=1)
        if self.padding_idx is not None:
            emb[self.padding_idx, :] = 0
        return emb.unsqueeze(0).unsqueeze(0)

    def forward(self, seq_len: int, *args, **kwargs) ->Tensor:
        pos_embed = self.pos_embed
        if seq_len != self.num_embeddings:
            pos_embed = F.interpolate(pos_embed, size=(seq_len, self.embedding_dim), mode=self.interpolation_mode)
        if self.sequence_first:
            return pos_embed.reshape(seq_len, 1, self.embedding_dim)
        else:
            return pos_embed.reshape(1, seq_len, self.embedding_dim)

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(num_embeddings={}, embedding_dim={}, padding_idx={}, sequence_first={})'.format(self.__class__.__name__, self.num_embeddings, self.embedding_dim, self.padding_idx, self.sequence_first)


class PositionalEmbedding(BaseLayer):

    def __init__(self, opts, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int]=None, is_learnable: Optional[bool]=False, sequence_first: Optional[bool]=False, interpolation_mode: Optional[str]='bilinear', *args, **kwargs):
        super().__init__(*args, **kwargs)
        module = LearnablePositionalEmbedding if is_learnable else SinusoidalPositionalEmbedding
        self.pos_embed = module(opts, *args, num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=padding_idx, sequence_first=sequence_first, interpolation_mode=interpolation_mode, **kwargs)

    def forward(self, seq_len: int, *args, **kwargs) ->Tensor:
        return self.pos_embed(seq_len, *args, **kwargs)

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        return input, 0.0, 0.0

    def __repr__(self):
        return self.pos_embed.__repr__()


class SinusoidalPositionalEncoding(BaseLayer):
    """
    This layer adds sinusoidal positional embeddings to a 3D input tensor. The code has been adapted from
    `Pytorch tutorial <https://pytorch.org/tutorials/beginner/transformer_tutorial.html>`_

    Args:
        d_model (int): dimension of the input tensor
        dropout (Optional[float]): Dropout rate. Default: 0.0
        max_len (Optional[int]): Max. number of patches (or seq. length). Default: 5000
        channels_last (Optional[bool]): Channels dimension is the last in the input tensor

    Shape:
        - Input: :math:`(N, C, P)` or :math:`(N, P, C)` where :math:`N` is the batch size, :math:`C` is the embedding dimension,
            :math:`P` is the number of patches
        - Output: same shape as the input

    """

    def __init__(self, d_model: int, dropout: Optional[float]=0.0, max_len: Optional[int]=5000, channels_last: Optional[bool]=True, *args, **kwargs) ->None:
        position_last = not channels_last
        pos_encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pos_encoding[:, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 1::2] = torch.cos(position * div_term)
        pos_encoding = pos_encoding.unsqueeze(0)
        patch_dim = -2
        if position_last:
            pos_encoding = pos_encoding.transpose(1, 2)
            patch_dim = -1
        super().__init__()
        self.dropout = Dropout(p=dropout)
        self.patch_dim = patch_dim
        self.register_buffer('pe', pos_encoding)

    def forward_patch_last(self, x, indices: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if indices is None:
            x = x + self.pe[..., :x.shape[-1]]
        else:
            ndim = x.ndim
            repeat_size = [x.shape[0]] + [-1] * (ndim - 1)
            pe = self.pe.expand(repeat_size)
            selected_pe = torch.gather(pe, index=indices, dim=-1)
            x = x + selected_pe
        return self.dropout(x)

    def forward_others(self, x, indices: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if indices is None:
            x = x + self.pe[..., :x.shape[-2], :]
        else:
            ndim = x.ndim
            repeat_size = [x.shape[0]] + [-1] * (ndim - 1)
            pe = self.pe.expand(repeat_size)
            selected_pe = torch.gather(pe, index=indices, dim=-2)
            x = x + selected_pe
        return self.dropout(x)

    def forward(self, x, indices: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if self.patch_dim == -1:
            return self.forward_patch_last(x, indices=indices)
        else:
            return self.forward_others(x, indices=indices)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(dropout={})'.format(self.__class__.__name__, self.dropout.p)


class LearnablePositionEncoding(BaseLayer):
    """
    This layer adds learnable positional embeddings to a 3D input tensor.

    Args:
        embed_dim (int): dimension of the input tensor
        num_embeddings (int): number of input embeddings. This is similar to vocab size in NLP.
        dropout (Optional[float]): Dropout rate. Default: 0.0
        channels_last (Optional[bool]): Channels dimension is the last in the input tensor

    Shape:
        - Input: :math:`(N, *, C, P)` or :math:`(N, *, P, C)` where :math:`N` is the batch size, :math:`C` is the embedding dimension,
            :math:`P` is the number of patches
        - Output: same shape as the input

    """

    def __init__(self, embed_dim: int, num_embeddings: int, dropout: Optional[float]=0.0, channels_last: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__()
        self.pos_emb = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_dim)
        self.channel_last = channels_last
        self.dropout = Dropout(p=dropout)

    def forward(self, x, *args, **kwargs) ->Tensor:
        num_embeddings = x.shape[-2] if self.channel_last else x.shape[-1]
        posistions = torch.arange(num_embeddings, dtype=torch.int64, device=x.device)
        position_emb = self.pos_emb(posistions)
        position_emb = position_emb.expand_as(x)
        x = x + position_emb
        return self.dropout(x)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0

    def __repr__(self):
        return '{}(embed_dim={}, vocab_size={}, dropout={})'.format(self.__class__.__name__, self.pos_emb.embedding_dim, self.pos_emb.num_embeddings, self.dropout.p)


def bound_fn(min_val: Union[float, int], max_val: Union[float, int], value: Union[float, int]) ->Union[float, int]:
    return max(min_val, min(max_val, value))


class RandomApply(BaseLayer):
    """
    This layer randomly applies a list of modules during training.

    Args:
        module_list (List): List of modules
        keep_p (Optional[float]): Keep P modules from the list during training. Default: 0.8 (or 80%)
    """

    def __init__(self, module_list: List, keep_p: Optional[float]=0.8, *args, **kwargs) ->None:
        super().__init__()
        n_modules = len(module_list)
        self.module_list = module_list
        self.module_indexes = [i for i in range(1, n_modules)]
        k = int(round(n_modules * keep_p))
        self.keep_k = bound_fn(min_val=1, max_val=n_modules, value=k)

    def forward(self, x: Tensor) ->Tensor:
        if self.training:
            indexes = [0] + sorted(random.sample(self.module_indexes, k=self.keep_k))
            for idx in indexes:
                x = self.module_list[idx](x)
        else:
            for layer in self.module_list:
                x = layer(x)
        return x

    def profile_module(self, x, *args, **kwargs) ->Tuple[Tensor, float, float]:
        params, macs = 0.0, 0.0
        for layer in self.module_list:
            x, p, m = layer.profile_module(x)
            params += p
            macs += m
        return x, params, macs

    def __repr__(self):
        format_string = '{}(apply_k (N={})={}, '.format(self.__class__.__name__, len(self.module_list), self.keep_k)
        for layer in self.module_list:
            format_string += '\n\t {}'.format(layer)
        format_string += '\n)'
        return format_string


class SingleHeadAttention(BaseLayer):
    """
    This layer applies a single-head attention as described in `DeLighT <https://arxiv.org/abs/2008.00623>`_ paper

    Args:
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`
        attn_dropout (Optional[float]): Attention dropout. Default: 0.0
        bias (Optional[bool]): Use bias or not. Default: ``True``

    Shape:
        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,
        and :math:`C_{in}` is input embedding dim
        - Output: same shape as the input

    """

    def __init__(self, embed_dim: int, attn_dropout: Optional[float]=0.0, bias: Optional[bool]=True, *args, **kwargs) ->None:
        super().__init__()
        self.qkv_proj = LinearLayer(in_features=embed_dim, out_features=3 * embed_dim, bias=bias)
        self.attn_dropout = Dropout(p=attn_dropout)
        self.out_proj = LinearLayer(in_features=embed_dim, out_features=embed_dim, bias=bias)
        self.softmax = nn.Softmax(dim=-1)
        self.embed_dim = embed_dim
        self.scaling = self.embed_dim ** -0.5

    def __repr__(self) ->str:
        return '{}(embed_dim={}, attn_dropout={})'.format(self.__class__.__name__, self.embed_dim, self.attn_dropout.p)

    def forward(self, x: Tensor, x_kv: Optional[Tensor]=None, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if x_kv is None:
            qkv = self.qkv_proj(x)
            query, key, value = torch.chunk(qkv, chunks=3, dim=-1)
        else:
            query = F.linear(x, weight=self.qkv_proj.weight[:self.embed_dim, ...], bias=self.qkv_proj.bias[:self.embed_dim])
            kv = F.linear(x_kv, weight=self.qkv_proj.weight[self.embed_dim:, ...], bias=self.qkv_proj.bias[self.embed_dim:])
            key, value = torch.chunk(kv, chunks=2, dim=-1)
        query = query * self.scaling
        key = key.transpose(-2, -1)
        attn = torch.matmul(query, key)
        if attn_mask is not None:
            assert list(attn_mask.shape) == list(attn.shape), 'Shape of attention mask and attn should be the same. Got: {} and {}'.format(attn_mask.shape, attn.shape)
            attn = attn + attn_mask
        if key_padding_mask is not None:
            batch_size, num_src_tokens, num_tgt_tokens = attn.shape
            assert key_padding_mask.dim() == 2 and list(key_padding_mask.shape) == [batch_size, num_tgt_tokens], 'Key_padding_mask should be 2-dimension with shape [{}, {}]. Got: {}'.format(batch_size, num_tgt_tokens, key_padding_mask.shape)
            attn = attn.masked_fill(key_padding_mask.unsqueeze(1), float('-inf'))
        attn = self.softmax(attn)
        attn = self.attn_dropout(attn)
        out = torch.matmul(attn, value)
        out = self.out_proj(out)
        return out

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        b_sz, seq_len, in_channels = input.shape
        params = macs = 0.0
        qkv, p, m = module_profile(module=self.qkv_proj, x=input)
        params += p
        macs += m * seq_len * b_sz
        m_qk = seq_len * in_channels * in_channels * b_sz
        macs += m_qk
        m_wt = seq_len * in_channels * in_channels * b_sz
        macs += m_wt
        out_p, p, m = module_profile(module=self.out_proj, x=input)
        params += p
        macs += m * seq_len * b_sz
        return input, params, macs


class Softmax(nn.Softmax):
    """
    Applies the Softmax function to an input tensor along the specified dimension

    Args:
        dim (int): Dimension along which softmax to be applied. Default: -1

    Shape:
        - Input: :math:`(*)` where :math:`*` is one or more dimensions
        - Output: same shape as the input
    """

    def __init__(self, dim: Optional[int]=-1, *args, **kwargs):
        super().__init__(dim=dim)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class UpSample(nn.Upsample):
    """
    This layer upsamples a given input tensor.

    Args:
        size (Optional[Union[int, Tuple[int, ...]]): Output spatial size. Default: None
        scale_factor (Optional[float]): Scale each spatial dimension of the input by this factor. Default: None
        mode (Optional[str]): Upsampling algorithm (``'nearest'``, ``'linear'``, ``'bilinear'``, ``'bicubic'`` and ``'trilinear'``. Default: ``'nearest'``
        align_corners (Optional[bool]): if ``True``, the corner pixels of the input and output tensors are aligned, and thus preserving the values at
            those pixels. This only has effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'``, or ``'trilinear'``.
            Default: ``None``

    Shape:
        - Input: :math:`(N, C, W_{in})` or :math:`(N, C, H_{in}, W_{in})` or :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, W_{out})` or :math:`(N, C, H_{out}, W_{out})` or :math:`(N, C, D_{out}, H_{out}, W_{out})`
    """

    def __init__(self, size: Optional[Union[int, Tuple[int, ...]]]=None, scale_factor: Optional[float]=None, mode: Optional[str]='nearest', align_corners: Optional[bool]=None, *args, **kwargs) ->None:
        super().__init__(size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        input = self.forward(input)
        return input, 0.0, 0.0


class UniformSampler(nn.Module):

    def __init__(self, low: float, high: float, min_fn: Optional[nn.Module]=Identity(), max_fn: Optional[nn.Module]=Identity(), *args, **kwargs):
        super().__init__()
        self._low = nn.Parameter(torch.tensor(low, dtype=torch.float))
        self._high = nn.Parameter(torch.tensor(high, dtype=torch.float))
        self.min_fn = min_fn
        self.max_fn = max_fn

    def forward(self, sample_shape=(), data_type=torch.float, device=torch.device('cpu')) ->Tensor:
        rand_tensor = torch.rand(sample_shape, dtype=data_type, device=device)
        return self.low + rand_tensor * (self.high - self.low)

    @property
    def high(self):
        return self.max_fn(self._high)

    @property
    def low(self):
        return self.min_fn(self._low)

    def __repr__(self):
        return '{}(min_fn={}, max_fn={})'.format(self.__class__.__name__, self.min_fn, self.max_fn)


_distribution_tuple = UniformSampler,


def random_brightness(x: Tensor, magnitude: Tensor, *args, **kwargs) ->Tensor:
    """
    Brightness function.
    """
    x = x * magnitude
    return x


def random_contrast(x: Tensor, magnitude: Tensor, *args, **kwargs) ->Tensor:
    per_channel_mean = torch.mean(x, dim=[-1, -2], keepdim=True)
    x = (1.0 - magnitude) * per_channel_mean + x * magnitude
    return x


def random_noise(x: Tensor, variance: Tensor, *args, **kwargs) ->Tensor:
    """Apply random noise sampled."""
    noise = torch.randn_like(x) * variance
    x = x + noise
    return x


class BaseNeuralAugmentor(nn.Module):
    """
    Base class for `neural (or range) augmentation <https://arxiv.org/abs/2212.10553>`_
    """

    def __init__(self, opts, *args, **kwargs):
        super().__init__()
        self.opts = opts
        self.lr_multiplier = getattr(opts, 'model.learn_augmentation.lr_multiplier', 1.0)
        self.brightness = None
        self.contrast = None
        self.noise = None
        self.aug_fns = []

    def _is_valid_aug_fn_list(self, aug_fns):
        if self.training:
            if len(aug_fns) == 0:
                logger.error('{} needs at least one learnable function.'.format(self.__class__.__name__))

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        """Get trainable parameters"""
        param_list = parameter_list(named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias)
        return param_list, [self.lr_multiplier] * len(param_list)

    def __repr__(self):
        aug_str = '{}('.format(self.__class__.__name__)
        if self.brightness is not None:
            aug_str += '\n\tBrightness={}, '.format(self.brightness.data.shape if isinstance(self.brightness, nn.Parameter) else self.brightness)
        if self.contrast is not None:
            aug_str += '\n\tContrast={}, '.format(self.contrast.data.shape if isinstance(self.contrast, nn.Parameter) else self.contrast)
        if self.noise is not None:
            aug_str += '\n\tNoise={}, '.format(self.noise.data.shape if isinstance(self.noise, nn.Parameter) else self.noise)
        aug_str += self.extra_repr()
        aug_str += ')'
        return aug_str

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        """Add model-specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.learn-augmentation.mode', type=str, default=None, choices=['basic', 'distribution'], help='Neural augmentation mode')
        group.add_argument('--model.learn-augmentation.brightness', action='store_true', help='Learn parameters for brightness')
        group.add_argument('--model.learn-augmentation.contrast', action='store_true', help='Learn parameters for contrast')
        group.add_argument('--model.learn-augmentation.noise', action='store_true', help='Learn parameters for noise')
        group.add_argument('--model.learn-augmentation.lr-multiplier', type=float, default=1.0, help='LR multiplier for neural aug parameters')
        return parser

    def _build_aug_fns(self, opts) ->List:
        raise NotImplementedError

    def _apply_brightness(self, x: Tensor, *args, **kwargs) ->Tensor:
        """
        Apply brightness augmentation function with learnable parameters.
        """
        x_shape = [*x.shape]
        x_shape[1:] = [1] * (len(x_shape) - 1)
        if isinstance(self.brightness, nn.Parameter):
            magnitude = self.brightness
        elif isinstance(self.brightness, _distribution_tuple):
            magnitude = self.brightness(x_shape, device=x.device, data_type=x.dtype)
        else:
            raise NotImplementedError
        return random_brightness(x, magnitude, *args, **kwargs)

    def _apply_contrast(self, x: Tensor, *args, **kwargs) ->Tensor:
        """
        Apply contrast augmentation function with learnable parameters.
        """
        x_shape = [*x.shape]
        x_shape[1:] = [1] * (len(x_shape) - 1)
        if isinstance(self.contrast, nn.Parameter):
            magnitude = self.contrast
        elif isinstance(self.contrast, _distribution_tuple):
            magnitude = self.contrast(x_shape, device=x.device, data_type=x.dtype)
        else:
            raise NotImplementedError
        return random_contrast(x, magnitude, *args, *kwargs)

    def _apply_noise(self, x: Tensor, *args, **kwargs) ->Tensor:
        x_shape = [*x.shape]
        x_shape[1:] = [1] * (len(x_shape) - 1)
        if isinstance(self.noise, nn.Parameter):
            variance = self.noise
        elif isinstance(self.noise, _distribution_tuple):
            variance = self.noise(x_shape, device=x.device, data_type=x.dtype)
        else:
            raise NotImplementedError
        return random_noise(x, variance, *args, *kwargs)

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        batch_size, in_channels, in_height, in_width = x.shape
        n_aug_samples = max(1, batch_size // 2)
        random.shuffle(self.aug_fns)
        for aug_fn in self.aug_fns:
            sample_ids = torch.randperm(n=batch_size, dtype=torch.long, device=x.device)[:n_aug_samples]
            x_aug = torch.index_select(x, dim=0, index=sample_ids)
            x_aug = aug_fn(x=x_aug)
            x = torch.index_copy(x, dim=0, source=x_aug, index=sample_ids)
        x = torch.clip(x, min=0.0, max=1.0)
        return x


class Clip(nn.Module):

    def __init__(self, min_val: float, max_val: float, hard_clip: Optional[bool]=False, *args, **kwargs) ->None:
        super().__init__()
        self.min_val = min_val
        self.max_val = max_val
        self.hard_clip = hard_clip

    def forward(self, x: Any) ->Any:
        if self.hard_clip:
            with torch.no_grad():
                return x.clamp_(min=self.min_val, max=self.max_val)
        else:
            return torch.sigmoid(x) * (self.max_val - self.min_val) + self.min_val

    def __repr__(self):
        return '{}(min={}, max={}, clipping={})'.format(self.__class__.__name__, self.min_val, self.max_val, 'hard' if self.hard_clip else 'soft')


class FixedSampler(nn.Module):

    def __init__(self, value: float, clip_fn: Optional[nn.Module]=Identity(), *args, **kwargs):
        super().__init__()
        self._value = nn.Parameter(torch.FloatTensor(1, 3, 1, 1).fill_(value))
        self.clip_fn = clip_fn

    def forward(self, sample_shape=(), data_type=torch.float, device=torch.device('cpu')) ->Tensor:
        return self.clip_fn(self._value)

    def __repr__(self):
        return '{}(clip_fn={})'.format(self.__class__.__name__, self.clip_fn)


class BasicNeuralAugmentor(BaseNeuralAugmentor):
    """
    Basic neural augmentation. This class learns per-channel augmentation parameters
    and apply the same parameter to all images in a batch.

    See `neural (or range) augmentation <https://arxiv.org/abs/2212.10553>`_ paper for details.
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(*args, opts=opts, **kwargs)
        aug_fns = self._build_aug_fns(opts=opts)
        self._is_valid_aug_fn_list(aug_fns)
        self.aug_fns = aug_fns

    def _build_aug_fns(self, opts) ->List:
        aug_fns = []
        if getattr(opts, 'model.learn_augmentation.brightness', False):
            self.brightness = FixedSampler(value=1.0, clip_fn=Clip(min_val=0.1, max_val=10.0))
            aug_fns.append(self._apply_brightness)
        if getattr(opts, 'model.learn_augmentation.contrast', False):
            self.contrast = FixedSampler(value=1.0, clip_fn=Clip(min_val=0.1, max_val=10.0))
            aug_fns.append(self._apply_contrast)
        if getattr(opts, 'model.learn_augmentation.noise', False):
            self.noise = FixedSampler(value=0.0, clip_fn=Clip(min_val=0.0, max_val=1.0))
            aug_fns.append(self._apply_noise)
        return aug_fns


class DistributionNeuralAugmentor(BaseNeuralAugmentor):
    """
    Distribution-based neural (or range) augmentation. This class samples the augmentation parameters
    from a specified distribution with learnable range.

    See `neural (or range) augmentation <https://arxiv.org/abs/2212.10553>`_ paper for details.
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(*args, opts=opts, **kwargs)
        aug_fns = self._build_aug_fns_with_uniform_dist(opts=opts)
        self._is_valid_aug_fn_list(aug_fns)
        self.aug_fns = aug_fns

    def _build_aug_fns_with_uniform_dist(self, opts) ->List:
        aug_fns = []
        if getattr(opts, 'model.learn_augmentation.brightness', False):
            self.brightness = UniformSampler(low=0.5, high=1.5, min_fn=Clip(min_val=0.1, max_val=0.9), max_fn=Clip(min_val=1.1, max_val=10.0))
            aug_fns.append(self._apply_brightness)
        if getattr(opts, 'model.learn_augmentation.contrast', False):
            self.contrast = UniformSampler(low=0.5, high=1.5, min_fn=Clip(min_val=0.1, max_val=0.9), max_fn=Clip(min_val=1.1, max_val=10.0))
            aug_fns.append(self._apply_contrast)
        if getattr(opts, 'model.learn_augmentation.noise', False):
            self.noise = UniformSampler(low=0.0, high=0.1, min_fn=Clip(min_val=0.0, max_val=5e-05), max_fn=Clip(min_val=0.0001, max_val=1.0))
            aug_fns.append(self._apply_noise)
        return aug_fns


def build_neural_augmentor(opts, *args, **kwargs):
    mode = getattr(opts, 'model.learn_augmentation.mode', None)
    if mode is None:
        mode = 'none'
    mode = mode.lower()
    if mode == 'distribution':
        return DistributionNeuralAugmentor(*args, opts=opts, **kwargs)
    elif mode == 'basic':
        return BasicNeuralAugmentor(*args, opts=opts, **kwargs)
    else:
        return None


supported_conv_inits = ['kaiming_normal', 'kaiming_uniform', 'xavier_normal', 'xavier_uniform', 'normal', 'trunc_normal']


def _init_nn_layers(module, init_method: Optional[str]='kaiming_normal', std_val: Optional[float]=None) ->None:
    """
    Helper function to initialize neural network module
    """
    init_method = init_method.lower()
    if init_method == 'kaiming_normal':
        if module.weight is not None:
            nn.init.kaiming_normal_(module.weight, mode='fan_out')
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'kaiming_uniform':
        if module.weight is not None:
            nn.init.kaiming_uniform_(module.weight, mode='fan_out')
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'xavier_normal':
        if module.weight is not None:
            nn.init.xavier_normal_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'xavier_uniform':
        if module.weight is not None:
            nn.init.xavier_uniform_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'normal':
        if module.weight is not None:
            std = 1.0 / module.weight.size(1) ** 0.5 if std_val is None else std_val
            nn.init.normal_(module.weight, mean=0.0, std=std)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'trunc_normal':
        if module.weight is not None:
            std = 1.0 / module.weight.size(1) ** 0.5 if std_val is None else std_val
            nn.init.trunc_normal_(module.weight, mean=0.0, std=std)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    else:
        supported_conv_message = 'Supported initialization methods are:'
        for i, l in enumerate(supported_conv_inits):
            supported_conv_message += '\n \t {}) {}'.format(i, l)
        logger.error('{} \n Got: {}'.format(supported_conv_message, init_method))


def initialize_fc_layer(module, init_method: Optional[str]='normal', std_val: Optional[float]=0.01) ->None:
    """Helper function to initialize fully-connected layers"""
    if hasattr(module, 'layer'):
        _init_nn_layers(module=module.layer, init_method=init_method, std_val=std_val)
    else:
        _init_nn_layers(module=module, init_method=init_method, std_val=std_val)


def initialize_conv_layer(module, init_method: Optional[str]='kaiming_normal', std_val: Optional[float]=0.01) ->None:
    """Helper function to initialize convolution layers"""
    _init_nn_layers(module=module, init_method=init_method, std_val=std_val)


def initialize_norm_layers(module) ->None:
    """Helper function to initialize normalization layers"""

    def _init_fn(module):
        if hasattr(module, 'weight') and module.weight is not None:
            nn.init.ones_(module.weight)
        if hasattr(module, 'bias') and module.bias is not None:
            nn.init.zeros_(module.bias)
    _init_fn(module.layer) if hasattr(module, 'layer') else _init_fn(module=module)


norm_layers_tuple = tuple(NORM_LAYER_CLS)


def initialize_weights(opts, modules) ->None:
    """Helper function to initialize differnet layers in a model"""
    conv_init_type = getattr(opts, 'model.layer.conv_init', 'kaiming_normal')
    linear_init_type = getattr(opts, 'model.layer.linear_init', 'normal')
    conv_std = getattr(opts, 'model.layer.conv_init_std_dev', None)
    linear_std = getattr(opts, 'model.layer.linear_init_std_dev', 0.01)
    group_linear_std = getattr(opts, 'model.layer.group_linear_init_std_dev', 0.01)
    if isinstance(modules, nn.Sequential):
        for m in modules:
            if isinstance(m, (nn.Conv2d, nn.Conv3d)):
                initialize_conv_layer(module=m, init_method=conv_init_type, std_val=conv_std)
            elif isinstance(m, norm_layers_tuple):
                initialize_norm_layers(module=m)
            elif isinstance(m, (nn.Linear, LinearLayer)):
                initialize_fc_layer(module=m, init_method=linear_init_type, std_val=linear_std)
            elif isinstance(m, GroupLinear):
                initialize_fc_layer(module=m, init_method=linear_init_type, std_val=group_linear_std)
    elif isinstance(modules, (nn.Conv2d, nn.Conv3d)):
        initialize_conv_layer(module=modules, init_method=conv_init_type, std_val=conv_std)
    elif isinstance(modules, norm_layers_tuple):
        initialize_norm_layers(module=modules)
    elif isinstance(modules, (nn.Linear, LinearLayer)):
        initialize_fc_layer(module=modules, init_method=linear_init_type, std_val=linear_std)
    elif isinstance(modules, GroupLinear):
        initialize_fc_layer(module=modules, init_method=linear_init_type, std_val=group_linear_std)


class BaseEncoder(nn.Module):
    """
    Base class for different classification models
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__()
        self.conv_1 = None
        self.layer_1 = None
        self.layer_2 = None
        self.layer_3 = None
        self.layer_4 = None
        self.layer_5 = None
        self.conv_1x1_exp = None
        self.classifier = None
        self.round_nearest = 8
        self.dilation = 1
        output_stride = kwargs.get('output_stride', None)
        self.dilate_l4 = False
        self.dilate_l5 = False
        if output_stride == 8:
            self.dilate_l4 = True
            self.dilate_l5 = True
        elif output_stride == 16:
            self.dilate_l5 = True
        self.model_conf_dict = dict()
        self.neural_augmentor = build_neural_augmentor(*args, opts=opts, **kwargs)
        self.gradient_checkpointing = getattr(opts, 'model.classification.gradient_checkpointing', False)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        """Add model-specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.classifier-dropout', type=float, default=0.0, help='Dropout rate in classifier')
        group.add_argument('--model.classification.name', type=str, default=None, help='Model name')
        group.add_argument('--model.classification.n-classes', type=int, default=1000, help='Number of classes in the dataset')
        group.add_argument('--model.classification.pretrained', type=str, default=None, help='Path of the pretrained backbone')
        group.add_argument('--model.classification.freeze-batch-norm', action='store_true', help='Freeze batch norm layers')
        group.add_argument('--model.classification.activation.name', default=None, type=str, help='Non-linear function name (e.g., relu)')
        group.add_argument('--model.classification.activation.inplace', action='store_true', help='Inplace non-linear functions')
        group.add_argument('--model.classification.activation.neg-slope', default=0.1, type=float, help='Negative slope in leaky relu')
        group.add_argument('--model.classification.finetune-pretrained-model', action='store_true', help='Finetune a pretrained model')
        group.add_argument('--model.classification.n-pretrained-classes', type=int, default=None, help='Number of pre-trained classes')
        group.add_argument('--model.classification.gradient-checkpointing', action='store_true', help='Checkpoint output of each spatial level in the classification backbone. Note thatwe only take care of checkpointing in {}. If custom forward functions are used, pleaseimplement checkpointing accordingly')
        return parser

    def check_model(self):
        assert self.model_conf_dict, 'Model configuration dictionary should not be empty'
        assert self.conv_1 is not None, 'Please implement self.conv_1'
        assert self.layer_1 is not None, 'Please implement self.layer_1'
        assert self.layer_2 is not None, 'Please implement self.layer_2'
        assert self.layer_3 is not None, 'Please implement self.layer_3'
        assert self.layer_4 is not None, 'Please implement self.layer_4'
        assert self.layer_5 is not None, 'Please implement self.layer_5'
        assert self.conv_1x1_exp is not None, 'Please implement self.conv_1x1_exp'
        assert self.classifier is not None, 'Please implement self.classifier'

    def reset_parameters(self, opts):
        """Initialize model weights"""
        initialize_weights(opts=opts, modules=self.modules())

    def update_classifier(self, opts, n_classes: int) ->None:
        """
        This function updates the classification layer in a model. Useful for finetuning purposes.
        """
        linear_init_type = getattr(opts, 'model.layer.linear_init', 'normal')
        if isinstance(self.classifier, nn.Sequential):
            in_features = self.classifier[-1].in_features
            layer = LinearLayer(in_features=in_features, out_features=n_classes, bias=True)
            initialize_fc_layer(layer, init_method=linear_init_type)
            self.classifier[-1] = layer
        else:
            in_features = self.classifier.in_features
            layer = LinearLayer(in_features=in_features, out_features=n_classes, bias=True)
            initialize_fc_layer(layer, init_method=linear_init_type)
            head_init_scale = 0.001
            layer.weight.data.mul_(head_init_scale)
            layer.bias.data.mul_(head_init_scale)
            self.classifier = layer

    def _forward_layer(self, layer: nn.Module, x: Tensor) ->Tensor:
        return gradient_checkpoint_fn(layer, x) if self.gradient_checkpointing else layer(x)

    def extract_end_points_all(self, x: Tensor, use_l5: Optional[bool]=True, use_l5_exp: Optional[bool]=False, *args, **kwargs) ->Dict[str, Tensor]:
        out_dict = {}
        if self.training and self.neural_augmentor is not None:
            x = self.neural_augmentor(x)
            out_dict['augmented_tensor'] = x
        x = self._forward_layer(self.conv_1, x)
        x = self._forward_layer(self.layer_1, x)
        out_dict['out_l1'] = x
        x = self._forward_layer(self.layer_2, x)
        out_dict['out_l2'] = x
        x = self._forward_layer(self.layer_3, x)
        out_dict['out_l3'] = x
        x = self._forward_layer(self.layer_4, x)
        out_dict['out_l4'] = x
        if use_l5:
            x = self._forward_layer(self.layer_5, x)
            out_dict['out_l5'] = x
            if use_l5_exp:
                x = self._forward_layer(self.conv_1x1_exp, x)
                out_dict['out_l5_exp'] = x
        return out_dict

    def extract_end_points_l4(self, x: Tensor, *args, **kwargs) ->Dict[str, Tensor]:
        return self.extract_end_points_all(x, use_l5=False)

    def _extract_features(self, x: Tensor, *args, **kwargs) ->Tensor:
        x = self._forward_layer(self.conv_1, x)
        x = self._forward_layer(self.layer_1, x)
        x = self._forward_layer(self.layer_2, x)
        x = self._forward_layer(self.layer_3, x)
        x = self._forward_layer(self.layer_4, x)
        x = self._forward_layer(self.layer_5, x)
        x = self._forward_layer(self.conv_1x1_exp, x)
        return x

    def _forward_classifier(self, x: Tensor, *args, **kwargs) ->Tensor:
        x = self._extract_features(x)
        x = self.classifier(x)
        return x

    def forward(self, x: Any, *args, **kwargs) ->Any:
        if self.neural_augmentor is not None:
            if self.training:
                x_aug = self.neural_augmentor(x)
                prediction = self._forward_classifier(x_aug)
                out_dict = {'augmented_tensor': x_aug, 'logits': prediction}
            else:
                out_dict = {'augmented_tensor': None, 'logits': self._forward_classifier(x)}
            return out_dict
        else:
            x = self._forward_classifier(x, *args, **kwargs)
            return x

    def freeze_norm_layers(self) ->None:
        """Freeze normalization layers"""
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        """Get trainable parameters"""
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    @staticmethod
    def _profile_layers(layers, input, overall_params, overall_macs, *args, **kwargs) ->Tuple[Tensor, float, float]:
        if not isinstance(layers, list):
            layers = [layers]
        for layer in layers:
            if layer is None:
                continue
            input, layer_param, layer_macs = module_profile(module=layer, x=input)
            overall_params += layer_param
            overall_macs += layer_macs
            if isinstance(layer, nn.Sequential):
                module_name = '\n+'.join([l.__class__.__name__ for l in layer])
            else:
                module_name = layer.__class__.__name__
            None
            logger.singe_dash_line()
        return input, overall_params, overall_macs

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        img_channels = 3
        height = 224
        width = 224
        n_labels = 10
        img_tensor = torch.randn(batch_size, img_channels, height, width, dtype=torch.float)
        label_tensor = torch.randint(low=0, high=n_labels, size=(batch_size,)).long()
        return {'samples': img_tensor, 'targets': label_tensor}

    def profile_model(self, input: Tensor, is_classification: Optional[bool]=True, *args, **kwargs) ->Tuple[Union[Tensor, Dict[str, Tensor]], float, float]:
        """
        Helper function to profile a model.

        .. note::
            Model profiling is for reference only and may contain errors as it solely relies on user implementation to
            compute theoretical FLOPs
        """
        overall_params, overall_macs = 0.0, 0.0
        input_fvcore = input.clone()
        if is_classification:
            logger.log('Model statistics for an input of size {}'.format(input.size()))
            logger.double_dash_line(dashes=65)
            None
            logger.double_dash_line(dashes=65)
        out_dict = {}
        input, overall_params, overall_macs = self._profile_layers([self.conv_1, self.layer_1], input=input, overall_params=overall_params, overall_macs=overall_macs)
        out_dict['out_l1'] = input
        input, overall_params, overall_macs = self._profile_layers(self.layer_2, input=input, overall_params=overall_params, overall_macs=overall_macs)
        out_dict['out_l2'] = input
        input, overall_params, overall_macs = self._profile_layers(self.layer_3, input=input, overall_params=overall_params, overall_macs=overall_macs)
        out_dict['out_l3'] = input
        input, overall_params, overall_macs = self._profile_layers(self.layer_4, input=input, overall_params=overall_params, overall_macs=overall_macs)
        out_dict['out_l4'] = input
        input, overall_params, overall_macs = self._profile_layers(self.layer_5, input=input, overall_params=overall_params, overall_macs=overall_macs)
        out_dict['out_l5'] = input
        if self.conv_1x1_exp is not None:
            input, overall_params, overall_macs = self._profile_layers(self.conv_1x1_exp, input=input, overall_params=overall_params, overall_macs=overall_macs)
            out_dict['out_l5_exp'] = input
        if is_classification:
            classifier_params, classifier_macs = 0.0, 0.0
            if self.classifier is not None:
                input, classifier_params, classifier_macs = module_profile(module=self.classifier, x=input)
                None
            overall_params += classifier_params
            overall_macs += classifier_macs
            logger.double_dash_line(dashes=65)
            None
            overall_params_py = sum([p.numel() for p in self.parameters()])
            None
            None
            try:
                flop_analyzer = FlopCountAnalysis(self.eval(), input_fvcore)
                flop_analyzer.unsupported_ops_warnings(False)
                flop_analyzer.uncalled_modules_warnings(False)
                flops_fvcore = flop_analyzer.total()
                None
                None
            except Exception:
                pass
            None
            logger.double_dash_line(dashes=65)
        return out_dict, overall_params, overall_macs


class BaseModule(nn.Module):
    """Base class for all modules"""

    def __init__(self, *args, **kwargs):
        super(BaseModule, self).__init__()

    def forward(self, x: Any, *args, **kwargs) ->Any:
        raise NotImplementedError

    def profile_module(self, input: Any, *args, **kwargs) ->Tuple[Any, float, float]:
        raise NotImplementedError

    def __repr__(self):
        return '{}'.format(self.__class__.__name__)


def make_divisible(v: Union[float, int], divisor: Optional[int]=8, min_value: Optional[Union[float, int]]=None) ->Union[float, int]:
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    :param v:
    :param divisor:
    :param min_value:
    :return:
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class SqueezeExcitation(BaseModule):
    """
    This class defines the Squeeze-excitation module, in the `SENet paper <https://arxiv.org/abs/1709.01507>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        squeeze_factor (Optional[int]): Reduce :math:`C` by this factor. Default: 4
        scale_fn_name (Optional[str]): Scaling function name. Default: sigmoid

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)`
    """

    def __init__(self, opts, in_channels: int, squeeze_factor: Optional[int]=4, scale_fn_name: Optional[str]='sigmoid', *args, **kwargs) ->None:
        squeeze_channels = max(make_divisible(in_channels // squeeze_factor, 8), 32)
        fc1 = ConvLayer(opts=opts, in_channels=in_channels, out_channels=squeeze_channels, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=True)
        fc2 = ConvLayer(opts=opts, in_channels=squeeze_channels, out_channels=in_channels, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=False)
        act_fn = get_activation_fn(act_type=scale_fn_name, inplace=True)
        super().__init__()
        self.se_layer = nn.Sequential()
        self.se_layer.add_module(name='global_pool', module=AdaptiveAvgPool2d(output_size=1))
        self.se_layer.add_module(name='fc1', module=fc1)
        self.se_layer.add_module(name='fc2', module=fc2)
        self.se_layer.add_module(name='scale_act', module=act_fn)
        self.in_channels = in_channels
        self.squeeze_factor = squeeze_factor
        self.scale_fn = scale_fn_name

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        return x * self.se_layer(x)

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        _, params, macs = module_profile(module=self.se_layer, x=input)
        return input, params, macs

    def __repr__(self) ->str:
        return '{}(in_channels={}, squeeze_factor={}, scale_fn={})'.format(self.__class__.__name__, self.in_channels, self.squeeze_factor, self.scale_fn)


class InvertedResidualSE(BaseModule):
    """
    This class implements the inverted residual block with squeeze-excitation unit, as described in
    `MobileNetv3 <https://arxiv.org/abs/1905.02244>`_ paper

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out)`
        expand_ratio (Union[int, float]): Expand the input channels by this factor in depth-wise conv
        dilation (Optional[int]): Use conv with dilation. Default: 1
        stride (Optional[int]): Use convolutions with a stride. Default: 1
        use_se (Optional[bool]): Use squeeze-excitation block. Default: False
        act_fn_name (Optional[str]): Activation function name. Default: relu
        se_scale_fn_name (Optional [str]): Scale activation function inside SE unit. Defaults to hard_sigmoid
        kernel_size (Optional[int]): Kernel size in depth-wise convolution. Defaults to 3.
        squeeze_factor (Optional[bool]): Squeezing factor in SE unit. Defaults to 4.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`
    """

    def __init__(self, opts, in_channels: int, out_channels: int, expand_ratio: Union[int, float], dilation: Optional[int]=1, stride: Optional[int]=1, use_se: Optional[bool]=False, act_fn_name: Optional[str]='relu', se_scale_fn_name: Optional[str]='hard_sigmoid', kernel_size: Optional[int]=3, squeeze_factor: Optional[int]=4, *args, **kwargs) ->None:
        hidden_dim = make_divisible(int(round(in_channels * expand_ratio)), 8)
        act_fn = get_activation_fn(act_type=act_fn_name, inplace=True)
        super().__init__()
        block = nn.Sequential()
        if expand_ratio != 1:
            block.add_module(name='exp_1x1', module=ConvLayer(opts, in_channels=in_channels, out_channels=hidden_dim, kernel_size=1, use_act=False, use_norm=True))
            block.add_module(name='act_fn_1', module=act_fn)
        block.add_module(name='conv_3x3', module=ConvLayer(opts, in_channels=hidden_dim, out_channels=hidden_dim, stride=stride, kernel_size=kernel_size, groups=hidden_dim, use_act=False, use_norm=True, dilation=dilation))
        block.add_module(name='act_fn_2', module=act_fn)
        if use_se:
            se = SqueezeExcitation(opts=opts, in_channels=hidden_dim, squeeze_factor=squeeze_factor, scale_fn_name=se_scale_fn_name)
            block.add_module(name='se', module=se)
        block.add_module(name='red_1x1', module=ConvLayer(opts, in_channels=hidden_dim, out_channels=out_channels, kernel_size=1, use_act=False, use_norm=True))
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.exp = expand_ratio
        self.dilation = dilation
        self.use_se = use_se
        self.stride = stride
        self.act_fn_name = act_fn_name
        self.kernel_size = kernel_size
        self.use_res_connect = self.stride == 1 and in_channels == out_channels

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        y = self.block(x)
        return x + y if self.use_res_connect else y

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return module_profile(module=self.block, x=input)

    def __repr__(self) ->str:
        return '{}(in_channels={}, out_channels={}, stride={}, exp={}, dilation={}, use_se={}, kernel_size={}, act_fn={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.stride, self.exp, self.dilation, self.use_se, self.kernel_size, self.act_fn_name)


class StochasticDepth(StochasticDepthTorch):
    """
    Implements the Stochastic Depth `"Deep Networks with Stochastic Depth"
    <https://arxiv.org/abs/1603.09382>`_ used for randomly dropping residual
    branches of residual architectures.
    """

    def __init__(self, p: float, mode: str) ->None:
        super().__init__(p=p, mode=mode)

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        return input, 0.0, 0.0


class EfficientNetBlock(InvertedResidualSE):
    """
    This class implements a variant of the inverted residual block with squeeze-excitation unit,
    as described in `MobileNetv3 <https://arxiv.org/abs/1905.02244>`_ paper. This variant
    includes stochastic depth, as used in `EfficientNet <https://arxiv.org/abs/1905.11946>`_ paper.

    Args:
        stochastic_depth_prob: float,
        For other arguments, refer to the parent class.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`
    """

    def __init__(self, stochastic_depth_prob: float, *args, **kwargs) ->None:
        super().__init__(*args, **kwargs)
        self.stochastic_depth = StochasticDepth(p=stochastic_depth_prob, mode='row')

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        y = self.block(x)
        if self.use_res_connect:
            y = self.stochastic_depth(y)
            y = y + x
        return y

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return super().profile_module(input=input)

    def __repr__(self) ->str:
        return super().__repr__()[:-1] + f', stochastic_depth_prob={self.stochastic_depth.p})'


def get_configuration(opts) ->Dict:
    network_mode = getattr(opts, 'model.classification.efficientnet.mode')
    if network_mode is None:
        logger.error("EfficientNet mode can't be none. Please specify --model.classification.efficientnet.mode")
    network_mode = network_mode.lower()
    network_config = dict()
    compound_scaling_cfg = {'b0': CompoundScalingConfig(1.0, 1.0, 224), 'b1': CompoundScalingConfig(1.0, 1.1, 240), 'b2': CompoundScalingConfig(1.1, 1.2, 260), 'b3': CompoundScalingConfig(1.2, 1.4, 300), 'b4': CompoundScalingConfig(1.4, 1.8, 380), 'b5': CompoundScalingConfig(1.6, 2.2, 456), 'b6': CompoundScalingConfig(1.8, 2.6, 528), 'b7': CompoundScalingConfig(2.0, 3.1, 600), 'b8': CompoundScalingConfig(2.2, 3.6, 672)}
    if network_mode in compound_scaling_cfg:
        compound_scaling_cfg_mode_i = compound_scaling_cfg[network_mode]
        width_mult = compound_scaling_cfg_mode_i.width_mult
        depth_mult = compound_scaling_cfg_mode_i.depth_mult
        block_builder = partial(EfficientNetBlockConfig, width_mult=width_mult, depth_mult=depth_mult)
        network_config['layer_1'] = [block_builder(1, 3, 1, 32, 16, 1)]
        network_config['layer_2'] = [block_builder(6, 3, 2, 16, 24, 2)]
        network_config['layer_3'] = [block_builder(6, 5, 2, 24, 40, 2)]
        network_config['layer_4'] = [block_builder(6, 3, 2, 40, 80, 3), block_builder(6, 5, 1, 80, 112, 3)]
        network_config['layer_5'] = [block_builder(6, 5, 2, 112, 192, 4), block_builder(6, 3, 1, 192, 320, 1)]
        network_config['last_channels'] = 4 * network_config['layer_5'][-1].out_channels
    else:
        logger.error('Current supported modes for EfficientNet are b[0-7]. Got: {}'.format(network_mode))
    total_layers = 0
    for layer_name in ['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5']:
        for block_config in network_config[layer_name]:
            total_layers += block_config.num_layers
    network_config['total_layers'] = total_layers
    return network_config


CLS_MODEL_REGISTRY = {}


def register_cls_models(name):

    def register_model_class(cls):
        if name in CLS_MODEL_REGISTRY:
            raise ValueError('Cannot register duplicate model ({})'.format(name))
        if not issubclass(cls, BaseEncoder):
            raise ValueError('Model ({}: {}) must extend BaseEncoder'.format(name, cls.__name__))
        CLS_MODEL_REGISTRY[name] = cls
        return cls
    return register_model_class


class EfficientNet(BaseEncoder):
    """
    This class defines the `EfficientNet architecture <https://arxiv.org/abs/1905.11946>`_
    """

    def __init__(self, opts, *args, **kwargs: Any) ->None:
        super().__init__(opts, *args, **kwargs)
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout')
        network_config = get_configuration(opts)
        last_channels = network_config['last_channels']
        total_layers = network_config['total_layers']
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        stochastic_depth_prob = getattr(opts, 'model.classification.efficientnet.stochastic_depth_prob', 0.2)
        image_channels = 3
        in_channels = network_config['layer_1'][0].in_channels
        self.conv_1 = ConvLayer(opts=opts, in_channels=image_channels, out_channels=in_channels, kernel_size=3, stride=2, use_norm=True, use_act=True)
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': in_channels}
        prev_layers_cnt = 0
        for layer_name in ['layer_1', 'layer_2', 'layer_3', 'layer_4', 'layer_5']:
            dilation = False
            if layer_name == 'layer_4':
                dilation = self.dilate_l4
            elif layer_name == 'layer_5':
                dilation = self.dilate_l5
            layer, prev_layers_cnt = self._make_layer(opts=opts, block_config=network_config[layer_name], stochastic_depth_prob=stochastic_depth_prob, prev_layers_cnt=prev_layers_cnt, total_layers=total_layers, dilate=dilation)
            setattr(self, layer_name, layer)
            self.model_conf_dict[layer_name.replace('_', '')] = {'in': network_config[layer_name][0].in_channels, 'out': network_config[layer_name][-1].out_channels}
        in_channels = network_config['layer_5'][-1].out_channels
        out_channels = last_channels
        self.conv_1x1_exp = ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, use_act=True, use_norm=True)
        self.model_conf_dict['exp_before_cls'] = {'in': in_channels, 'out': out_channels}
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='classifier_dropout', module=Dropout(p=classifier_dropout, inplace=True))
        self.classifier.add_module(name='classifier_fc', module=LinearLayer(in_features=out_channels, out_features=num_classes, bias=True))

    def _make_layer(self, opts, block_config, stochastic_depth_prob: float, prev_layers_cnt: int, total_layers: int, dilate: Optional[bool]=False, *args, **kwargs) ->Tuple[nn.Module, int]:
        prev_dilation = self.dilation
        block = []
        count = 0
        for layer_config in block_config:
            assert isinstance(layer_config, EfficientNetBlockConfig)
            in_channels = layer_config.in_channels
            out_channels = layer_config.out_channels
            for layer_idx in range(layer_config.num_layers):
                stride = layer_config.stride if layer_idx == 0 else 1
                if dilate and stride == 2:
                    self.dilation *= stride
                    stride = 1
                    dilate = False
                sd_prob = stochastic_depth_prob * float(prev_layers_cnt + count) / total_layers
                sd_prob = round(sd_prob, 4)
                efficient_net_layer = EfficientNetBlock(stochastic_depth_prob=sd_prob, opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=layer_config.kernel, stride=stride, expand_ratio=layer_config.expand_ratio, dilation=prev_dilation if count == 0 else self.dilation, use_hs=False, use_se=True, use_input_as_se_dim=True, squeeze_factor=layer_config.expand_ratio * 4, act_fn_name='swish', se_scale_fn_name='sigmoid')
                block.append(efficient_net_layer)
                count += 1
                in_channels = out_channels
        prev_layers_cnt += count
        return nn.Sequential(*block), prev_layers_cnt

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.efficientnet.mode', type=str, choices=[f'b{i}' for i in range(8)])
        group.add_argument('--model.classification.efficientnet.stochastic-depth-prob', type=float, default=0.0)
        return parser


class MobileNetv1(BaseEncoder):
    """
    This class defines the `MobileNet architecture <https://arxiv.org/abs/1704.04861>`_
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        image_channels = 3
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout', 0.0)
        if classifier_dropout == 0.0:
            width_mult = getattr(opts, 'model.classification.mobilenetv1.width_multiplier', 1.0)
            val = round(0.1 * width_mult, 3)
            classifier_dropout = bound_fn(min_val=0.0, max_val=0.1, value=val)
        super().__init__(opts, *args, **kwargs)
        cfg = get_configuration(opts=opts)
        self.model_conf_dict = dict()
        input_channels = cfg['conv1_out']
        self.conv_1 = ConvLayer(opts=opts, in_channels=image_channels, out_channels=input_channels, kernel_size=3, stride=2, use_norm=True, use_act=True)
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': input_channels}
        self.layer_1, out_channels = self._make_layer(opts=opts, mv1_config=cfg['layer1'], input_channel=input_channels)
        self.model_conf_dict['layer1'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_2, out_channels = self._make_layer(opts=opts, mv1_config=cfg['layer2'], input_channel=input_channels)
        self.model_conf_dict['layer2'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_3, out_channels = self._make_layer(opts=opts, mv1_config=cfg['layer3'], input_channel=input_channels)
        self.model_conf_dict['layer3'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_4, out_channels = self._make_layer(opts=opts, mv1_config=cfg['layer4'], input_channel=input_channels, dilate=self.dilate_l4)
        self.model_conf_dict['layer4'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_5, out_channels = self._make_layer(opts=opts, mv1_config=cfg['layer5'], input_channel=input_channels, dilate=self.dilate_l5)
        self.model_conf_dict['layer5'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.conv_1x1_exp = Identity()
        self.model_conf_dict['exp_before_cls'] = {'in': input_channels, 'out': input_channels}
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='classifier_dropout', module=Dropout(p=classifier_dropout))
        self.classifier.add_module(name='classifier_fc', module=LinearLayer(in_features=input_channels, out_features=num_classes, bias=True))
        self.model_conf_dict['cls'] = {'in': input_channels, 'out': num_classes}
        self.check_model()
        self.reset_parameters(opts=opts)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """Add model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.mobilenetv1.width-multiplier', type=float, default=1.0, help='Width multiplier for MobileNetv1. Default: 1.0')
        return parser

    def _make_layer(self, opts, mv1_config: (Dict or List), input_channel: int, dilate: Optional[bool]=False, *args, **kwargs) ->Tuple[nn.Module, int]:
        prev_dilation = self.dilation
        mv1_block = []
        out_channels = mv1_config.get('out_channels')
        stride = mv1_config.get('stride', 1)
        n_repeat = mv1_config.get('repeat', 0)
        if stride == 2:
            if dilate:
                self.dilation *= stride
                stride = 1
            mv1_block.append(SeparableConv(opts=opts, in_channels=input_channel, out_channels=out_channels, kernel_size=3, stride=stride, use_norm=True, use_act=True, dilation=prev_dilation))
            input_channel = out_channels
        for i in range(n_repeat):
            mv1_block.append(SeparableConv(opts=opts, in_channels=input_channel, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=True, dilation=self.dilation))
            input_channel = out_channels
        return nn.Sequential(*mv1_block), input_channel


class InvertedResidual(BaseModule):
    """
    This class implements the inverted residual block, as described in `MobileNetv2 <https://arxiv.org/abs/1801.04381>`_ paper

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out)`
        stride (Optional[int]): Use convolutions with a stride. Default: 1
        expand_ratio (Union[int, float]): Expand the input channels by this factor in depth-wise conv
        dilation (Optional[int]): Use conv with dilation. Default: 1
        skip_connection (Optional[bool]): Use skip-connection. Default: True

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    .. note::
        If `in_channels =! out_channels` and `stride > 1`, we set `skip_connection=False`

    """

    def __init__(self, opts, in_channels: int, out_channels: int, stride: int, expand_ratio: Union[int, float], dilation: int=1, skip_connection: Optional[bool]=True, *args, **kwargs) ->None:
        assert stride in [1, 2]
        hidden_dim = make_divisible(int(round(in_channels * expand_ratio)), 8)
        super().__init__()
        block = nn.Sequential()
        if expand_ratio != 1:
            block.add_module(name='exp_1x1', module=ConvLayer(opts, in_channels=in_channels, out_channels=hidden_dim, kernel_size=1, use_act=True, use_norm=True))
        block.add_module(name='conv_3x3', module=ConvLayer(opts, in_channels=hidden_dim, out_channels=hidden_dim, stride=stride, kernel_size=3, groups=hidden_dim, use_act=True, use_norm=True, dilation=dilation))
        block.add_module(name='red_1x1', module=ConvLayer(opts, in_channels=hidden_dim, out_channels=out_channels, kernel_size=1, use_act=False, use_norm=True))
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.exp = expand_ratio
        self.dilation = dilation
        self.stride = stride
        self.use_res_connect = self.stride == 1 and in_channels == out_channels and skip_connection

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        if self.use_res_connect:
            return x + self.block(x)
        else:
            return self.block(x)

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        return module_profile(module=self.block, x=input)

    def __repr__(self) ->str:
        return '{}(in_channels={}, out_channels={}, stride={}, exp={}, dilation={}, skip_conn={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.stride, self.exp, self.dilation, self.use_res_connect)


class MobileNetV2(BaseEncoder):
    """
    This class defines the `MobileNetv2 architecture <https://arxiv.org/abs/1801.04381>`_
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        width_mult = getattr(opts, 'model.classification.mobilenetv2.width_multiplier', 1.0)
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        cfg = get_configuration(opts=opts)
        image_channels = 3
        input_channels = 32
        last_channel = 1280
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout', 0.0)
        if classifier_dropout == 0.0 or classifier_dropout is None:
            val = round(0.2 * width_mult, 3)
            classifier_dropout = bound_fn(min_val=0.0, max_val=0.2, value=val)
        super().__init__(opts, *args, **kwargs)
        last_channel = make_divisible(last_channel * max(1.0, width_mult), self.round_nearest)
        self.model_conf_dict = dict()
        self.conv_1 = ConvLayer(opts=opts, in_channels=image_channels, out_channels=input_channels, kernel_size=3, stride=2, use_norm=True, use_act=True)
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': input_channels}
        self.layer_1, out_channels = self._make_layer(opts=opts, mv2_config=cfg['layer1'], width_mult=width_mult, input_channel=input_channels)
        self.model_conf_dict['layer1'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_2, out_channels = self._make_layer(opts=opts, mv2_config=cfg['layer2'], width_mult=width_mult, input_channel=input_channels)
        self.model_conf_dict['layer2'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_3, out_channels = self._make_layer(opts=opts, mv2_config=cfg['layer3'], width_mult=width_mult, input_channel=input_channels)
        self.model_conf_dict['layer3'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_4, out_channels = self._make_layer(opts=opts, mv2_config=[cfg['layer4'], cfg['layer4_a']], width_mult=width_mult, input_channel=input_channels, dilate=self.dilate_l4)
        self.model_conf_dict['layer4'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_5, out_channels = self._make_layer(opts=opts, mv2_config=[cfg['layer5'], cfg['layer5_a']], width_mult=width_mult, input_channel=input_channels, dilate=self.dilate_l5)
        self.model_conf_dict['layer5'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.conv_1x1_exp = ConvLayer(opts=opts, in_channels=input_channels, out_channels=last_channel, kernel_size=1, stride=1, use_act=True, use_norm=True)
        self.model_conf_dict['exp_before_cls'] = {'in': input_channels, 'out': last_channel}
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='classifier_dropout', module=Dropout(p=classifier_dropout))
        self.classifier.add_module(name='classifier_fc', module=LinearLayer(in_features=last_channel, out_features=num_classes, bias=True))
        self.model_conf_dict['cls'] = {'in': last_channel, 'out': num_classes}
        self.check_model()
        self.reset_parameters(opts=opts)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.mobilenetv2.width-multiplier', type=float, default=1.0, help='Width multiplier for MobileNetv2. Default: 1.0')
        return parser

    def _make_layer(self, opts, mv2_config: (Dict or List), width_mult: float, input_channel: int, dilate: Optional[bool]=False, *args, **kwargs) ->Tuple[nn.Module, int]:
        prev_dilation = self.dilation
        mv2_block = nn.Sequential()
        count = 0
        if isinstance(mv2_config, Dict):
            mv2_config = [mv2_config]
        for cfg in mv2_config:
            t = cfg.get('expansion_ratio')
            c = cfg.get('out_channels')
            n = cfg.get('num_blocks')
            s = cfg.get('stride')
            output_channel = make_divisible(c * width_mult, self.round_nearest)
            for block_idx in range(n):
                stride = s if block_idx == 0 else 1
                block_name = 'mv2_block_{}'.format(count)
                if dilate and count == 0:
                    self.dilation *= stride
                    stride = 1
                layer = InvertedResidual(opts=opts, in_channels=input_channel, out_channels=output_channel, stride=stride, expand_ratio=t, dilation=prev_dilation if count == 0 else self.dilation)
                mv2_block.add_module(name=block_name, module=layer)
                count += 1
                input_channel = output_channel
        return mv2_block, input_channel


class MobileNetV3(BaseEncoder):
    """
    This class implements the `MobileNetv3 architecture <https://arxiv.org/abs/1905.02244>`_
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        width_mult = getattr(opts, 'model.classification.mobilenetv3.width_multiplier', 1.0)
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout', 0.0)
        if classifier_dropout == 0.0 or classifier_dropout is None:
            val = round(0.2 * width_mult, 3)
            classifier_dropout = bound_fn(min_val=0.0, max_val=0.2, value=val)
        image_channels = 3
        input_channels = make_divisible(16 * width_mult, 8)
        mv3_config = get_configuration(opts)
        super().__init__(opts, *args, **kwargs)
        self.conv_1 = nn.Sequential()
        self.conv_1.add_module(name='conv_3x3_bn', module=ConvLayer(opts=opts, in_channels=image_channels, out_channels=input_channels, kernel_size=3, stride=2, use_norm=True, use_act=False))
        self.conv_1.add_module(name='act', module=get_activation_fn(act_type='hard_swish', inplace=True))
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': input_channels}
        self.layer_1, out_channels = self._make_layer(opts=opts, mv3_config=mv3_config['layer_1'], width_mult=width_mult, input_channel=input_channels)
        self.model_conf_dict['layer1'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_2, out_channels = self._make_layer(opts=opts, mv3_config=mv3_config['layer_2'], width_mult=width_mult, input_channel=input_channels)
        self.model_conf_dict['layer2'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_3, out_channels = self._make_layer(opts=opts, mv3_config=mv3_config['layer_3'], width_mult=width_mult, input_channel=input_channels)
        self.model_conf_dict['layer3'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_4, out_channels = self._make_layer(opts=opts, mv3_config=mv3_config['layer_4'], width_mult=width_mult, input_channel=input_channels, dilate=self.dilate_l4)
        self.model_conf_dict['layer4'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_5, out_channels = self._make_layer(opts=opts, mv3_config=mv3_config['layer_5'], width_mult=width_mult, input_channel=input_channels, dilate=self.dilate_l5)
        self.model_conf_dict['layer5'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.conv_1x1_exp = nn.Sequential()
        out_channels = 6 * input_channels
        self.conv_1x1_exp.add_module(name='conv_1x1', module=ConvLayer(opts=opts, in_channels=input_channels, out_channels=out_channels, kernel_size=1, stride=1, use_act=False, use_norm=True))
        self.conv_1x1_exp.add_module(name='act', module=get_activation_fn(act_type='hard_swish', inplace=True))
        self.model_conf_dict['exp_before_cls'] = {'in': input_channels, 'out': out_channels}
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        last_channels = mv3_config['last_channels']
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        self.classifier.add_module(name='fc1', module=LinearLayer(in_features=out_channels, out_features=last_channels, bias=True))
        self.classifier.add_module(name='act', module=get_activation_fn(act_type='hard_swish', inplace=True))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='classifier_dropout', module=Dropout(p=classifier_dropout))
        self.classifier.add_module(name='classifier_fc', module=LinearLayer(in_features=last_channels, out_features=num_classes, bias=True))
        self.model_conf_dict['cls'] = {'in': 6 * input_channels, 'out': num_classes}

    def _make_layer(self, opts, mv3_config, width_mult: float, input_channel: int, dilate: Optional[bool]=False, *args, **kwargs) ->Tuple[nn.Module, int]:
        prev_dilation = self.dilation
        mv3_block = nn.Sequential()
        count = 0
        for i in range(len(mv3_config)):
            for kernel_size, expansion_factor, in_channels, use_se, use_hs, stride in [mv3_config[i]]:
                block_name = 'mv3_s_{}_idx_{}'.format(stride, count)
                output_channel = make_divisible(in_channels * width_mult, self.round_nearest)
                if dilate and count == 0:
                    self.dilation *= stride
                    stride = 1
                layer = InvertedResidualSE(opts=opts, in_channels=input_channel, out_channels=output_channel, stride=stride, expand_ratio=expansion_factor, dilation=prev_dilation if count == 0 else self.dilation, act_fn_name='hard_swish' if use_hs else 'relu', use_se=use_se)
                mv3_block.add_module(name=block_name, module=layer)
                count += 1
                input_channel = output_channel
        return mv3_block, input_channel

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.mobilenetv3.mode', type=str, default='large', help='Configuration for mobilenetv3. Default: large', choices=('small', 'large'))
        group.add_argument('--model.classification.mobilenetv3.width-multiplier', type=float, default=1.0, help='Width multiplier for mobilenetv3. Default: 1.0')
        return parser


class TransformerEncoder(BaseModule):
    """
    This class defines the pre-norm `Transformer encoder <https://arxiv.org/abs/1706.03762>`_
    Args:
        opts: command line arguments
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`
        ffn_latent_dim (int): Inner dimension of the FFN
        num_heads (Optional[int]) : Number of heads in multi-head attention. Default: 8
        attn_dropout (Optional[float]): Dropout rate for attention in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers. Default: 0.0
        transformer_norm_layer (Optional[str]): Normalization layer. Default: layer_norm

    Shape:
        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,
        and :math:`C_{in}` is input embedding dim
        - Output: same shape as the input
    """

    def __init__(self, opts, embed_dim: int, ffn_latent_dim: int, num_heads: Optional[int]=8, attn_dropout: Optional[float]=0.0, dropout: Optional[float]=0.0, ffn_dropout: Optional[float]=0.0, transformer_norm_layer: Optional[str]='layer_norm', *args, **kwargs) ->None:
        super().__init__()
        attn_unit = SingleHeadAttention(embed_dim=embed_dim, attn_dropout=attn_dropout, bias=True)
        if num_heads > 1:
            attn_unit = MultiHeadAttention(embed_dim, num_heads, attn_dropout=attn_dropout, bias=True, coreml_compatible=getattr(opts, 'common.enable_coreml_compatible_module', False))
        self.pre_norm_mha = nn.Sequential(get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=embed_dim), attn_unit, Dropout(p=dropout))
        act_name = self.build_act_layer(opts=opts)
        self.pre_norm_ffn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=embed_dim), LinearLayer(in_features=embed_dim, out_features=ffn_latent_dim, bias=True), act_name, Dropout(p=ffn_dropout), LinearLayer(in_features=ffn_latent_dim, out_features=embed_dim, bias=True), Dropout(p=dropout))
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_latent_dim
        self.ffn_dropout = ffn_dropout
        self.std_dropout = dropout
        self.attn_fn_name = attn_unit.__class__.__name__
        self.act_fn_name = act_name.__class__.__name__
        self.norm_type = transformer_norm_layer

    @staticmethod
    def build_act_layer(opts) ->nn.Module:
        act_type = getattr(opts, 'model.activation.name', 'relu')
        neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        inplace = getattr(opts, 'model.activation.inplace', False)
        act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=1)
        return act_layer

    def __repr__(self) ->str:
        return '{}(embed_dim={}, ffn_dim={}, dropout={}, ffn_dropout={}, attn_fn={}, act_fn={}, norm_fn={})'.format(self.__class__.__name__, self.embed_dim, self.ffn_dim, self.std_dropout, self.ffn_dropout, self.attn_fn_name, self.act_fn_name, self.norm_type)

    def forward(self, x: Tensor, x_prev: Optional[Tensor]=None, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        res = x
        x = self.pre_norm_mha[0](x)
        x = self.pre_norm_mha[1](*args, x_q=x, x_kv=x_prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask, **kwargs)
        x = self.pre_norm_mha[2](x)
        x = x + res
        x = x + self.pre_norm_ffn(x)
        return x

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        b_sz, seq_len = input.shape[:2]
        out, p_mha, m_mha = module_profile(module=self.pre_norm_mha, x=input)
        out, p_ffn, m_ffn = module_profile(module=self.pre_norm_ffn, x=input)
        m_ffn = m_ffn * b_sz * seq_len
        macs = m_mha + m_ffn
        params = p_mha + p_ffn
        return input, params, macs


class MobileViTBlock(BaseModule):
    """
    This class defines the `MobileViT block <https://arxiv.org/abs/2110.02178?context=cs.LG>`_

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        transformer_dim (int): Input dimension to the transformer unit
        ffn_dim (int): Dimension of the FFN block
        n_transformer_blocks (Optional[int]): Number of transformer blocks. Default: 2
        head_dim (Optional[int]): Head dimension in the multi-head attention. Default: 32
        attn_dropout (Optional[float]): Dropout in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers in transformer. Default: 0.0
        patch_h (Optional[int]): Patch height for unfolding operation. Default: 8
        patch_w (Optional[int]): Patch width for unfolding operation. Default: 8
        transformer_norm_layer (Optional[str]): Normalization layer in the transformer block. Default: layer_norm
        conv_ksize (Optional[int]): Kernel size to learn local representations in MobileViT block. Default: 3
        dilation (Optional[int]): Dilation rate in convolutions. Default: 1
        no_fusion (Optional[bool]): Do not combine the input and output feature maps. Default: False
    """

    def __init__(self, opts, in_channels: int, transformer_dim: int, ffn_dim: int, n_transformer_blocks: Optional[int]=2, head_dim: Optional[int]=32, attn_dropout: Optional[float]=0.0, dropout: Optional[int]=0.0, ffn_dropout: Optional[int]=0.0, patch_h: Optional[int]=8, patch_w: Optional[int]=8, transformer_norm_layer: Optional[str]='layer_norm', conv_ksize: Optional[int]=3, dilation: Optional[int]=1, no_fusion: Optional[bool]=False, *args, **kwargs) ->None:
        conv_3x3_in = ConvLayer(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True, dilation=dilation)
        conv_1x1_in = ConvLayer(opts=opts, in_channels=in_channels, out_channels=transformer_dim, kernel_size=1, stride=1, use_norm=False, use_act=False)
        conv_1x1_out = ConvLayer(opts=opts, in_channels=transformer_dim, out_channels=in_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        conv_3x3_out = None
        if not no_fusion:
            conv_3x3_out = ConvLayer(opts=opts, in_channels=2 * in_channels, out_channels=in_channels, kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True)
        super().__init__()
        self.local_rep = nn.Sequential()
        self.local_rep.add_module(name='conv_3x3', module=conv_3x3_in)
        self.local_rep.add_module(name='conv_1x1', module=conv_1x1_in)
        assert transformer_dim % head_dim == 0
        num_heads = transformer_dim // head_dim
        global_rep = [TransformerEncoder(opts=opts, embed_dim=transformer_dim, ffn_latent_dim=ffn_dim, num_heads=num_heads, attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, transformer_norm_layer=transformer_norm_layer) for _ in range(n_transformer_blocks)]
        global_rep.append(get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=transformer_dim))
        self.global_rep = nn.Sequential(*global_rep)
        self.conv_proj = conv_1x1_out
        self.fusion = conv_3x3_out
        self.patch_h = patch_h
        self.patch_w = patch_w
        self.patch_area = self.patch_w * self.patch_h
        self.cnn_in_dim = in_channels
        self.cnn_out_dim = transformer_dim
        self.n_heads = num_heads
        self.ffn_dim = ffn_dim
        self.dropout = dropout
        self.attn_dropout = attn_dropout
        self.ffn_dropout = ffn_dropout
        self.dilation = dilation
        self.n_blocks = n_transformer_blocks
        self.conv_ksize = conv_ksize

    def __repr__(self) ->str:
        repr_str = '{}('.format(self.__class__.__name__)
        repr_str += '\n\t Local representations'
        if isinstance(self.local_rep, nn.Sequential):
            for m in self.local_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.local_rep)
        repr_str += '\n\t Global representations with patch size of {}x{}'.format(self.patch_h, self.patch_w)
        if isinstance(self.global_rep, nn.Sequential):
            for m in self.global_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.global_rep)
        if isinstance(self.conv_proj, nn.Sequential):
            for m in self.conv_proj:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.conv_proj)
        if self.fusion is not None:
            repr_str += '\n\t Feature fusion'
            if isinstance(self.fusion, nn.Sequential):
                for m in self.fusion:
                    repr_str += '\n\t\t {}'.format(m)
            else:
                repr_str += '\n\t\t {}'.format(self.fusion)
        repr_str += '\n)'
        return repr_str

    def unfolding(self, feature_map: Tensor) ->Tuple[Tensor, Dict]:
        patch_w, patch_h = self.patch_w, self.patch_h
        patch_area = int(patch_w * patch_h)
        batch_size, in_channels, orig_h, orig_w = feature_map.shape
        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)
        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)
        interpolate = False
        if new_w != orig_w or new_h != orig_h:
            feature_map = F.interpolate(feature_map, size=(new_h, new_w), mode='bilinear', align_corners=False)
            interpolate = True
        num_patch_w = new_w // patch_w
        num_patch_h = new_h // patch_h
        num_patches = num_patch_h * num_patch_w
        reshaped_fm = feature_map.reshape(batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w)
        transposed_fm = reshaped_fm.transpose(1, 2)
        reshaped_fm = transposed_fm.reshape(batch_size, in_channels, num_patches, patch_area)
        transposed_fm = reshaped_fm.transpose(1, 3)
        patches = transposed_fm.reshape(batch_size * patch_area, num_patches, -1)
        info_dict = {'orig_size': (orig_h, orig_w), 'batch_size': batch_size, 'interpolate': interpolate, 'total_patches': num_patches, 'num_patches_w': num_patch_w, 'num_patches_h': num_patch_h}
        return patches, info_dict

    def folding(self, patches: Tensor, info_dict: Dict) ->Tensor:
        n_dim = patches.dim()
        assert n_dim == 3, 'Tensor should be of shape BPxNxC. Got: {}'.format(patches.shape)
        patches = patches.contiguous().view(info_dict['batch_size'], self.patch_area, info_dict['total_patches'], -1)
        batch_size, pixels, num_patches, channels = patches.size()
        num_patch_h = info_dict['num_patches_h']
        num_patch_w = info_dict['num_patches_w']
        patches = patches.transpose(1, 3)
        feature_map = patches.reshape(batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w)
        feature_map = feature_map.transpose(1, 2)
        feature_map = feature_map.reshape(batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w)
        if info_dict['interpolate']:
            feature_map = F.interpolate(feature_map, size=info_dict['orig_size'], mode='bilinear', align_corners=False)
        return feature_map

    def forward_spatial(self, x: Tensor) ->Tensor:
        res = x
        fm = self.local_rep(x)
        patches, info_dict = self.unfolding(fm)
        for transformer_layer in self.global_rep:
            patches = transformer_layer(patches)
        fm = self.folding(patches=patches, info_dict=info_dict)
        fm = self.conv_proj(fm)
        if self.fusion is not None:
            fm = self.fusion(torch.cat((res, fm), dim=1))
        return fm

    def forward_temporal(self, x: Tensor, x_prev: Optional[Tensor]=None) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        res = x
        fm = self.local_rep(x)
        patches, info_dict = self.unfolding(fm)
        for global_layer in self.global_rep:
            if isinstance(global_layer, TransformerEncoder):
                patches = global_layer(x=patches, x_prev=x_prev)
            else:
                patches = global_layer(patches)
        fm = self.folding(patches=patches, info_dict=info_dict)
        fm = self.conv_proj(fm)
        if self.fusion is not None:
            fm = self.fusion(torch.cat((res, fm), dim=1))
        return fm, patches

    def forward(self, x: Union[Tensor, Tuple[Tensor]], *args, **kwargs) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        if isinstance(x, Tuple) and len(x) == 2:
            return self.forward_temporal(x=x[0], x_prev=x[1])
        elif isinstance(x, Tensor):
            return self.forward_spatial(x)
        else:
            raise NotImplementedError

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        params = macs = 0.0
        res = input
        out, p, m = module_profile(module=self.local_rep, x=input)
        params += p
        macs += m
        patches, info_dict = self.unfolding(feature_map=out)
        patches, p, m = module_profile(module=self.global_rep, x=patches)
        params += p
        macs += m
        fm = self.folding(patches=patches, info_dict=info_dict)
        out, p, m = module_profile(module=self.conv_proj, x=fm)
        params += p
        macs += m
        if self.fusion is not None:
            out, p, m = module_profile(module=self.fusion, x=torch.cat((out, res), dim=1))
            params += p
            macs += m
        return res, params, macs


class MobileViT(BaseEncoder):
    """
    This class implements the `MobileViT architecture <https://arxiv.org/abs/2110.02178?context=cs.LG>`_
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout', 0.0)
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        image_channels = 3
        out_channels = 16
        mobilevit_config = get_configuration(opts=opts)
        super().__init__(opts, *args, **kwargs)
        self.model_conf_dict = dict()
        self.conv_1 = ConvLayer(opts=opts, in_channels=image_channels, out_channels=out_channels, kernel_size=3, stride=2, use_norm=True, use_act=True)
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_1, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer1'])
        self.model_conf_dict['layer1'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_2, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer2'])
        self.model_conf_dict['layer2'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_3, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer3'])
        self.model_conf_dict['layer3'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_4, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer4'], dilate=self.dilate_l4)
        self.model_conf_dict['layer4'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_5, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer5'], dilate=self.dilate_l5)
        self.model_conf_dict['layer5'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        exp_channels = min(mobilevit_config['last_layer_exp_factor'] * in_channels, 960)
        self.conv_1x1_exp = ConvLayer(opts=opts, in_channels=in_channels, out_channels=exp_channels, kernel_size=1, stride=1, use_act=True, use_norm=True)
        self.model_conf_dict['exp_before_cls'] = {'in': in_channels, 'out': exp_channels}
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='dropout', module=Dropout(p=classifier_dropout, inplace=True))
        self.classifier.add_module(name='fc', module=LinearLayer(in_features=exp_channels, out_features=num_classes, bias=True))
        self.check_model()
        self.reset_parameters(opts=opts)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.mit.mode', type=str, default='small', choices=['xx_small', 'x_small', 'small'], help='MobileViT mode. Defaults to small')
        group.add_argument('--model.classification.mit.attn-dropout', type=float, default=0.0, help='Dropout in attention layer. Defaults to 0.0')
        group.add_argument('--model.classification.mit.ffn-dropout', type=float, default=0.0, help='Dropout between FFN layers. Defaults to 0.0')
        group.add_argument('--model.classification.mit.dropout', type=float, default=0.0, help='Dropout in Transformer layer. Defaults to 0.0')
        group.add_argument('--model.classification.mit.transformer-norm-layer', type=str, default='layer_norm', help='Normalization layer in transformer. Defaults to LayerNorm')
        group.add_argument('--model.classification.mit.no-fuse-local-global-features', action='store_true', help='Do not combine local and global features in MobileViT block')
        group.add_argument('--model.classification.mit.conv-kernel-size', type=int, default=3, help='Kernel size of Conv layers in MobileViT block')
        group.add_argument('--model.classification.mit.head-dim', type=int, default=None, help='Head dimension in transformer')
        group.add_argument('--model.classification.mit.number-heads', type=int, default=None, help='Number of heads in transformer')
        return parser

    def _make_layer(self, opts, input_channel, cfg: Dict, dilate: Optional[bool]=False, *args, **kwargs) ->Tuple[nn.Sequential, int]:
        block_type = cfg.get('block_type', 'mobilevit')
        if block_type.lower() == 'mobilevit':
            return self._make_mit_layer(opts=opts, input_channel=input_channel, cfg=cfg, dilate=dilate)
        else:
            return self._make_mobilenet_layer(opts=opts, input_channel=input_channel, cfg=cfg)

    @staticmethod
    def _make_mobilenet_layer(opts, input_channel: int, cfg: Dict, *args, **kwargs) ->Tuple[nn.Sequential, int]:
        output_channels = cfg.get('out_channels')
        num_blocks = cfg.get('num_blocks', 2)
        expand_ratio = cfg.get('expand_ratio', 4)
        block = []
        for i in range(num_blocks):
            stride = cfg.get('stride', 1) if i == 0 else 1
            layer = InvertedResidual(opts=opts, in_channels=input_channel, out_channels=output_channels, stride=stride, expand_ratio=expand_ratio)
            block.append(layer)
            input_channel = output_channels
        return nn.Sequential(*block), input_channel

    def _make_mit_layer(self, opts, input_channel, cfg: Dict, dilate: Optional[bool]=False, *args, **kwargs) ->Tuple[nn.Sequential, int]:
        prev_dilation = self.dilation
        block = []
        stride = cfg.get('stride', 1)
        if stride == 2:
            if dilate:
                self.dilation *= 2
                stride = 1
            layer = InvertedResidual(opts=opts, in_channels=input_channel, out_channels=cfg.get('out_channels'), stride=stride, expand_ratio=cfg.get('mv_expand_ratio', 4), dilation=prev_dilation)
            block.append(layer)
            input_channel = cfg.get('out_channels')
        head_dim = cfg.get('head_dim', 32)
        transformer_dim = cfg['transformer_channels']
        ffn_dim = cfg.get('ffn_dim')
        if head_dim is None:
            num_heads = cfg.get('num_heads', 4)
            if num_heads is None:
                num_heads = 4
            head_dim = transformer_dim // num_heads
        if transformer_dim % head_dim != 0:
            logger.error('Transformer input dimension should be divisible by head dimension. Got {} and {}.'.format(transformer_dim, head_dim))
        block.append(MobileViTBlock(opts=opts, in_channels=input_channel, transformer_dim=transformer_dim, ffn_dim=ffn_dim, n_transformer_blocks=cfg.get('transformer_blocks', 1), patch_h=cfg.get('patch_h', 2), patch_w=cfg.get('patch_w', 2), dropout=getattr(opts, 'model.classification.mit.dropout', 0.1), ffn_dropout=getattr(opts, 'model.classification.mit.ffn_dropout', 0.0), attn_dropout=getattr(opts, 'model.classification.mit.attn_dropout', 0.1), head_dim=head_dim, no_fusion=getattr(opts, 'model.classification.mit.no_fuse_local_global_features', False), conv_ksize=getattr(opts, 'model.classification.mit.conv_kernel_size', 3)))
        return nn.Sequential(*block), input_channel


class MobileViTv2(BaseEncoder):
    """
    This class defines the `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ architecture
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        mobilevit_config = get_configuration(opts=opts)
        image_channels = mobilevit_config['layer0']['img_channels']
        out_channels = mobilevit_config['layer0']['out_channels']
        super().__init__(opts, *args, **kwargs)
        self.model_conf_dict = dict()
        self.conv_1 = ConvLayer(opts=opts, in_channels=image_channels, out_channels=out_channels, kernel_size=3, stride=2, use_norm=True, use_act=True)
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_1, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer1'])
        self.model_conf_dict['layer1'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_2, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer2'])
        self.model_conf_dict['layer2'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_3, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer3'])
        self.model_conf_dict['layer3'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_4, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer4'], dilate=self.dilate_l4)
        self.model_conf_dict['layer4'] = {'in': in_channels, 'out': out_channels}
        in_channels = out_channels
        self.layer_5, out_channels = self._make_layer(opts=opts, input_channel=in_channels, cfg=mobilevit_config['layer5'], dilate=self.dilate_l5)
        self.model_conf_dict['layer5'] = {'in': in_channels, 'out': out_channels}
        self.conv_1x1_exp = Identity()
        self.model_conf_dict['exp_before_cls'] = {'in': out_channels, 'out': out_channels}
        self.classifier = nn.Sequential(GlobalPool(pool_type=pool_type, keep_dim=False), LinearLayer(in_features=out_channels, out_features=num_classes, bias=True))
        self.check_model()
        self.reset_parameters(opts=opts)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.mitv2.attn-dropout', type=float, default=0.0, help='Dropout in attention layer. Defaults to 0.0')
        group.add_argument('--model.classification.mitv2.ffn-dropout', type=float, default=0.0, help='Dropout between FFN layers. Defaults to 0.0')
        group.add_argument('--model.classification.mitv2.dropout', type=float, default=0.0, help='Dropout in attention layer. Defaults to 0.0')
        group.add_argument('--model.classification.mitv2.width-multiplier', type=float, default=1.0, help='Width multiplier. Defaults to 1.0')
        group.add_argument('--model.classification.mitv2.attn-norm-layer', type=str, default='layer_norm_2d', help='Norm layer in attention block. Defaults to LayerNorm')
        return parser

    def _make_layer(self, opts, input_channel, cfg: Dict, dilate: Optional[bool]=False) ->Tuple[nn.Sequential, int]:
        block_type = cfg.get('block_type', 'mobilevit')
        if block_type.lower() == 'mobilevit':
            return self._make_mit_layer(opts=opts, input_channel=input_channel, cfg=cfg, dilate=dilate)
        else:
            return self._make_mobilenet_layer(opts=opts, input_channel=input_channel, cfg=cfg)

    @staticmethod
    def _make_mobilenet_layer(opts, input_channel: int, cfg: Dict) ->Tuple[nn.Sequential, int]:
        output_channels = cfg.get('out_channels')
        num_blocks = cfg.get('num_blocks', 2)
        expand_ratio = cfg.get('expand_ratio', 4)
        block = []
        for i in range(num_blocks):
            stride = cfg.get('stride', 1) if i == 0 else 1
            layer = InvertedResidual(opts=opts, in_channels=input_channel, out_channels=output_channels, stride=stride, expand_ratio=expand_ratio)
            block.append(layer)
            input_channel = output_channels
        return nn.Sequential(*block), input_channel

    def _make_mit_layer(self, opts, input_channel, cfg: Dict, dilate: Optional[bool]=False) ->Tuple[nn.Sequential, int]:
        prev_dilation = self.dilation
        block = []
        stride = cfg.get('stride', 1)
        if stride == 2:
            if dilate:
                self.dilation *= 2
                stride = 1
            layer = InvertedResidual(opts=opts, in_channels=input_channel, out_channels=cfg.get('out_channels'), stride=stride, expand_ratio=cfg.get('mv_expand_ratio', 4), dilation=prev_dilation)
            block.append(layer)
            input_channel = cfg.get('out_channels')
        attn_unit_dim = cfg['attn_unit_dim']
        ffn_multiplier = cfg.get('ffn_multiplier')
        dropout = getattr(opts, 'model.classification.mitv2.dropout', 0.0)
        block.append(Block(opts=opts, in_channels=input_channel, attn_unit_dim=attn_unit_dim, ffn_multiplier=ffn_multiplier, n_attn_blocks=cfg.get('attn_blocks', 1), patch_h=cfg.get('patch_h', 2), patch_w=cfg.get('patch_w', 2), dropout=dropout, ffn_dropout=getattr(opts, 'model.classification.mitv2.ffn_dropout', 0.0), attn_dropout=getattr(opts, 'model.classification.mitv2.attn_dropout', 0.0), conv_ksize=3, attn_norm_layer=getattr(opts, 'model.classification.mitv2.attn_norm_layer', 'layer_norm_2d'), dilation=self.dilation))
        return nn.Sequential(*block), input_channel


class BasicResNetBlock(BaseModule):
    """
    This class defines the Basic block in the `ResNet model <https://arxiv.org/abs/1512.03385>`_
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        mid_channels (int): :math:`C_{mid}` from an expected tensor of size :math:`(N, C_{mid}, H_{out}, W_{out})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        stride (Optional[int]): Stride for convolution. Default: 1
        dilation (Optional[int]): Dilation for convolution. Default: 1
        dropout (Optional[float]): Dropout after second convolution. Default: 0.0

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    """
    expansion: int = 1

    def __init__(self, opts, in_channels: int, mid_channels: int, out_channels: int, stride: Optional[int]=1, dilation: Optional[int]=1, dropout: Optional[float]=0.0, *args, **kwargs) ->None:
        act_type = getattr(opts, 'model.activation.name', 'relu')
        neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        inplace = getattr(opts, 'model.activation.inplace', False)
        cbr_1 = ConvLayer(opts=opts, in_channels=in_channels, out_channels=mid_channels, kernel_size=3, stride=stride, dilation=dilation, use_norm=True, use_act=True)
        cb_2 = ConvLayer(opts=opts, in_channels=mid_channels, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=False, dilation=dilation)
        block = nn.Sequential()
        block.add_module(name='conv_batch_act_1', module=cbr_1)
        block.add_module(name='conv_batch_2', module=cb_2)
        if 0.0 < dropout < 1.0:
            block.add_module(name='dropout', module=Dropout(p=dropout))
        down_sample = Identity()
        if stride == 2:
            down_sample = ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, use_norm=True, use_act=False)
        super().__init__()
        self.block = block
        self.down_sample = down_sample
        self.final_act = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
        self.stride = stride
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.dilation = dilation
        self.dropout = dropout

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        out = self.block(x)
        res = self.down_sample(x)
        out = out + res
        return self.final_act(out)

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        out, n_params, n_macs = module_profile(module=self.block, x=input)
        _, n_params_down, n_macs_down = module_profile(module=self.down_sample, x=input)
        return out, n_params + n_params_down, n_macs + n_macs_down

    def __repr__(self) ->str:
        return '{}(in_channels={}, out_channels={}, stride={}, dilation={}, dropout={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.stride, self.dilation, self.dropout)


class BottleneckResNetBlock(BaseModule):
    """
    This class defines the Bottleneck block in the `ResNet model <https://arxiv.org/abs/1512.03385>`_
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        mid_channels (int): :math:`C_{mid}` from an expected tensor of size :math:`(N, C_{mid}, H_{out}, W_{out})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        stride (Optional[int]): Stride for convolution. Default: 1
        dilation (Optional[int]): Dilation for convolution. Default: 1
        dropout (Optional[float]): Dropout after third convolution. Default: 0.0

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    """
    expansion: int = 4

    def __init__(self, opts, in_channels: int, mid_channels: int, out_channels: int, stride: Optional[int]=1, dilation: Optional[int]=1, dropout: Optional[float]=0.0, *args, **kwargs) ->None:
        act_type = getattr(opts, 'model.activation.name', 'relu')
        neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        inplace = getattr(opts, 'model.activation.inplace', False)
        cbr_1 = ConvLayer(opts=opts, in_channels=in_channels, out_channels=mid_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        cbr_2 = ConvLayer(opts=opts, in_channels=mid_channels, out_channels=mid_channels, kernel_size=3, stride=stride, use_norm=True, use_act=True, dilation=dilation)
        cb_3 = ConvLayer(opts=opts, in_channels=mid_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=False)
        block = nn.Sequential()
        block.add_module(name='conv_batch_act_1', module=cbr_1)
        block.add_module(name='conv_batch_act_2', module=cbr_2)
        block.add_module(name='conv_batch_3', module=cb_3)
        if 0.0 < dropout < 1.0:
            block.add_module(name='dropout', module=Dropout(p=dropout))
        down_sample = Identity()
        if stride == 2:
            down_sample = ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, use_norm=True, use_act=False)
        elif in_channels != out_channels:
            down_sample = ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=False)
        super().__init__()
        self.block = block
        self.down_sample = down_sample
        self.final_act = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
        self.stride = stride
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.mid_channels = mid_channels
        self.dilation = dilation
        self.dropout = dropout

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        out = self.block(x)
        res = self.down_sample(x)
        out = out + res
        return self.final_act(out)

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        out, n_params, n_macs = module_profile(module=self.block, x=input)
        _, n_params_down, n_macs_down = module_profile(module=self.down_sample, x=input)
        return out, n_params + n_params_down, n_macs + n_macs_down

    def __repr__(self) ->str:
        return '{}(in_channels={}, mid_channels={}, out_channels={}, stride={}, dilation={}, dropout={})'.format(self.__class__.__name__, self.in_channels, self.mid_channels, self.out_channels, self.stride, self.dilation, self.dropout)


class ResNet(BaseEncoder):
    """
    This class implements the `ResNet architecture <https://arxiv.org/pdf/1512.03385.pdf>`_

    .. note::
        Our ResNet implementation is different from the original implementation in two ways:
        1. First 7x7 strided conv is replaced with 3x3 strided conv
        2. MaxPool operation is replaced with another 3x3 strided depth-wise conv
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        image_channels = 3
        input_channels = 64
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout', 0.2)
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        cfg = get_configuration(opts=opts)
        super().__init__(opts, *args, **kwargs)
        self.model_conf_dict = dict()
        self.conv_1 = ConvLayer(opts=opts, in_channels=image_channels, out_channels=input_channels, kernel_size=3, stride=2, use_norm=True, use_act=True)
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': input_channels}
        self.layer_1 = ConvLayer(opts=opts, in_channels=input_channels, out_channels=input_channels, kernel_size=3, stride=2, use_norm=True, use_act=True, groups=input_channels)
        self.model_conf_dict['layer1'] = {'in': input_channels, 'out': input_channels}
        self.layer_2, out_channels = self._make_layer(opts=opts, in_channels=input_channels, layer_config=cfg['layer2'])
        self.model_conf_dict['layer2'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_3, out_channels = self._make_layer(opts=opts, in_channels=input_channels, layer_config=cfg['layer3'])
        self.model_conf_dict['layer3'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_4, out_channels = self._make_layer(opts=opts, in_channels=input_channels, layer_config=cfg['layer4'], dilate=self.dilate_l4)
        self.model_conf_dict['layer4'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.layer_5, out_channels = self._make_layer(opts=opts, in_channels=input_channels, layer_config=cfg['layer5'], dilate=self.dilate_l5)
        self.model_conf_dict['layer5'] = {'in': input_channels, 'out': out_channels}
        input_channels = out_channels
        self.conv_1x1_exp = Identity()
        self.model_conf_dict['exp_before_cls'] = {'in': input_channels, 'out': input_channels}
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='classifier_dropout', module=Dropout(p=classifier_dropout))
        self.classifier.add_module(name='classifier_fc', module=LinearLayer(in_features=input_channels, out_features=num_classes, bias=True))
        self.model_conf_dict['cls'] = {'in': input_channels, 'out': num_classes}
        self.check_model()
        self.reset_parameters(opts=opts)

    def _make_layer(self, opts, in_channels: int, layer_config: Dict, dilate: bool=False, *args, **kwargs) ->Tuple[nn.Sequential, int]:
        block_type = BottleneckResNetBlock if layer_config.get('block_type', 'bottleneck').lower() == 'bottleneck' else BasicResNetBlock
        mid_channels = layer_config.get('mid_channels')
        num_blocks = layer_config.get('num_blocks', 2)
        stride = layer_config.get('stride', 1)
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        out_channels = block_type.expansion * mid_channels
        dropout = getattr(opts, 'model.classification.resnet.dropout', 0.0)
        block = nn.Sequential()
        block.add_module(name='block_0', module=block_type(opts=opts, in_channels=in_channels, mid_channels=mid_channels, out_channels=out_channels, stride=stride, dilation=previous_dilation, dropout=dropout))
        for block_idx in range(1, num_blocks):
            block.add_module(name='block_{}'.format(block_idx), module=block_type(opts=opts, in_channels=out_channels, mid_channels=mid_channels, out_channels=out_channels, stride=1, dilation=self.dilation, dropout=dropout))
        return block, out_channels

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.resnet.depth', type=int, default=50)
        group.add_argument('--model.classification.resnet.dropout', type=float, default=0.0, help='Dropout in Resnet blocks')
        return parser


def _patch_merging_pad(x):
    H, W, _ = x.shape[-3:]
    x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))
    return x


class PatchMerging(BaseModule):
    """Patch Merging Layer.
    Args:
        dim (int): Number of input channels.
        norm_layer (str): Normalization layer name.
        strided (Optional[bool]): Down-sample the input by a factor of 2. Default is True.
    """

    def __init__(self, opts, dim: int, norm_layer: str, strided: Optional[bool]=True):
        super().__init__()
        self.dim = dim
        self.reduction = LinearLayer(in_features=4 * dim, out_features=2 * dim, bias=False)
        self.norm = get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=4 * dim)
        self.strided = strided

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        """
        Args:
            x (Tensor): input tensor with expected layout of [..., H, W, C]
        Returns:
            Tensor with layout of [..., H/2, W/2, 2*C]
        """
        x = _patch_merging_pad(x)
        if self.strided:
            x0 = x[..., 0::2, 0::2, :]
            x1 = x[..., 1::2, 0::2, :]
            x2 = x[..., 0::2, 1::2, :]
            x3 = x[..., 1::2, 1::2, :]
            x = torch.cat([x0, x1, x2, x3], -1)
        else:
            x = torch.cat([x, x, x, x], -1)
        x = self.norm(x)
        x = self.reduction(x)
        return x

    def __repr__(self) ->str:
        s = f'{self.__class__.__name__}(dim={self.dim})'
        return s


class Permute(BaseModule):
    """This module returns a view of the tensor input with its dimensions permuted.
    Args:
        dims (List[int]): The desired ordering of dimensions
    """

    def __init__(self, dims: List[int]):
        super().__init__()
        self.dims = dims

    def forward(self, x: Tensor) ->Tensor:
        return torch.permute(x, self.dims)

    def __repr__(self) ->str:
        s = f'{self.__class__.__name__}(dims={self.dims})'
        return s


def shifted_window_attention(input: Tensor, qkv_weight: Tensor, proj_weight: Tensor, relative_position_bias: Tensor, window_size: List[int], num_heads: int, shift_size: List[int], attention_dropout: float=0.0, dropout: float=0.0, qkv_bias: Optional[Tensor]=None, proj_bias: Optional[Tensor]=None):
    """
    Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        input (Tensor[N, H, W, C]): The input tensor or 4-dimensions.
        qkv_weight (Tensor[in_dim, out_dim]): The weight tensor of query, key, value.
        proj_weight (Tensor[out_dim, out_dim]): The weight tensor of projection.
        relative_position_bias (Tensor): The learned relative position bias added to attention.
        window_size (List[int]): Window size.
        num_heads (int): Number of attention heads.
        shift_size (List[int]): Shift size for shifted window attention.
        attention_dropout (float): Dropout ratio of attention weight. Default: 0.0.
        dropout (float): Dropout ratio of output. Default: 0.0.
        qkv_bias (Tensor[out_dim], optional): The bias tensor of query, key, value. Default: None.
        proj_bias (Tensor[out_dim], optional): The bias tensor of projection. Default: None.
    Returns:
        Tensor[N, H, W, C]: The output tensor after shifted window attention.
    """
    B, H, W, C = input.shape
    pad_r = (window_size[1] - W % window_size[1]) % window_size[1]
    pad_b = (window_size[0] - H % window_size[0]) % window_size[0]
    x = F.pad(input, (0, 0, 0, pad_r, 0, pad_b))
    _, pad_H, pad_W, _ = x.shape
    shift_size = shift_size.copy()
    if window_size[0] >= pad_H:
        shift_size[0] = 0
    if window_size[1] >= pad_W:
        shift_size[1] = 0
    if sum(shift_size) > 0:
        x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))
    num_windows = pad_H // window_size[0] * (pad_W // window_size[1])
    x = x.view(B, pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1], C)
    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B * num_windows, window_size[0] * window_size[1], C)
    qkv = F.linear(x, qkv_weight, qkv_bias)
    qkv = qkv.reshape(x.size(0), x.size(1), 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)
    q, k, v = qkv[0], qkv[1], qkv[2]
    q = q * (C // num_heads) ** -0.5
    attn = q.matmul(k.transpose(-2, -1))
    attn = attn + relative_position_bias
    if sum(shift_size) > 0:
        attn_mask = x.new_zeros((pad_H, pad_W))
        h_slices = (0, -window_size[0]), (-window_size[0], -shift_size[0]), (-shift_size[0], None)
        w_slices = (0, -window_size[1]), (-window_size[1], -shift_size[1]), (-shift_size[1], None)
        count = 0
        for h in h_slices:
            for w in w_slices:
                attn_mask[h[0]:h[1], w[0]:w[1]] = count
                count += 1
        attn_mask = attn_mask.view(pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1])
        attn_mask = attn_mask.permute(0, 2, 1, 3).reshape(num_windows, window_size[0] * window_size[1])
        attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        attn = attn.view(x.size(0) // num_windows, num_windows, num_heads, x.size(1), x.size(1))
        attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)
        attn = attn.view(-1, num_heads, x.size(1), x.size(1))
    attn = F.softmax(attn, dim=-1)
    attn = F.dropout(attn, p=attention_dropout)
    x = attn.matmul(v).transpose(1, 2).reshape(x.size(0), x.size(1), C)
    x = F.linear(x, proj_weight, proj_bias)
    x = F.dropout(x, p=dropout)
    x = x.view(B, pad_H // window_size[0], pad_W // window_size[1], window_size[0], window_size[1], C)
    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, pad_H, pad_W, C)
    if sum(shift_size) > 0:
        x = torch.roll(x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))
    x = x[:, :H, :W, :].contiguous()
    return x


class ShiftedWindowAttention(BaseModule):
    """
    See :func:`shifted_window_attention`.
    """

    def __init__(self, dim: int, window_size: List[int], shift_size: List[int], num_heads: int, qkv_bias: bool=True, proj_bias: bool=True, attention_dropout: float=0.0, dropout: float=0.0):
        super().__init__()
        if len(window_size) != 2 or len(shift_size) != 2:
            raise ValueError('window_size and shift_size must be of length 2')
        self.window_size = window_size
        self.shift_size = shift_size
        self.num_heads = num_heads
        self.attention_dropout = attention_dropout
        self.dropout = dropout
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim, bias=proj_bias)
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1).view(-1)
        self.register_buffer('relative_position_index', relative_position_index)
        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)
        self.embed_dim = dim

    def __repr__(self) ->str:
        return '{}(embed_dim={}, window_size={}, shift_size={}, num_heads={}, dropout={}, attn_dropout={}, dropout={})'.format(self.__class__.__name__, self.embed_dim, self.window_size, self.shift_size, self.num_heads, self.attention_dropout, self.dropout)

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        """
        Args:
            x (Tensor): Tensor with layout of [B, H, W, C]
        Returns:
            Tensor with same layout as input, i.e. [B, H, W, C]
        """
        N = self.window_size[0] * self.window_size[1]
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index]
        relative_position_bias = relative_position_bias.view(N, N, -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)
        return shifted_window_attention(x, self.qkv.weight, self.proj.weight, relative_position_bias, self.window_size, self.num_heads, shift_size=self.shift_size, attention_dropout=self.attention_dropout, dropout=self.dropout, qkv_bias=self.qkv.bias, proj_bias=self.proj.bias)


class SwinTransformerBlock(BaseModule):
    """
    Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (List[int]): Window size.
        shift_size (List[int]): Shift size for shifted window attention.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.
        dropout (float): Dropout rate. Default: 0.0.
        attention_dropout (float): Attention dropout rate. Default: 0.0.
        stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0.
        norm_layer (nn.Module): Normalization layer.  Default: nn.LayerNorm.
        attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttention
    """

    def __init__(self, opts, embed_dim: int, num_heads: int, window_size: List[int], shift_size: List[int], mlp_ratio: float=4.0, dropout: float=0.0, attn_dropout: Optional[float]=0.0, ffn_dropout: Optional[float]=0.0, stochastic_depth_prob: float=0.0, norm_layer: Optional[str]='layer_norm'):
        super().__init__()
        attn_unit = ShiftedWindowAttention(embed_dim, window_size, shift_size, num_heads, attention_dropout=attn_dropout, dropout=dropout)
        self.attn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), attn_unit, Dropout(p=dropout))
        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, 'row')
        ffn_latent_dim = int(embed_dim * mlp_ratio)
        act_name = self.build_act_layer(opts=opts)
        self.mlp = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), LinearLayer(in_features=embed_dim, out_features=ffn_latent_dim, bias=True), act_name, Dropout(p=ffn_dropout), LinearLayer(in_features=ffn_latent_dim, out_features=embed_dim, bias=True), Dropout(p=dropout))
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_latent_dim
        self.ffn_dropout = ffn_dropout
        self.std_dropout = dropout
        self.attn_fn_name = attn_unit.__class__.__name__
        self.act_fn_name = act_name.__class__.__name__
        self.norm_type = norm_layer

    @staticmethod
    def build_act_layer(opts) ->nn.Module:
        act_type = getattr(opts, 'model.activation.name', 'gelu')
        neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        inplace = getattr(opts, 'model.activation.inplace', False)
        act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=1)
        return act_layer

    def __repr__(self) ->str:
        return '{}(embed_dim={}, ffn_dim={}, dropout={}, ffn_dropout={}, attn_fn={}, act_fn={}, norm_fn={})'.format(self.__class__.__name__, self.embed_dim, self.ffn_dim, self.std_dropout, self.ffn_dropout, self.attn_fn_name, self.act_fn_name, self.norm_type)

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        x = x + self.stochastic_depth(self.attn(x))
        x = x + self.stochastic_depth(self.mlp(x))
        return x


class SwinTransformer(BaseEncoder):
    """
    Implements Swin Transformer from the `"Swin Transformer: Hierarchical Vision Transformer using
    Shifted Windows" <https://arxiv.org/pdf/2103.14030>`_ paper.

    The code is adapted from `"Torchvision repository" <https://github.com/pytorch/vision/blob/main/torchvision/models/swin_transformer.py>`_
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        image_channels = 3
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        classifier_dropout = getattr(opts, 'model.classification.classifier_dropout', 0.0)
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        super().__init__(opts, *args, **kwargs)
        cfg = get_configuration(opts=opts)
        patch_size = cfg['patch_size']
        embed_dim = cfg['embed_dim']
        depths = cfg['depths']
        window_size = cfg['window_size']
        mlp_ratio = cfg['mlp_ratio']
        num_heads = cfg['num_heads']
        dropout = cfg['dropout']
        attn_dropout = cfg['attn_dropout']
        ffn_dropout = cfg['ffn_dropout']
        stochastic_depth_prob = cfg['stochastic_depth_prob']
        norm_layer = cfg['norm_layer']
        self.model_conf_dict = dict()
        self.conv_1 = nn.Sequential(*[ConvLayer(opts=opts, in_channels=image_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size, use_norm=False, use_act=False), Permute([0, 2, 3, 1]), get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim)])
        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': embed_dim}
        in_channels = embed_dim
        self.model_conf_dict['layer1'] = {'in': embed_dim, 'out': embed_dim}
        layers: List[nn.Module] = []
        total_stage_blocks = sum(depths)
        stage_block_id = 0
        for i_stage in range(len(depths)):
            stage: List[nn.Module] = []
            dim = embed_dim * 2 ** i_stage
            for i_layer in range(depths[i_stage]):
                sd_prob = stochastic_depth_prob * float(stage_block_id) / (total_stage_blocks - 1)
                stage.append(SwinTransformerBlock(opts, dim, num_heads[i_stage], window_size=window_size, shift_size=[(0 if i_layer % 2 == 0 else w // 2) for w in window_size], mlp_ratio=mlp_ratio, dropout=dropout, attn_dropout=attn_dropout, ffn_dropout=ffn_dropout, stochastic_depth_prob=sd_prob, norm_layer=norm_layer))
                stage_block_id += 1
            if i_stage < len(depths) - 1:
                stage += [PatchMerging(opts, dim, norm_layer)]
            layers.append(nn.Sequential(*stage))
            self.model_conf_dict['layer{}'.format(i_stage + 2)] = {'in': in_channels, 'out': dim}
            in_channels = dim
        self.layer_1, self.layer_2, self.layer_3, self.layer_4 = layers
        if self.dilate_l5:
            for m in self.layer_3.modules():
                if isinstance(m, PatchMerging):
                    m.strided = False
        if self.dilate_l4:
            for m in self.layer_2.modules():
                if isinstance(m, PatchMerging):
                    m.strided = False
        self.layer_5 = nn.Sequential(*[get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=in_channels), Permute([0, 3, 1, 2])])
        self.conv_1x1_exp = Identity()
        self.model_conf_dict['exp_before_cls'] = {'in': in_channels, 'out': in_channels}
        self.classifier = nn.Sequential()
        self.classifier.add_module(name='global_pool', module=GlobalPool(pool_type=pool_type, keep_dim=False))
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='classifier_dropout', module=Dropout(p=classifier_dropout))
        self.classifier.add_module(name='classifier_fc', module=LinearLayer(in_features=in_channels, out_features=num_classes, bias=True))
        self.model_conf_dict['cls'] = {'in': in_channels, 'out': num_classes}
        extract_enc_point_format = getattr(opts, 'model.classification.swin.extract_end_point_format', 'nchw')
        if extract_enc_point_format not in ['nchw', 'nhwc']:
            logger.error('End point extraction format should be either nchw or nhwc. Got: {}'.format(extract_enc_point_format))
        self.extract_end_point_nchw_format = extract_enc_point_format == 'nchw'
        self.check_model()
        self.reset_parameters(opts=opts)

    def extract_end_points_all(self, x: Tensor, use_l5: Optional[bool]=True, use_l5_exp: Optional[bool]=False, *args, **kwargs) ->Dict[str, Tensor]:
        out_dict = {}
        if self.training and self.neural_augmentor is not None:
            x = self.neural_augmentor(x)
            out_dict['augmented_tensor'] = x
        x = self.conv_1(x)
        if self.extract_end_point_nchw_format:
            x_nchw = torch.permute(x, dims=(0, 3, 1, 2))
            out_dict['out_l1'] = x_nchw
            out_dict['out_l2'] = x_nchw
        else:
            out_dict['out_l1'] = x
            out_dict['out_l2'] = x
        x = self.layer_1(x)
        out_dict['out_l3'] = torch.permute(x, dims=(0, 3, 1, 2)) if self.extract_end_point_nchw_format else x
        x = self.layer_2(x)
        out_dict['out_l4'] = torch.permute(x, dims=(0, 3, 1, 2)) if self.extract_end_point_nchw_format else x
        if use_l5:
            x = self.layer_3(x)
            x = self.layer_4(x)
            x = self.layer_5(x)
            out_dict['out_l5'] = x if self.extract_end_point_nchw_format else torch.permute(x, dims=(0, 2, 3, 1))
            if use_l5_exp:
                x = self.conv_1x1_exp(x)
                out_dict['out_l5_exp'] = x
        return out_dict

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.swin.mode', type=str, default='tiny', help='SwinTransformer mode. Default is swin_t')
        group.add_argument('--model.classification.swin.stochastic-depth-prob', type=float, default=None)
        group.add_argument('--model.classification.swin.extract-end-point-format', type=str, default='nchw', choices=['nchw', 'nhwc'], help='End point extraction format in Swin Transformer. This is useful for down-stream tasks where task-specific heads are either in nhwc format or nchw format. Defaults to nchw.')
        return parser

    def profile_model(self, input: Tensor, is_classification: Optional[bool]=True, *args, **kwargs) ->Tuple[Union[Tensor, Dict[str, Tensor]], float, float]:
        """
        Helper function to profile SwinTrasnformer Model
        """
        logger.log('Model statistics for an input of size {}'.format(input.size()))
        logger.double_dash_line(dashes=65)
        None
        logger.double_dash_line(dashes=65)
        overall_params = sum([p.numel() for p in self.parameters()])
        None
        overall_macs = 0.0
        try:
            flop_analyzer = FlopCountAnalysis(self.eval(), input)
            flop_analyzer.unsupported_ops_warnings(False)
            flop_analyzer.uncalled_modules_warnings(False)
            overall_macs = flop_analyzer.total()
            None
        except Exception:
            pass
        None
        logger.double_dash_line(dashes=65)
        out_dict = self.extract_end_points_all(x=input)
        return out_dict, overall_params, overall_macs


class VisionTransformer(BaseEncoder):
    """
    This class defines the `Vision Transformer architecture <https://arxiv.org/abs/2010.11929>`_. Our model implementation
    is inspired from `Early Convolutions Help Transformers See Better <https://arxiv.org/abs/2106.14881>`_

    .. note::
        Our implementation is different from the original implementation in two ways:
        1. Kernel size is odd.
        2. Our positional encoding implementation allows us to use ViT with any multiple input scales
        3. We do not use StochasticDepth
        4. We do not add positional encoding to class token (if enabled), as suggested in `DeiT-3 paper <https://arxiv.org/abs/2204.07118>`_
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        image_channels = 3
        num_classes = getattr(opts, 'model.classification.n_classes', 1000)
        pytorch_mha = getattr(opts, 'model.classification.vit.use_pytorch_mha', False)
        super().__init__(opts, *args, **kwargs)
        if pytorch_mha and self.gradient_checkpointing:
            logger.error('Current version of ViT supports PyTorch MHA without gradient checkpointing. Please use either of them, but not both')
        vit_config = get_configuration(opts)
        kernel_sizes_conv_stem = [4, 2, 2]
        num_embeddings = (224 // 16) ** 2
        embed_dim = vit_config['embed_dim']
        ffn_dim = vit_config['ffn_dim']
        pos_emb_drop_p = vit_config['pos_emb_drop_p']
        n_transformer_layers = vit_config['n_transformer_layers']
        num_heads = vit_config['n_attn_heads']
        attn_dropout = vit_config['attn_dropout']
        dropout = vit_config['dropout']
        ffn_dropout = vit_config['ffn_dropout']
        norm_layer = vit_config['norm_layer']
        conv_stem_proj_dim = max(32, embed_dim // 4)
        patch_emb = [ConvLayer(opts=opts, in_channels=image_channels, out_channels=conv_stem_proj_dim, kernel_size=kernel_sizes_conv_stem[0], stride=kernel_sizes_conv_stem[0], bias=False, use_norm=True, use_act=True), ConvLayer(opts=opts, in_channels=conv_stem_proj_dim, out_channels=conv_stem_proj_dim, kernel_size=kernel_sizes_conv_stem[1], stride=kernel_sizes_conv_stem[1], bias=False, use_norm=True, use_act=True), ConvLayer(opts=opts, in_channels=conv_stem_proj_dim, out_channels=embed_dim, kernel_size=kernel_sizes_conv_stem[2], stride=kernel_sizes_conv_stem[2], bias=True, use_norm=False, use_act=False)]
        self.patch_emb = nn.Sequential(*patch_emb)
        use_cls_token = not getattr(opts, 'model.classification.vit.no_cls_token', False)
        transformer_blocks = [TransformerEncoder(opts=opts, embed_dim=embed_dim, ffn_latent_dim=ffn_dim, num_heads=num_heads, attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, transformer_norm_layer=norm_layer) for _ in range(n_transformer_layers)]
        self.post_transformer_norm = get_normalization_layer(opts=opts, num_features=embed_dim, norm_type=norm_layer)
        self.transformer = nn.Sequential(*transformer_blocks)
        self.classifier = LinearLayer(embed_dim, num_classes)
        self.reset_parameters(opts=opts)
        if use_cls_token:
            self.cls_token = nn.Parameter(torch.zeros(size=(1, 1, embed_dim)))
            torch.nn.init.trunc_normal_(self.cls_token, std=0.02)
        else:
            self.cls_token = None
        self.pos_embed = PositionalEmbedding(opts=opts, num_embeddings=num_embeddings, embedding_dim=embed_dim, sequence_first=False, padding_idx=None, is_learnable=not getattr(opts, 'model.classification.vit.sinusoidal_pos_emb', False), interpolation_mode='bilinear')
        self.emb_dropout = Dropout(p=pos_emb_drop_p)
        self.use_pytorch_mha = pytorch_mha
        self.embed_dim = embed_dim
        self.checkpoint_segments = getattr(opts, 'model.classification.vit.checkpoint_segments', 4)
        self.model_conf_dict = {'conv1': {'in': image_channels, 'out': embed_dim}, 'layer1': {'in': embed_dim, 'out': embed_dim}, 'layer2': {'in': embed_dim, 'out': embed_dim}, 'layer3': {'in': embed_dim, 'out': embed_dim}, 'layer4': {'in': embed_dim, 'out': embed_dim}, 'layer5': {'in': embed_dim, 'out': embed_dim}, 'exp_before_cls': {'in': embed_dim, 'out': embed_dim}, 'cls': {' in ': embed_dim, 'out': num_classes}}
        use_simple_fpn = getattr(opts, 'model.classification.vit.use_simple_fpn', False)
        self.simple_fpn = None
        if use_simple_fpn:
            self.simple_fpn = self.build_simple_fpn_layers(opts, embed_dim)
            self.reset_simple_fpn_params()
        self.update_layer_norm_eps()

    def update_layer_norm_eps(self):
        for m in self.modules():
            if isinstance(m, nn.LayerNorm):
                m.eps = 1e-06

    def reset_simple_fpn_params(self):
        for m in self.simple_fpn.modules():
            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                initialize_conv_layer(m, init_method='kaiming_uniform')

    @staticmethod
    def build_simple_fpn_layers(opts, embed_dim: int) ->nn.ModuleDict:
        layer_l2 = nn.Sequential(TransposeConvLayer(opts, in_channels=embed_dim, out_channels=embed_dim, kernel_size=2, stride=2, padding=0, output_padding=0, groups=1, use_norm=True, use_act=True), TransposeConvLayer(opts, in_channels=embed_dim, out_channels=embed_dim, kernel_size=2, stride=2, padding=0, output_padding=0, groups=1, use_norm=False, use_act=False, bias=True))
        layer_l3 = TransposeConvLayer(opts, in_channels=embed_dim, out_channels=embed_dim, kernel_size=2, stride=2, padding=0, output_padding=0, groups=1, use_norm=False, use_act=False, bias=True)
        layer_l4 = Identity()
        layer_l5 = MaxPool2d(kernel_size=2, stride=2, padding=0)
        simple_fpn_layers = nn.ModuleDict({'out_l2': layer_l2, 'out_l3': layer_l3, 'out_l4': layer_l4, 'out_l5': layer_l5})
        return simple_fpn_layers

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.classification.vit.mode', type=str, default='tiny', help='ViT mode. Default is Tiny')
        group.add_argument('--model.classification.vit.dropout', type=float, default=0.0, help='Dropout in ViT layers. Defaults to 0.0')
        group.add_argument('--model.classification.vit.norm-layer', type=str, default='layer_norm', help='Normalization layer in ViT')
        group.add_argument('--model.classification.vit.sinusoidal-pos-emb', action='store_true', help='Use sinusoidal positional encoding instead of learnable')
        group.add_argument('--model.classification.vit.no-cls-token', action='store_true', help='Do not use classification token')
        group.add_argument('--model.classification.vit.use-pytorch-mha', action='store_true', help="Use PyTorch's native multi-head attention")
        group.add_argument('--model.classification.vit.use-simple-fpn', action='store_true', help='Add simple FPN for down-stream tasks')
        group.add_argument('--model.classification.vit.checkpoint-segments', type=int, default=4, help='Number of checkpoint segments')
        return parser

    def _extract_features(self, x: Tensor, *args, **kwargs) ->Tensor:
        raise NotImplementedError('ViT does not support feature extraction the same way as CNN.')

    def extract_patch_embeddings(self, x: Tensor) ->Tuple[Tensor, Tuple[int, int]]:
        batch_size = x.shape[0]
        patch_emb = self.patch_emb(x)
        n_h, n_w = patch_emb.shape[-2:]
        patch_emb = patch_emb.flatten(2)
        patch_emb = patch_emb.transpose(1, 2).contiguous()
        n_patches = patch_emb.shape[1]
        pos_emb = self.pos_embed(n_patches)
        patch_emb = pos_emb + patch_emb
        if self.cls_token is not None:
            cls_tokens = self.cls_token.expand(batch_size, -1, -1)
            patch_emb = torch.cat((cls_tokens, patch_emb), dim=1)
        patch_emb = self.emb_dropout(patch_emb)
        return patch_emb, (n_h, n_w)

    def _forward_classifier(self, x: Tensor, *args, **kwargs) ->Tensor:
        x, _ = self.extract_patch_embeddings(x)
        if self.use_pytorch_mha:
            x = x.transpose(0, 1)
        if self.gradient_checkpointing:
            x = gradient_checkpoint_fn(self.transformer, self.checkpoint_segments, x)
        else:
            for layer in self.transformer:
                x = layer(x, use_pytorch_mha=self.use_pytorch_mha)
        x = self.post_transformer_norm(x)
        if self.cls_token is not None:
            x = x[0] if self.use_pytorch_mha else x[:, 0]
        else:
            x = torch.mean(x, dim=0) if self.use_pytorch_mha else torch.mean(x, dim=1)
        x = self.classifier(x)
        return x

    def extract_end_points_all(self, x: Tensor, use_l5: Optional[bool]=True, use_l5_exp: Optional[bool]=False, *args, **kwargs) ->Dict[str, Tensor]:
        if not self.simple_fpn:
            logger.error('Please enable simple FPN for down-stream tasks')
        if self.cls_token:
            logger.error('Please disable cls token for down-stream tasks')
        batch_size, in_dim, in_height, in_width = x.shape
        out_dict = {}
        if self.training and self.neural_augmentor is not None:
            x = self.neural_augmentor(x)
            out_dict['augmented_tensor'] = x
        x, (n_h, n_w) = self.extract_patch_embeddings(x)
        if self.gradient_checkpointing:
            x = gradient_checkpoint_fn(self.transformer, self.checkpoint_segments, input=x)
        else:
            x = self.transformer(x)
        x = self.post_transformer_norm(x)
        x = x.transpose(1, 2)
        x = x.reshape(batch_size, x.shape[1], n_h, n_w)
        for k, extra_layer in self.simple_fpn.items():
            out_dict[k] = extra_layer(x)
        return out_dict

    def profile_model(self, input: Tensor, *args, **kwargs) ->None:
        logger.log('Model statistics for an input of size {}'.format(input.size()))
        logger.double_dash_line(dashes=65)
        None
        logger.double_dash_line(dashes=65)
        overall_params, overall_macs = 0.0, 0.0
        patch_emb, overall_params, overall_macs = self._profile_layers(self.patch_emb, input=input, overall_params=overall_params, overall_macs=overall_macs)
        patch_emb = patch_emb.flatten(2)
        patch_emb = patch_emb.transpose(1, 2)
        if self.cls_token is not None:
            cls_tokens = self.cls_token.expand(patch_emb.shape[0], -1, -1)
            patch_emb = torch.cat((cls_tokens, patch_emb), dim=1)
        patch_emb, overall_params, overall_macs = self._profile_layers(self.transformer, input=patch_emb, overall_params=overall_params, overall_macs=overall_macs)
        patch_emb, overall_params, overall_macs = self._profile_layers(self.classifier, input=patch_emb[:, 0], overall_params=overall_params, overall_macs=overall_macs)
        logger.double_dash_line(dashes=65)
        None
        None
        overall_params_py = sum([p.numel() for p in self.parameters()])
        None
        logger.double_dash_line(dashes=65)


def _check_out_channels(config: dict, layer_name: str) ->int:
    enc_ch_l: dict = config.get(layer_name, None)
    if enc_ch_l is None or not enc_ch_l:
        logger.error('Encoder does not define input-output mapping for {}: Got: {}'.format(layer_name, config))
    enc_ch_l_out = enc_ch_l.get('out', None)
    if enc_ch_l_out is None or not enc_ch_l_out:
        logger.error('Output channels are not defined in {} of the encoder. Got: {}'.format(layer_name, enc_ch_l))
    return enc_ch_l_out


class BaseDetection(nn.Module):
    """
    Base class for the task of object detection
    """

    def __init__(self, opts, encoder: BaseEncoder) ->None:
        super().__init__()
        assert isinstance(encoder, BaseEncoder)
        self.encoder: BaseEncoder = encoder
        self.n_detection_classes = getattr(opts, 'model.detection.n_classes', 80)
        enc_conf = self.encoder.model_conf_dict
        enc_ch_l5_out_proj = _check_out_channels(enc_conf, 'exp_before_cls')
        enc_ch_l5_out = _check_out_channels(enc_conf, 'layer5')
        enc_ch_l4_out = _check_out_channels(enc_conf, 'layer4')
        enc_ch_l3_out = _check_out_channels(enc_conf, 'layer3')
        enc_ch_l2_out = _check_out_channels(enc_conf, 'layer2')
        enc_ch_l1_out = _check_out_channels(enc_conf, 'layer1')
        self.enc_l5_channels = enc_ch_l5_out
        self.enc_l5_channels_exp = enc_ch_l5_out_proj
        self.enc_l4_channels = enc_ch_l4_out
        self.enc_l3_channels = enc_ch_l3_out
        self.enc_l2_channels = enc_ch_l2_out
        self.enc_l1_channels = enc_ch_l1_out
        self.opts = opts

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """Add model specific arguments"""
        return parser

    @staticmethod
    def reset_layer_parameters(layer, opts) ->None:
        """Initialize weights of a given layer"""
        initialize_weights(opts=opts, modules=layer.modules())

    def get_trainable_parameters(self, weight_decay: float=0.0, no_decay_bn_filter_bias: bool=False, *args, **kwargs):
        """Returns a list of trainable parameters"""
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    @staticmethod
    def profile_layer(layer, input: Tensor) ->Tuple[Tensor, float, float]:
        block_params = block_macs = 0.0
        if isinstance(layer, nn.Sequential):
            for layer_i in range(len(layer)):
                input, layer_param, layer_macs = layer[layer_i].profile_module(input)
                block_params += layer_param
                block_macs += layer_macs
                None
        else:
            input, layer_param, layer_macs = layer.profile_module(input)
            block_params += layer_param
            block_macs += layer_macs
            None
        return input, block_params, block_macs

    def profile_model(self, input: Tensor):
        """
        Child classes must implement this function to compute FLOPs and parameters
        """
        raise NotImplementedError

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        raise NotImplementedError


class MaskRCNNEncoder(nn.Module):

    def __init__(self, opts, encoder: BaseEncoder, output_strides: List, projection_channels: int) ->None:
        use_fpn = not getattr(opts, 'model.detection.mask_rcnn.disable_fpn', False)
        super().__init__()
        encoder.conv_1x1_exp = Identity()
        encoder.classifier = Identity()
        backbone_proj_layers = nn.ModuleDict()
        self.backbone_output_strides = sorted(list({4, 8, 16, 32}.intersection(output_strides)))
        model_config = encoder.model_conf_dict
        self.backbone_map = {}
        fpn_proj_layers = nn.ModuleDict() if use_fpn else None
        for os in self.backbone_output_strides:
            if os == 4:
                in_channels = model_config['layer2']['out']
                backbone_os_str = 'out_l2'
            elif os == 8:
                in_channels = model_config['layer3']['out']
                backbone_os_str = 'out_l3'
            elif os == 16:
                in_channels = model_config['layer4']['out']
                backbone_os_str = 'out_l4'
            elif os == 32:
                in_channels = model_config['layer5']['out']
                backbone_os_str = 'out_l5'
            else:
                raise NotImplementedError
            conv_layer = ConvLayer(opts=opts, in_channels=in_channels, out_channels=projection_channels, kernel_size=1, use_norm=True, use_act=False)
            backbone_proj_layers.add_module(str(os), conv_layer)
            self.backbone_map[os] = backbone_os_str
            if use_fpn:
                fpn_layer = ConvLayer(opts=opts, in_channels=projection_channels, out_channels=projection_channels, kernel_size=3, use_norm=True, use_act=False)
                fpn_proj_layers.add_module(str(os), fpn_layer)
        extra_layers = nn.ModuleDict()
        extra_layer_os = sorted(list(set(self.backbone_output_strides) ^ set(output_strides)))
        for os in extra_layer_os:
            conv_layer = ConvLayer(opts=opts, in_channels=projection_channels, out_channels=projection_channels, kernel_size=3, stride=2, use_norm=True, use_act=False)
            extra_layers.add_module(str(os), conv_layer)
        self.encoder = encoder
        self.backbone_proj_layers = backbone_proj_layers
        self.fpn_proj_layers = fpn_proj_layers
        self.use_fpn = use_fpn
        self.extra_layers = extra_layers
        self.out_channels = projection_channels
        self.augmented_tensor = None

    def get_augmented_tensor(self) ->Tensor:
        return self.augmented_tensor

    def forward(self, x: Tensor) ->Dict[str, Tensor]:
        enc_end_points: Dict = self.encoder.extract_end_points_all(x)
        self.augmented_tensor = enc_end_points.pop('augmented_tensor', None)
        outputs_backbone: Dict = {}
        for os, enc_key_name in self.backbone_map.items():
            x_proj = self.backbone_proj_layers[str(os)](enc_end_points.pop(enc_key_name))
            outputs_backbone[f'{os}'] = x_proj
        if self.fpn_proj_layers:
            last_os = self.backbone_output_strides[-1]
            prev_fm = outputs_backbone[f'{last_os}']
            prev_fm = self.fpn_proj_layers[f'{last_os}'](prev_fm)
            for os in self.backbone_output_strides[:-1][::-1]:
                curr_fm = outputs_backbone[f'{os}']
                feat_shape = curr_fm.shape[-2:]
                inner_top_down = F.interpolate(prev_fm, size=feat_shape, mode='nearest')
                prev_fm = self.fpn_proj_layers[f'{os}'](curr_fm + inner_top_down)
                outputs_backbone[f'{os}'] = prev_fm
        if self.extra_layers:
            prev_os = self.backbone_output_strides[-1]
            for os, extra_layer in self.extra_layers.items():
                x_proj = extra_layer(outputs_backbone[f'{prev_os}'])
                outputs_backbone[f'{os}'] = x_proj
                prev_os = os
        return outputs_backbone


DetectionPredTuple = namedtuple(typename='DetectionPredTuple', field_names=('labels', 'scores', 'boxes', 'masks'), defaults=(None, None, None, None))


def replace_syncbn_with_syncbnfp32(opts, num_features: int) ->nn.Module:
    norm_layer = getattr(opts, 'model.normalization.name', None)
    if norm_layer.find('sync') > -1:
        return get_normalization_layer(opts, num_features=num_features, norm_type='sync_batch_norm_fp32')
    else:
        return get_normalization_layer(opts=opts, num_features=num_features)


class FastRCNNConvFCHead(nn.Sequential):

    def __init__(self, opts, input_size: Tuple[int, int, int], conv_layers: List[int], fc_layers: List[int], *args, **kwargs):
        """
        Args:
            input_size (Tuple[int, int, int]): the input size in CHW format.
            conv_layers (list): feature dimensions of each Convolution layer
            fc_layers (list): feature dimensions of each FCN layer
        """
        in_channels, in_height, in_width = input_size
        blocks = []
        previous_channels = in_channels
        for current_channels in conv_layers:
            blocks.extend([ConvLayer(opts, in_channels=previous_channels, out_channels=current_channels, kernel_size=3, stride=1, use_norm=False, use_act=False), replace_syncbn_with_syncbnfp32(opts, num_features=current_channels), nn.ReLU(inplace=False)])
            previous_channels = current_channels
        blocks.append(nn.Flatten())
        previous_channels = previous_channels * in_height * in_width
        for current_channels in fc_layers:
            blocks.append(LinearLayer(previous_channels, current_channels, bias=True))
            blocks.append(nn.ReLU(inplace=True))
            previous_channels = current_channels
        super().__init__(*blocks)
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')
            elif isinstance(layer, LinearLayer):
                initialize_fc_layer(module=layer, init_method='kaiming_uniform')


class FastRCNNPredictor(nn.Module):
    """
    Standard classification + bounding box regression layers
    for Fast R-CNN.

    Args:
        in_channels (int): number of input channels
        num_classes (int): number of output classes (including background)
    """

    def __init__(self, in_channels: int, num_classes: int) ->None:
        super().__init__()
        self.cls_score = LinearLayer(in_channels, num_classes, bias=True)
        self.bbox_pred = LinearLayer(in_channels, num_classes * 4, bias=True)
        for layer in self.modules():
            if isinstance(layer, LinearLayer):
                initialize_fc_layer(module=layer, init_method='kaiming_uniform')

    def forward(self, x: Tensor) ->Tuple[Tensor, Tensor]:
        if x.dim() == 4:
            torch._assert(list(x.shape[2:]) == [1, 1], f'x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}')
        x = x.flatten(start_dim=1)
        scores = self.cls_score(x)
        bbox_deltas = self.bbox_pred(x)
        return scores, bbox_deltas


class MaskRCNNHeads(nn.Sequential):

    def __init__(self, opts, in_channels: int, layers: List, dilation: int):
        """
        Args:
            in_channels (int): number of input channels
            layers (list): feature dimensions of each FCN layer
            dilation (int): dilation rate of kernel
            norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None
        """
        blocks = []
        next_feature = in_channels
        for layer_features in layers:
            blocks.extend([ConvLayer(opts=opts, in_channels=next_feature, out_channels=layer_features, kernel_size=3, stride=1, dilation=dilation, use_norm=False, use_act=False, bias=False), replace_syncbn_with_syncbnfp32(opts=opts, num_features=layer_features), nn.ReLU(inplace=False)])
            next_feature = layer_features
        super().__init__(*blocks)
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')


class MaskRCNNPredictor(nn.Sequential):

    def __init__(self, opts, in_channels: int, dim_reduced: int, num_classes: int) ->None:
        super().__init__(*[TransposeConvLayer(opts, in_channels=in_channels, out_channels=dim_reduced, kernel_size=2, stride=2, padding=0, output_padding=0, use_norm=False, use_act=False, bias=False, groups=1), replace_syncbn_with_syncbnfp32(opts, num_features=dim_reduced), nn.ReLU(inplace=False), ConvLayer(opts, in_channels=dim_reduced, out_channels=num_classes, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=False)])
        for layer in self.modules():
            if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')


class RPNHead(nn.Module):
    """
    Adds a simple RPN Head with classification and regression heads

    Args:
        in_channels (int): number of channels of the input feature
        num_anchors (int): number of anchors to be predicted
        conv_depth (int, optional): number of convolutions
    """

    def __init__(self, opts, in_channels: int, num_anchors: int, conv_depth=1) ->None:
        super().__init__()
        convs = []
        for _ in range(conv_depth):
            convs.extend([ConvLayer(opts, in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, use_norm=False, use_act=False, bias=False), replace_syncbn_with_syncbnfp32(opts, num_features=in_channels), nn.ReLU(inplace=False)])
        self.conv = nn.Sequential(*convs)
        self.cls_logits = ConvLayer(opts, in_channels=in_channels, out_channels=num_anchors, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True)
        self.bbox_pred = ConvLayer(opts, in_channels=in_channels, out_channels=num_anchors * 4, kernel_size=1, stride=1, use_act=False, use_norm=False, bias=True)
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='normal', std_val=0.01)

    def forward(self, x: List[Tensor]) ->Tuple[List[Tensor], List[Tensor]]:
        logits = []
        bbox_reg = []
        for feature in x:
            t = self.conv(feature)
            logits.append(self.cls_logits(t))
            bbox_reg.append(self.bbox_pred(t))
        return logits, bbox_reg


DETECT_MODEL_REGISTRY = {}


def register_detection_models(name):

    def register_model_class(cls):
        if name in DETECT_MODEL_REGISTRY:
            raise ValueError('Cannot register duplicate model ({})'.format(name))
        if not issubclass(cls, BaseDetection):
            raise ValueError('Model ({}: {}) must extend BaseDetection'.format(name, cls.__name__))
        DETECT_MODEL_REGISTRY[name] = cls
        return cls
    return register_model_class


class MaskRCNNDetector(BaseDetection):
    """
    This class implements a `Mask RCNN style object detector <https://arxiv.org/abs/1703.06870>`

    Args:
        opts: command-line arguments
        encoder (BaseEncoder): Encoder network (e.g., ResNet or MobileViT)
    """

    def __init__(self, opts, encoder: BaseEncoder):
        super().__init__(opts, encoder)
        default_norm = self.set_norm_layer_opts()
        output_strides = getattr(opts, 'model.detection.mask_rcnn.output_strides', [4, 8, 16, 32, 64])
        if len(output_strides) == 0:
            logger.error('Please specify output strides for extracting backbone feature maps using --model.detection.mask-rcnn.output-strides')
        output_strides = sorted(output_strides)
        projection_channels = getattr(opts, 'model.detection.mask_rcnn.backbone_projection_channels', 256)
        anchor_sizes = getattr(opts, 'model.detection.mask_rcnn.anchor_sizes', [32, 64, 128, 256, 512])
        if anchor_sizes is None:
            logger.error("Anchor sizes can't be None")
        elif len(anchor_sizes) != len(output_strides):
            logger.error('Number of anchor sizes should be the same as the output stride. Got: {} and {}'.format(anchor_sizes, output_strides))
        elif isinstance(anchor_sizes, List) and isinstance(anchor_sizes[0], List):
            anchor_sizes = tuple([tuple(a_size) for a_size in anchor_sizes])
        elif isinstance(anchor_sizes, List) and isinstance(anchor_sizes[0], int):
            anchor_sizes = tuple([(a_size,) for a_size in anchor_sizes])
        else:
            raise NotImplementedError
        aspect_ratios = getattr(opts, 'model.detection.mask_rcnn.aspect_ratio', [0.5, 1.0, 2.0])
        if aspect_ratios is None:
            logger.error("Aspect ratios can't be None")
        elif isinstance(aspect_ratios, (int, float)):
            aspect_ratios = ((aspect_ratios,),) * len(anchor_sizes)
        elif isinstance(aspect_ratios, List):
            aspect_ratios = (tuple(aspect_ratios),) * len(anchor_sizes)
        else:
            raise NotImplementedError
        box_fm_size = getattr(opts, 'model.detection.mask_rcnn.bbox_head_fm_size', 7)
        mask_fm_size = getattr(opts, 'model.detection.mask_rcnn.mask_head_fm_size', 14)
        backbone = MaskRCNNEncoder(opts, encoder=encoder, output_strides=output_strides, projection_channels=projection_channels)
        rpn_anchor_generator = AnchorGenerator(sizes=anchor_sizes, aspect_ratios=aspect_ratios)
        rpn_head = RPNHead(opts=opts, in_channels=projection_channels, num_anchors=rpn_anchor_generator.num_anchors_per_location()[0], conv_depth=2)
        representation_size = getattr(opts, 'model.detection.mask_rcnn.representation_size', 1024)
        output_strides_str = [str(os) for os in output_strides]
        box_roi_pool = MultiScaleRoIAlign(featmap_names=output_strides_str, output_size=box_fm_size, sampling_ratio=2)
        box_fm_size_conv_layer = getattr(opts, 'model.detection.mask_rcnn.box_fm_size_conv_layer', [256] * 4)
        box_head = FastRCNNConvFCHead(opts=opts, input_size=(projection_channels, box_fm_size, box_fm_size), conv_layers=box_fm_size_conv_layer, fc_layers=[representation_size])
        box_predictor = FastRCNNPredictor(in_channels=representation_size, num_classes=self.n_detection_classes)
        mask_fm_size_conv_layer = getattr(opts, 'model.detection.mask_rcnn.mask_fm_size_conv_layer', [256] * 4)
        mask_dilation = getattr(opts, 'model.detection.mask_rcnn.mask_dilation', 1)
        mask_roi_pool = MultiScaleRoIAlign(featmap_names=output_strides_str, output_size=mask_fm_size, sampling_ratio=2)
        mask_dilation = mask_dilation
        mask_head = MaskRCNNHeads(opts=opts, in_channels=projection_channels, layers=mask_fm_size_conv_layer, dilation=mask_dilation)
        mask_predictor = MaskRCNNPredictor(opts=opts, in_channels=mask_fm_size_conv_layer[-1], dim_reduced=256, num_classes=self.n_detection_classes)
        rpn_pre_nms_top_n_train = getattr(opts, 'model.detection.mask_rcnn.rpn_pre_nms_top_n_train', 2000)
        rpn_pre_nms_top_n_test = getattr(opts, 'model.detection.mask_rcnn.rpn_pre_nms_top_n_test', 1000)
        rpn_post_nms_top_n_train = getattr(opts, 'model.detection.mask_rcnn.rpn_post_nms_top_n_train', 2000)
        rpn_post_nms_top_n_test = getattr(opts, 'model.detection.mask_rcnn.rpn_post_nms_top_n_test', 1000)
        rpn_nms_thresh = getattr(opts, 'model.detection.mask_rcnn.rpn_nms_thresh', 0.7)
        rpn_fg_iou_thresh = getattr(opts, 'model.detection.mask_rcnn.rpn_fg_iou_thresh', 0.7)
        rpn_bg_iou_thresh = getattr(opts, 'model.detection.mask_rcnn.rpn_bg_iou_thresh', 0.3)
        rpn_batch_size_per_image = getattr(opts, 'model.detection.mask_rcnn.rpn_batch_size_per_image', 256)
        rpn_positive_fraction = getattr(opts, 'model.detection.mask_rcnn.rpn_positive_fraction', 0.5)
        rpn_score_thresh = getattr(opts, 'model.detection.mask_rcnn.rpn_score_thresh', 0.0)
        box_score_thresh = getattr(opts, 'model.detection.mask_rcnn.box_score_thresh', 0.05)
        box_nms_thresh = getattr(opts, 'model.detection.mask_rcnn.box_nms_thresh', 0.5)
        box_detections_per_img = getattr(opts, 'model.detection.mask_rcnn.box_detections_per_img', 100)
        box_fg_iou_thresh = getattr(opts, 'model.detection.mask_rcnn.box_fg_iou_thresh', 0.5)
        box_bg_iou_thresh = getattr(opts, 'model.detection.mask_rcnn.box_bg_iou_thresh', 0.5)
        box_batch_size_per_image = getattr(opts, 'model.detection.mask_rcnn.box_batch_size_per_image', 512)
        box_positive_fraction = getattr(opts, 'model.detection.mask_rcnn.box_positive_fraction', 0.25)
        self.model = MaskRCNN(backbone=backbone, image_mean=[0.0] * 3, image_std=[1.0] * 3, rpn_anchor_generator=rpn_anchor_generator, rpn_head=rpn_head, rpn_pre_nms_top_n_train=rpn_pre_nms_top_n_train, rpn_pre_nms_top_n_test=rpn_pre_nms_top_n_test, rpn_post_nms_top_n_train=rpn_post_nms_top_n_train, rpn_post_nms_top_n_test=rpn_post_nms_top_n_test, rpn_nms_thresh=rpn_nms_thresh, rpn_fg_iou_thresh=rpn_fg_iou_thresh, rpn_bg_iou_thresh=rpn_bg_iou_thresh, rpn_batch_size_per_image=rpn_batch_size_per_image, rpn_positive_fraction=rpn_positive_fraction, rpn_score_thresh=rpn_score_thresh, box_roi_pool=box_roi_pool, box_head=box_head, box_score_thresh=box_score_thresh, box_nms_thresh=box_nms_thresh, box_detections_per_img=box_detections_per_img, box_fg_iou_thresh=box_fg_iou_thresh, box_bg_iou_thresh=box_bg_iou_thresh, box_batch_size_per_image=box_batch_size_per_image, box_positive_fraction=box_positive_fraction, bbox_reg_weights=None, box_predictor=box_predictor, mask_roi_pool=mask_roi_pool, mask_head=mask_head, mask_predictor=mask_predictor)
        self.backbone_lr_multiplier = getattr(opts, 'model.detection.mask_rcnn.backbone_lr_multiplier', 1.0)
        del self.encoder
        self.reset_norm_layer_opts(default_norm=default_norm)

    def set_norm_layer_opts(self):
        mask_rcnn_norm_layer = getattr(self.opts, 'model.detection.mask_rcnn.norm_layer', None)
        if mask_rcnn_norm_layer is None:
            logger.error('Please specify norm layer')
        default_norm = getattr(self.opts, 'model.normalization.name', None)
        setattr(self.opts, 'model.normalization.name', mask_rcnn_norm_layer)
        return default_norm

    def reset_norm_layer_opts(self, default_norm):
        setattr(self.opts, 'model.normalization.name', default_norm)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """Add model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.detection.mask-rcnn.backbone-projection-channels', type=int, default=256, help='Projection channels for the encoder in Mask-RCNN')
        group.add_argument('--model.detection.mask-rcnn.backbone-lr-multiplier', type=float, default=1.0, help='LR multiplier for MASK RCNN head')
        group.add_argument('--model.detection.mask-rcnn.output-strides', type=int, nargs='+', default=[4, 8, 16, 32, 64], help='Extract backbone feature maps from these output strides. If output stride is greater than 32, extra layers are added.')
        group.add_argument('--model.detection.mask-rcnn.anchor-sizes', type=int, nargs='+', action='append', default=[32, 64, 128, 256, 512], help='Anchor sizes at each output stride')
        group.add_argument('--model.detection.mask-rcnn.aspect-ratio', type=float, nargs='+', default=[0.5, 1.0, 2.0], help='Aspect ratios. These are the same for all feature maps')
        group.add_argument('--model.detection.mask-rcnn.bbox-head-fm-size', type=int, default=7, help='Feature map size for the box head')
        group.add_argument('--model.detection.mask-rcnn.mask-head-fm-size', type=int, default=14, help='Feature map size for the max head')
        group.add_argument('--model.detection.mask-rcnn.representation-size', type=int, default=1024, help='Size of the intermediate representation in Mask RCNN')
        group.add_argument('--model.detection.mask-rcnn.box-fm-size-conv-layer', type=int, nargs='+', default=[256] * 4, help='Feature dim of each Convolution layer in the Faster RCNN head. Defaults to [256, 256, 256, 256]')
        group.add_argument('--model.detection.mask-rcnn.mask-fm-size-conv-layer', type=int, nargs='+', default=[256] * 4, help='Feature dim of each Convolution layer in the Mask RCNN head. Defaults to [256, 256, 256, 256]')
        group.add_argument('--model.detection.mask-rcnn.mask-dilation', type=int, default=1, help='Dilation rate in Mask RCNN head. Defaults to 1')
        group.add_argument('--model.detection.mask-rcnn.rpn-pre-nms-top-n-train', type=int, default=2000, help='Number of proposals to keep before applying NMS during training')
        group.add_argument('--model.detection.mask-rcnn.rpn-pre-nms-top-n-test', type=int, default=1000, help='Number of proposals to keep before applying NMS during test')
        group.add_argument('--model.detection.mask-rcnn.rpn-post-nms-top-n-train', type=int, default=2000, help='Number of proposals to keep after applying NMS during training')
        group.add_argument('--model.detection.mask-rcnn.rpn-post-nms-top-n-test', type=int, default=1000, help='Number of proposals to keep after applying NMS during test')
        group.add_argument('--model.detection.mask-rcnn.rpn-nms-thresh', type=float, default=0.7, help='NMS threshold used for postprocessing the RPN proposals')
        group.add_argument('--model.detection.mask-rcnn.rpn-fg-iou-thresh', type=float, default=0.7, help='minimum IoU between the anchor and the GT box so that they can be considered as positive during training of the RPN.')
        group.add_argument('--model.detection.mask-rcnn.rpn-bg-iou-thresh', type=float, default=0.7, help='minimum IoU between the anchor and the GT box so that they can be considered as negative during training of the RPN.')
        group.add_argument('--model.detection.mask-rcnn.rpn-batch-size-per-image', type=int, default=256, help='Number of anchors that are sampled during training of the RPN for computing the loss')
        group.add_argument('--model.detection.mask-rcnn.rpn-positive-fraction', type=float, default=0.5, help='Proportion of positive anchors in a mini-batch during training of the RPN')
        group.add_argument('--model.detection.mask-rcnn.rpn-score-thresh', type=float, default=0.0, help='During inference, only return proposals with a classification score greater than rpn_score_thresh')
        group.add_argument('--model.detection.mask-rcnn.box-score-thresh', type=float, default=0.05, help='During inference, only return proposals with a classification score greater than box_score_thresh')
        group.add_argument('--model.detection.mask-rcnn.box-nms-thresh', type=float, default=0.5, help='During inference, NMS threshold for the prediction head.')
        group.add_argument('--model.detection.mask-rcnn.box-detections-per-img', type=int, default=100, help='Maximum number of detections per image, for all classes')
        group.add_argument('--model.detection.mask-rcnn.box-fg-iou-thresh', type=float, default=0.5, help='Minimum IoU between the proposals and the GT box so that they can be considered as positive during training of the classification head')
        group.add_argument('--model.detection.mask-rcnn.box-bg-iou-thresh', type=float, default=0.5, help='Minimum IoU between the proposals and the GT box so that they can be considered as negative during training of the classification head')
        group.add_argument('--model.detection.mask-rcnn.box-batch-size-per-image', type=int, default=512, help='Number of proposals that are sampled during training of the classification head')
        group.add_argument('--model.detection.mask-rcnn.box-positive-fraction', type=float, default=0.25, help='Proportion of positive proposals in a mini-batch during training of the classification head')
        group.add_argument('--model.detection.mask-rcnn.norm-layer', type=str, default=None, help='Mask RCNN Norm layer')
        group.add_argument('--model.detection.mask-rcnn.disable-fpn', action='store_true', help='Do not use FPN')
        return parser

    def reset_generalized_rcnn_transform(self, height, width):
        self.model.transform.fixed_size = width, height

    def get_trainable_parameters(self, weight_decay: float=0.0, no_decay_bn_filter_bias: bool=False, *args, **kwargs):
        """Returns a list of trainable parameters"""
        if self.backbone_lr_multiplier == 1.0:
            return super(MaskRCNNDetector, self).get_trainable_parameters(*args, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        else:
            all_params = []
            all_params_lr = []
            backbone_param_list = parameter_list(*args, named_parameters=self.model.backbone.encoder.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='model.backbone.encoder.', **kwargs)
            all_params.extend(backbone_param_list)
            all_params_lr.extend([self.backbone_lr_multiplier] * len(backbone_param_list))
            if self.model.backbone.backbone_proj_layers:
                projection_param_list = parameter_list(*args, named_parameters=self.model.backbone.backbone_proj_layers.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='model.backbone.backbone_proj_layers.', **kwargs)
                all_params.extend(projection_param_list)
                all_params_lr.extend([1.0] * len(projection_param_list))
            if self.model.backbone.fpn_proj_layers:
                fpn_projection_param_list = parameter_list(*args, named_parameters=self.model.backbone.fpn_proj_layers.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='model.backbone.fpn_proj_layers.', **kwargs)
                all_params.extend(fpn_projection_param_list)
                all_params_lr.extend([1.0] * len(fpn_projection_param_list))
            if self.model.backbone.extra_layers:
                extra_layer_param_list = parameter_list(*args, named_parameters=self.model.backbone.extra_layers.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='model.backbone.extra_layers.', **kwargs)
                all_params.extend(extra_layer_param_list)
                all_params_lr.extend([1.0] * len(extra_layer_param_list))
            rpn_param_list = parameter_list(*args, named_parameters=self.model.rpn.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='model.rpn.', **kwargs)
            all_params.extend(rpn_param_list)
            all_params_lr.extend([1.0] * len(rpn_param_list))
            roi_param_list = parameter_list(*args, named_parameters=self.model.roi_heads.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='model.roi_heads.', **kwargs)
            all_params.extend(roi_param_list)
            all_params_lr.extend([1.0] * len(roi_param_list))
            return all_params, all_params_lr

    def forward(self, x: Dict, *args, **kwargs) ->Union[Tuple[Tensor, ...], Tuple[Any, ...], Dict]:
        if isinstance(x, Dict):
            input_tensor = x['image']
            input_labels = x['label']
        else:
            raise NotImplementedError('Input to MaskRCNN should be a Dict of List of Tensors')
        assert isinstance(input_tensor, List)
        assert isinstance(input_labels, List)
        in_channels, in_height, in_width = input_tensor[0].shape
        self.reset_generalized_rcnn_transform(height=in_height, width=in_width)
        outputs = self.model(input_tensor, targets=input_labels)
        if not self.training:
            detections = []
            for i, elem in enumerate(outputs):
                elem['boxes'][:, 0::2] /= input_tensor[i].shape[2]
                elem['boxes'][:, 1::2] /= input_tensor[i].shape[1]
                masks = elem['masks']
                masks = masks.squeeze(1)
                elem_detections = DetectionPredTuple(labels=elem['labels'], scores=elem['scores'], boxes=elem['boxes'], masks=masks)
                detections.append(elem_detections)
            return {'detections': detections}
        if hasattr(self.model.backbone, 'get_augmented_tensor'):
            outputs['augmented_tensor'] = self.model.backbone.get_augmented_tensor()
        return outputs

    @torch.no_grad()
    def predict(self, x: Tensor, *args, **kwargs) ->DetectionPredTuple:
        """Predict the bounding boxes given an image tensor"""
        assert isinstance(x, Tensor) and x.ndim == 4, 'Expected 4D tensor as an input'
        bsz, channels, in_height, in_width = x.shape
        if bsz != 1:
            logger.error('Prediction is supported with a batch size of 1 in {}'.format(self.__class__.__name__))
        self.reset_generalized_rcnn_transform(height=in_height, width=in_width)
        outputs = self.model(x)
        if isinstance(outputs, List) and len(outputs) == 1:
            outputs = outputs[0]
        if isinstance(outputs, Dict) and {'boxes', 'labels', 'scores'}.issubset(outputs.keys()):
            outputs['boxes'][:, 0::2] /= in_width
            outputs['boxes'][:, 1::2] /= in_height
            masks = outputs['masks']
            masks = masks.squeeze(1)
            detections = DetectionPredTuple(labels=outputs['labels'], scores=outputs['scores'], boxes=outputs['boxes'], masks=masks)
            return detections
        else:
            logger.error('Output should be a dict with boxes, scores, and labels as keys. Got: {}'.format(type(outputs)))

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes."""
        img_channels = 3
        height = 320
        width = 320
        n_classes = 80
        n_boxes = 1
        gt_boxes = torch.tensor([2, 20, 3, 40]).reshape(-1, 4).float()
        gt_box_labels = torch.randint(low=0, high=n_classes, size=(n_boxes,), dtype=torch.long)
        img_tensor = torch.randn(img_channels, height, width, dtype=torch.float)
        labels = {'box_labels': gt_box_labels, 'box_coordinates': gt_boxes}
        return {'samples': {'image': [img_tensor] * batch_size, 'label': [{'labels': gt_box_labels, 'boxes': gt_boxes, 'masks': torch.zeros(1, height, width, dtype=torch.long)}] * batch_size}, 'targets': labels}


class SSDHead(BaseModule):
    """
    This class defines the `SSD object detection Head <https://arxiv.org/abs/1512.02325>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        n_anchors (int): Number of anchors
        n_classes (int): Number of classes in the dataset
        n_coordinates (Optional[int]): Number of coordinates. Default: 4 (x, y, w, h)
        proj_channels (Optional[int]): Number of projected channels. If `-1`, then projection layer is not used
        kernel_size (Optional[int]): Kernel size in convolutional layer. If kernel_size=1, then standard
            point-wise convolution is used. Otherwise, separable convolution is used
        stride (Optional[int]): stride for feature map. If stride > 1, then feature map is sampled at this rate
            and predictions are made on fewer pixels as compared to the input tensor. Default: 1
    """

    def __init__(self, opts, in_channels: int, n_anchors: int, n_classes: int, n_coordinates: Optional[int]=4, proj_channels: Optional[int]=-1, kernel_size: Optional[int]=3, stride: Optional[int]=1, *args, **kwargs) ->None:
        super().__init__()
        proj_layer = None
        self.proj_channels = None
        if proj_channels != -1 and proj_channels != in_channels and kernel_size > 1:
            proj_layer = ConvLayer(opts=opts, in_channels=in_channels, out_channels=proj_channels, kernel_size=1, stride=1, groups=1, bias=False, use_norm=True, use_act=True)
            in_channels = proj_channels
            self.proj_channels = proj_channels
        self.proj_layer = proj_layer
        conv_fn = ConvLayer if kernel_size == 1 else SeparableConv
        if kernel_size > 1 and stride > 1:
            kernel_size = max(kernel_size, stride if stride % 2 != 0 else stride + 1)
        self.loc_cls_layer = conv_fn(opts=opts, in_channels=in_channels, out_channels=n_anchors * (n_coordinates + n_classes), kernel_size=kernel_size, stride=1, groups=1, bias=True, use_norm=False, use_act=False)
        self.n_coordinates = n_coordinates
        self.n_classes = n_classes
        self.n_anchors = n_anchors
        self.k_size = kernel_size
        self.stride = stride
        self.in_channel = in_channels
        self.reset_parameters()

    def __repr__(self) ->str:
        repr_str = '{}(in_channels={}, n_anchors={}, n_classes={}, n_coordinates={}, kernel_size={}, stride={}'.format(self.__class__.__name__, self.in_channel, self.n_anchors, self.n_classes, self.n_coordinates, self.k_size, self.stride)
        if self.proj_layer is not None:
            repr_str += ', proj=True, proj_channels={}'.format(self.proj_channels)
        repr_str += ')'
        return repr_str

    def reset_parameters(self) ->None:
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='xavier_uniform')

    def _sample_fm(self, x: Tensor) ->Tensor:
        height, width = x.shape[-2:]
        device = x.device
        start_step = max(0, self.stride // 2)
        indices_h = torch.arange(start=start_step, end=height, step=self.stride, dtype=torch.int64, device=device)
        indices_w = torch.arange(start=start_step, end=width, step=self.stride, dtype=torch.int64, device=device)
        x_sampled = torch.index_select(x, dim=-1, index=indices_w)
        x_sampled = torch.index_select(x_sampled, dim=-2, index=indices_h)
        return x_sampled

    def forward(self, x: Tensor, *args, **kwargs) ->Tuple[Tensor, Tensor]:
        batch_size = x.shape[0]
        if self.proj_layer is not None:
            x = self.proj_layer(x)
        x = self.loc_cls_layer(x)
        if self.stride > 1:
            x = self._sample_fm(x)
        x = x.permute(0, 2, 3, 1)
        x = x.contiguous().view(batch_size, -1, self.n_coordinates + self.n_classes)
        box_locations, box_classes = torch.split(x, [self.n_coordinates, self.n_classes], dim=-1)
        return box_locations, box_classes

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        params = macs = 0.0
        if self.proj_layer is not None:
            input, p, m = module_profile(module=self.proj_layer, x=input)
            params += p
            macs += m
        x, p, m = module_profile(module=self.loc_cls_layer, x=input)
        params += p
        macs += m
        return input, params, macs


def is_master(opts) ->bool:
    node_rank = getattr(opts, 'ddp.rank', 0)
    return node_rank == 0


def build_anchor_generator(opts, *args, **kwargs):
    """Build anchor generator for object detection"""
    anchor_gen_name = getattr(opts, 'anchor_generator.name', None)
    anchor_gen = None
    if anchor_gen_name in ANCHOR_GEN_REGISTRY:
        anchor_gen = ANCHOR_GEN_REGISTRY[anchor_gen_name](opts, *args, **kwargs)
    else:
        supported_anchor_gens = list(ANCHOR_GEN_REGISTRY.keys())
        supp_anchor_gen_str = 'Got {} as anchor generator. Supported anchor generators are:'.format(anchor_gen_name)
        for i, m_name in enumerate(supported_anchor_gens):
            supp_anchor_gen_str += '\n\t {}: {}'.format(i, logger.color_text(m_name))
        if is_master(opts):
            logger.error(supp_anchor_gen_str)
    return anchor_gen


MATCHER_REGISTRY = {}


def build_matcher(opts, *args, **kwargs):
    matcher_name = getattr(opts, 'matcher.name', None)
    matcher = None
    if matcher_name in MATCHER_REGISTRY:
        matcher = MATCHER_REGISTRY[matcher_name](opts, *args, **kwargs)
    else:
        supported_matchers = list(MATCHER_REGISTRY.keys())
        supp_matcher_str = 'Got {} as matcher. Supported matchers are:'.format(matcher_name)
        for i, m_name in enumerate(supported_matchers):
            supp_matcher_str += '\n\t {}: {}'.format(i, logger.color_text(m_name))
        if is_master(opts):
            logger.error(supp_matcher_str)
    return matcher


def is_coreml_conversion(opts) ->bool:
    if getattr(opts, 'common.enable_coreml_compatible_module', False):
        return True
    return False


class SingleShotMaskDetector(BaseDetection):
    """
    This class implements a `Single Shot Object Detector <https://arxiv.org/abs/1512.02325>`_

    Args:
        opts: command-line arguments
        encoder (BaseEncoder): Encoder network (e.g., ResNet or MobileViT)
    """
    coordinates = 4

    def __init__(self, opts, encoder: BaseEncoder) ->None:
        anchor_gen_name = getattr(opts, 'anchor_generator.name', None)
        if anchor_gen_name is None or anchor_gen_name != 'ssd':
            logger.error('For SSD, we need --anchor-generator.name to be ssd')
        anchor_box_generator = build_anchor_generator(opts=opts)
        output_strides_aspect_ratio = anchor_box_generator.output_strides_aspect_ratio
        output_strides = list(output_strides_aspect_ratio.keys())
        anchors_aspect_ratio = list(output_strides_aspect_ratio.values())
        n_os = len(output_strides)
        if getattr(opts, 'matcher.name') != 'ssd':
            logger.error('For SSD, we need --matcher.name as ssd')
        super().__init__(opts=opts, encoder=encoder)
        self.encoder.classifier = None
        self.encoder.conv_1x1_exp = None
        proj_channels = getattr(opts, 'model.detection.ssd.proj_channels', [512, 256, 256, 128, 128, 64])
        proj_channels = proj_channels + [128] * (n_os - len(proj_channels))
        if n_os != len(anchors_aspect_ratio) != len(proj_channels):
            logger.error('SSD model requires anchors to be defined for feature maps from each output stride. Alsolen(anchors_aspect_ratio) == len(output_strides) == len(proj_channels). Got len(output_strides)={}, len(anchors_aspect_ratio)={}, len(proj_channels)={}. Please specify correct arguments using following arguments: \n--model.detection.ssd.anchors-aspect-ratio \n--model.detection.ssd.output-strides\n--model.detection.ssd.proj-channels'.format(n_os, len(anchors_aspect_ratio), len(proj_channels)))
        extra_layers = {}
        enc_channels_list = []
        in_channels = self.enc_l5_channels
        extra_proj_list = [256] * (len(output_strides) - len(proj_channels))
        proj_channels = proj_channels + extra_proj_list
        for idx, os in enumerate(output_strides):
            out_channels = proj_channels[idx]
            if os == 8:
                enc_channels_list.append(self.enc_l3_channels)
            elif os == 16:
                enc_channels_list.append(self.enc_l4_channels)
            elif os == 32:
                enc_channels_list.append(self.enc_l5_channels)
            elif os > 32 and os != -1:
                extra_layers['os_{}'.format(os)] = SeparableConv(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=3, use_act=True, use_norm=True, stride=2)
                enc_channels_list.append(out_channels)
                in_channels = out_channels
            elif os == -1:
                extra_layers['os_{}'.format(os)] = nn.Sequential(AdaptiveAvgPool2d(output_size=1), ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, use_act=True, use_norm=False))
                enc_channels_list.append(out_channels)
                in_channels = out_channels
            else:
                raise NotImplementedError
        self.extra_layers = None if not extra_layers else nn.ModuleDict(extra_layers)
        if self.extra_layers is not None:
            self.reset_layers(module=self.extra_layers)
        self.fpn = None
        if getattr(opts, 'model.detection.ssd.use_fpn', False):
            fpn_channels = getattr(opts, 'model.detection.ssd.fpn_out_channels', 256)
            self.fpn = FeaturePyramidNetwork(opts=opts, in_channels=enc_channels_list, output_strides=output_strides, out_channels=fpn_channels)
            enc_channels_list = [fpn_channels] * len(output_strides)
            proj_channels = enc_channels_list
        self.conf_threshold = getattr(opts, 'model.detection.ssd.conf_threshold', 0.01)
        self.nms_threshold = getattr(opts, 'model.detection.ssd.nms_iou_threshold', 0.5)
        self.top_k = getattr(opts, 'model.detection.ssd.top_k', 400)
        self.objects_per_image = getattr(opts, 'model.detection.ssd.objects_per_image', 200)
        self.anchor_box_generator = anchor_box_generator
        anchors_aspect_ratio = self.anchor_box_generator.num_anchors_per_os()
        anchor_steps = self.anchor_box_generator.step
        self.ssd_heads = nn.ModuleList()
        for os, in_dim, proj_dim, n_anchors, step in zip(output_strides, enc_channels_list, proj_channels, anchors_aspect_ratio, anchor_steps):
            self.ssd_heads += [SSDHead(opts=opts, in_channels=in_dim, n_classes=self.n_detection_classes, n_coordinates=self.coordinates, n_anchors=n_anchors, proj_channels=proj_dim, kernel_size=3 if os != -1 else 1, stride=step)]
        self.anchors_aspect_ratio = anchors_aspect_ratio
        self.output_strides = output_strides
        self.match_prior = build_matcher(opts=opts)
        self.step = self.anchor_box_generator.step

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.detection.ssd.anchors-aspect-ratio', type=int, nargs='+', action='append', default=[[2, 3]] * 4, help='Anchors aspect ratio in each feature map obtained at different output strides.')
        group.add_argument('--model.detection.ssd.output-strides', type=int, nargs='+', default=[16, 32, 64, 128], help='Extract feature maps from these output strides.')
        group.add_argument('--model.detection.ssd.proj-channels', type=int, nargs='+', default=[512] * 4, help='Projection channels for feature map obtained at each output stride')
        group.add_argument('--model.detection.ssd.min-box-size', type=float, default=None, help='Min. box size. Value between 0 and 1. Good default value is 0.1')
        group.add_argument('--model.detection.ssd.max-box-size', type=float, default=None, help='Max. box size. Value between 0 and 1. Good default value is 1.05')
        group.add_argument('--model.detection.ssd.center-variance', type=float, default=None, help='Center variance.')
        group.add_argument('--model.detection.ssd.size-variance', type=float, default=None, help='Size variance.')
        group.add_argument('--model.detection.ssd.iou-threshold', type=float, default=None, help='IOU Threshold.')
        group.add_argument('--model.detection.ssd.conf-threshold', type=float, default=0.01, help='Confidence threshold. For evaluation on COCO, set to 0.01, so that we can compute mAP')
        group.add_argument('--model.detection.ssd.top-k', type=int, default=400, help='Keep only top-k objects before NMS')
        group.add_argument('--model.detection.ssd.objects-per-image', type=int, default=200, help='Keep only these many objects after NMS')
        group.add_argument('--model.detection.ssd.nms-iou-threshold', type=float, default=0.5, help='NMS IoU threshold ')
        group.add_argument('--model.detection.ssd.fpn-out-channels', type=int, default=256, help='Number of output channels in FPN')
        group.add_argument('--model.detection.ssd.use-fpn', action='store_true', help='Use SSD with FPN')
        return parser

    @staticmethod
    def reset_layers(module) ->None:
        for layer in module.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='xavier_uniform')

    @staticmethod
    def process_anchors_ar(anchor_ar: List) ->List:
        assert isinstance(anchor_ar, list)
        new_ar = []
        for ar in anchor_ar:
            if ar in new_ar:
                continue
            new_ar.append(ar)
        return new_ar

    def get_backbone_features(self, x: Tensor) ->Dict[str, Tensor]:
        enc_end_points: Dict = self.encoder.extract_end_points_all(x)
        end_points: Dict = dict()
        for idx, os in enumerate(self.output_strides):
            if os == 8:
                end_points['os_{}'.format(os)] = enc_end_points.pop('out_l3')
            elif os == 16:
                end_points['os_{}'.format(os)] = enc_end_points.pop('out_l4')
            elif os == 32:
                end_points['os_{}'.format(os)] = enc_end_points.pop('out_l5')
            else:
                x = end_points['os_{}'.format(self.output_strides[idx - 1])]
                end_points['os_{}'.format(os)] = self.extra_layers['os_{}'.format(os)](x)
        if self.fpn is not None:
            end_points = self.fpn(end_points)
        return end_points

    def ssd_forward(self, end_points: Dict[str, Tensor], device: Optional[torch.device]=torch.device('cpu'), *args, **kwargs) ->Union[Tuple[Tensor, Tensor, Tensor], Tuple[Tensor, ...]]:
        locations = []
        confidences = []
        anchors = []
        for os, ssd_head in zip(self.output_strides, self.ssd_heads):
            x = end_points['os_{}'.format(os)]
            fm_h, fm_w = x.shape[2:]
            loc, pred = ssd_head(x)
            locations.append(loc)
            confidences.append(pred)
            anchors_fm_ctr = self.anchor_box_generator(fm_height=fm_h, fm_width=fm_w, fm_output_stride=os, device=device)
            anchors.append(anchors_fm_ctr)
        locations = torch.cat(locations, dim=1)
        confidences = torch.cat(confidences, dim=1)
        anchors = torch.cat(anchors, dim=0)
        anchors = anchors.unsqueeze(dim=0)
        return confidences, locations, anchors

    def forward(self, x: Union[Tensor, Dict]) ->Union[Tuple[Tensor, ...], Tuple[Any, ...], Dict]:
        if isinstance(x, Dict):
            input_tensor = x['image']
        elif isinstance(x, Tensor):
            input_tensor = x
        else:
            raise NotImplementedError('Input to SSD should be either a Tensor or a Dict of Tensors')
        device = input_tensor.device
        backbone_end_points: Dict = self.get_backbone_features(input_tensor)
        if not is_coreml_conversion(self.opts):
            confidences, locations, anchors = self.ssd_forward(end_points=backbone_end_points, device=device)
            output_dict = {'scores': confidences, 'boxes': locations}
            if not self.training:
                scores = nn.Softmax(dim=-1)(confidences)
                boxes = self.match_prior.convert_to_boxes(pred_locations=locations, anchors=anchors)
                detections = self.postprocess_detections(boxes=boxes, scores=scores)
                output_dict['detections'] = detections
            return output_dict
        else:
            return self.ssd_forward(end_points=backbone_end_points, is_prediction=False)

    @torch.no_grad()
    def predict(self, x: Tensor, *args, **kwargs) ->DetectionPredTuple:
        """Predict the bounding boxes given an image tensor"""
        bsz, channels, width, height = x.shape
        if bsz != 1:
            logger.error('Prediction is supported with a batch size of 1 in {}'.format(self.__class__.__name__))
        device = x.device
        enc_end_points: Dict = self.get_backbone_features(x)
        confidences, locations, anchors = self.ssd_forward(end_points=enc_end_points, device=device)
        scores = nn.Softmax(dim=-1)(confidences)
        boxes = self.match_prior.convert_to_boxes(pred_locations=locations, anchors=anchors)
        detections = self.postprocess_detections(boxes=boxes, scores=scores)[0]
        return detections

    @torch.no_grad()
    def postprocess_detections(self, boxes: Tensor, scores: Tensor) ->List[DetectionPredTuple]:
        """Post process detections, including NMS"""
        batch_size = boxes.shape[0]
        n_classes = scores.shape[-1]
        device = boxes.device
        box_dtype = boxes.dtype
        scores_dtype = scores.dtype
        results = []
        for b_id in range(batch_size):
            object_labels = []
            object_boxes = []
            object_scores = []
            for class_index in range(1, n_classes):
                probs = scores[b_id, :, class_index]
                mask = probs > self.conf_threshold
                probs = probs[mask]
                if probs.size(0) == 0:
                    continue
                masked_boxes = boxes[b_id, mask, :]
                num_topk = min(self.top_k, probs.size(0))
                probs, idxs = probs.topk(num_topk)
                masked_boxes = masked_boxes[idxs, ...]
                object_boxes.append(masked_boxes)
                object_scores.append(probs)
                object_labels.append(torch.full_like(probs, fill_value=class_index, dtype=torch.int64, device=device))
            if len(object_scores) == 0:
                output = DetectionPredTuple(labels=torch.empty(0, device=device, dtype=torch.long), scores=torch.empty(0, device=device, dtype=scores_dtype), boxes=torch.empty(0, 4, device=device, dtype=box_dtype))
            else:
                object_scores = torch.cat(object_scores, dim=0)
                object_boxes = torch.cat(object_boxes, dim=0)
                object_labels = torch.cat(object_labels, dim=0)
                keep = batched_nms(object_boxes, object_scores, object_labels, self.nms_threshold)
                keep = keep[:self.objects_per_image]
                output = DetectionPredTuple(labels=object_labels[keep], scores=object_scores[keep], boxes=object_boxes[keep])
            results.append(output)
        return results

    def profile_backbone(self, x: Tensor) ->Tuple[Dict[str, Tensor], float, float]:
        params, macs = 0.0, 0.0
        enc_end_points, p, m = self.encoder.profile_model(x, is_classification=False)
        params += p
        macs += m
        end_points = dict()
        for idx, os in enumerate(self.output_strides):
            if os == 8:
                end_points['os_{}'.format(os)] = enc_end_points.pop('out_l3')
            elif os == 16:
                end_points['os_{}'.format(os)] = enc_end_points.pop('out_l4')
            elif os == 32:
                end_points['os_{}'.format(os)] = enc_end_points.pop('out_l5')
            else:
                x = end_points['os_{}'.format(self.output_strides[idx - 1])]
                x, p, m = module_profile(module=self.extra_layers['os_{}'.format(os)], x=x)
                end_points['os_{}'.format(os)] = x
                params += p
                macs += m
        if self.fpn is not None:
            end_points, p, m = self.fpn.profile_module(end_points)
            params += p
            macs += m
            enc_str = logger.text_colors['logs'] + logger.text_colors['bold'] + 'FPN  ' + logger.text_colors['end_color']
            None
            None
            logger.singe_dash_line()
        return end_points, params, macs

    def profile_model(self, input: Tensor) ->None:
        """
        This function computes layer-wise FLOPs and parameters for SSD

        .. note::
             Model profiling is for reference only and may contain errors as it relies heavily on user
             to implement the underlying functions accurately.
        """
        overall_params, overall_macs = 0.0, 0.0
        input_fvcore = input.clone()
        logger.log('Model statistics for an input of size {}'.format(input.size()))
        logger.double_dash_line(dashes=65)
        None
        logger.double_dash_line(dashes=65)
        enc_str = logger.text_colors['logs'] + logger.text_colors['bold'] + 'Encoder  ' + logger.text_colors['end_color']
        None
        backbone_end_points, encoder_params, encoder_macs = self.profile_backbone(x=input)
        ssd_head_params = ssd_head_macs = 0.0
        for os, ssd_head in zip(self.output_strides, self.ssd_heads):
            _, p, m = module_profile(module=ssd_head, x=backbone_end_points['os_{}'.format(os)])
            ssd_head_params += p
            ssd_head_macs += m
        overall_params += encoder_params + ssd_head_params
        overall_macs += encoder_macs + ssd_head_macs
        ssd_str = logger.text_colors['logs'] + logger.text_colors['bold'] + 'SSD  ' + logger.text_colors['end_color']
        None
        None
        logger.double_dash_line(dashes=65)
        None
        overall_params_py = sum([p.numel() for p in self.parameters()])
        None
        None
        try:
            flop_analyzer = FlopCountAnalysis(self.eval(), input_fvcore)
            flop_analyzer.unsupported_ops_warnings(False)
            flop_analyzer.uncalled_modules_warnings(False)
            flops_fvcore = flop_analyzer.total()
            None
            None
        except Exception:
            pass
        None
        logger.double_dash_line(dashes=65)

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes."""
        img_channels = 3
        height = 320
        width = 320
        n_classes = 80

        def generate_anchors(height, width):
            """Generate anchors **on-the-fly** based on the input resolution."""
            anchors = []
            for output_stride in self.output_strides:
                if output_stride == -1:
                    fm_width = fm_height = 1
                else:
                    fm_width = int(math.ceil(width / output_stride))
                    fm_height = int(math.ceil(height / output_stride))
                fm_anchor = self.anchor_box_generator(fm_height=fm_height, fm_width=fm_width, fm_output_stride=output_stride)
                anchors.append(fm_anchor)
            anchors = torch.cat(anchors, dim=0)
            return anchors
        gt_boxes = generate_anchors(height=height, width=width)
        gt_boxes = gt_boxes.unsqueeze(0).expand(batch_size, -1, -1)
        gt_box_labels = torch.randint(low=0, high=n_classes, size=(batch_size, gt_boxes.shape[1]), dtype=torch.long)
        img_tensor = torch.randn(batch_size, img_channels, height, width, dtype=torch.float)
        labels = {'box_labels': gt_box_labels, 'box_coordinates': gt_boxes}
        return {'samples': img_tensor, 'targets': labels}


class BaseMultiModalImageText(nn.Module):
    """Base class for multi-modal image-text data"""

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__()
        self.lr_multiplier_img_encoder = getattr(opts, 'model.multi_modal_image_text.lr_multiplier_img_encoder', 1.0)
        self.lr_multiplier_text_encoder = getattr(opts, 'model.multi_modal_image_text.lr_multiplier_text_encoder', 1.0)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        """Add model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.multi-modal-image-text.name', type=str, default=None, help='Name of the multi-modal image-text model')
        group.add_argument('--model.multi-modal-image-text.lr-multiplier-img-encoder', type=float, default=1.0, help='LR multiplier for the image encoder in {}'.format(cls.__name__))
        group.add_argument('--model.multi-modal-image-text.lr-multiplier-text-encoder', type=float, default=1.0, help='LR multiplier for the text encoder in {}'.format(cls.__name__))
        group.add_argument('--model.multi-modal-image-text.pretrained', type=str, default=None, help='Path of the pretrained backbone')
        group.add_argument('--model.multi-modal-image-text.freeze-batch-norm', action='store_true', help='Freeze batch norm layers')
        return parser

    def reset_parameters(self) ->None:
        """Reset weights of a given layer"""
        initialize_weights(opts=self.opts, modules=self.modules())

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        raise NotImplementedError

    def profile_model(self, input: Tensor) ->Optional[Tuple[Tensor, float, float]]:
        """
        Child classes must implement this function to compute FLOPs and parameters
        """
        raise NotImplementedError

    def freeze_norm_layers(self) ->None:
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        raise NotImplementedError

    def forward(self, input: Dict, *args, **kwargs) ->Dict:
        raise NotImplementedError


class BaseTextEncoder(nn.Module):
    """Base class for text encoder"""

    def __init__(self, opts, projection_dim: int, *args, **kwargs) ->None:
        is_master_node = is_master(opts)
        vocab_size = getattr(opts, 'dataset.text_vocab_size', None)
        if getattr(opts, 'common.debug_mode', False):
            vocab_size = 100
        if vocab_size is None and is_master_node:
            logger.error("Vocabulary size can't be None or -1 in {}. Got: {}".format(self.__class__.__name__, vocab_size))
        super(BaseTextEncoder, self).__init__()
        self.opts = opts
        self.projection_dim = projection_dim
        self.is_master_node = is_master_node
        self.vocab_size = vocab_size

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """Add model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.text.name', type=str, default=None, help='Name of the text encoder')
        return parser

    def reset_parameters(self):
        """Initialize model weights"""
        initialize_weights(opts=self.opts, modules=self.modules())

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    def profile_model(self, input: Tensor) ->Optional[Tuple[Tensor, float, float]]:
        """
        Child classes must implement this function to compute FLOPs and parameters
        """
        raise NotImplementedError

    def freeze_norm_layers(self) ->None:
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def forward(self, text_tokens: Tensor, key_padding_mask: Optional[Tensor]=None, attn_mask: Optional[Tensor]=None, *args, **kwargs) ->Any:
        raise NotImplementedError

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        seq_length = 77
        vocab_size = 10
        text_tensor = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_length)).long()
        return {'text': text_tensor}


def check_frozen_norm_layer(model: torch.nn.Module) ->Tuple[bool, int]:
    if hasattr(model, 'module'):
        model = model.module
    count_norm = 0
    frozen_state = False
    for m in model.modules():
        if isinstance(m, norm_layers_tuple):
            frozen_state = m.weight.requires_grad
    return frozen_state, count_norm


def clean_strip(obj: Union[str, List[str]], sep: Optional[str]=',', strip: bool=True) ->List[str]:
    if isinstance(obj, list):
        strings = obj
    else:
        strings = obj.split(sep)
    if strip:
        strings = [x.strip() for x in strings]
    strings = [x for x in strings if x]
    return strings


def is_start_rank_node(opts) ->bool:
    node_rank = getattr(opts, 'ddp.rank', 0)
    def_rank = getattr(opts, 'ddp.start_rank', 0)
    return node_rank == def_rank


def load_pretrained_model(model: torch.nn.Module, wt_loc: str, opts: Dict[str, Any], *args, **kwargs) ->torch.nn.Module:
    """
    Helper function to load pre-trained weights
    """
    if not os.path.isfile(wt_loc):
        logger.error('Pretrained file is not found here: {}'.format(wt_loc))
    wts = torch.load(wt_loc, map_location='cpu')
    is_master_node = is_start_rank_node(opts)
    exclude_scopes = getattr(opts, 'model.resume_exclude_scopes', '')
    exclude_scopes: List[str] = clean_strip(exclude_scopes)
    missing_scopes = getattr(opts, 'model.ignore_missing_scopes', '')
    missing_scopes: List[str] = clean_strip(missing_scopes)
    rename_scopes_map: List[List[str]] = getattr(opts, 'model.rename_scopes_map', [])
    if rename_scopes_map:
        for entry in rename_scopes_map:
            if len(entry) != 2:
                raise ValueError('Every entry in model.rename_scopes_map must contain exactly two string elements for before and after. Got {}.'.format(str(entry)))
    missing_scopes += exclude_scopes
    if exclude_scopes:
        for key in wts.copy():
            if any([re.match(x, key) for x in exclude_scopes]):
                del wts[key]
    if rename_scopes_map:
        for before, after in rename_scopes_map:
            wts = {re.sub(before, after, key): value for key, value in wts.items()}
    strict = not bool(missing_scopes)
    try:
        module = model.module if hasattr(model, 'module') else model
        missing_keys, unexpected_keys = module.load_state_dict(wts, strict=strict)
        if unexpected_keys:
            raise Exception('Found unexpected keys: {}.You can ignore these keys using `model.resume_exclude_scopes`.'.format(','.join(unexpected_keys)))
        missing_keys = [key for key in missing_keys if not any([re.match(x, key) for x in missing_scopes])]
        if missing_keys:
            raise Exception('Missing keys detected. Did not find the following keys in pre-trained model: {}. You can ignore the keys using `model.ignore_missing_scopes`.'.format(','.join(missing_keys)))
        if is_master_node:
            logger.log('Pretrained weights are loaded from {}'.format(wt_loc))
    except Exception as e:
        if is_master_node:
            logger.error('Unable to load pretrained weights from {}. Error: {}'.format(wt_loc, e))
    return model


SUPPORTED_TASKS = []


TASK_REGISTRY = {}


def register_tasks(name):

    def register_task_class(cls):
        if name in TASK_REGISTRY:
            raise ValueError('Cannot register duplicate task ({})'.format(name))
        TASK_REGISTRY[name] = cls
        SUPPORTED_TASKS.append(name)
        return cls
    return register_task_class


@register_tasks(name='classification')
def build_classification_model(opts, *args, **kwargs):
    model_name = getattr(opts, 'model.classification.name', None)
    model = None
    is_master_node = is_master(opts)
    if model_name in CLS_MODEL_REGISTRY:
        cls_act_fn = getattr(opts, 'model.classification.activation.name', None)
        if cls_act_fn is not None:
            gen_act_fn = getattr(opts, 'model.activation.name', 'relu')
            gen_act_inplace = getattr(opts, 'model.activation.inplace', False)
            gen_act_neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
            setattr(opts, 'model.activation.name', cls_act_fn)
            setattr(opts, 'model.activation.inplace', getattr(opts, 'model.classification.activation.inplace', False))
            setattr(opts, 'model.activation.neg_slope', getattr(opts, 'model.classification.activation.neg_slope', 0.1))
            model = CLS_MODEL_REGISTRY[model_name](opts, *args, **kwargs)
            setattr(opts, 'model.activation.name', gen_act_fn)
            setattr(opts, 'model.activation.inplace', gen_act_inplace)
            setattr(opts, 'model.activation.neg_slope', gen_act_neg_slope)
        else:
            model = CLS_MODEL_REGISTRY[model_name](opts, *args, **kwargs)
    else:
        supported_models = list(CLS_MODEL_REGISTRY.keys())
        supp_model_str = 'Supported models are:'
        for i, m_name in enumerate(supported_models):
            supp_model_str += '\n\t {}: {}'.format(i, logger.color_text(m_name))
        if is_master_node:
            logger.error(supp_model_str + 'Got: {}'.format(model_name))
    finetune_task = getattr(opts, 'model.classification.finetune_pretrained_model', False)
    pretrained = getattr(opts, 'model.classification.pretrained', None)
    if finetune_task:
        n_pretrained_classes = getattr(opts, 'model.classification.n_pretrained_classes', None)
        n_classes = getattr(opts, 'model.classification.n_classes', None)
        assert n_pretrained_classes is not None
        assert n_classes is not None
        model.update_classifier(opts, n_classes=n_pretrained_classes)
        if pretrained is not None:
            pretrained = get_local_path(opts, path=pretrained)
            model = load_pretrained_model(model=model, wt_loc=pretrained, opts=opts)
        model.update_classifier(opts, n_classes=n_classes)
    elif pretrained is not None:
        pretrained = get_local_path(opts, path=pretrained)
        model = load_pretrained_model(model=model, wt_loc=pretrained, opts=opts)
    freeze_norm_layers = getattr(opts, 'model.classification.freeze_batch_norm', False)
    if freeze_norm_layers:
        model.freeze_norm_layers()
        frozen_state, count_norm = check_frozen_norm_layer(model)
        if count_norm > 0 and frozen_state and is_master_node:
            logger.error('Something is wrong while freezing normalization layers. Please check')
        if is_master_node:
            logger.log('Normalization layers are frozen')
    return model


TEXT_ENCODER_REGISTRY = {}


def supported_text_encoder_str(text_encoder_name: Optional[str]=None) ->None:
    """Helper utility to print supported text_encoder names in case specified text_encoder
    name is not part of the implemented text encoders.
    """
    supp_list = list(TEXT_ENCODER_REGISTRY.keys())
    if text_encoder_name is None:
        supp_str = "Text encoder name can't be None. \n Supported text encoders are:"
    else:
        supp_str = 'Text encoder ({}) is not yet supported. \n Supported text encoders are:'.format(text_encoder_name)
    for t_name in supp_list:
        supp_str += '\n\t{}'.format(t_name)
    logger.error(supp_str + '\n')


def build_text_encoder(opts, projection_dim: int, *args, **kwargs) ->BaseTextEncoder:
    """Helper function to build the text encoder"""
    text_encoder_name = getattr(opts, 'model.text.name', None)
    if text_encoder_name is None:
        supported_text_encoder_str(text_encoder_name)
    if text_encoder_name in list(TEXT_ENCODER_REGISTRY.keys()):
        return TEXT_ENCODER_REGISTRY[text_encoder_name](opts, projection_dim, *args, **kwargs)
    else:
        supported_text_encoder_str(text_encoder_name)


MULTI_MODAL_IMAGE_TEXT_REGISTRY = {}


def register_multi_modal_image_text(name):

    def register_multi_modal_image_text_class(cls):
        if name in MULTI_MODAL_IMAGE_TEXT_REGISTRY:
            raise ValueError('Cannot register duplicate multi-modal-image-text class ({})'.format(name))
        if not issubclass(cls, BaseMultiModalImageText):
            raise ValueError('Multi-modal image text class ({}: {}) must extend BaseMultiModalImageText'.format(name, cls.__name__))
        MULTI_MODAL_IMAGE_TEXT_REGISTRY[name] = cls
        return cls
    return register_multi_modal_image_text_class


def supported_str(layer_name: Optional[str]=None) ->None:
    """Helper utility to print supported image projection heads."""
    supp_list = list(IMAGE_PROJECTION_HEAD_REGISTRY.keys())
    if layer_name is None:
        supp_str = "Image projection head name can't be None. \n Supported heads are:"
    else:
        supp_str = 'Image projection head ({}) is not yet supported. \n Supported heads are:'.format(layer_name)
    for t_name in supp_list:
        supp_str += '\n\t{}'.format(t_name)
    logger.error(supp_str + '\n')


def build_image_projection_head(opts, in_dim: int, out_dim: int, *args, **kwargs) ->BaseImageProjectionHead:
    """Helper function to build the text encoder"""
    projection_head_name = getattr(opts, 'model.image_projection_head.name', None)
    if projection_head_name is None:
        supported_str(projection_head_name)
    if projection_head_name in list(IMAGE_PROJECTION_HEAD_REGISTRY.keys()):
        return IMAGE_PROJECTION_HEAD_REGISTRY[projection_head_name](opts, in_dim, out_dim, *args, **kwargs)
    else:
        supported_str(projection_head_name)


def update_image_classifier(opts, image_classifier: nn.Module, projection_dim: int, *args, **kwargs) ->nn.Module:
    in_features = None
    if isinstance(image_classifier, nn.Sequential):
        for layer in image_classifier:
            if isinstance(layer, (nn.Linear, LinearLayer)):
                in_features = layer.in_features
                break
    elif isinstance(image_classifier, (nn.Linear, LinearLayer)):
        in_features = image_classifier.in_features
    else:
        raise NotImplementedError
    new_img_classifier = build_image_projection_head(opts, in_dim=in_features, out_dim=projection_dim)
    return new_img_classifier


class CLIP(BaseMultiModalImageText):
    """Base class for multi-modal image-text data"""

    def __init__(self, opts, *args, **kwargs) ->None:
        projection_dim = getattr(opts, 'model.multi_modal_image_text.clip.projection_dim', -1)
        if projection_dim < 1:
            logger.error('Projection dimension should be > 1. Got: {}')
        image_encoder: BaseEncoder = build_classification_model(*args, opts=opts, **kwargs)
        text_encoder: BaseTextEncoder = build_text_encoder(*args, opts=opts, projection_dim=projection_dim, **kwargs)
        image_encoder.classifier = update_image_classifier(opts, image_classifier=image_encoder.classifier, projection_dim=projection_dim)
        super().__init__(*args, opts=opts, **kwargs)
        self.image_encoder: BaseEncoder = image_encoder
        self.text_encoder: BaseTextEncoder = text_encoder
        self.logit_scale = nn.Parameter(torch.ones([]) * math.log(1.0 / 0.07))
        self.projection_dim = projection_dim
        self.use_distributed = getattr(opts, 'ddp.use_distributed', False)
        self.cache_text_features_zero_shot = getattr(opts, 'model.multi_modal_image_text.clip.cache_text_features_zero_shot', False)
        self.cached_text_features = None
        self.reset_parameters()

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        """Add model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.multi-modal-image-text.clip.projection-dim', type=int, default=256, help='Project image and text features to this dimensionality')
        group.add_argument('--model.multi-modal-image-text.clip.cache-text-features-zero-shot', action='store_true', help='Cache text features for zero-shot during inference')
        return parser

    def reset_parameters(self) ->None:
        """Reset weights image and text models"""
        torch.nn.init.constant_(self.logit_scale, math.log(1.0 / 0.07))

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        image_param_list, image_lr_mult = self.image_encoder.get_trainable_parameters(*args, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='image_encoder.', **kwargs)
        image_lr_mult = [self.lr_multiplier_img_encoder] * len(image_lr_mult)
        text_param_list, text_lr_mult = self.text_encoder.get_trainable_parameters(*args, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='text_encoder.', **kwargs)
        text_lr_mult = [self.lr_multiplier_text_encoder] * len(text_lr_mult)
        logit_scale_param_list = [{'params': self.logit_scale, 'weight_decay': 0.0, 'param_names': 'logit_scale'}]
        logit_scale_lr_mult = [1.0] * len(logit_scale_param_list)
        return image_param_list + text_param_list + logit_scale_param_list, image_lr_mult + text_lr_mult + logit_scale_lr_mult

    def profile_model(self, input: Tensor) ->Optional[Tuple[Tensor, float, float]]:
        """
        Child classes must implement this function to compute FLOPs and parameters
        """
        inputs = self.dummy_input_and_label(batch_size=1)
        logger.double_dash_line(dashes=65)
        overall_params_py = sum([p.numel() for p in self.parameters()])
        None
        try:
            flop_analyzer = FlopCountAnalysis(self.eval(), inputs['image'])
            flop_analyzer.unsupported_ops_warnings(False)
            flop_analyzer.uncalled_modules_warnings(False)
            flops_fvcore = flop_analyzer.total()
            None
        except Exception:
            None
            pass
        logger.double_dash_line(dashes=65)
        return None

    def freeze_norm_layers(self) ->None:
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        img_channels = 3
        height = 224
        width = 224
        vocab_size = 10
        seq_length = 5
        img_tensor = torch.randn(batch_size, img_channels, height, width, dtype=torch.float)
        text_tensor = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_length)).long()
        return {'samples': {'image': img_tensor, 'text': text_tensor}, 'targets': text_tensor}

    def _exponentiate_and_clip_logits(self, max_scale: float=100.0):
        scale = self.logit_scale.exp()
        scale = torch.clamp(scale, 0, max_scale)
        return scale

    def forward(self, input: Dict, *args, **kwargs) ->Dict:
        images = input.get('image', None)
        text_tokens = input.get('text', None)
        padding_mask = input.get('padding_mask', None)
        image_encoder_out = self.image_encoder(images)
        augmented_tensor = None
        if isinstance(image_encoder_out, Dict):
            if not {'augmented_tensor', 'logits'}.issubset(image_encoder_out.keys()):
                logger.error('Output of image classifier must contain logits and augmented_tensor as keys. Got keys: {}'.format(image_encoder_out.keys()))
            image_embeddings = image_encoder_out['logits']
            augmented_tensor = image_encoder_out['augmented_tensor']
        elif isinstance(image_encoder_out, Tensor):
            image_embeddings = image_encoder_out
        else:
            logger.error('The output of image encoder should be either Dict or Tensor')
        if self.cache_text_features_zero_shot and not self.training:
            if self.cached_text_features is None:
                text_embeddings = self.text_encoder(text_tokens=text_tokens, key_padding_mask=padding_mask)
                self.cached_text_features = text_embeddings
            text_embeddings = self.cached_text_features
        else:
            text_embeddings = self.text_encoder(text_tokens=text_tokens, key_padding_mask=padding_mask)
        if not self.training and text_embeddings.shape[0] == image_embeddings.shape[1] and text_embeddings.shape[1] != image_embeddings.shape[0]:
            zero_shot_image_logits = 100.0 * image_embeddings @ text_embeddings
            return {'image': None, 'text': None, 'logit_scale': self._exponentiate_and_clip_logits(), 'zero_shot_image_logits': zero_shot_image_logits, 'augmented_tensor': None}
        else:
            return {'image': image_embeddings, 'text': text_embeddings, 'logit_scale': self._exponentiate_and_clip_logits(), 'zero_shot_image_logits': None, 'augmented_tensor': augmented_tensor}


class BaseSegmentation(nn.Module):
    """Base class for segmentation networks"""

    def __init__(self, opts, encoder: BaseEncoder, *args, **kwargs) ->None:
        super().__init__()
        self.lr_multiplier = getattr(opts, 'model.segmentation.lr_multiplier', 1.0)
        assert isinstance(encoder, BaseEncoder), 'encoder should be an instance of BaseEncoder'
        self.encoder: BaseEncoder = encoder

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        """Add segmentation model specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.segmentation.name', type=str, default=None, help='Model name')
        group.add_argument('--model.segmentation.n-classes', type=int, default=20, help='Number of classes in the dataset')
        group.add_argument('--model.segmentation.pretrained', type=str, default=None, help='Path of the pretrained segmentation model. Useful for evaluation')
        group.add_argument('--model.segmentation.lr-multiplier', type=float, default=1.0, help='Multiply the learning rate in segmentation network (e.g., decoder)')
        group.add_argument('--model.segmentation.classifier-dropout', type=float, default=0.1, help='Dropout rate in classifier')
        group.add_argument('--model.segmentation.use-aux-head', action='store_true', help='Use auxiliary output')
        group.add_argument('--model.segmentation.aux-dropout', default=0.1, type=float, help='Dropout in auxiliary branch')
        group.add_argument('--model.segmentation.output-stride', type=int, default=None, help='Output stride in classification network')
        group.add_argument('--model.segmentation.replace-stride-with-dilation', action='store_true', help='Replace stride with dilation')
        group.add_argument('--model.segmentation.activation.name', default=None, type=str, help='Non-linear function type')
        group.add_argument('--model.segmentation.activation.inplace', action='store_true', help='Inplace non-linear functions')
        group.add_argument('--model.segmentation.activation.neg-slope', default=0.1, type=float, help='Negative slope in leaky relu')
        group.add_argument('--model.segmentation.freeze-batch-norm', action='store_true', help='Freeze batch norm layers')
        group.add_argument('--model.segmentation.use-level5-exp', action='store_true', help='Use output of Level 5 expansion layer in base feature extractor')
        group.add_argument('--model.segmentation.finetune-pretrained-model', action='store_true', help='Finetune a pretrained model')
        group.add_argument('--model.segmentation.n-pretrained-classes', type=int, default=None, help='Number of pre-trained classes')
        return parser

    @staticmethod
    def reset_layer_parameters(layer, opts) ->None:
        """Reset weights of a given layer"""
        initialize_weights(opts=opts, modules=layer.modules())

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    def profile_model(self, input: Tensor) ->Optional[Tuple[Tensor, float, float]]:
        """
        Child classes must implement this function to compute FLOPs and parameters
        """
        raise NotImplementedError

    def freeze_norm_layers(self) ->None:
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        img_channels = 3
        height = 224
        width = 224
        n_classes = 10
        img_tensor = torch.randn(batch_size, img_channels, height, width, dtype=torch.float)
        label_tensor = torch.randint(low=0, high=n_classes, size=(batch_size, height, width)).long()
        return {'samples': img_tensor, 'targets': label_tensor}

    def update_classifier(self, opts, n_classes: int) ->None:
        """
        This function updates the classification layer in a model. Useful for finetuning purposes.
        """
        raise NotImplementedError


SEG_HEAD_REGISTRY = {}


def build_segmentation_head(opts, enc_conf: Dict, use_l5_exp: bool=False):
    seg_model_name = getattr(opts, 'model.segmentation.seg_head', 'lr_aspp')
    seg_head = None
    if seg_model_name in SEG_HEAD_REGISTRY:
        seg_head = SEG_HEAD_REGISTRY[seg_model_name](opts=opts, enc_conf=enc_conf, use_l5_exp=use_l5_exp)
    else:
        supported_heads = list(SEG_HEAD_REGISTRY.keys())
        supp_model_str = 'Supported segmentation heads are:'
        for i, m_name in enumerate(supported_heads):
            supp_model_str += '\n\t {}: {}'.format(i, logger.color_text(m_name))
        logger.error(supp_model_str)
    return seg_head


SEG_MODEL_REGISTRY = {}


def register_segmentation_models(name):

    def register_model_class(cls):
        if name in SEG_MODEL_REGISTRY:
            raise ValueError('Cannot register duplicate model ({})'.format(name))
        if not issubclass(cls, BaseSegmentation):
            raise ValueError('Model ({}: {}) must extend BaseSegmentation'.format(name, cls.__name__))
        SEG_MODEL_REGISTRY[name] = cls
        return cls
    return register_model_class


class SegEncoderDecoder(BaseSegmentation):
    """
    This class defines a encoder-decoder architecture for the task of semantic segmentation. Different segmentation
    heads (e.g., PSPNet and DeepLabv3) can be used

    Args:
        opts: command-line arguments
        encoder (BaseEncoder): Backbone network (e.g., MobileViT or ResNet)
    """

    def __init__(self, opts, encoder: BaseEncoder, *args, **kwargs) ->None:
        super().__init__(opts=opts, encoder=encoder)
        self.encoder.classifier = None
        use_l5_exp = getattr(opts, 'model.segmentation.use_level5_exp', False)
        if not use_l5_exp:
            self.encoder.conv_1x1_exp = None
        self.seg_head = build_segmentation_head(opts=opts, enc_conf=self.encoder.model_conf_dict, use_l5_exp=use_l5_exp)
        self.use_l5_exp = use_l5_exp

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        """This function separates the parameters for backbone and segmentation head, so that
        different learning rates can be used for backbone and segmentation head
        """
        encoder_params, enc_lr_mult = self.encoder.get_trainable_parameters(*args, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='encoder.', **kwargs)
        decoder_params, dec_lr_mult = self.seg_head.get_trainable_parameters(*args, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name='seg_head.', **kwargs)
        total_params = sum([p.numel() for p in self.parameters()])
        encoder_params_count = sum([p.numel() for p in self.encoder.parameters()])
        decoder_params_count = sum([p.numel() for p in self.seg_head.parameters()])
        assert total_params == encoder_params_count + decoder_params_count, 'Total network parameters are not equal to the sum of encoder and decoder. {} != {} + {}'.format(total_params, encoder_params_count, decoder_params_count)
        return encoder_params + decoder_params, enc_lr_mult + dec_lr_mult

    def forward(self, x: Tensor, *args, **kwargs) ->Union[Tuple[Tensor, Tensor], Tensor, Dict]:
        enc_end_points: Dict = self.encoder.extract_end_points_all(x, use_l5=True, use_l5_exp=self.use_l5_exp)
        if 'augmented_tensor' in enc_end_points:
            output_dict = {'augmented_tensor': enc_end_points.pop('augmented_tensor'), 'segmentation_output': self.seg_head(*args, enc_out=enc_end_points, **kwargs)}
            return output_dict
        else:
            return self.seg_head(*args, enc_out=enc_end_points, **kwargs)

    def update_classifier(self, opts, n_classes: int) ->None:
        """
        This function updates the classification layer in a model. Useful for finetuning purposes.
        """
        if hasattr(self.seg_head, 'update_classifier'):
            self.seg_head.update_classifier(opts, n_classes)

    def profile_model(self, input: Tensor) ->None:
        """
        This function computes layer-wise FLOPs and parameters for segmentation network

        .. note::
             Model profiling is for reference only and may contain errors as it relies heavily on user
             to implement the underlying functions accurately.
        """
        overall_params, overall_macs = 0.0, 0.0
        input_fvcore = input.clone()
        logger.log('Model statistics for an input of size {}'.format(input.size()))
        logger.double_dash_line(dashes=65)
        None
        logger.double_dash_line(dashes=65)
        enc_str = logger.text_colors['logs'] + logger.text_colors['bold'] + 'Encoder  ' + logger.text_colors['end_color']
        None
        enc_end_points, encoder_params, encoder_macs = self.encoder.profile_model(input, is_classification=False)
        overall_params += encoder_params
        overall_macs += encoder_macs
        dec_str = logger.text_colors['logs'] + logger.text_colors['bold'] + 'Decoder  ' + logger.text_colors['end_color']
        None
        out, decoder_params, decoder_macs = self.seg_head.profile_module(enc_end_points)
        overall_params += decoder_params
        overall_macs += decoder_macs
        logger.double_dash_line(dashes=65)
        None
        overall_params_py = sum([p.numel() for p in self.parameters()])
        None
        None
        try:
            flop_analyzer = FlopCountAnalysis(self.eval(), input_fvcore)
            flop_analyzer.unsupported_ops_warnings(False)
            flop_analyzer.uncalled_modules_warnings(False)
            flops_fvcore = flop_analyzer.total()
            None
            None
        except ModuleNotFoundError as mnfe:
            logger.warning('Please install fvcore to profile {} model'.format(self.__class__.__name__))
        except Exception:
            pass
        logger.double_dash_line(dashes=65)


class BaseSegHead(nn.Module):
    """
    Base class for segmentation heads
    """

    def __init__(self, opts, enc_conf: dict, use_l5_exp: Optional[bool]=False):
        enc_ch_l5_exp_out = _check_out_channels(enc_conf, 'exp_before_cls')
        enc_ch_l5_out = _check_out_channels(enc_conf, 'layer5')
        enc_ch_l4_out = _check_out_channels(enc_conf, 'layer4')
        enc_ch_l3_out = _check_out_channels(enc_conf, 'layer3')
        enc_ch_l2_out = _check_out_channels(enc_conf, 'layer2')
        enc_ch_l1_out = _check_out_channels(enc_conf, 'layer1')
        super().__init__()
        self.use_l5_exp = use_l5_exp
        self.enc_l5_exp_channels = enc_ch_l5_exp_out
        self.enc_l5_channels = enc_ch_l5_out
        self.enc_l4_channels = enc_ch_l4_out
        self.enc_l3_channels = enc_ch_l3_out
        self.enc_l2_channels = enc_ch_l2_out
        self.enc_l1_channels = enc_ch_l1_out
        self.n_seg_classes = getattr(opts, 'model.segmentation.n_classes', 20)
        self.lr_multiplier = getattr(opts, 'model.segmentation.lr_multiplier', 1.0)
        self.classifier_dropout = getattr(opts, 'model.segmentation.classifier_dropout', 0.1)
        self.output_stride = getattr(opts, 'model.segmentation.output_stride', 16)
        self.aux_head = None
        if getattr(opts, 'model.segmentation.use_aux_head', False):
            drop_aux = getattr(opts, 'model.segmentation.aux_dropout', 0.1)
            inner_channels = max(int(self.enc_l4_channels // 4), 128)
            self.aux_head = nn.Sequential(ConvLayer(opts=opts, in_channels=self.enc_l4_channels, out_channels=inner_channels, kernel_size=3, stride=1, use_norm=True, use_act=True, bias=False, groups=1), Dropout2d(drop_aux), ConvLayer(opts=opts, in_channels=inner_channels, out_channels=self.n_seg_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True, groups=1))
        self.upsample_seg_out = None
        if self.output_stride != 1.0:
            self.upsample_seg_out = UpSample(scale_factor=self.output_stride, mode='bilinear', align_corners=True)

    def forward_aux_head(self, enc_out: Dict) ->Tensor:
        aux_out = self.aux_head(enc_out['out_l4'])
        return aux_out

    def forward_seg_head(self, enc_out: Dict) ->Tensor:
        raise NotImplementedError

    def forward(self, enc_out: Dict, *args, **kwargs) ->(Tensor or Tuple[Tensor]):
        out = self.forward_seg_head(enc_out=enc_out)
        if self.upsample_seg_out is not None:
            mask_size = kwargs.get('orig_size', None)
            if mask_size is not None:
                self.upsample_seg_out.scale_factor = None
                self.upsample_seg_out.size = mask_size
            out = self.upsample_seg_out(out)
        if self.aux_head is not None and self.training:
            aux_out = self.forward_aux_head(enc_out=enc_out)
            return out, aux_out
        return out

    def reset_head_parameters(self, opts) ->None:
        initialize_weights(opts=opts, modules=self.modules())

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """Add segmentation head specific arguments"""
        group = parser.add_argument_group(title='Segmentation head arguments', description='Segmentation head arguments')
        group.add_argument('--model.segmentation.seg-head', type=str, default=None, help='Segmentation head')
        return parser

    def profile_module(self, x: Tensor) ->Tuple[Tensor, float, float]:
        """
        Child classes must implement this function to compute FLOPs and parameters
        """
        raise NotImplementedError

    def get_trainable_parameters(self, weight_decay: float=0.0, no_decay_bn_filter_bias: bool=False, *args, **kwargs):
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [self.lr_multiplier] * len(param_list)

    def update_classifier(self, opts, n_classes: int) ->None:
        """
        This function updates the classification layer in a model. Useful for finetuning purposes.
        """
        raise NotImplementedError


class ASPPConv(ConvLayer):
    """
    Convolution with a dilation  for the ASPP module
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        dilation (int): Dilation rate

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: int, out_channels: int, dilation: int, *args, **kwargs) ->None:
        super().__init__(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=True, dilation=dilation)

    def adjust_atrous_rate(self, rate: int) ->None:
        """This function allows to adjust the dilation rate"""
        self.block.conv.dilation = rate
        self.block.conv.padding = rate


class ASPPPooling(BaseLayer):
    """
    ASPP pooling layer
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: int, out_channels: int, *args, **kwargs) ->None:
        super().__init__()
        self.aspp_pool = nn.Sequential()
        self.aspp_pool.add_module(name='global_pool', module=AdaptiveAvgPool2d(output_size=1))
        self.aspp_pool.add_module(name='conv_1x1', module=ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=True))
        self.in_channels = in_channels
        self.out_channels = out_channels

    def forward(self, x: Tensor) ->Tensor:
        x_size = x.shape[-2:]
        x = self.aspp_pool(x)
        x = F.interpolate(x, size=x_size, mode='bilinear', align_corners=False)
        return x

    def profile_module(self, input: Tensor) ->Tuple[Tensor, float, float]:
        out, params, macs = module_profile(module=self.aspp_pool, x=input)
        out = F.interpolate(out, size=input.shape[-2:], mode='bilinear', align_corners=False)
        return out, params, macs

    def __repr__(self):
        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__, self.in_channels, self.out_channels)


class ASPPSeparableConv(SeparableConv):
    """
    Separable Convolution with a dilation for the ASPP module
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        dilation (int): Dilation rate

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: int, out_channels: int, dilation: int, *args, **kwargs) ->None:
        super().__init__(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, dilation=dilation, use_norm=True, use_act=True)

    def adjust_atrous_rate(self, rate: int) ->None:
        """This function allows to adjust the dilation rate"""
        self.dw_conv.block.conv.dilation = rate
        self.dw_conv.block.conv.padding = rate


class ASPP(BaseModule):
    """
    ASPP module defined in DeepLab papers, `here <https://arxiv.org/abs/1606.00915>`_ and `here <https://arxiv.org/abs/1706.05587>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        atrous_rates (Tuple[int]): atrous rates for different branches.
        is_sep_conv (Optional[bool]): Use separable convolution instead of standaard conv. Default: False
        dropout (Optional[float]): Apply dropout. Default is 0.0

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: int, out_channels: int, atrous_rates: Tuple[int], is_sep_conv: Optional[bool]=False, dropout: Optional[float]=0.0, *args, **kwargs) ->None:
        in_proj = ConvLayer(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        out_proj = ConvLayer(opts=opts, in_channels=5 * out_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        aspp_layer = ASPPSeparableConv if is_sep_conv else ASPPConv
        assert len(atrous_rates) == 3
        modules = [in_proj]
        modules.extend([aspp_layer(opts=opts, in_channels=in_channels, out_channels=out_channels, dilation=rate) for rate in atrous_rates])
        modules.append(ASPPPooling(opts=opts, in_channels=in_channels, out_channels=out_channels))
        if not 0.0 <= dropout < 1.0:
            if is_master(opts):
                logger.warning('Dropout value in {} should be between 0 and 1. Got: {}. Setting it to 0.0'.format(self.__class__.__name__, dropout))
            dropout = 0.0
        super().__init__()
        self.convs = nn.ModuleList(modules)
        self.project = out_proj
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.atrous_rates = atrous_rates
        self.is_sep_conv_layer = is_sep_conv
        self.n_atrous_branches = len(atrous_rates)
        self.dropout_layer = Dropout2d(p=dropout)

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        out = []
        for conv in self.convs:
            out.append(conv(x))
        out = torch.cat(out, dim=1)
        out = self.project(out)
        out = self.dropout_layer(out)
        return out

    def profile_module(self, input: Tensor, *args, **kwargs) ->(Tensor, float, float):
        params, macs = 0.0, 0.0
        res = []
        for c in self.convs:
            out, p, m = module_profile(module=c, x=input)
            params += p
            macs += m
            res.append(out)
        res = torch.cat(res, dim=1)
        out, p, m = module_profile(module=self.project, x=res)
        params += p
        macs += m
        return out, params, macs

    def __repr__(self):
        return '{}(in_channels={}, out_channels={}, atrous_rates={}, is_aspp_sep={}, dropout={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.atrous_rates, self.is_sep_conv_layer, self.dropout_layer.p)


def register_segmentation_head(name):

    def register_model_class(cls):
        if name in SEG_HEAD_REGISTRY:
            raise ValueError('Cannot register duplicate model ({})'.format(name))
        if not issubclass(cls, BaseSegHead):
            raise ValueError('Model ({}: {}) must extend BaseSegHead'.format(name, cls.__name__))
        SEG_HEAD_REGISTRY[name] = cls
        return cls
    return register_model_class


class DeeplabV3(BaseSegHead):
    """
    This class defines the segmentation head in `DeepLabv3 architecture <https://arxiv.org/abs/1706.05587>`_
    Args:
        opts: command-line arguments
        enc_conf (Dict): Encoder input-output configuration at each spatial level
        use_l5_exp (Optional[bool]): Use features from expansion layer in Level5 in the encoder
    """

    def __init__(self, opts, enc_conf: Dict, use_l5_exp: Optional[bool]=False, *args, **kwargs) ->None:
        atrous_rates = getattr(opts, 'model.segmentation.deeplabv3.aspp_rates', (6, 12, 18))
        out_channels = getattr(opts, 'model.segmentation.deeplabv3.aspp_out_channels', 256)
        is_sep_conv = getattr(opts, 'model.segmentation.deeplabv3.aspp_sep_conv', False)
        dropout = getattr(opts, 'model.segmentation.deeplabv3.aspp_dropout', 0.1)
        super().__init__(opts=opts, enc_conf=enc_conf, use_l5_exp=use_l5_exp)
        self.aspp = nn.Sequential()
        aspp_in_channels = self.enc_l5_channels if not self.use_l5_exp else self.enc_l5_exp_channels
        self.aspp.add_module(name='aspp_layer', module=ASPP(opts=opts, in_channels=aspp_in_channels, out_channels=out_channels, atrous_rates=atrous_rates, is_sep_conv=is_sep_conv, dropout=dropout))
        self.classifier = ConvLayer(opts=opts, in_channels=out_channels, out_channels=self.n_seg_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True)
        self.reset_head_parameters(opts=opts)

    def update_classifier(self, opts, n_classes: int) ->None:
        """
        This function updates the classification layer in a model. Useful for finetuning purposes.
        """
        in_channels = self.classifier.in_channels
        conv_layer = ConvLayer(opts=opts, in_channels=in_channels, out_channels=n_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True)
        initialize_weights(opts, modules=conv_layer)
        self.classifier = conv_layer

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        """DeepLabv3 specific arguments"""
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.segmentation.deeplabv3.aspp-rates', type=tuple, default=(6, 12, 18), help='Atrous rates in DeepLabV3+ model')
        group.add_argument('--model.segmentation.deeplabv3.aspp-out-channels', type=int, default=256, help='Output channels of ASPP module')
        group.add_argument('--model.segmentation.deeplabv3.aspp-sep-conv', action='store_true', help='Separable conv in ASPP module')
        group.add_argument('--model.segmentation.deeplabv3.aspp-dropout', type=float, default=0.1, help='Dropout in ASPP module')
        return parser

    def forward_seg_head(self, enc_out: Dict) ->Tensor:
        x = enc_out['out_l5_exp'] if self.use_l5_exp else enc_out['out_l5']
        x = self.aspp(x)
        x = self.classifier(x)
        return x

    def profile_module(self, enc_out: Dict) ->Tuple[Tensor, float, float]:
        params, macs = 0.0, 0.0
        if self.use_l5_exp:
            x, p, m = module_profile(module=self.aspp, x=enc_out['out_l5_exp'])
        else:
            x, p, m = module_profile(module=self.aspp, x=enc_out['out_l5'])
        params += p
        macs += m
        out, p, m = module_profile(module=self.classifier, x=x)
        params += p
        macs += m
        None
        return out, params, macs


class PSP(BaseModule):
    """
    This class defines the Pyramid Scene Parsing module in the `PSPNet paper <https://arxiv.org/abs/1612.01105>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        pool_sizes Optional[Tuple[int, ...]]: List or Tuple of pool sizes. Default: (1, 2, 3, 6)
        dropout (Optional[float]): Apply dropout. Default is 0.0
    """

    def __init__(self, opts, in_channels: int, out_channels: int, pool_sizes: Optional[Tuple[int, ...]]=(1, 2, 3, 6), dropout: Optional[float]=0.0, *args, **kwargs) ->None:
        if not 0.0 <= dropout < 1.0:
            logger.error('Dropout value in {} should be between 0 and 1. Got: {}'.format(self.__class__.__name__, dropout))
        reduction_dim = in_channels // len(pool_sizes)
        reduction_dim = reduction_dim // 16 * 16
        channels_after_concat = reduction_dim * len(pool_sizes) + in_channels
        super().__init__()
        self.psp_branches = nn.ModuleList([self._make_psp_layer(opts, o_size=ps, in_channels=in_channels, out_channels=reduction_dim) for ps in pool_sizes])
        self.fusion = nn.Sequential(ConvLayer(opts=opts, in_channels=channels_after_concat, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=True), Dropout2d(p=dropout))
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.pool_sizes = pool_sizes
        self.inner_channels = reduction_dim
        self.dropout = dropout

    @staticmethod
    def _make_psp_layer(opts, o_size: int, in_channels: int, out_channels: int) ->nn.Module:
        return nn.Sequential(AdaptiveAvgPool2d(output_size=(o_size, o_size)), ConvLayer(opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False, use_norm=True, use_act=True))

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        x_size = x.shape[2:]
        out = [x] + [F.interpolate(input=psp_branch(x), size=x_size, mode='bilinear', align_corners=True) for psp_branch in self.psp_branches]
        out = torch.cat(out, dim=1)
        out = self.fusion(out)
        return out

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        params, macs = 0.0, 0.0
        res = [input]
        input_size = input.size()
        for psp_branch in self.psp_branches:
            out, p, m = module_profile(module=psp_branch, x=input)
            out = F.interpolate(out, input_size[2:], mode='bilinear', align_corners=True)
            params += p
            macs += m
            res.append(out)
        res = torch.cat(res, dim=1)
        res, p, m = module_profile(module=self.fusion, x=res)
        return res, params + p, macs + m

    def __repr__(self):
        return '{}(in_channels={}, out_channels={}, pool_sizes={}, inner_channels={}, dropout_2d={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.pool_sizes, self.inner_channels, self.dropout)


class PSPNet(BaseSegHead):
    """
    This class defines the segmentation head in `PSPNet architecture <https://arxiv.org/abs/1612.01105>`_
    Args:
        opts: command-line arguments
        enc_conf (Dict): Encoder input-output configuration at each spatial level
        use_l5_exp (Optional[bool]): Use features from expansion layer in Level5 in the encoder
    """

    def __init__(self, opts, enc_conf: dict, use_l5_exp: Optional[bool]=False, *args, **kwargs) ->None:
        psp_out_channels = getattr(opts, 'model.segmentation.pspnet.psp_out_channels', 512)
        psp_pool_sizes = getattr(opts, 'model.segmentation.pspnet.psp_pool_sizes', [1, 2, 3, 6])
        psp_dropout = getattr(opts, 'model.segmentation.pspnet.psp_dropout', 0.1)
        super().__init__(opts=opts, enc_conf=enc_conf, use_l5_exp=use_l5_exp)
        psp_in_channels = self.enc_l5_channels if not self.use_l5_exp else self.enc_l5_exp_channels
        self.psp_layer = PSP(opts=opts, in_channels=psp_in_channels, out_channels=psp_out_channels, pool_sizes=psp_pool_sizes, dropout=psp_dropout)
        self.classifier = ConvLayer(opts=opts, in_channels=psp_out_channels, out_channels=self.n_seg_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True)
        self.reset_head_parameters(opts=opts)

    def update_classifier(self, opts, n_classes: int) ->None:
        """
        This function updates the classification layer in a model. Useful for finetuning purposes.
        """
        in_channels = self.classifier.in_channels
        conv_layer = ConvLayer(opts=opts, in_channels=in_channels, out_channels=n_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True)
        initialize_weights(opts, modules=conv_layer)
        self.classifier = conv_layer

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.segmentation.pspnet.psp-pool-sizes', type=int, nargs='+', default=[1, 2, 3, 6], help='Pool sizes in the PSPNet module')
        group.add_argument('--model.segmentation.pspnet.psp-out-channels', type=int, default=512, help='Output channels of PSPNet module')
        group.add_argument('--model.segmentation.pspnet.psp-dropout', type=float, default=0.1, help='Dropout in the PSPNet module')
        return parser

    def forward_seg_head(self, enc_out: Dict) ->Tensor:
        x = enc_out['out_l5_exp'] if self.use_l5_exp else enc_out['out_l5']
        x = self.psp_layer(x)
        out = self.classifier(x)
        return out

    def profile_module(self, enc_out: Dict) ->Tuple[Tensor, float, float]:
        params, macs = 0.0, 0.0
        if self.use_l5_exp:
            x, p, m = module_profile(module=self.psp_layer, x=enc_out['out_l5_exp'])
        else:
            x, p, m = module_profile(module=self.psp_layer, x=enc_out['out_l5'])
        params += p
        macs += m
        out, p, m = module_profile(module=self.classifier, x=x)
        params += p
        macs += m
        None
        return out, params, macs


class BaseVideoEncoder(nn.Module):
    """
    Base class for the video backbones
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__()
        self.round_nearest = 8
        self.model_conf_dict = dict()
        self.inference_mode = getattr(opts, 'model.video_classification.inference_mode', False)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        return parser

    def reset_parameters(self, opts) ->None:
        """Reset parameters for all modules in the network"""
        initialize_weights(opts=opts, modules=self.modules())

    @staticmethod
    def reset_module_parameters(opts, module) ->None:
        """Reset parameters for a specific module in the network"""
        initialize_weights(opts=opts, modules=module)

    def extract_end_points_all(self, x: Tensor, use_l5: Optional[bool]=True, use_l5_exp: Optional[bool]=False, *args, **kwargs) ->Dict:
        raise NotImplementedError

    def extract_end_points_l4(self, x: Tensor, *args, **kwargs) ->Dict:
        raise NotImplementedError

    def _extract_features(self, x: Tensor, *args, **kwargs) ->Tensor:
        raise NotImplementedError

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        raise NotImplementedError

    def freeze_norm_layers(self) ->None:
        """Freeze normalization layers in the network"""
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def get_trainable_parameters(self, weight_decay: Optional[float]=0.0, no_decay_bn_filter_bias: Optional[bool]=False, *args, **kwargs):
        """
        Get trainable parameters for the network
        """
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    def profile_model(self, input: Tensor, *args, **kwargs) ->None:
        """
        This function computes FLOPs using fvcore (if installed).
        """
        logger.double_dash_line(dashes=65)
        None
        logger.double_dash_line(dashes=65)
        overall_params_py = sum([p.numel() for p in self.parameters()])
        try:
            flop_analyzer = FlopCountAnalysis(self.eval(), input)
            flop_analyzer.unsupported_ops_warnings(False)
            total_flops = flop_analyzer.total()
            None
        except ModuleNotFoundError:
            pass
        None
        logger.double_dash_line(dashes=65)

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        raise NotImplementedError


def register_video_cls_models(name):

    def register_model_class(cls):
        if name in CLS_MODEL_REGISTRY:
            raise ValueError('Cannot register duplicate model ({})'.format(name))
        if not issubclass(cls, BaseVideoEncoder):
            raise ValueError('Model ({}: {}) must extend BaseEncoder'.format(name, cls.__name__))
        CLS_MODEL_REGISTRY[name] = cls
        return cls
    return register_model_class


class SpatioTemporalMobileViT(BaseVideoEncoder):
    """
    This class defines the spatio-temporal `MobileViT <https://arxiv.org/abs/2110.02178>`_ model for the
    task of video classification
    """
    os_keys = ['os_8', 'os_16', 'os_32']

    def __init__(self, opts, *args, **kwargs) ->None:
        mobilevit_img_model = build_classification_model(opts=opts)
        mobilevit_img_model.conv_1x1_exp = None
        mobilevit_img_model.classifier = None
        num_classes = getattr(opts, 'model.video_classification.n_classes', 400)
        classifier_dropout = getattr(opts, 'model.video_classification.classifier_dropout', 0.2)
        pool_type = getattr(opts, 'model.layer.global_pool', 'mean')
        super(SpatioTemporalMobileViT, self).__init__(opts=opts)
        self.mobilevit_model = mobilevit_img_model
        in_channels = mobilevit_img_model.model_conf_dict['exp_before_cls']['in']
        out_channels = mobilevit_img_model.model_conf_dict['exp_before_cls']['out']
        self.temporal_conv = nn.Sequential(ConvLayer3d(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1, use_norm=True, use_act=False, groups=in_channels), ConvLayer3d(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=1, use_norm=True, use_act=True))
        self.global_pool = GlobalPool(pool_type=pool_type, keep_dim=False)
        self.classifier = nn.Sequential()
        if 0.0 < classifier_dropout < 1.0:
            self.classifier.add_module(name='dropout', module=Dropout(p=classifier_dropout, inplace=True))
        self.classifier.add_module(name='fc', module=LinearLayer(in_features=out_channels, out_features=num_classes, bias=True))
        self.reset_module_parameters(opts=opts, module=self.temporal_conv)
        self.reset_module_parameters(opts=opts, module=self.classifier)
        self.num_classes = num_classes

    def _init_cache(self) ->Dict[str, Optional[Tensor]]:
        """Initialize a cache to store intermediate results at time step t so that they can be used at step t+1"""
        return dict(zip(self.os_keys, [None] * len(self.os_keys)))

    def _extract_mobilevit_features(self, layer, x: Tensor, cached_fm: Tensor) ->Tuple[Tensor, Tensor]:
        x = layer[0](x)
        x, cached_fm = layer[1]((x, cached_fm))
        return x, cached_fm

    def _forward_cnn(self, x: Tensor) ->Tensor:
        x = self.mobilevit_model.conv_1(x)
        x = self.mobilevit_model.layer_1(x)
        x = self.mobilevit_model.layer_2(x)
        return x

    def _forward_fn(self, x: Tensor, spatial_cache: Optional[Dict[str, Optional[Tensor]]]=None, *args, **kwargs) ->Tuple[Tensor, Dict[str, Tensor]]:
        x = self._forward_cnn(x)
        mvit_cache = dict()
        for os, layers_os in zip(self.os_keys, [self.mobilevit_model.layer_3, self.mobilevit_model.layer_4, self.mobilevit_model.layer_5]):
            x, curr_patches = self._extract_mobilevit_features(layer=layers_os, x=x, cached_fm=spatial_cache[os].clone() if spatial_cache[os] is not None else None)
            mvit_cache[os] = curr_patches
        return x, mvit_cache

    def _forward_train(self, x: Tensor) ->Tensor:
        assert x.dim() == 5, 'Input should be 5-dimensional [B,N,C,H,W]'
        num_frames = x.shape[1]
        spatial_cache = self._init_cache()
        outputs = []
        for i in range(num_frames):
            out, spatial_cache = self._forward_fn(x=x[:, i], spatial_cache=spatial_cache)
            outputs.append(out)
        outputs = torch.stack(outputs, dim=2)
        outputs = self.temporal_conv(outputs)
        outputs = self.global_pool(outputs)
        return self.classifier(outputs)

    def _forward_inference(self, x: Tensor) ->Tensor:
        assert x.dim() == 5, 'Input should be 5-dimensional [B,N,C,H,W]'
        batch_size, num_frames = x.shape[:2]
        batch_outputs = torch.zeros(size=(batch_size, self.num_classes), dtype=x.dtype, device=x.device)
        for b in range(batch_size):
            spatial_cache = self._init_cache()
            frame_outputs = []
            for i in range(num_frames):
                out, spatial_cache = self._forward_fn(x=x[b, i].unsqueeze(0), spatial_cache=spatial_cache)
                frame_outputs.append(out)
            frame_outputs = torch.stack(frame_outputs, dim=2)
            frame_outputs = self.temporal_conv(frame_outputs)
            frame_outputs = self.global_pool(frame_outputs)
            frame_outputs = self.classifier(frame_outputs)
            batch_outputs[b] = frame_outputs[0]
        return batch_outputs

    def forward(self, x: Tensor, *args, **kwargs) ->Tensor:
        if self.inference_mode and not self.training:
            return self._forward_inference(x)
        else:
            return self._forward_train(x)

    def dummy_input_and_label(self, batch_size: int) ->Dict:
        """Create dummy input and labels for CI/CD purposes."""
        num_frames = 2
        img_channels = 3
        height = 224
        width = 224
        n_classes = 10
        img_tensor = torch.randn(batch_size, num_frames, img_channels, height, width, dtype=torch.float)
        label_tensor = torch.randint(low=0, high=n_classes, size=(batch_size,)).long()
        return {'samples': img_tensor, 'targets': label_tensor}


class FeaturePyramidNetwork(BaseModule):
    """
    This class implements the `Feature Pyramid Network <https://arxiv.org/abs/1612.03144>`_ module for object detection.

    Args:
        opts: command-line arguments
        in_channels (List[int]): List of channels at different output strides
        output_strides (List[int]): Feature maps from these output strides will be used in FPN
        out_channels (int): Output channels

    """

    def __init__(self, opts, in_channels: List[int], output_strides: List[str], out_channels: int, *args, **kwargs) ->None:
        if isinstance(in_channels, int):
            in_channels = [in_channels]
        if isinstance(output_strides, int):
            output_strides = [output_strides]
        if len(in_channels) != len(output_strides):
            logger.error('For {}, we need the length of input_channels to be the same as the length of output stride. Got: {} and {}'.format(self.__class__.__name__, len(in_channels), len(output_strides)))
        assert len(in_channels) == len(output_strides)
        super().__init__(*args, **kwargs)
        self.proj_layers = nn.ModuleDict()
        self.nxn_convs = nn.ModuleDict()
        for os, in_channel in zip(output_strides, in_channels):
            proj_layer = ConvLayer(opts=opts, in_channels=in_channel, out_channels=out_channels, kernel_size=1, bias=False, use_norm=True, use_act=False)
            nxn_conv = ConvLayer(opts=opts, in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=False, use_norm=True, use_act=False)
            self.proj_layers.add_module(name='os_{}'.format(os), module=proj_layer)
            self.nxn_convs.add_module(name='os_{}'.format(os), module=nxn_conv)
        self.num_fpn_layers = len(in_channels)
        self.out_channels = out_channels
        self.in_channels = in_channels
        self.output_strides = output_strides
        self.reset_weights()

    def reset_weights(self) ->None:
        """Resets the weights of FPN layers"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                initialize_conv_layer(m, init_method='xavier_uniform')
            elif isinstance(m, norm_layers_tuple):
                initialize_norm_layers(m)

    def forward(self, x: Dict[str, Tensor], *args, **kwargs) ->Dict[str, Tensor]:
        assert len(x) == self.num_fpn_layers
        fpn_out_dict = {'os_'.format(os): None for os in self.output_strides}
        os_key = 'os_{}'.format(self.output_strides[-1])
        prev_x = self.proj_layers[os_key](x[os_key])
        prev_x = self.nxn_convs[os_key](prev_x)
        fpn_out_dict[os_key] = prev_x
        remaining_output_strides = self.output_strides[:-1]
        for os in remaining_output_strides[::-1]:
            os_key = 'os_{}'.format(os)
            curr_x = self.proj_layers[os_key](x[os_key])
            prev_x = F.interpolate(prev_x, size=curr_x.shape[-2:], mode='nearest')
            prev_x = curr_x + prev_x
            prev_x = self.nxn_convs[os_key](prev_x)
            fpn_out_dict[os_key] = prev_x
        return fpn_out_dict

    def profile_module(self, input: Dict[str, Tensor], *args, **kwargs) ->(Dict[str, Tensor], float, float):
        params, macs = 0.0, 0.0
        fpn_out_dict = {'os_{}'.format(os): None for os in self.output_strides}
        os_key = 'os_{}'.format(self.output_strides[-1])
        prev_x, p, m = module_profile(module=self.proj_layers[os_key], x=input[os_key])
        params += p
        macs += m
        prev_x, p, m = module_profile(module=self.nxn_convs[os_key], x=prev_x)
        params += p
        macs += m
        fpn_out_dict[os_key] = prev_x
        remaining_output_strides = self.output_strides[:-1]
        for os in remaining_output_strides[::-1]:
            os_key = 'os_{}'.format(os)
            curr_x, p, m = module_profile(module=self.proj_layers[os_key], x=input[os_key])
            params += p
            macs += m
            prev_x = F.interpolate(prev_x, size=curr_x.shape[-2:], mode='nearest')
            prev_x = curr_x + prev_x
            prev_x, p, m = module_profile(module=self.nxn_convs[os_key], x=prev_x)
            params += p
            macs += m
            fpn_out_dict[os_key] = prev_x
        return fpn_out_dict, params, macs

    def __repr__(self):
        return '{}(in_channels={}, output_strides={} out_channels={})'.format(self.__class__.__name__, self.in_channels, self.output_strides, self.out_channels)


class LinearAttnFFN(BaseModule):
    """
    This class defines the pre-norm transformer encoder with linear self-attention in `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ paper
    Args:
        opts: command line arguments
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(B, C_{in}, P, N)`
        ffn_latent_dim (int): Inner dimension of the FFN
        attn_dropout (Optional[float]): Dropout rate for attention in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers. Default: 0.0
        norm_layer (Optional[str]): Normalization layer. Default: layer_norm_2d

    Shape:
        - Input: :math:`(B, C_{in}, P, N)` where :math:`B` is batch size, :math:`C_{in}` is input embedding dim,
            :math:`P` is number of pixels in a patch, and :math:`N` is number of patches,
        - Output: same shape as the input
    """

    def __init__(self, opts, embed_dim: int, ffn_latent_dim: int, attn_dropout: Optional[float]=0.0, dropout: Optional[float]=0.1, ffn_dropout: Optional[float]=0.0, norm_layer: Optional[str]='layer_norm_2d', *args, **kwargs) ->None:
        super().__init__()
        attn_unit = LinearSelfAttention(opts, embed_dim=embed_dim, attn_dropout=attn_dropout, bias=True)
        self.pre_norm_attn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), attn_unit, Dropout(p=dropout))
        self.pre_norm_ffn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), ConvLayer(opts=opts, in_channels=embed_dim, out_channels=ffn_latent_dim, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=True), Dropout(p=ffn_dropout), ConvLayer(opts=opts, in_channels=ffn_latent_dim, out_channels=embed_dim, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=False), Dropout(p=dropout))
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_latent_dim
        self.ffn_dropout = ffn_dropout
        self.std_dropout = dropout
        self.attn_fn_name = attn_unit.__repr__()
        self.norm_name = norm_layer

    @staticmethod
    def build_act_layer(opts) ->nn.Module:
        act_type = getattr(opts, 'model.activation.name', 'relu')
        neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        inplace = getattr(opts, 'model.activation.inplace', False)
        act_layer = get_activation_fn(act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=1)
        return act_layer

    def __repr__(self) ->str:
        return '{}(embed_dim={}, ffn_dim={}, dropout={}, ffn_dropout={}, attn_fn={}, norm_layer={})'.format(self.__class__.__name__, self.embed_dim, self.ffn_dim, self.std_dropout, self.ffn_dropout, self.attn_fn_name, self.norm_name)

    def forward(self, x: Tensor, x_prev: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if x_prev is None:
            x = x + self.pre_norm_attn(x)
        else:
            res = x
            x = self.pre_norm_attn[0](x)
            x = self.pre_norm_attn[1](x, x_prev)
            x = self.pre_norm_attn[2](x)
            x = x + res
        x = x + self.pre_norm_ffn(x)
        return x

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        out, p_mha, m_mha = module_profile(module=self.pre_norm_attn, x=input)
        out, p_ffn, m_ffn = module_profile(module=self.pre_norm_ffn, x=input)
        macs = m_mha + m_ffn
        params = p_mha + p_ffn
        return input, params, macs


class MobileViTBlockv2(BaseModule):
    """
    This class defines the `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ block

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        attn_unit_dim (int): Input dimension to the attention unit
        ffn_multiplier (int): Expand the input dimensions by this factor in FFN. Default is 2.
        n_attn_blocks (Optional[int]): Number of attention units. Default: 2
        attn_dropout (Optional[float]): Dropout in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers in transformer. Default: 0.0
        patch_h (Optional[int]): Patch height for unfolding operation. Default: 8
        patch_w (Optional[int]): Patch width for unfolding operation. Default: 8
        conv_ksize (Optional[int]): Kernel size to learn local representations in MobileViT block. Default: 3
        dilation (Optional[int]): Dilation rate in convolutions. Default: 1
        attn_norm_layer (Optional[str]): Normalization layer in the attention block. Default: layer_norm_2d
    """

    def __init__(self, opts, in_channels: int, attn_unit_dim: int, ffn_multiplier: Optional[Union[Sequence[Union[int, float]], int, float]]=2.0, n_attn_blocks: Optional[int]=2, attn_dropout: Optional[float]=0.0, dropout: Optional[float]=0.0, ffn_dropout: Optional[float]=0.0, patch_h: Optional[int]=8, patch_w: Optional[int]=8, conv_ksize: Optional[int]=3, dilation: Optional[int]=1, attn_norm_layer: Optional[str]='layer_norm_2d', *args, **kwargs) ->None:
        cnn_out_dim = attn_unit_dim
        conv_3x3_in = ConvLayer(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True, dilation=dilation, groups=in_channels)
        conv_1x1_in = ConvLayer(opts=opts, in_channels=in_channels, out_channels=cnn_out_dim, kernel_size=1, stride=1, use_norm=False, use_act=False)
        super(MobileViTBlockv2, self).__init__()
        self.local_rep = nn.Sequential(conv_3x3_in, conv_1x1_in)
        self.global_rep, attn_unit_dim = self._build_attn_layer(opts=opts, d_model=attn_unit_dim, ffn_mult=ffn_multiplier, n_layers=n_attn_blocks, attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, attn_norm_layer=attn_norm_layer)
        self.conv_proj = ConvLayer(opts=opts, in_channels=cnn_out_dim, out_channels=in_channels, kernel_size=1, stride=1, use_norm=True, use_act=False)
        self.patch_h = patch_h
        self.patch_w = patch_w
        self.patch_area = self.patch_w * self.patch_h
        self.cnn_in_dim = in_channels
        self.cnn_out_dim = cnn_out_dim
        self.transformer_in_dim = attn_unit_dim
        self.dropout = dropout
        self.attn_dropout = attn_dropout
        self.ffn_dropout = ffn_dropout
        self.n_blocks = n_attn_blocks
        self.conv_ksize = conv_ksize
        self.enable_coreml_compatible_fn = getattr(opts, 'common.enable_coreml_compatible_module', False)
        if self.enable_coreml_compatible_fn:
            self.register_buffer(name='unfolding_weights', tensor=self._compute_unfolding_weights(), persistent=False)

    def _compute_unfolding_weights(self) ->Tensor:
        weights = torch.eye(self.patch_h * self.patch_w, dtype=torch.float)
        weights = weights.reshape((self.patch_h * self.patch_w, 1, self.patch_h, self.patch_w))
        weights = weights.repeat(self.cnn_out_dim, 1, 1, 1)
        return weights

    def _build_attn_layer(self, opts, d_model: int, ffn_mult: Union[Sequence, int, float], n_layers: int, attn_dropout: float, dropout: float, ffn_dropout: float, attn_norm_layer: str, *args, **kwargs) ->Tuple[nn.Module, int]:
        if isinstance(ffn_mult, Sequence) and len(ffn_mult) == 2:
            ffn_dims = np.linspace(ffn_mult[0], ffn_mult[1], n_layers, dtype=float) * d_model
        elif isinstance(ffn_mult, Sequence) and len(ffn_mult) == 1:
            ffn_dims = [ffn_mult[0] * d_model] * n_layers
        elif isinstance(ffn_mult, (int, float)):
            ffn_dims = [ffn_mult * d_model] * n_layers
        else:
            raise NotImplementedError
        ffn_dims = [int(d // 16 * 16) for d in ffn_dims]
        global_rep = [LinearAttnFFN(opts=opts, embed_dim=d_model, ffn_latent_dim=ffn_dims[block_idx], attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, norm_layer=attn_norm_layer) for block_idx in range(n_layers)]
        global_rep.append(get_normalization_layer(opts=opts, norm_type=attn_norm_layer, num_features=d_model))
        return nn.Sequential(*global_rep), d_model

    def __repr__(self) ->str:
        repr_str = '{}('.format(self.__class__.__name__)
        repr_str += '\n\t Local representations'
        if isinstance(self.local_rep, nn.Sequential):
            for m in self.local_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.local_rep)
        repr_str += '\n\t Global representations with patch size of {}x{}'.format(self.patch_h, self.patch_w)
        if isinstance(self.global_rep, nn.Sequential):
            for m in self.global_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.global_rep)
        if isinstance(self.conv_proj, nn.Sequential):
            for m in self.conv_proj:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.conv_proj)
        repr_str += '\n)'
        return repr_str

    def unfolding_pytorch(self, feature_map: Tensor) ->Tuple[Tensor, Tuple[int, int]]:
        batch_size, in_channels, img_h, img_w = feature_map.shape
        patches = F.unfold(feature_map, kernel_size=(self.patch_h, self.patch_w), stride=(self.patch_h, self.patch_w))
        patches = patches.reshape(batch_size, in_channels, self.patch_h * self.patch_w, -1)
        return patches, (img_h, img_w)

    def folding_pytorch(self, patches: Tensor, output_size: Tuple[int, int]) ->Tensor:
        batch_size, in_dim, patch_size, n_patches = patches.shape
        patches = patches.reshape(batch_size, in_dim * patch_size, n_patches)
        feature_map = F.fold(patches, output_size=output_size, kernel_size=(self.patch_h, self.patch_w), stride=(self.patch_h, self.patch_w))
        return feature_map

    def unfolding_coreml(self, feature_map: Tensor) ->Tuple[Tensor, Tuple[int, int]]:
        batch_size, in_channels, img_h, img_w = feature_map.shape
        patches = F.conv2d(feature_map, self.unfolding_weights, bias=None, stride=(self.patch_h, self.patch_w), padding=0, dilation=1, groups=in_channels)
        patches = patches.reshape(batch_size, in_channels, self.patch_h * self.patch_w, -1)
        return patches, (img_h, img_w)

    def folding_coreml(self, patches: Tensor, output_size: Tuple[int, int]) ->Tensor:
        batch_size, in_dim, patch_size, n_patches = patches.shape
        n_patches_h = output_size[0] // self.patch_h
        n_patches_w = output_size[1] // self.patch_w
        feature_map = patches.reshape(batch_size, in_dim * self.patch_h * self.patch_w, n_patches_h, n_patches_w)
        assert self.patch_h == self.patch_w, 'For Coreml, we need patch_h and patch_w are the same'
        feature_map = F.pixel_shuffle(feature_map, upscale_factor=self.patch_h)
        return feature_map

    def resize_input_if_needed(self, x):
        batch_size, in_channels, orig_h, orig_w = x.shape
        if orig_h % self.patch_h != 0 or orig_w % self.patch_w != 0:
            new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)
            new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)
            x = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=True)
        return x

    def forward_spatial(self, x: Tensor, *args, **kwargs) ->Tensor:
        x = self.resize_input_if_needed(x)
        fm = self.local_rep(x)
        if self.enable_coreml_compatible_fn:
            patches, output_size = self.unfolding_coreml(fm)
        else:
            patches, output_size = self.unfolding_pytorch(fm)
        patches = self.global_rep(patches)
        if self.enable_coreml_compatible_fn:
            fm = self.folding_coreml(patches=patches, output_size=output_size)
        else:
            fm = self.folding_pytorch(patches=patches, output_size=output_size)
        fm = self.conv_proj(fm)
        return fm

    def forward_temporal(self, x: Tensor, x_prev: Tensor, *args, **kwargs) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        x = self.resize_input_if_needed(x)
        fm = self.local_rep(x)
        if self.enable_coreml_compatible_fn:
            patches, output_size = self.unfolding_coreml(fm)
        else:
            patches, output_size = self.unfolding_pytorch(fm)
        for global_layer in self.global_rep:
            if isinstance(global_layer, LinearAttnFFN):
                patches = global_layer(x=patches, x_prev=x_prev)
            else:
                patches = global_layer(patches)
        if self.enable_coreml_compatible_fn:
            fm = self.folding_coreml(patches=patches, output_size=output_size)
        else:
            fm = self.folding_pytorch(patches=patches, output_size=output_size)
        fm = self.conv_proj(fm)
        return fm, patches

    def forward(self, x: Union[Tensor, Tuple[Tensor]], *args, **kwargs) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        if isinstance(x, Tuple) and len(x) == 2:
            return self.forward_temporal(x=x[0], x_prev=x[1])
        elif isinstance(x, Tensor):
            return self.forward_spatial(x)
        else:
            raise NotImplementedError

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        params = macs = 0.0
        input = self.resize_input_if_needed(input)
        res = input
        out, p, m = module_profile(module=self.local_rep, x=input)
        params += p
        macs += m
        patches, output_size = self.unfolding_pytorch(feature_map=out)
        patches, p, m = module_profile(module=self.global_rep, x=patches)
        params += p
        macs += m
        fm = self.folding_pytorch(patches=patches, output_size=output_size)
        out, p, m = module_profile(module=self.conv_proj, x=fm)
        params += p
        macs += m
        return res, params, macs


class SSDInstanceHead(BaseModule):
    """
    Instance segmentation head for SSD model.
    """

    def __init__(self, opts, in_channels: int, n_classes: Optional[int]=1, inner_dim: Optional[int]=256, output_stride: Optional[int]=1, output_size: Optional[int]=8, *args, **kwargs) ->None:
        """

        Args:
            opts: command-line arguments
            in_channels (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
            n_classes (Optional[int]): Number of classes. Default: 1
            inner_dim: (Optional[int]): Inner dimension of the instance head. Default: 256
            output_stride (Optional[int]): Output stride of the feature map. Output stride is the ratio of input to
                the feature map size. Default: 1
            output_size (Optional[int]): Output size of the instances extracted from RoIAlign layer. Default: 8
        """
        super().__init__()
        self.roi_align = RoIAlign(output_size=output_size, spatial_scale=1.0 / output_stride, sampling_ratio=2, aligned=True)
        self.seg_head = nn.Sequential(TransposeConvLayer(opts=opts, in_channels=in_channels, out_channels=inner_dim, kernel_size=2, stride=2, bias=True, use_norm=False, use_act=True, auto_padding=False, padding=0, output_padding=0), ConvLayer(opts=opts, in_channels=inner_dim, out_channels=n_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True))
        self.inner_channels = inner_dim
        self.in_channels = in_channels
        self.mask_classes = n_classes
        self.reset_parameters()

    def __repr__(self) ->str:
        return '{}(in_channels={}, up_out_channels={}, n_classes={})'.format(self.__class__.__name__, self.in_channels, self.inner_channels, self.mask_classes)

    def reset_parameters(self) ->None:
        for layer in self.modules():
            if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')

    def forward(self, x: Tensor, boxes: Tensor, *args, **kwargs) ->Tensor:
        rois = self.roi_align(x, boxes)
        rois = self.seg_head(rois)
        return rois

    def profile_module(self, input: Tensor, *args, **kwargs) ->Tuple[Tensor, float, float]:
        input, params, macs = module_profile(module=self.seg_head, x=input)
        return input, params, macs


def register_text_encoder(name):

    def register_text_encoder_class(cls):
        if name in TEXT_ENCODER_REGISTRY:
            raise ValueError('Cannot register duplicate text_encoder class ({})'.format(name))
        if not issubclass(cls, BaseTextEncoder):
            raise ValueError('Text encoder class ({}: {}) must extend BaseTextEncoder'.format(name, cls.__name__))
        TEXT_ENCODER_REGISTRY[name] = cls
        return cls
    return register_text_encoder_class


class TextTransformer(BaseTextEncoder):

    def __init__(self, opts, projection_dim: int, *args, **kwargs) ->None:
        model_dim = getattr(opts, 'model.text.transformer.model_dim', 512)
        no_scale_embedding = getattr(opts, 'model.text.transformer.no_scale_embedding', False)
        no_pos_embedding = getattr(opts, 'model.text.transformer.no_pos_embedding', False)
        embed_dropout = getattr(opts, 'model.text.transformer.embed_dropout', 0.0)
        dropout = getattr(opts, 'model.text.transformer.dropout', 0.0)
        attn_dropout = getattr(opts, 'model.text.transformer.attn_dropout', 0.0)
        ffn_dropout = getattr(opts, 'model.text.transformer.ffn_dropout', 0.0)
        norm_layer = getattr(opts, 'model.text.transformer.norm_layer', None)
        gradient_ckpt = getattr(opts, 'model.text.transformer.gradient_checkpoint', False)
        if norm_layer is None:
            logger.error('Normalization layer can not be None in {}'.format(self.__class__.__name__))
        super().__init__(*args, opts=opts, projection_dim=projection_dim, **kwargs)
        padding_index = getattr(opts, 'dataset.padding_index', None)
        self.embedding_layer = Embedding(opts=opts, embedding_dim=model_dim, padding_idx=padding_index, num_embeddings=self.vocab_size)
        self.embed_scale = 1.0 if no_scale_embedding else model_dim ** -0.5
        context_length = getattr(opts, 'dataset.text_context_length', None)
        if getattr(opts, 'common.debug_mode', False):
            context_length = 77
        assert context_length is not None, "Context length can't be None. Please set dataset.text_context_length argument in your dataset class"
        self.positional_embedding = None if no_pos_embedding else PositionalEmbedding(opts=opts, num_embeddings=context_length, embedding_dim=model_dim, padding_idx=getattr(opts, 'dataset.padding_index', None), is_learnable=not getattr(opts, 'model.text.transformer.sinusoidal_pos_emb', False))
        self.embedding_dropout = Dropout(p=embed_dropout)
        n_transformer_layers = getattr(opts, 'model.text.transformer.n_transformer_layers', 6)
        ffn_multipliers = getattr(opts, 'model.text.transformer.ffn_multiplier_per_layer', 4.0)
        if isinstance(ffn_multipliers, (float, int)):
            ffn_multipliers = [ffn_multipliers] * n_transformer_layers
        if not isinstance(ffn_multipliers, Sequence):
            logger.error('{} expects FFN multipliers as a list, whose length is the same as number of transformer layers. Got: {}'.format(self.__class__.__name__, type(ffn_multipliers)))
        elif isinstance(ffn_multipliers, Sequence) and len(ffn_multipliers) != n_transformer_layers:
            logger.error('We need FFN multiplier for each transformer layer. Got {} ffn multipliers while number of transformer layers = {}'.format(len(ffn_multipliers), n_transformer_layers))
        ffn_dims = [int(math.ceil(model_dim * ffn_mult / 16.0) * 16.0) for ffn_mult in ffn_multipliers]
        mha_heads = getattr(opts, 'model.text.transformer.n_heads_per_layer', 8)
        if isinstance(mha_heads, int):
            mha_heads = [mha_heads] * n_transformer_layers
        if not isinstance(mha_heads, Sequence):
            logger.error('{} expects MHA heads as a list, whose length is the same as number of transformer layers. Got: {}'.format(self.__class__.__name__, type(mha_heads)))
        elif isinstance(mha_heads, Sequence) and len(mha_heads) != n_transformer_layers:
            logger.error('{} needs MHA heads for each transformer layer. Got {} mha heads while number of transformer layers = {}'.format(self.__class__.__name__, len(mha_heads), n_transformer_layers))
        self.transformer = nn.ModuleList([TransformerEncoder(opts=opts, embed_dim=model_dim, num_heads=mha_heads[layer_idx], ffn_latent_dim=ffn_dims[layer_idx], attn_dropout=attn_dropout, ffn_dropout=ffn_dropout, dropout=dropout, transformer_norm_layer=norm_layer) for layer_idx in range(n_transformer_layers)])
        self.final_layer_norm = get_normalization_layer(opts, num_features=model_dim, norm_type=norm_layer)
        self.projection_layer = nn.Parameter(torch.empty(model_dim, self.projection_dim))
        self.model_dim = model_dim
        self.reset_parameters_clip_style()
        self.gradient_ckpt = gradient_ckpt
        self.use_pytorch_mha = False
        self.causal_masking = getattr(opts, 'model.text.transformer.causal_masking', False)
        self.classes_per_split_zero_shot = max(1, int(getattr(opts, 'model.text.transformer.classes_per_split_zero_shot', 1)))

    def reset_parameters_clip_style(self):
        """This function resets the weights of Transformer model as done in the CLIP paper"""
        nn.init.normal_(self.embedding_layer.weight, mean=0.0, std=0.02)
        attn_std = self.model_dim ** -0.5
        proj_std = attn_std * (2 * len(self.transformer)) ** -0.5
        fc_std = (2 * self.model_dim) ** -0.5
        for block in self.transformer:
            nn.init.normal_(block.pre_norm_mha[1].qkv_proj.weight, mean=0.0, std=attn_std)
            nn.init.normal_(block.pre_norm_mha[1].out_proj.weight, mean=0.0, std=proj_std)
            nn.init.normal_(block.pre_norm_ffn[1].weight, mean=0.0, std=fc_std)
            nn.init.normal_(block.pre_norm_ffn[4].weight, mean=0.0, std=proj_std)
        nn.init.normal_(self.projection_layer, mean=0.0, std=attn_std)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--model.text.transformer.model-dim', type=int, default=512, help='Model dimension of the transformer model')
        group.add_argument('--model.text.transformer.no-scale-embedding', action='store_true', help='Do not scale the output of embedding layer in {}'.format(cls.__name__))
        group.add_argument('--model.text.transformer.no-pos-embedding', action='store_true', help='Do not add positional embeddings to the output of embedding layer in {}'.format(cls.__name__))
        group.add_argument('--model.text.transformer.embed-dropout', type=float, default=0.0, help='Dropout in embedding layer')
        default_layers = 6
        group.add_argument('--model.text.transformer.n-transformer-layers', type=int, default=default_layers, help='Number of transformer layers in {}'.format(cls.__name__))
        group.add_argument('--model.text.transformer.n-heads-per-layer', type=int, default=[8] * default_layers, nargs='+', help='Number of transformer heads per transformer layer')
        group.add_argument('--model.text.transformer.ffn-multiplier-per-layer', type=float, default=[4.0] * default_layers, nargs='+', help='FFN multiplier for each transformer layer')
        group.add_argument('--model.text.transformer.attn-dropout', type=float, default=0.0, help='Dropout in multi-head attention')
        group.add_argument('--model.text.transformer.ffn-dropout', type=float, default=0.0, help='Dropout between linear layers in FFN')
        group.add_argument('--model.text.transformer.dropout', type=float, default=0.0, help='Dropout in transformer')
        group.add_argument('--model.text.transformer.norm-layer', type=str, default='layer_norm', help='Normalization layer')
        group.add_argument('--model.text.transformer.sinusoidal-pos-emb', action='store_true', help='Use sinusoidal positional embedding')
        group.add_argument('--model.text.transformer.gradient-checkpoint', action='store_true', help='Use gradient checkpointing')
        group.add_argument('--model.text.transformer.num-checkpoint-segments', type=int, default=1, help='Number of gradient checkpoint segments')
        group.add_argument('--model.text.transformer.causal-masking', action='store_true', help='Use causal masking')
        group.add_argument('--model.text.transformer.classes-per-split-zero-shot', type=int, default=20, help='Divide zero-shot classes into these many chunks, for faster processing')
        return parser

    def forward_embedding(self, text_tokens: Tensor):
        token_emb = self.embedding_layer(text_tokens)
        seq_len = token_emb.shape[1]
        if self.positional_embedding is not None:
            token_emb = token_emb + self.positional_embedding(seq_len)
        token_emb = self.embedding_dropout(token_emb)
        return token_emb

    def build_attention_mask(self, context_length: int, batch_size: int):
        mask = torch.empty(context_length, context_length)
        mask.fill_(float('-inf'))
        mask.triu_(1)
        if not self.use_pytorch_mha:
            mask = mask.unsqueeze(0)
            mask = mask.expand(batch_size, -1, -1)
        return mask

    def encode_text(self, text_tokens: Tensor, key_padding_mask: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        token_emb = self.forward_embedding(text_tokens)
        attn_mask = None
        if self.causal_masking:
            attn_mask = self.build_attention_mask(context_length=text_tokens.shape[1], batch_size=text_tokens.shape[0])
            attn_mask = attn_mask
            key_padding_mask = None
        if self.use_pytorch_mha:
            token_emb = token_emb.transpose(0, 1)
        for layer in self.transformer:
            if self.gradient_ckpt:
                token_emb = gradient_checkpoint_fn(layer, token_emb, None, key_padding_mask, attn_mask)
            else:
                token_emb = layer(token_emb, key_padding_mask=key_padding_mask, attn_mask=attn_mask, use_pytorch_mha=self.use_pytorch_mha)
        token_emb = self.final_layer_norm(token_emb)
        if self.use_pytorch_mha:
            token_emb = token_emb[text_tokens.argmax(dim=-1), torch.arange(text_tokens.shape[0])]
        else:
            token_emb = token_emb[torch.arange(text_tokens.shape[0]), text_tokens.argmax(dim=-1)]
        token_emb = token_emb @ self.projection_layer
        token_emb = F.normalize(token_emb, dim=-1)
        return token_emb

    def forward_zero_shot(self, text_tokens: Tensor, key_padding_mask: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if self.training:
            raise NotImplementedError('Zero-shot evaluation is only supported with eval mode')
        if text_tokens.ndim != 4:
            logger.error('For zero-shot evaluation, expected size of text is [Batch, Num_classes, num_captions, context_len]')
        batch_size, num_classes, num_captions, context_len = text_tokens.shape
        if batch_size != 1:
            logger.error('For zero-shot evaluation, text templates are the same across all images in the batch.Therefore, batch size should be 1. Got: {}'.format(batch_size))
        text_features = []
        for start_idx in range(0, num_classes, self.classes_per_split_zero_shot):
            end_idx = min(start_idx + self.classes_per_split_zero_shot, num_classes)
            text_tokens_split = text_tokens[0, start_idx:end_idx, ...]
            num_classes_split = text_tokens_split.shape[0]
            text_tokens_split = text_tokens_split.reshape(num_classes_split * num_captions, context_len)
            key_padding_mask_split = None
            if key_padding_mask is not None:
                key_padding_mask_split = key_padding_mask[0, start_idx:end_idx, ...]
                key_padding_mask_split = key_padding_mask_split.reshape(num_classes_split * num_captions, context_len)
            class_embedding_split = self.encode_text(text_tokens=text_tokens_split, key_padding_mask=key_padding_mask_split)
            class_embedding_split = class_embedding_split.reshape(num_classes_split, num_captions, class_embedding_split.shape[-1])
            mean_class_embedding_split = class_embedding_split.mean(dim=1)
            mean_class_embedding_split = F.normalize(mean_class_embedding_split, dim=-1)
            text_features.append(mean_class_embedding_split)
        text_features = torch.cat(text_features, dim=0)
        text_features = text_features.transpose(0, 1)
        return text_features

    def forward(self, text_tokens: Tensor, key_padding_mask: Optional[Tensor]=None, *args, **kwargs) ->Tensor:
        if text_tokens.dim() == 4:
            return self.forward_zero_shot(*args, text_tokens=text_tokens, key_padding_mask=key_padding_mask, **kwargs)
        elif text_tokens.dim() == 2:
            text_tokens = self.encode_text(*args, text_tokens=text_tokens, key_padding_mask=key_padding_mask, **kwargs)
            return text_tokens
        else:
            raise NotImplementedError


class BaseTokenizer(nn.Module):

    def __init__(self, opts, *args, **kwargs):
        super().__init__()
        self.opts = opts

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--text-tokenizer.name', type=str, default=None, help='Name of the text tokenizer.')
        return parser

    def get_vocab_size(self):
        raise NotImplementedError

    def get_eot_token(self):
        raise NotImplementedError

    def get_sot_token(self):
        raise NotImplementedError

    def get_encodings(self):
        raise NotImplementedError

    def forward(self, input_sentence: Any, *args, **kwargs) ->Any:
        raise NotImplementedError


TOKENIZER_REGISTRY = {}


def register_tokenizer(name):

    def register_tokenizer_class(cls):
        if name in TOKENIZER_REGISTRY:
            raise ValueError('Cannot register duplicate text_tokenizer class ({})'.format(name))
        if not issubclass(cls, BaseTokenizer):
            raise ValueError('Tokenizer ({}: {}) must extend BaseTokenizer'.format(name, cls.__name__))
        TOKENIZER_REGISTRY[name] = cls
        return cls
    return register_tokenizer_class


class ClipTokenizer(BaseTokenizer):

    def __init__(self, opts, *args, **kwargs):
        merges_path = getattr(opts, 'text_tokenizer.clip.merges_path', None)
        if merges_path is None:
            logger.error('Please specify BPE merge file using --text-tokenizer.clip.merges-path argument')
        merges_path = get_local_path(opts, path=merges_path)
        encoder_json_path = getattr(opts, 'text_tokenizer.clip.encoder_json_path', None)
        if encoder_json_path is None:
            logger.error('Please specify Encoder JSON file using --text-tokenizer.clip.encoder-json-path argument')
        encoder_json_path = get_local_path(opts, path=encoder_json_path)
        super().__init__(opts, *args, **kwargs)
        self.tokenizer = CLIPTokenizer(merges_path=merges_path, encoder_json_path=encoder_json_path)
        self.bpe_encodings = self.tokenizer.bpe.bpe_encoder_

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--text-tokenizer.clip.merges-path', type=str, default=None, help='Path to bpe merges file.')
        group.add_argument('--text-tokenizer.clip.encoder-json-path', type=str, default=None, help='Optional, path to BPE encoder json file. When specified, this is used to infer num_merges.')
        return parser

    def get_vocab_size(self):
        return len(self.bpe_encodings)

    def get_encodings(self):
        return self.bpe_encodings

    def get_eot_token(self):
        return int(self.tokenizer('<|endoftext|>')[0])

    def get_sot_token(self):
        return int(self.tokenizer('<|startoftext|>')[0])

    def forward(self, input_sentence: str, *args, **kwargs) ->Tensor:
        input_sentence = '<|startoftext|> ' + input_sentence + ' <|endoftext|>'
        tokenized_sentence = self.tokenizer(input_sentence)
        tokenized_sentence = torch.tensor([int(cap) for cap in tokenized_sentence], dtype=torch.long)
        return tokenized_sentence


class BaseCriteria(nn.Module):

    def __init__(self, *args, **kwargs):
        super(BaseCriteria, self).__init__()
        self.eps = 1e-07

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def forward(self, input_sample: Any, prediction: Any, target: Any, *args, **kwargs) ->Tensor:
        raise NotImplementedError

    @staticmethod
    def _class_weights(target: Tensor, n_classes: int, norm_val: float=1.1) ->Tensor:
        class_hist: Tensor = torch.histc(target.float(), bins=n_classes, min=0, max=n_classes - 1)
        mask_indices = class_hist == 0
        norm_hist = torch.div(class_hist, class_hist.sum())
        norm_hist = torch.add(norm_hist, norm_val)
        class_wts = torch.div(torch.ones_like(class_hist), torch.log(norm_hist))
        class_wts[mask_indices] = 0.0
        return class_wts

    def extra_repr(self) ->str:
        return ''

    def __repr__(self):
        return '{}({}\n)'.format(self.__class__.__name__, self.extra_repr())


def cosine_curriculumn(start, end, period):
    """This function implements cosine curriculumn"""
    curr = [(end + 0.5 * (start - end) * (1 + math.cos(math.pi * i / (period + 1)))) for i in range(period + 1)]
    curr = torch.tensor(curr, dtype=torch.float)
    return curr


def linear_curriculumn(start, end, period):
    """This function implements linear curriculumn"""
    return torch.linspace(start=start, end=end, steps=period + 1, dtype=torch.float)


CURRICULUMN_METHOD = {'linear': linear_curriculumn, 'cosine': cosine_curriculumn}


class BaseNeuralAug(BaseCriteria):
    __supported_metrics = ['psnr']

    def __init__(self, opts, *args, **kwargs):
        super().__init__(opts, *args, **kwargs)
        perceptual_metric = getattr(opts, 'loss.neural_aug.perceptual_metric', 'psnr')
        is_master_node = is_master(opts)
        if perceptual_metric is None and is_master_node:
            logger.error("Perceptual metric can't be none. Please specify perceptual metric using --loss.neural-aug.perceptual-metric argument")
        if not isinstance(perceptual_metric, str) and is_master_node:
            logger.error('The type of perceptual metric is not string. Got: {}'.format(type(perceptual_metric)))
        perceptual_metric = perceptual_metric.lower()
        target_value = getattr(opts, 'loss.neural_aug.target_value', None)
        self.curriculumn_learning = False
        self.iteration_based_training = getattr(opts, 'scheduler.is_iteration_based', False)
        self.target_str = f'{target_value}'
        if perceptual_metric == 'psnr':
            if target_value is None and is_master_node:
                logger.error('Target PSNR value can not be None.')
            if isinstance(target_value, (int, float)):
                if target_value < 0:
                    if is_master_node:
                        logger.error('PSNR value should be >= 0 in {}. Got: {}'.format(self.__class__.__name__, target_value))
                target_mse = 10.0 ** ((20.0 * math.log10(255.0) - target_value) / 10.0)
                self.target_value = torch.ones(size=(1,), dtype=torch.float).fill_(target_mse)
                self.target_str = f'{target_value}'
            elif isinstance(target_value, (list, tuple)) and len(target_value) == 2:
                start_target_value = target_value[0]
                end_target_value = target_value[1]
                if start_target_value < 0 or end_target_value < 0:
                    if is_master_node:
                        logger.error('PSNR value should be >= 0 in {}. Got: {}'.format(self.__class__.__name__, target_value))
                start_target_mse = 10.0 ** ((20.0 * math.log10(255.0) - start_target_value) / 10.0)
                end_target_mse = 10.0 ** ((20.0 * math.log10(255.0) - end_target_value) / 10.0)
                max_steps = getattr(opts, 'scheduler.max_iterations', None) if self.iteration_based_training else getattr(opts, 'scheduler.max_epochs', None)
                if max_steps is None and is_master_node:
                    logger.error('Please specify {}. Got None.'.format('--scheduler.max-iterations' if self.iteration_based_training else '--scheduler.max-epochs'))
                curriculum_method = getattr(opts, 'loss.neural_aug.curriculum_method', None)
                if curriculum_method in CURRICULUMN_METHOD.keys():
                    self.target_value = CURRICULUMN_METHOD[curriculum_method](start=start_target_mse, end=end_target_mse, period=max_steps)
                else:
                    raise NotImplementedError
                self.curriculumn_learning = True
                self.target_str = f'[{start_target_value}, {end_target_value}]'
            else:
                raise NotImplementedError
            self.alpha = 100.0 / 65025.0
        elif is_master_node:
            logger.error('Supported perceptual metrics are: {}. Got: {}'.format(self.__supported_metrics, perceptual_metric))
        self.perceptual_metric = perceptual_metric

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def _forward_psnr(self, input_tensor: Tensor, augmented_tensor: Tensor, *args, **kwargs) ->Tensor:
        squared_err = ((augmented_tensor - input_tensor) * 255.0) ** 2
        pred_mse = torch.mean(squared_err, dim=[1, 2, 3])
        if self.curriculumn_learning:
            step = kwargs.get('iterations', 0) if self.iteration_based_training else kwargs.get('epoch', 0)
            if step >= len(self.target_value):
                step = -1
            target_mse = self.target_value[step]
        else:
            target_mse = self.target_value
        loss_na = F.smooth_l1_loss(input=pred_mse, target=target_mse.expand_as(pred_mse), reduction='mean')
        loss_na = loss_na * self.alpha
        return loss_na

    def forward_neural_aug(self, input_tensor: Tensor, augmented_tensor: Tensor, *args, **kwargs) ->Tensor:
        if self.perceptual_metric == 'psnr':
            loss_na = self._forward_psnr(*args, input_tensor=input_tensor, augmented_tensor=augmented_tensor, **kwargs)
            return loss_na
        else:
            logger.error('Supported perceptual metrics are {}. Got: {}'.format(self.__supported_metrics, self.perceptual_metric))

    def repr_na(self):
        return '\n\ttarget_metric={}\n\ttarget_value={}\n\tcurriculum_learning={}'.format(self.perceptual_metric, self.target_str, self.curriculumn_learning)

    def __repr__(self):
        return '{}()'.format(self.__class__.__name__)


CLS_LOSS_FN_REGISTRY = {}


def arguments_cls_loss_fn(parser: argparse.ArgumentParser):
    for k, v in CLS_LOSS_FN_REGISTRY.items():
        parser = v.add_arguments(parser=parser)
    return parser


SUPPORTED_CLS_LOSS_FNS = []


def supported_loss_fn_str(loss_fn_name):
    supp_str = 'Loss function ({}) is not yet supported. \n Supported functions are:'.format(loss_fn_name)
    for i, fn_name in enumerate(SUPPORTED_CLS_LOSS_FNS):
        supp_str += '{} \t'.format(fn_name)
    logger.error(supp_str)


def get_classification_loss(opts, *args, **kwargs):
    loss_fn_name = getattr(opts, 'loss.classification.name', 'cross_entropy')
    if loss_fn_name in SUPPORTED_CLS_LOSS_FNS:
        return CLS_LOSS_FN_REGISTRY[loss_fn_name](opts, *args, **kwargs)
    else:
        supported_loss_fn_str(loss_fn_name)
        return None


LOSS_REGISTRY = {}


def register_loss_fn(name):

    def register_loss_fn_class(cls):
        if name in LOSS_REGISTRY:
            raise ValueError('Cannot register duplicate loss function ({})'.format(name))
        if not issubclass(cls, BaseCriteria):
            raise ValueError('Criteria ({}: {}) must extend BaseCriteria'.format(name, cls.__name__))
        LOSS_REGISTRY[name] = cls
        return cls
    return register_loss_fn_class


class ClassificationLoss(BaseCriteria):

    def __init__(self, opts, *args, **kwargs):
        super().__init__(opts, *args, **kwargs)
        self.criteria = get_classification_loss(*args, opts=opts, **kwargs)

    def forward(self, input_sample: Tensor, prediction: Tensor, target: Tensor, *args, **kwargs) ->Tensor:
        return self.criteria(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.classification.name', type=str, default='cross_entropy', help='Loss function name')
        parser = arguments_cls_loss_fn(parser)
        return parser

    def __repr__(self):
        return self.criteria.__repr__()


def register_classification_loss_fn(name):

    def register_fn(cls):
        if name in SUPPORTED_CLS_LOSS_FNS:
            raise ValueError('Cannot register duplicate classfication loss function ({})'.format(name))
        if not issubclass(cls, BaseCriteria):
            raise ValueError('Loss function ({}: {}) must extend BaseCriteria'.format(name, cls.__name__))
        CLS_LOSS_FN_REGISTRY[name] = cls
        SUPPORTED_CLS_LOSS_FNS.append(name)
        return cls
    return register_fn


class ClsBinaryCrossEntropy(BaseCriteria):
    """Binary CE for classification tasks"""

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)

    def forward(self, input_sample: Tensor, prediction: Tensor, target: Tensor, *args, **kwargs) ->Tensor:
        if target.dim() != prediction.dim():
            target = F.one_hot(target, num_classes=prediction.shape[-1])
        return F.binary_cross_entropy_with_logits(input=prediction, target=target, weight=None, reduction='sum')

    def __repr__(self) ->str:
        return '{}()'.format(self.__class__.__name__)


class ClsCrossEntropy(BaseCriteria):
    """Cross entropy for classification tasks"""

    def __init__(self, opts, *args, **kwargs):
        ignore_idx = getattr(opts, 'loss.ignore_idx', -1)
        use_class_wts = getattr(opts, 'loss.classification.cross_entropy.class_weights', False)
        super().__init__(opts, *args, **kwargs)
        self.ignore_idx = ignore_idx
        self.use_class_wts = use_class_wts
        self.label_smoothing = getattr(opts, 'loss.classification.label_smoothing', 0.0)

    def forward(self, input_sample: Tensor, prediction: Tensor, target: Tensor, *args, **kwargs) ->Tensor:
        weight = None
        if self.use_class_wts and self.training:
            n_classes = prediction.shape[1]
            weight = self._class_weights(target=target, n_classes=n_classes)
        return F.cross_entropy(input=prediction, target=target, weight=weight, ignore_index=self.ignore_idx, label_smoothing=self.label_smoothing if self.training else 0.0)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.classification.cross-entropy.class-weights', action='store_true', help='Use class weights in loss function')
        group.add_argument('--loss.classification.label-smoothing', type=float, default=0.0, help='Label smoothing value')
        return parser

    def __repr__(self):
        return '{}(\n\tignore_idx={}\n\tclass_wts={}\n\tlabel_smoothing={}\n)'.format(self.__class__.__name__, self.ignore_idx, self.use_class_wts, self.label_smoothing)


class CrossEntropyWithNA(ClsCrossEntropy, BaseNeuralAug):
    """Cross entropy with Perceptual loss for classification tasks with neural augmentation"""

    def __init__(self, opts, *args, **kwargs):
        ClsCrossEntropy.__init__(self, opts, *args, **kwargs)
        BaseNeuralAug.__init__(self, opts, *args, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def forward(self, input_sample: Tensor, prediction: Dict, target: Tensor, *args, **kwargs) ->Dict[str, Tensor]:
        if not isinstance(prediction, Dict):
            logger.error('Prediction needs to be an instance of Dict and must contain logits and augmented_tensor as keys')
        if not {'augmented_tensor', 'logits'}.issubset(prediction.keys()):
            logger.error('Prediction needs to be an instance of Dict and must contain logits and augmented_tensor as keys. Got keys: {}'.format(prediction.keys()))
        augmented_tensor = prediction.get('augmented_tensor', None)
        logits = prediction.get('logits', None)
        if augmented_tensor is None:
            ce_loss = ClsCrossEntropy.forward(self, *args, input_sample=input_sample, prediction=logits, target=target, **kwargs)
            return {'total_loss': ce_loss}
        loss_na = self.forward_neural_aug(*args, input_tensor=input_sample, augmented_tensor=augmented_tensor, **kwargs)
        ce_loss = ClsCrossEntropy.forward(self, *args, input_sample=augmented_tensor, prediction=logits, target=target, **kwargs)
        return {'total_loss': loss_na + ce_loss, 'na_loss': loss_na, 'cls_loss': ce_loss}

    def __repr__(self):
        repr_str = '{}(\n\tignore_idx={}\n\tclass_wts={}\n\tlabel_smoothing={}{}\n)'.format(self.__class__.__name__, self.ignore_idx, self.use_class_wts, self.label_smoothing, self.repr_na())
        return repr_str


DETECTION_LOSS_FN_REGISTRY = {}


def arguments_detection_loss_fn(parser: argparse.ArgumentParser):
    for k, v in DETECTION_LOSS_FN_REGISTRY.items():
        parser = v.add_arguments(parser=parser)
    return parser


SUPPORTED_DETECTION_LOSS_FNS = []


def get_detection_loss(opts):
    loss_fn_name = getattr(opts, 'loss.detection.name', 'cross_entropy')
    if loss_fn_name in SUPPORTED_DETECTION_LOSS_FNS:
        return DETECTION_LOSS_FN_REGISTRY[loss_fn_name](opts)
    else:
        supported_loss_fn_str(loss_fn_name)
        return None


class DetectionLoss(BaseCriteria):

    def __init__(self, opts, *args, **kwargs):
        super().__init__(opts, *args, **kwargs)
        self.criteria = get_detection_loss(opts=opts)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.detection.name', type=str, default='cross_entropy', help='Detection loss function name')
        parser = arguments_detection_loss_fn(parser)
        return parser

    def forward(self, input_sample: Tensor, prediction: Union[Tensor, Union[Tensor, Tensor]], target: Tensor, *args, **kwargs) ->Tensor:
        loss = self.criteria(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)
        return loss

    def __repr__(self):
        return self.criteria.__repr__()


def register_detection_loss_fn(name):

    def register_fn(cls):
        if name in SUPPORTED_DETECTION_LOSS_FNS:
            raise ValueError('Cannot register duplicate detection loss function ({})'.format(name))
        if not issubclass(cls, BaseCriteria):
            raise ValueError('Loss function ({}: {}) must extend BaseCriteria'.format(name, cls.__name__))
        DETECTION_LOSS_FN_REGISTRY[name] = cls
        SUPPORTED_DETECTION_LOSS_FNS.append(name)
        return cls
    return register_fn


class MaskRCNNLoss(BaseCriteria):
    """Mask RCNN Loss"""

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser) ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.detection.mask-rcnn-loss.classifier-weight', type=float, default=1, help='Weight for classifier.')
        group.add_argument('--loss.detection.mask-rcnn-loss.box-reg-weight', type=float, default=1, help='Weight for box reg.')
        group.add_argument('--loss.detection.mask-rcnn-loss.mask-weight', type=float, default=1, help='Weight for mask.')
        group.add_argument('--loss.detection.mask-rcnn-loss.objectness-weight', type=float, default=1, help='Weight for objectness.')
        group.add_argument('--loss.detection.mask-rcnn-loss.rpn-box-reg', type=float, default=1, help='Weight for rpn box reg.')
        return parser

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        self.classifier_weight = getattr(opts, 'loss.detection.mask_rcnn_loss.classifier_weight')
        self.box_reg_weight = getattr(opts, 'loss.detection.mask_rcnn_loss.box_reg_weight')
        self.mask_weight = getattr(opts, 'loss.detection.mask_rcnn_loss.mask_weight')
        self.objectness_weight = getattr(opts, 'loss.detection.mask_rcnn_loss.objectness_weight')
        self.rpn_box_reg = getattr(opts, 'loss.detection.mask_rcnn_loss.rpn_box_reg')

    def extra_repr(self) ->str:
        return f'\n\tclassifier_wt={self.classifier_weight}\n\tbox_reg_weight={self.box_reg_weight}\n\tmask_weight={self.mask_weight}\n\tobjectness_weight={self.objectness_weight}\n\trpn_box_reg={self.rpn_box_reg}'

    def forward(self, input_sample: Dict[str, List], prediction: Dict[str, Tensor], *args, **kwargs) ->Dict[str, Tensor]:
        try:
            total_loss = self.classifier_weight * prediction['loss_classifier'] + self.box_reg_weight * prediction['loss_box_reg'] + self.mask_weight * prediction['loss_mask'] + self.objectness_weight * prediction['loss_objectness'] + self.rpn_box_reg * prediction['loss_rpn_box_reg']
            return {'total_loss': total_loss, **prediction}
        except KeyError:
            device = input_sample['image'][0].device
            return {'total_loss': torch.tensor(0.0, device=device)}


class MaskRCNNLossWithNA(MaskRCNNLoss, BaseNeuralAug):
    """Mask RCNN loss with neural augmentation"""

    def __init__(self, opts, *args, **kwargs):
        MaskRCNNLoss.__init__(self, opts, *args, **kwargs)
        BaseNeuralAug.__init__(self, opts, *args, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def forward(self, input_sample: Dict[str, List], prediction: Dict[str, Tensor], *args, **kwargs) ->Dict[str, Tensor]:
        if not isinstance(prediction, Dict):
            logger.error('Prediction needs to be an instance of Dict and must contain logits and augmented_tensor as keys')
        augmented_tensor = prediction.pop('augmented_tensor', None)
        if augmented_tensor is None:
            loss = MaskRCNNLoss.forward(self, *args, input_sample=input_sample, prediction=prediction, **kwargs)
            return loss
        if not isinstance(input_sample, Dict):
            logger.error('Input is expected as a Dictionary containing atleast image as a key')
        if not {'image'}.issubset(input_sample.keys()):
            logger.error('Input is expected as a Dictionary containing atleast image as a key. Got: {}'.format(input_sample.keys()))
        input_image_sample = input_sample['image']
        if isinstance(input_image_sample, List):
            input_image_sample = torch.stack(input_image_sample, dim=0)
        loss_na = self.forward_neural_aug(*args, input_tensor=input_image_sample, augmented_tensor=augmented_tensor, **kwargs)
        loss = MaskRCNNLoss.forward(self, *args, input_sample=input_sample, prediction=prediction, **kwargs)
        loss['total_loss'] += loss_na
        loss['na_loss'] = loss_na
        return loss

    def __repr__(self) ->str:
        return f'{self.__class__.__name__}(\n{self.extra_repr() + self.repr_na()}\n)'.replace('\n\n', '\n')


def hard_negative_mining(loss: Tensor, labels: Tensor, neg_pos_ratio: int, *args, **kwargs) ->Tensor:
    """
    This function is used to suppress the presence of a large number of negative predictions. For any example/image,
    it keeps all the positive predictions and cut the number of negative predictions to make sure the ratio
    between the negative examples and positive examples is no more than the given ratio for an image.
    Args:
        loss (Tensor): the loss for each example and has shape (N, num_priors).
        labels (Tensor): the labels and has shape (N, num_priors).
        neg_pos_ratio (int):  the ratio between the negative examples and positive examples. Usually, it is set as 3.

    """
    pos_mask = labels > 0
    num_pos = pos_mask.long().sum(dim=1, keepdim=True)
    num_neg = num_pos * neg_pos_ratio
    loss[pos_mask] = -math.inf
    _, indexes = loss.sort(dim=1, descending=True)
    _, orders = indexes.sort(dim=1)
    neg_mask = orders < num_neg
    return pos_mask | neg_mask


def reduce_tensor(inp_tensor: torch.Tensor) ->torch.Tensor:
    size = dist.get_world_size() if dist.is_initialized() else 1
    inp_tensor_clone = inp_tensor.clone().detach()
    dist.all_reduce(inp_tensor_clone, op=dist.ReduceOp.SUM)
    inp_tensor_clone /= size
    return inp_tensor_clone


def tensor_to_python_float(inp_tensor: Union[int, float, torch.Tensor], is_distributed: bool) ->Union[int, float, np.ndarray]:
    if is_distributed and isinstance(inp_tensor, torch.Tensor):
        inp_tensor = reduce_tensor(inp_tensor=inp_tensor)
    if isinstance(inp_tensor, torch.Tensor) and inp_tensor.numel() > 1:
        return inp_tensor.cpu().numpy()
    elif hasattr(inp_tensor, 'item'):
        return inp_tensor.item()
    elif isinstance(inp_tensor, (int, float)):
        return inp_tensor * 1.0
    else:
        raise NotImplementedError('The data type is not supported yet in tensor_to_python_float function')


class SSDLoss(BaseCriteria):
    """SSD Loss"""

    def __init__(self, opts, *args, **kwargs):
        super().__init__(opts, *args, **kwargs)
        self.unscaled_reg_loss = 1e-07
        self.unscaled_conf_loss = 1e-07
        self.neg_pos_ratio = getattr(opts, 'loss.detection.ssd_multibox_loss.neg_pos_ratio', 3)
        self.wt_loc = 1.0
        self.curr_iter = 0
        self.max_iter = getattr(opts, 'loss.detection.ssd_multibox_loss.max_monitor_iter', -1)
        self.update_inter = getattr(opts, 'loss.detection.ssd_multibox_loss.update_wt_freq', 200)
        self.is_distributed = getattr(opts, 'ddp.use_distributed', False)
        self.is_master = is_master(opts)
        self.label_smoothing = getattr(opts, 'loss.detection.ssd_multibox_loss.label_smoothing', 0.0)
        if not 0.0 <= self.label_smoothing < 1.0:
            logger.error('The value of --loss.detection.ssd-multibox-loss.label-smoothing should be between 0 and 1. Got: {}'.format(self.label_smoothing))
        self.reset_unscaled_loss_values()

    def reset_unscaled_loss_values(self):
        self.unscaled_conf_loss = 1e-07
        self.unscaled_reg_loss = 1e-07

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.detection.ssd-multibox-loss.neg-pos-ratio', type=int, default=3, help='Negative positive ratio in SSD loss')
        group.add_argument('--loss.detection.ssd-multibox-loss.max-monitor-iter', type=int, default=-1, help='Number of iterations for monitoring location and classification loss.')
        group.add_argument('--loss.detection.ssd-multibox-loss.update-wt-freq', type=int, default=200, help='Update the weights after N number of iterations')
        group.add_argument('--loss.detection.ssd-multibox-loss.label-smoothing', type=float, default=0.0, help='Label smoothing for classification labels in SSD')
        return parser

    def __repr__(self):
        return '{}(\n\tneg_pos_ratio={}\n\tbox_loss=SmoothL1\n\tclass_loss=CrossEntropy\n\twt_loss={}\n)'.format(self.__class__.__name__, self.neg_pos_ratio, True if self.max_iter > 0 else False)

    def _forward_detection_loss(self, prediction: Dict, target: Dict, *args, **kwargs) ->Dict[str, Tensor]:
        confidence = prediction['scores']
        predicted_locations = prediction['boxes']
        gt_labels = target['box_labels']
        gt_locations = target['box_coordinates']
        num_classes = confidence.shape[-1]
        num_coordinates = predicted_locations.shape[-1]
        pos_mask = gt_labels > 0
        predicted_locations = predicted_locations[pos_mask].reshape(-1, num_coordinates)
        gt_locations = gt_locations[pos_mask].reshape(-1, num_coordinates)
        num_pos = max(1, gt_locations.shape[0])
        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, reduction='sum')
        with torch.no_grad():
            loss = -F.log_softmax(confidence, dim=2)[:, :, 0]
            mask = hard_negative_mining(loss, gt_labels, self.neg_pos_ratio)
        confidence = confidence[mask, :]
        label_smoothing = self.label_smoothing if self.training else 0.0
        classification_loss = F.cross_entropy(input=confidence.reshape(-1, num_classes), target=gt_labels[mask], reduction='sum', label_smoothing=label_smoothing)
        if self.curr_iter <= self.max_iter and self.training:
            self.unscaled_conf_loss += tensor_to_python_float(classification_loss, is_distributed=self.is_distributed)
            self.unscaled_reg_loss += tensor_to_python_float(smooth_l1_loss, is_distributed=self.is_distributed)
            if (self.curr_iter + 1) % self.update_inter == 0 or self.curr_iter == self.max_iter:
                before_update = round(tensor_to_python_float(self.wt_loc, is_distributed=self.is_distributed), 4)
                self.wt_loc = self.unscaled_conf_loss / self.unscaled_reg_loss
                self.reset_unscaled_loss_values()
                if self.is_master:
                    after_update = round(tensor_to_python_float(self.wt_loc, is_distributed=self.is_distributed), 4)
                    logger.log(f'Updating localization loss multiplier from {before_update} to {after_update}')
            self.curr_iter += 1
        if self.training and self.wt_loc > 0.0:
            smooth_l1_loss = smooth_l1_loss * self.wt_loc
        return {'total_loss': (smooth_l1_loss + classification_loss) / num_pos, 'reg_loss': smooth_l1_loss / num_pos, 'cls_loss': classification_loss / num_pos}

    def forward(self, input_sample: Tensor, prediction: Dict, target: Dict, *args, **kwargs) ->Dict[str, Tensor]:
        detection_loss = self._forward_detection_loss(prediction=prediction, target=target)
        return detection_loss


DISTILL_LOSS_FN_REGISTRY = {}


def arguments_distill_loss_fn(parser: argparse.ArgumentParser):
    for k, v in DISTILL_LOSS_FN_REGISTRY.items():
        parser = v.add_arguments(parser=parser)
    return parser


SUPPORTED_DISTILL_LOSS_FNS = []


def get_distillation_loss(opts, *args, **kwargs):
    loss_fn_name = getattr(opts, 'loss.distillation.name', None)
    if loss_fn_name in SUPPORTED_DISTILL_LOSS_FNS:
        return DISTILL_LOSS_FN_REGISTRY[loss_fn_name](opts, *args, **kwargs)
    else:
        supported_loss_fn_str(loss_fn_name)
        return None


class DistillationLoss(BaseCriteria):

    def __init__(self, opts, *args, **kwargs):
        loss_fn_name = getattr(opts, 'loss.distillation.name', 'vanilla')
        super().__init__(opts, *args, **kwargs)
        self.criteria = get_distillation_loss(*args, opts=opts, **kwargs)

    def forward(self, input_sample: Tensor, prediction: Tensor, target: Tensor, *args, **kwargs) ->Tensor:
        return self.criteria(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.distillation.name', type=str, default='vanilla', help='Distillation loss function name')
        parser = arguments_distill_loss_fn(parser=parser)
        return parser

    def extra_repr(self) ->str:
        if hasattr(self.criteria, 'extra_repr'):
            return self.criteria.extra_repr()
        return ''

    def __repr__(self):
        return '{}({}\n)'.format(self.criteria.__class__.__name__, self.extra_repr())


def build_cls_teacher_from_opts(opts) ->nn.Module:
    """
    Helper function to build a classification teacher model from options
    """
    pretrained_model = getattr(opts, 'teacher.model.classification.pretrained', None)
    if not pretrained_model:
        logger.error('For distillation, please specify teacher weights using teacher.model.classification.pretrained')
    opts_dict = vars(opts)
    teacher_dict = {key.replace('teacher.', ''): value for key, value in opts_dict.items() if key.split('.')[0] == 'teacher'}
    teacher_opts = argparse.Namespace(**teacher_dict)
    return build_classification_model(teacher_opts)


def register_distillation_loss_fn(name):

    def register_fn(fn):
        if name in SUPPORTED_DISTILL_LOSS_FNS:
            raise ValueError('Cannot register duplicate distillation loss function ({})'.format(name))
        SUPPORTED_DISTILL_LOSS_FNS.append(name)
        DISTILL_LOSS_FN_REGISTRY[name] = fn
        return fn
    return register_fn


class ClsKLDivLoss(BaseCriteria):
    """
    KL Loss for classification
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        self.teacher = build_cls_teacher_from_opts(opts=opts)
        self.temperature = getattr(opts, 'loss.distillation.cls_kl_div_loss.temperature', 1.0)
        self.distillation_mode = getattr(opts, 'loss.distillation.cls_kl_div_loss.mode', 'soft')
        self.topk = getattr(opts, 'loss.distillation.cls_kl_div_loss.topk', 1)
        self.label_smoothing = getattr(opts, 'loss.distillation.cls_kl_div_loss.label-smoothing', 0.0)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.distillation.cls-kl-div-loss.temperature', type=float, default=1.0, help='Temperature for KL Div. loss')
        group.add_argument('--loss.distillation.cls-kl-div-loss.mode', type=str, default='soft', help='Distillation mode')
        group.add_argument('--loss.distillation.cls-kl-div-loss.topk', type=int, default=1, help='Distill top-k labels from teacher when using hard-labels')
        group.add_argument('--loss.distillation.cls-kl-div-loss.label-smoothing', type=float, default=0.0, help='Use label smoothing when using hard-labels')
        return parser

    def extra_repr(self) ->str:
        extra_repr_str = f'\n\ttemperature={self.temperature}\n\tmode={self.distillation_mode}'
        if self.distillation_mode.find('hard') > -1:
            extra_repr_str += f'\n\ttopk={self.topk}\n\tlabel_smoothing={self.label_smoothing}'
        return extra_repr_str

    def _forward_soft_labels(self, prediction: Tensor, teacher_logits: Tensor) ->Tensor:
        with torch.no_grad():
            teacher_lprobs = F.log_softmax(teacher_logits / self.temperature, dim=1).detach()
        student_lprobs = F.log_softmax(prediction / self.temperature, dim=-1)
        kl_loss = F.kl_div(student_lprobs, teacher_lprobs, reduction='batchmean', log_target=True)
        return kl_loss * self.temperature ** 2

    def _forward_hard_labels(self, prediction: Tensor, teacher_logits: Tensor) ->Tensor:
        with torch.no_grad():
            teacher_probs = F.softmax(teacher_logits, dim=-1).detach()
            _, teacher_topk_labels = torch.topk(teacher_probs, k=self.topk, dim=-1, largest=True, sorted=True)
        if self.topk > 1:
            num_classes = prediction.shape[-1]
            teacher_topk_labels = F.one_hot(teacher_topk_labels, num_classes=num_classes)
            teacher_topk_labels = teacher_topk_labels.sum(1)
            teacher_topk_labels = teacher_topk_labels
            smooth_class_p = (1.0 - self.label_smoothing) / self.topk
            smooth_non_class_p = self.label_smoothing / (num_classes - self.topk)
            teacher_topk_labels = torch.where(teacher_topk_labels == 1.0, smooth_class_p, smooth_non_class_p)
            loss = F.binary_cross_entropy_with_logits(input=prediction, target=teacher_topk_labels, reduction='mean') * num_classes
        else:
            teacher_topk_labels = teacher_topk_labels.reshape(-1)
            loss = F.cross_entropy(input=prediction, target=teacher_topk_labels, reduction='mean', label_smoothing=self.label_smoothing)
        return loss

    def forward(self, input_sample: Tensor, prediction: Tensor, target: Tensor, *args, **kwargs) ->Tensor:
        with torch.no_grad():
            self.teacher.eval()
            teacher_logits: Union[Tensor, Dict] = self.teacher(input_sample)
            if isinstance(teacher_logits, Dict):
                teacher_logits = teacher_logits['logits']
        if self.distillation_mode == 'soft':
            return self._forward_soft_labels(prediction=prediction, teacher_logits=teacher_logits)
        elif self.distillation_mode == 'hard':
            return self._forward_hard_labels(prediction=prediction, teacher_logits=teacher_logits)
        else:
            raise NotImplementedError


class ClsKLDivLossWithNA(ClsKLDivLoss, BaseNeuralAug):
    """
    KLDiv loss with Perceptual loss for distillation
    """

    def __init__(self, opts, *args, **kwargs):
        BaseNeuralAug.__init__(self, opts, *args, **kwargs)
        ClsKLDivLoss.__init__(self, opts, *args, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def extra_repr(self) ->str:
        return super().extra_repr() + self.repr_na()

    def forward(self, input_sample: Tensor, prediction: Union[Dict, Tensor], target: Tensor, *args, **kwargs) ->Dict:
        if isinstance(prediction, Tensor):
            kl_loss = super().forward(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)
            return {'total_loss': kl_loss}
        elif isinstance(prediction, Dict):
            if not isinstance(prediction, Dict):
                logger.error('Prediction needs to be an instance of Dict and must contain logits and augmented_tensor as keys')
            if not {'augmented_tensor', 'logits'}.issubset(prediction.keys()):
                logger.error('Prediction needs to be an instance of Dict and must contain logits and augmented_tensor as keys. Got keys: {}'.format(prediction.keys()))
            augmented_tensor = prediction.get('augmented_tensor', None)
            logits = prediction.get('logits', None)
            if augmented_tensor is None:
                kl_loss = ClsKLDivLoss.forward(self, *args, input_sample=input_sample, prediction=logits, target=target, **kwargs)
                return {'total_loss': kl_loss}
            kl_loss = ClsKLDivLoss.forward(self, *args, input_sample=augmented_tensor, prediction=logits, target=target, **kwargs)
            loss_na = self.forward_neural_aug(*args, input_tensor=input_sample, augmented_tensor=augmented_tensor, **kwargs)
            return {'total_loss': loss_na + kl_loss, 'na_loss': loss_na, 'kl_loss': kl_loss}
        else:
            raise NotImplementedError


MULTI_MODAL_IMG_TEXT_LOSS_FN_REGISTRY = {}


def arguments_multi_modal_img_text_loss_fn(parser: argparse.ArgumentParser):
    for k, v in MULTI_MODAL_IMG_TEXT_LOSS_FN_REGISTRY.items():
        parser = v.add_arguments(parser=parser)
    return parser


SUPPORTED_MULTI_MODAL_IMG_TEXT_LOSS_FNS = []


def get_multi_modal_img_text_loss(opts, *args, **kwargs):
    loss_name = getattr(opts, 'loss.multi_modal_image_text.name', None)
    if loss_name in SUPPORTED_MULTI_MODAL_IMG_TEXT_LOSS_FNS:
        return MULTI_MODAL_IMG_TEXT_LOSS_FN_REGISTRY[loss_name](opts, *args, **kwargs)
    else:
        supported_loss_fn_str(loss_name)


class MultiModalImageTextLoss(BaseCriteria):

    def __init__(self, opts, *args, **kwargs):
        super().__init__()
        self.criteria = get_multi_modal_img_text_loss(*args, opts=opts, **kwargs)

    def forward(self, input_sample: Any, prediction: Any, target: Any, *args, **kwargs) ->Any:
        return self.criteria(input_sample=input_sample, prediction=prediction, target=target)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.multi-modal-image-text.name', type=str, default='clip', help='Loss function name')
        parser = arguments_multi_modal_img_text_loss_fn(parser)
        return parser

    def __repr__(self):
        return self.criteria.__repr__()


def gather_all_features(features: Tensor, dim=0):
    return torch.cat(all_gather_with_backward(features), dim=dim)


def gather_features(image_features: Tensor, text_features: Tensor, use_distributed: bool) ->Tuple[Tensor, Tensor]:
    """
    Helper function that allows us to gather image and text features from all DDP ranks in a differentiable manner
    """
    if use_distributed:
        gathered_image_features = gather_all_features(features=image_features, dim=0)
        gathered_text_features = gather_all_features(features=text_features, dim=0)
        return gathered_image_features, gathered_text_features
    return image_features, text_features


def register_multi_modal_img_text_loss_fns(name):

    def register_fn(cls):
        if name in SUPPORTED_MULTI_MODAL_IMG_TEXT_LOSS_FNS:
            raise ValueError('Cannot register duplicate multi-modal image-text loss function ({})'.format(name))
        if not issubclass(cls, BaseCriteria):
            raise ValueError('Loss function ({}: {}) must extend BaseCriteria'.format(name, cls.__name__))
        MULTI_MODAL_IMG_TEXT_LOSS_FN_REGISTRY[name] = cls
        SUPPORTED_MULTI_MODAL_IMG_TEXT_LOSS_FNS.append(name)
        return cls
    return register_fn


class ContrastiveLossClip(BaseCriteria):
    """CLIP Loss function for multi-modal image-text training"""

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        self.rank = getattr(opts, 'ddp.rank', 0)
        self.use_distributed = getattr(opts, 'ddp.use_distributed', False)
        self.device = getattr(opts, 'dev.device', torch.device('cpu'))

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def forward(self, input_sample: Any, prediction: Dict, target: Any, *args, **kwargs) ->Dict[str, Tensor]:
        image_features = prediction.pop('image', None)
        text_features = prediction.pop('text', None)
        if image_features is None or text_features is None:
            return {'total_loss': torch.tensor(0.0, dtype=torch.float, device=self.device)}
        assert image_features is not None
        assert text_features is not None
        logit_scale = prediction.pop('logit_scale', 1.0)
        gathered_image_features, gathered_text_features = gather_features(image_features=image_features, text_features=text_features, use_distributed=self.use_distributed)
        logits_per_image = logit_scale * (image_features @ gathered_text_features.transpose(0, 1))
        logits_per_text = logit_scale * (text_features @ gathered_image_features.transpose(0, 1))
        num_logits = logits_per_image.shape[0]
        contrastive_labels = torch.arange(num_logits, device=logits_per_image.device, dtype=torch.long)
        contrastive_labels = contrastive_labels + num_logits * self.rank
        text_loss = F.cross_entropy(logits_per_text, contrastive_labels) * 0.5
        image_loss = F.cross_entropy(logits_per_image, contrastive_labels) * 0.5
        total_loss = image_loss + text_loss
        return {'total_loss': total_loss, 'image_loss': image_loss, 'text_loss': text_loss, 'logit_scale': logit_scale}

    def __repr__(self) ->str:
        return '{}()'.format(self.__class__.__name__)


SEG_LOSS_FN_REGISTRY = {}


def arguments_seg_loss_fn(parser: argparse.ArgumentParser):
    for k, v in SEG_LOSS_FN_REGISTRY.items():
        parser = v.add_arguments(parser=parser)
    return parser


SUPPORTED_SEG_LOSS_FNS = []


def get_segmentation_loss(opts, *args, **kwargs):
    loss_fn_name = getattr(opts, 'loss.segmentation.name', 'cross_entropy')
    if loss_fn_name in SUPPORTED_SEG_LOSS_FNS:
        return SEG_LOSS_FN_REGISTRY[loss_fn_name](opts, *args, **kwargs)
    else:
        supported_loss_fn_str(loss_fn_name)
        return None


class SegmentationLoss(BaseCriteria):

    def __init__(self, opts, *args, **kwargs):
        super().__init__(opts, *args, **kwargs)
        self.criteria = get_segmentation_loss(*args, opts=opts, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.segmentation.name', type=str, default='cross_entropy', help='Segmentation loss function name')
        parser = arguments_seg_loss_fn(parser=parser)
        return parser

    def forward(self, input_sample: Any, prediction: Any, target: Any, *args, **kwargs) ->Tensor:
        return self.criteria(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)

    def __repr__(self):
        return self.criteria.__repr__()


def register_segmentation_loss_fn(name):

    def register_fn(cls):
        if name in SUPPORTED_SEG_LOSS_FNS:
            raise ValueError('Cannot register duplicate segmentation loss function ({})'.format(name))
        if not issubclass(cls, BaseCriteria):
            raise ValueError('Loss function ({}: {}) must extend BaseCriteria'.format(name, cls.__name__))
        SUPPORTED_SEG_LOSS_FNS.append(name)
        SEG_LOSS_FN_REGISTRY[name] = cls
        return cls
    return register_fn


class SegCrossEntropy(BaseCriteria):
    """Cross entropy loss for the task of semantic segmentation"""

    def __init__(self, opts, *args, **kwargs):
        super().__init__(opts, *args, **kwargs)
        self.ignore_idx = getattr(opts, 'loss.ignore_idx', -1)
        self.weighted_loss = getattr(opts, 'loss.segmentation.cross_entropy.class_weights', False)
        self.aux_wt = getattr(opts, 'loss.segmentation.cross_entropy.aux_weight', 0.4)
        self.label_smoothing = getattr(opts, 'loss.segmentation.cross_entropy.label_smoothing', 0.0)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        group = parser.add_argument_group(title=''.format(cls.__name__), description=''.format(cls.__name__))
        group.add_argument('--loss.segmentation.cross-entropy.class-weights', action='store_true', help='Use class weights in loss function')
        group.add_argument('--loss.segmentation.cross-entropy.aux-weight', type=float, default=0.4, help='Weight of auxiliary loss')
        group.add_argument('--loss.segmentation.cross-entropy.label-smoothing', type=float, default=0.0, help='Label smoothing in CE loss for the task of segmentation')
        return parser

    def _compute_loss(self, pred_mask: Tensor, target_mask: Tensor, weight: Optional[Tensor]=None):
        b, c, x_h, x_w = pred_mask.shape
        b, y_h, y_w = target_mask.shape
        label_smoothing = self.label_smoothing if self.training else 0.0
        if x_h != y_h or x_w != y_w:
            pred_mask = F.interpolate(pred_mask, size=(y_h, y_w), mode='bilinear', align_corners=True)
        loss = F.cross_entropy(input=pred_mask, target=target_mask, weight=weight, ignore_index=self.ignore_idx, label_smoothing=label_smoothing)
        return loss

    def forward(self, input_sample: Tensor, prediction: Union[Tensor, Tuple[Tensor, Tensor]], target: Tensor, *args, **kwargs) ->Tensor:
        aux_out = None
        if isinstance(prediction, Tuple) and len(prediction) == 2:
            mask, aux_out = prediction
            assert isinstance(mask, Tensor)
            assert isinstance(aux_out, Tensor)
        elif isinstance(prediction, Tensor):
            mask = prediction
            assert isinstance(mask, Tensor)
        else:
            raise NotImplementedError('For computing loss for segmentation task, we need prediction to be an instance of Tuple or Tensor')
        cls_wts = None
        if self.training:
            if self.weighted_loss:
                n_classes = mask.size(1)
                cls_wts = self._class_weights(target=target, n_classes=n_classes)
            total_loss = self._compute_loss(pred_mask=mask, target_mask=target, weight=cls_wts)
            if aux_out is not None:
                loss_aux = self._compute_loss(pred_mask=aux_out, target_mask=target, weight=cls_wts)
                total_loss = total_loss + self.aux_wt * loss_aux
            return total_loss
        else:
            return self._compute_loss(pred_mask=mask, target_mask=target, weight=None)

    def __repr__(self):
        repr_str = '{}(\n\tweighted_loss={}\n\tignore_idx={}\n\tlabel_smoothing={}'.format(self.__class__.__name__, self.weighted_loss, self.ignore_idx, self.label_smoothing)
        if self.aux_wt > 0:
            repr_str += '\n\taux_wt={}'.format(self.aux_wt)
        return repr_str + '\n)'


class SegCrossEntropyWithNA(SegCrossEntropy, BaseNeuralAug):
    """Cross entropy with Perceptual loss for segmentation tasks with neural augmentation"""

    def __init__(self, opts, *args, **kwargs):
        SegCrossEntropy.__init__(self, opts, *args, **kwargs)
        BaseNeuralAug.__init__(self, opts, *args, **kwargs)

    @classmethod
    def add_arguments(cls, parser: argparse.ArgumentParser):
        return parser

    def forward(self, input_sample: Tensor, prediction: Union[Dict, Tensor, Tuple[Tensor, Tensor]], target: Tensor, *args, **kwargs) ->Dict:
        if isinstance(prediction, (Tuple, Tensor)):
            seg_loss = super().forward(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)
            return {'total_loss': seg_loss}
        elif isinstance(prediction, Dict):
            if not {'augmented_tensor', 'segmentation_output'}.issubset(prediction.keys()):
                logger.error('Prediction needs to be an instance of Dict and must contain segmentation_output and augmented_tensor as keys. Got keys: {}'.format(prediction.keys()))
            augmented_tensor = prediction.get('augmented_tensor', None)
            segmentation_output = prediction.get('segmentation_output', None)
            if augmented_tensor is None:
                seg_loss = SegCrossEntropy.forward(self, *args, input_sample=input_sample, prediction=segmentation_output, target=target, **kwargs)
                return {'total_loss': seg_loss}
            seg_loss = SegCrossEntropy.forward(self, *args, input_sample=input_sample, prediction=segmentation_output, target=target, **kwargs)
            loss_na = self.forward_neural_aug(*args, input_tensor=input_sample, augmented_tensor=augmented_tensor, **kwargs)
            return {'total_loss': loss_na + seg_loss, 'na_loss': loss_na, 'seg_loss': seg_loss}
        else:
            raise NotImplementedError

    def __repr__(self):
        repr_str = '{}(\n\tweighted_loss={}\n\tignore_idx={}\n\tlabel_smoothing={}{}'.format(self.__class__.__name__, self.weighted_loss, self.ignore_idx, self.label_smoothing, self.repr_na())
        if self.aux_wt > 0:
            repr_str += '\n\taux_wt={}'.format(self.aux_wt)
        return repr_str + '\n)'


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AdaptiveAvgPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (AvgPool2d,
     lambda: ([], {'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BaseLayer,
     lambda: ([], {}),
     lambda: ([], {}),
     False),
    (BaseNeuralAugmentor,
     lambda: ([], {'opts': _mock_config(model.learn_augmentation.lr_multiplier=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BatchNorm1d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (BatchNorm2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BatchNorm2dFP32,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BatchNorm3d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     True),
    (Clip,
     lambda: ([], {'min_val': 4, 'max_val': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Conv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DistributionNeuralAugmentor,
     lambda: ([], {'opts': _mock_config(model.learn_augmentation.lr_multiplier=4, model.learn_augmentation.brightness=4, model.learn_augmentation.contrast=4, model.learn_augmentation.noise=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Dropout,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Dropout2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FixedSampler,
     lambda: ([], {'value': 4}),
     lambda: ([], {}),
     False),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GELU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GlobalPool,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalPool2D,
     lambda: ([], {'opts': _mock_config(model.image_projection_head.lr_multiplier=4, model.image_projection_head.global_pool_nchw2nc.no_feature_normalization=4), 'in_dim': 4, 'out_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GroupLinear,
     lambda: ([], {'in_features': 4, 'out_features': 4, 'n_groups': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GroupNorm,
     lambda: ([], {'num_groups': 1, 'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Identity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (InstanceNorm1d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (InstanceNorm2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LayerNorm2D_NCHW,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LayerNormFP32,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LeakyReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LearnablePositionEncoding,
     lambda: ([], {'embed_dim': 4, 'num_embeddings': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MaxPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReLU6,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Sigmoid,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SimpleImageProjectionHead,
     lambda: ([], {'opts': _mock_config(model.image_projection_head.lr_multiplier=4, model.image_projection_head.simple_projection_nc2nc.no_feature_normalization=4), 'in_dim': 4, 'out_dim': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (SinusoidalPositionalEncoding,
     lambda: ([], {'d_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Softmax,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Swish,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SyncBatchNorm,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SyncBatchNormFP32,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Tanh,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (UniformSampler,
     lambda: ([], {'low': 4, 'high': 4}),
     lambda: ([], {}),
     False),
]

class Test_apple_ml_cvnets(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

