import sys
_module = sys.modules[__name__]
del sys
train_forest_cover_calibrated = _module
train_mushroom_edibility_calibrated = _module
model_training = _module
train = _module
display_kfold_cv_results = _module
prepare_classification_data_set = _module
model = _module
advanced_model_training = _module
assess_model_performance = _module
simple_model_training = _module
client_program = _module
train_higgs_medium = _module
train_higgs_small = _module
multiple_model_training = _module
train_twitter_bots = _module
train_nmt = _module
ludwig = _module
api = _module
api_annotations = _module
automl = _module
auto_tune_config = _module
base_config = _module
backend = _module
_ray112_compat = _module
base = _module
datasource = _module
horovod = _module
ray = _module
utils = _module
storage = _module
benchmarking = _module
artifacts = _module
benchmark = _module
process_config = _module
profiler = _module
profiler_callbacks = _module
profiler_dataclasses = _module
reporting = _module
summarize = _module
summary_dataclasses = _module
utils = _module
callbacks = _module
cli = _module
collect = _module
combiners = _module
combiners = _module
constants = _module
contrib = _module
contribs = _module
aim = _module
comet = _module
mlflow = _module
wandb = _module
data = _module
batcher = _module
bucketed = _module
iterable = _module
random_access = _module
cache = _module
manager = _module
types = _module
util = _module
concatenate_datasets = _module
dataframe = _module
dask = _module
modin = _module
pandas = _module
dataset = _module
dataset_synthesizer = _module
negative_sampling = _module
postprocessing = _module
preprocessing = _module
sampler = _module
split = _module
split_dataset = _module
datasets = _module
archives = _module
configs = _module
dataset_config = _module
kaggle = _module
loaders = _module
adult_census_income = _module
agnews = _module
allstate_claims_severity = _module
creditcard_fraud = _module
dataset_loader = _module
ethos_binary = _module
flickr8k = _module
forest_cover = _module
goemotions = _module
higgs = _module
ieee_fraud = _module
insurance_lite = _module
kdd_loader = _module
mnist = _module
naval = _module
rossman_store_sales = _module
santander_value_prediction = _module
sarcastic_headlines = _module
sarcos = _module
split_loaders = _module
sst = _module
model_configs = _module
decoders = _module
generic_decoders = _module
registry = _module
sequence_decoder_utils = _module
sequence_decoders = _module
sequence_tagger = _module
encoders = _module
bag_encoders = _module
binary_encoders = _module
category_encoders = _module
date_encoders = _module
generic_encoders = _module
h3_encoders = _module
image_encoders = _module
sequence_encoders = _module
set_encoders = _module
text_encoders = _module
error = _module
evaluate = _module
experiment = _module
explain = _module
captum = _module
captum_ray = _module
explainer = _module
explanation = _module
gbm = _module
export = _module
features = _module
audio_feature = _module
bag_feature = _module
base_feature = _module
binary_feature = _module
category_feature = _module
date_feature = _module
feature_registries = _module
feature_utils = _module
h3_feature = _module
image_feature = _module
number_feature = _module
sequence_feature = _module
set_feature = _module
text_feature = _module
timeseries_feature = _module
vector_feature = _module
globals = _module
hyperopt = _module
execution = _module
results = _module
run = _module
search_algos = _module
syncer = _module
hyperopt_cli = _module
models = _module
base = _module
calibrator = _module
ecd = _module
gbm = _module
inference = _module
predictor = _module
modules = _module
attention_modules = _module
convolutional_modules = _module
embedding_modules = _module
fully_connected_modules = _module
initializer_modules = _module
loss_modules = _module
metric_modules = _module
metric_registry = _module
mlp_mixer_modules = _module
normalization_modules = _module
optimization_modules = _module
recurrent_modules = _module
reduction_modules = _module
tabnet_modules = _module
predict = _module
preprocess = _module
profiling = _module
dataset_profile = _module
metrics = _module
proto = _module
dataset_profile_pb2 = _module
whylogs_messages_pb2 = _module
type_inference = _module
progress_bar = _module
schema = _module
common_transformer_options = _module
comparator = _module
concat = _module
project_aggregate = _module
sequence = _module
sequence_concat = _module
tab_transformer = _module
tabnet = _module
transformer = _module
defaults = _module
loss = _module
audio = _module
bag = _module
binary = _module
category = _module
date = _module
h3 = _module
image = _module
number = _module
set = _module
text = _module
timeseries = _module
vector = _module
executor = _module
scheduler = _module
search_algorithm = _module
metadata = _module
combiner_metadata = _module
decoder_metadata = _module
encoder_metadata = _module
feature_metadata = _module
parameter_metadata = _module
preprocessing_metadata = _module
trainer_metadata = _module
model_config = _module
optimizers = _module
trainer = _module
serve = _module
trainers = _module
trainer = _module
trainer_lightgbm = _module
algorithms_utils = _module
audio_utils = _module
data_source = _module
field_info = _module
ray_utils = _module
backward_compatibility = _module
calibration = _module
checkpoint_utils = _module
config_utils = _module
data_utils = _module
dataframe_utils = _module
dataset_utils = _module
date_utils = _module
entmax = _module
activations = _module
losses = _module
root_finding = _module
error_handling_utils = _module
eval_utils = _module
fs_utils = _module
h3_util = _module
heuristics = _module
horovod_utils = _module
html_utils = _module
image_utils = _module
inference_utils = _module
logging_utils = _module
loss_utils = _module
math_utils = _module
metric_utils = _module
misc_utils = _module
neuropod_utils = _module
nlp_utils = _module
numerical_test_utils = _module
output_feature_utils = _module
package_utils = _module
print_utils = _module
server_utils = _module
strings_utils = _module
structural_warning = _module
system_utils = _module
time_utils = _module
tokenizers = _module
torch_utils = _module
trainer_utils = _module
triton_utils = _module
types = _module
version_transformation = _module
visualization_utils = _module
visualize = _module
setup = _module
tests = _module
conftest = _module
integration_tests = _module
parameter_update_utils = _module
run_train_aim = _module
run_train_comet = _module
run_train_horovod = _module
run_train_wandb = _module
synthetic_test_data = _module
test_api = _module
test_audio_feature = _module
test_automl = _module
test_cache_manager = _module
test_class_imbalance_feature = _module
test_cli = _module
test_collect = _module
test_config_global_defaults = _module
test_contrib_aim = _module
test_contrib_comet = _module
test_contrib_wandb = _module
test_custom_components = _module
test_dependencies = _module
test_experiment = _module
test_explain = _module
test_gbm = _module
test_graph_execution = _module
test_horovod = _module
test_hyperopt = _module
test_hyperopt_ray = _module
test_hyperopt_ray_horovod = _module
test_input_feature_tied = _module
test_kfold_cv = _module
test_missing_value_strategy = _module
test_mlflow = _module
test_model_save_and_load = _module
test_model_training_options = _module
test_neuropod = _module
test_number_feature = _module
test_postprocessing = _module
test_preprocessing = _module
test_ray = _module
test_reducers = _module
test_regularizers = _module
test_remote = _module
test_reproducibility = _module
test_sequence_encoders = _module
test_sequence_features = _module
test_server = _module
test_simple_features = _module
test_timeseries_feature = _module
test_torchscript = _module
test_trainer = _module
test_triton = _module
test_visualization = _module
test_visualization_api = _module
utils = _module
test_base_config = _module
test_data_source = _module
test_tune_config = _module
test_utils = _module
test_benchmarking = _module
test_profiler = _module
test_combiners = _module
test_dataset_synthesizer = _module
test_negative_sampling = _module
test_postprocessing = _module
test_split = _module
check_dead_links = _module
download_all_datasets = _module
test_mnist_workflow = _module
train_all_model_configs = _module
test_dataset_configs = _module
test_datasets = _module
test_model_configs = _module
test_titanic_workflow = _module
test_sequence_decoder = _module
test_sequence_decoder_utils = _module
test_sequence_tagger = _module
test_bag_encoders = _module
test_binary_encoders = _module
test_category_encoders = _module
test_date_encoders = _module
test_generic_encoders = _module
test_h3_encoders = _module
test_image_encoders = _module
test_sequence_encoders = _module
test_set_encoders = _module
test_text_encoders = _module
test_audio_feature = _module
test_bag_feature = _module
test_binary_feature = _module
test_category_feature = _module
test_date_feature = _module
test_feature_utils = _module
test_h3_feature = _module
test_image_feature = _module
test_number_feature = _module
test_sequence_features = _module
test_set_feature = _module
test_text_feature = _module
test_timeseries_feature = _module
test_fields_misc = _module
test_fields_optimization = _module
test_fields_preprocessing = _module
test_marshmallow_misc = _module
test_training_determinism = _module
test_training_success = _module
test_attention = _module
test_convolutional_modules = _module
test_embedding_modules = _module
test_encoder = _module
test_fully_connected_modules = _module
test_initializer_modules = _module
test_loss_modules = _module
test_metric_modules = _module
test_mlp_mixer_modules = _module
test_normalization_modules = _module
test_recurrent_modules = _module
test_reduction_modules = _module
test_tabnet_modules = _module
test_utils = _module
test_dataset_profile = _module
test_metrics = _module
test_profiling_type_inference = _module
test_proto = _module
test_model_config = _module
test_validate_config_combiner = _module
test_validate_config_features = _module
test_validate_config_misc = _module
test_validate_config_preprocessing = _module
test_validate_config_trainer = _module
test_type_inference = _module
test_losses = _module
test_mask = _module
test_root_finding = _module
test_topk = _module
test_backward_compatibility = _module
test_calibration = _module
test_class_balancing = _module
test_config_utils = _module
test_data_utils = _module
test_dataframe_utils = _module
test_dataset_utils = _module
test_defaults = _module
test_errors = _module
test_fs_utils = _module
test_heuristics = _module
test_hyperopt_ray_utils = _module
test_image_utils = _module
test_metric_utils = _module
test_normalization = _module
test_numerical_test_utils = _module
test_output_feature_utils = _module
test_server_utils = _module
test_strings_utils = _module
test_tokenizers = _module
test_torch_utils = _module
test_trainer_utils = _module
test_version_transformation = _module
update_golden_types = _module
test_auto_type_inference = _module
expected_metric = _module
test_model_performance = _module
test_old_models = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import copy


import logging


from collections import OrderedDict


from typing import Any


from typing import ClassVar


from typing import Dict


from typing import List


from typing import Optional


from typing import Tuple


from typing import Union


import numpy as np


import pandas as pd


import torch


from abc import ABC


from abc import abstractmethod


from typing import Callable


import time


from functools import partial


from typing import TYPE_CHECKING


from queue import Empty as EmptyQueueException


from queue import Queue


from collections import Counter


from collections import defaultdict


from torch._C._autograd import _KinetoEvent


from torch.autograd import DeviceType


from torch.autograd import profiler_util


import functools


import uuid


from types import ModuleType


from functools import lru_cache


from torch.nn import Linear


from torch.nn import ModuleList


import random


import string


import torchaudio


import warnings


import math


import torch.nn as nn


from torch import nn


import numpy.typing as npt


from torch.autograd import Variable


from abc import abstractstaticmethod


from torch import Tensor


import re


import torchvision


from abc import ABCMeta


from torch.nn import functional as F


from copy import deepcopy


from torch.nn import BatchNorm1d


from torch.nn import BatchNorm2d


from torch.nn import Dropout


from torch.nn import LayerNorm


from torch.nn import L1Loss


from torch.nn import MSELoss as _MSELoss


from typing import Generator


from torch import tensor


from torch.nn import GRU


from torch.nn import LSTM


from torch.nn import RNN


from torchvision.io import decode_image


from torch.utils.tensorboard import SummaryWriter


from typing import Iterable


import torch.nn.functional as F


from typing import Type


from torch.autograd import Function


from collections.abc import Iterable


import torchvision.transforms.functional as F


from torchvision.io import ImageReadMode


from collections import namedtuple


from collections.abc import Mapping


import numpy


from torch.nn import Module


from torch.nn import ModuleDict


import torchtext


from typing import Set


from random import choice


from string import ascii_lowercase


from string import ascii_uppercase


from string import digits


from torch.autograd import gradcheck


from itertools import product


def _get_indent(docstring: str) ->int:
    """
    Example:
        >>> def f():
        ...     '''Docstring summary.'''
        >>> f.__doc__
        'Docstring summary.'
        >>> _get_indent(f.__doc__)
        0
        >>> def g(foo):
        ...     '''Docstring summary.
        ...
        ...     Args:
        ...         foo: Does bar.
        ...     '''
        >>> g.__doc__
        'Docstring summary.\\n\\n    Args:\\n        foo: Does bar.\\n    '
        >>> _get_indent(g.__doc__)
        4
        >>> class A:
        ...     def h():
        ...         '''Docstring summary.
        ...
        ...         Returns:
        ...             None.
        ...         '''
        >>> A.h.__doc__
        'Docstring summary.\\n\\n        Returns:\\n            None.\\n        '
        >>> _get_indent(A.h.__doc__)
        8
    """
    if not docstring:
        return 0
    non_empty_lines = list(filter(bool, docstring.splitlines()))
    if len(non_empty_lines) == 1:
        return 0
    return len(non_empty_lines[1]) - len(non_empty_lines[1].lstrip())


def _append_doc(obj, message: str, directive: Optional[str]=None) ->str:
    """
    Args:
        message: An additional message to append to the end of docstring for a class
                 or method that uses one of the API annotations
        directive: A shorter message that provides contexts for the message and indents it.
                For example, this could be something like 'warning' or 'info'.
    """
    if not obj.__doc__:
        obj.__doc__ = ''
    obj.__doc__ = obj.__doc__.rstrip()
    indent = _get_indent(obj.__doc__)
    obj.__doc__ += '\n\n'
    if directive is not None:
        obj.__doc__ += f"{' ' * indent}.. {directive}::\n"
        obj.__doc__ += f"{' ' * (indent + 4)}{message}"
    else:
        obj.__doc__ += f"{' ' * indent}{message}"
    obj.__doc__ += f"\n{' ' * indent}"


def _mark_annotated(obj) ->None:
    if hasattr(obj, '__name__'):
        obj._annotated = obj.__name__


def DeveloperAPI(*args, **kwargs):
    """Annotation for documenting developer APIs. Developer APIs are lower-level methods explicitly exposed to
    advanced Ludwig users and library developers. Their interfaces may change across minor Ludwig releases (for
    e.g., Ludwig 0.6.1 and Ludwig 0.6.2).

    Examples:
        >>> from api_annotations import DeveloperAPI
        >>> @DeveloperAPI
        ... def func(x):
        ...     return x
    """
    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
        return DeveloperAPI()(args[0])

    def wrap(obj):
        _append_doc(obj, message='DeveloperAPI: This API may change across minor Ludwig releases.')
        _mark_annotated(obj)
        return obj
    return wrap


@DeveloperAPI
class RNNDecoder(nn.Module):
    """GRU or RNN-based decoder."""

    def __init__(self, hidden_size: int, vocab_size: int, cell_type: str, num_layers: int=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        if cell_type == 'gru':
            self.rnn = nn.GRU(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)
        else:
            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers=num_layers, batch_first=True)
        self.out = nn.Linear(hidden_size, vocab_size)
        self.out.weight = self.embedding.weight

    def forward(self, input: torch.Tensor, hidden: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor]:
        """Runs a single decoding time step.

        Modeled off of https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.

        Args:
            input: [batch_size] tensor with the previous step's predicted symbol.
            hidden: [batch_size, hidden_size] tensor with the previous step's hidden state.

        Returns:
            Tuple of two tensors:
            - output: [batch_size, 1, vocab_size] tensor with the logits.
            - hidden: [num_layers, batch_size, hidden_size] tensor with the hidden state for the next time step.
        """
        input = input.unsqueeze(1)
        output = self.embedding(input)
        output, hidden = self.rnn(output, hidden)
        output_logits = self.out(output)
        return output_logits, hidden


@DeveloperAPI
class LSTMDecoder(nn.Module):
    """LSTM-based decoder."""

    def __init__(self, hidden_size: int, vocab_size: int, num_layers: int=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True, num_layers=num_layers)
        self.out = nn.Linear(hidden_size, vocab_size)
        self.out.weight = self.embedding.weight

    def forward(self, input: torch.Tensor, hidden_state: torch.Tensor, cell_state: torch.Tensor) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Runs a single decoding time step.

        Modeled off of https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html.

        Args:
            input: [batch_size] tensor with the previous step's predicted symbol.
            hidden_state: [batch_size, hidden_size] tensor with the previous step's hidden state.
            cell_state: [batch_size, hidden_size] tensor with the previous step's cell state.

        Returns:
            Tuple of 3 tensors:
            - output: [batch_size, vocab_size] tensor with the logits.
            - hidden_state: [batch_size, hidden_size] tensor with the hidden state for the next time step.
            - cell_state: [batch_size, hidden_size] tensor with the cell state for the next time step.
        """
        input = input.unsqueeze(1)
        output = self.embedding(input)
        output, (hidden_state, cell_state) = self.lstm(output, (hidden_state, cell_state))
        output_logits = self.out(output)
        return output_logits, hidden_state, cell_state


@DeveloperAPI
class LudwigModule(Module):

    def __init__(self):
        super().__init__()
        self._losses = {}
        self.register_buffer('device_tensor', torch.zeros(0))

    @property
    def device(self):
        return self.device_tensor.device

    def losses(self):
        collected_losses = []
        for loss in self._losses.values():
            collected_losses.append(loss)
        for child in self.children():
            if isinstance(child, LudwigModule):
                collected_losses.extend(child.losses())
            elif isinstance(child, ModuleDict):
                for c in child.values():
                    if hasattr(c, 'losses'):
                        collected_losses.extend(c.losses())
            elif isinstance(child, Module):
                pass
            else:
                raise ValueError
        return collected_losses

    def update_loss(self, key: str, loss: torch.Tensor):
        """This should be called in the forward pass to add a custom loss term to the combined loss."""
        self._losses[key] = loss

    @property
    def input_dtype(self):
        return torch.float32

    @property
    @abstractmethod
    def input_shape(self) ->torch.Size:
        """Returns size of the input tensor without the batch dimension."""
        pass

    @property
    def output_shape(self) ->torch.Size:
        """Returns size of the output tensor without the batch dimension."""
        return self._computed_output_shape()

    @lru_cache(maxsize=1)
    def _computed_output_shape(self) ->torch.Size:
        dummy_input = torch.rand(2, *self.input_shape, device=self.device)
        output_tensor = self.forward(dummy_input.type(self.input_dtype))
        if isinstance(output_tensor, torch.Tensor):
            return output_tensor.size()[1:]
        elif isinstance(output_tensor, dict) and 'encoder_output' in output_tensor:
            return output_tensor['encoder_output'].size()[1:]
        else:
            raise ValueError('Unknown output tensor type.')


@DeveloperAPI
def get_from_registry(key, registry):
    if hasattr(key, 'lower'):
        key = key.lower()
    if key in registry:
        return registry[key]
    else:
        raise ValueError(f'Key {key} not supported, available options: {registry.keys()}')


activations = {'elu': nn.ELU, 'leakyRelu': nn.LeakyReLU, 'logSigmoid': nn.LogSigmoid, 'relu': nn.ReLU, 'sigmoid': nn.Sigmoid, 'tanh': nn.Tanh, 'softmax': nn.Softmax, None: nn.Identity}


@DeveloperAPI
def get_activation(activation):
    return activations[activation]()


class FeedForwardAttentionReducer(LudwigModule):

    def __init__(self, input_size, hidden_size=256, activation='tanh'):
        super().__init__()
        self.fc_layer1 = nn.Linear(input_size, hidden_size)
        self.fc_layer1_activation = get_activation(activation)
        self.fc_layer2 = nn.Linear(hidden_size, 1, bias=False)

    def forward(self, inputs, mask=None):
        hidden = self.fc_layer1(inputs)
        hidden = self.fc_layer1_activation(hidden)
        hidden = self.fc_layer2(hidden)
        attention = F.softmax(hidden, dim=1)
        gated_inputs = torch.sum(attention * inputs, dim=1)
        return gated_inputs


class ReduceConcat(torch.nn.Module):

    def forward(self, inputs, mask=None):
        if inputs.dim() > 2:
            return inputs.reshape(-1, inputs.shape[-1] * inputs.shape[-2])
        return inputs


@DeveloperAPI
def sequence_length_3D(sequence: torch.Tensor) ->torch.Tensor:
    """Returns the number of non-zero elements per sequence in batch.

    :param sequence: (torch.Tensor) A 3D tensor of shape [batch size x max sequence length x hidden size].

    # Return
    :returns: (torch.Tensor) The count on non-zero elements per sequence.
    """
    used = torch.sign(torch.amax(torch.abs(sequence), dim=2))
    length = torch.sum(used, 1)
    length = length.int()
    return length


class ReduceLast(torch.nn.Module):

    def forward(self, inputs, mask=None):
        batch_size = inputs.shape[0]
        sequence_length = sequence_length_3D(inputs) - 1
        sequence_length[sequence_length < 0] = 0
        gathered = inputs[torch.arange(batch_size), sequence_length.type(torch.int64)]
        return gathered


class ReduceMax(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return torch.amax(inputs, dim=1)


class ReduceMean(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return torch.mean(inputs, dim=1)


class ReduceNone(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return inputs


class ReduceSum(torch.nn.Module):

    def forward(self, inputs, mask=None):
        return torch.sum(inputs, dim=1)


reduce_mode_registry = {'last': ReduceLast, 'sum': ReduceSum, 'mean': ReduceMean, 'avg': ReduceMean, 'max': ReduceMax, 'concat': ReduceConcat, 'attention': FeedForwardAttentionReducer, 'none': ReduceNone, 'None': ReduceNone, None: ReduceNone}


class SequenceReducer(LudwigModule):
    """Reduces the sequence dimension of an input tensor according to the specified reduce_mode.  Any additional
    kwargs are passed on to the reduce mode's constructor.  If using reduce_mode=="attention", the input_size kwarg
    must also be specified.

    A sequence is a tensor of 2 or more dimensions, where the shape is [batch size x sequence length x ...].

    :param reduce_mode: The reduction mode, one of {"last", "sum", "mean", "max", "concat", "attention", "none"}
    :param max_sequence_length The maximum sequence length.  Only used for computation of shapes - inputs passed
                               at runtime may have a smaller sequence length.
    :param encoding_size The size of each sequence element/embedding vector, or None if input is a sequence of scalars.
    """

    def __init__(self, reduce_mode: str=None, max_sequence_length: int=256, encoding_size: int=None, **kwargs):
        super().__init__()
        self._reduce_mode = reduce_mode
        self._max_sequence_length = max_sequence_length
        self._encoding_size = encoding_size
        if reduce_mode == 'attention' and encoding_size and 'input_size' not in kwargs:
            kwargs['input_size'] = encoding_size
        self._reduce_obj = get_from_registry(reduce_mode, reduce_mode_registry)(**kwargs)

    def forward(self, inputs, mask=None):
        """Forward pass of reducer.

        :param inputs: A tensor of 2 or more dimensions, where the shape is [batch size x sequence length x ...].
        :param mask: A mask tensor of 2 dimensions [batch size x sequence length].  Not yet implemented.

        :return: The input after applying the reduction operation to sequence dimension.
        """
        return self._reduce_obj(inputs, mask=mask)

    @property
    def input_shape(self) ->torch.Size:
        """Returns size of the input tensor without the batch dimension."""
        if self._encoding_size is None:
            return torch.Size([self._max_sequence_length])
        else:
            return torch.Size([self._max_sequence_length, self._encoding_size])

    @property
    def output_shape(self) ->torch.Size:
        """Returns size of the output tensor without the batch dimension."""
        input_shape = self.input_shape
        if self._reduce_mode in {None, 'none', 'None'}:
            return input_shape
        elif self._reduce_mode == 'concat':
            if len(input_shape) > 1:
                return input_shape[:-2] + (input_shape[-1] * input_shape[-2],)
            return input_shape
        else:
            return input_shape[1:]


ENCODER_OUTPUT_STATE = 'encoder_output_state'


HIDDEN = 'hidden'


def repeat_2D_tensor(tensor, k):
    """Repeats a 2D-tensor k times over the first dimension.

    For example:
    Input: Tensor of [batch_size, state_size], k=2
    Output: Tensor of [k, batch_size, state_size]
    """
    if len(tensor.size()) > 2:
        raise ValueError('Cannot repeat a non-2D tensor with this method.')
    return tensor.repeat(k, 1, 1)


def get_rnn_init_state(combiner_outputs: Dict[str, torch.Tensor], sequence_reducer: SequenceReducer, num_layers: int) ->torch.Tensor:
    """Computes the hidden state that the RNN decoder should start with.

    Args:
        combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
        sequence_reducer: SequenceReducer to reduce rank-3 to rank-2.
        num_layers: Number of layers the decoder uses.

    Returns:
        Tensor of [num_layers, batch_size, hidden_size].
    """
    if ENCODER_OUTPUT_STATE not in combiner_outputs:
        encoder_output_state = combiner_outputs[HIDDEN]
    else:
        encoder_output_state = combiner_outputs[ENCODER_OUTPUT_STATE]
        if isinstance(encoder_output_state, tuple):
            if len(encoder_output_state) == 2:
                encoder_output_state = encoder_output_state[0]
            elif len(encoder_output_state) == 4:
                encoder_output_state = torch.mean([encoder_output_state[0], encoder_output_state[2]])
            else:
                raise ValueError(f'Invalid sequence decoder inputs with keys: {combiner_outputs.keys()} with extracted encoder ' + f'state: {encoder_output_state.size()} that was invalid. Please double check the compatibility ' + 'of your encoder and decoder.')
    if len(encoder_output_state.size()) > 3:
        raise ValueError('Init state for RNN decoders only works for 1d or 2d tensors (encoder_output).')
    if len(encoder_output_state.size()) == 3:
        encoder_output_state = sequence_reducer(encoder_output_state)
    return repeat_2D_tensor(encoder_output_state, num_layers)


@DeveloperAPI
class SequenceRNNDecoder(nn.Module):
    """RNN-based decoder over multiple time steps."""

    def __init__(self, hidden_size: int, vocab_size: int, max_sequence_length: int, cell_type: str, num_layers: int=1, reduce_input='sum'):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.rnn_decoder = RNNDecoder(hidden_size, vocab_size, cell_type, num_layers=num_layers)
        self.max_sequence_length = max_sequence_length
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_input)
        self.num_layers = num_layers
        self.register_buffer('logits', torch.zeros([max_sequence_length, vocab_size]))
        self.register_buffer('decoder_input', torch.Tensor([strings_utils.SpecialSymbol.START.value]))

    def forward(self, combiner_outputs: Dict[str, torch.Tensor], target: torch.Tensor):
        """Runs max_sequence_length RNN decoding time steps.

        Args:
            combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
            target: Tensor [batch_size, max_sequence_length] with target symbols.

        Returns:
            Tensor of logits [batch_size, max_sequence_length, vocab_size].
        """
        decoder_hidden = get_rnn_init_state(combiner_outputs, self.reduce_sequence, self.num_layers)
        batch_size = decoder_hidden.size()[1]
        logits = self.logits.unsqueeze(0).repeat(batch_size, 1, 1)
        decoder_input = self.decoder_input.repeat(batch_size)
        for di in range(self.max_sequence_length):
            decoder_output, decoder_hidden = self.rnn_decoder(decoder_input, decoder_hidden)
            logits[:, di, :] = decoder_output.squeeze(1)
            if target is None:
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(1).squeeze(1).detach()
            else:
                decoder_input = target[:, di]
        return logits


def get_lstm_init_state(combiner_outputs: Dict[str, torch.Tensor], sequence_reducer: SequenceReducer, num_layers: int) ->Tuple[torch.Tensor, torch.Tensor]:
    """Returns the states that the LSTM decoder should start with.

    Args:
        combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
        sequence_reducer: SequenceReducer to reduce rank-3 to rank-2.
        num_layers: Number of layers the decoder uses.

    Returns:
        Tuple of 2 tensors (decoder hidden state, decoder cell state), each [num_layers, batch_size, hidden_size].
    """
    if 'encoder_output_state' not in combiner_outputs:
        decoder_hidden_state = combiner_outputs[HIDDEN]
        decoder_cell_state = torch.clone(decoder_hidden_state)
    else:
        encoder_output_state = combiner_outputs[ENCODER_OUTPUT_STATE]
        if not isinstance(encoder_output_state, tuple):
            decoder_hidden_state = encoder_output_state
            decoder_cell_state = decoder_hidden_state
        elif len(encoder_output_state) == 2:
            decoder_hidden_state, decoder_cell_state = encoder_output_state
        elif len(encoder_output_state) == 4:
            decoder_hidden_state = torch.mean([encoder_output_state[0], encoder_output_state[2]])
            decoder_cell_state = torch.mean([encoder_output_state[1], encoder_output_state[3]])
        else:
            raise ValueError(f'Invalid sequence decoder inputs with keys: {combiner_outputs.keys()} with extracted encoder ' + f'state: {encoder_output_state} that was invalid. Please double check the compatibility of your ' + 'encoder and decoder.')
    if len(decoder_hidden_state.size()) > 3 or len(decoder_cell_state.size()) > 3:
        raise ValueError(f'Invalid sequence decoder inputs with keys: {combiner_outputs.keys()} with extracted encoder ' + f'state: {decoder_hidden_state.size()} that was invalid. Please double check the compatibility ' + 'of your encoder and decoder.')
    if len(decoder_hidden_state.size()) == 3:
        decoder_hidden_state = sequence_reducer(decoder_hidden_state)
    if len(decoder_cell_state.size()) == 3:
        decoder_cell_state = sequence_reducer(decoder_cell_state)
    return repeat_2D_tensor(decoder_hidden_state, num_layers), repeat_2D_tensor(decoder_cell_state, num_layers)


@DeveloperAPI
class SequenceLSTMDecoder(nn.Module):
    """LSTM-based decoder over multiple time steps."""

    def __init__(self, hidden_size: int, vocab_size: int, max_sequence_length: int, reduce_input: str='sum', num_layers: int=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.vocab_size = vocab_size
        self.lstm_decoder = LSTMDecoder(hidden_size, vocab_size, num_layers)
        self.max_sequence_length = max_sequence_length
        self.reduce_sequence = SequenceReducer(reduce_mode=reduce_input)
        self.num_layers = num_layers
        self.register_buffer('logits', torch.zeros([max_sequence_length, vocab_size]))
        self.register_buffer('decoder_input', torch.Tensor([strings_utils.SpecialSymbol.START.value]))

    def forward(self, combiner_outputs: Dict[str, torch.Tensor], target: torch.Tensor) ->torch.Tensor:
        """Runs max_sequence_length LSTM decoding time steps.

        Args:
            combiner_outputs: Dictionary of tensors from the outputs of the combiner and other output features.
            target: Tensor [batch_size, max_sequence_length] with target symbols.

        Returns:
            Tensor of logits [batch_size, max_sequence_length, vocab_size].
        """
        decoder_hidden, decoder_cell_state = get_lstm_init_state(combiner_outputs, self.reduce_sequence, self.num_layers)
        batch_size = decoder_hidden.size()[1]
        decoder_input = self.decoder_input.repeat(batch_size)
        logits = self.logits.unsqueeze(0).repeat(batch_size, 1, 1)
        for di in range(self.max_sequence_length):
            decoder_output, decoder_hidden, decoder_cell_state = self.lstm_decoder(decoder_input, decoder_hidden, decoder_cell_state)
            logits[:, di, :] = decoder_output.squeeze(1)
            if target is None:
                _, topi = decoder_output.topk(1)
                decoder_input = topi.squeeze(1).squeeze(1).detach()
            else:
                decoder_input = target[:, di]
        return logits


CATEGORY = 'category'


DATE = 'date'


AUDIO = 'audio'


BAG = 'bag'


BINARY = 'binary'


H3 = 'h3'


IMAGE = 'image'


NUMBER = 'number'


SEQUENCE = 'sequence'


SET = 'set'


TEXT = 'text'


TIMESERIES = 'timeseries'


VECTOR = 'vector'


logger = logging.getLogger(__name__)


COMBINED = 'combined'


@DeveloperAPI
class Combiner(LudwigModule, ABC):
    """Base class for combiners, which implements common properties.

    Subclasses will usually override:     __init__()        to set properties and allocate resources.  Should call
    super().__init__(input_features).     forward()         performs the forward pass given a dictionary of encoder
    outputs.     get_schema_cls()  must returns the class of the corresponding schema for the combiner type.
    """

    def __init__(self, input_features: Dict[str, 'InputFeature']):
        super().__init__()
        self.input_features = input_features

    @property
    def concatenated_shape(self) ->torch.Size:
        shapes = [torch.prod(torch.Tensor([*self.input_features[k].output_shape])) for k in self.input_features]
        return torch.Size([torch.sum(torch.Tensor(shapes)).type(torch.int32)])

    @property
    def input_shape(self) ->Dict:
        return {k: self.input_features[k].output_shape for k in self.input_features}

    @property
    @lru_cache(maxsize=1)
    def output_shape(self) ->torch.Size:
        pseudo_input = {}
        for k in self.input_features:
            pseudo_input[k] = {'encoder_output': torch.rand(2, *self.input_features[k].output_shape, dtype=self.input_dtype, device=self.device)}
        output_tensor = self.forward(pseudo_input)
        return output_tensor['combiner_output'].size()[1:]


@DeveloperAPI
def get_torch_device():
    return 'cuda' if torch.cuda.is_available() else 'cpu'


DEVICE = get_torch_device()


NAME = 'name'


PREPROCESSING = 'preprocessing'


@DeveloperAPI
def hash_dict(d: dict, max_length: Union[int, None]=6) ->bytes:
    s = json.dumps(d, cls=NumpyEncoder, sort_keys=True, ensure_ascii=True)
    h = hashlib.md5(s.encode())
    d = h.digest()
    b = base64.b64encode(d, altchars=b'__')
    return b[:max_length]


def sanitize(name):
    """Replaces invalid id characters."""
    return re.sub('\\W|^(?=\\d)', '_', name)


def compute_feature_hash(feature: dict) ->str:
    """
    This function computes a hash for each feature based on the preprocessing dictionary associated with each feature.
    The input is always the feature dict, however sometimes, this is called from BaseFeature which dumps the feature
    dict from a ConfigObject and the preprocessing key corresponds to a nested Preprocessing config. This is why it
    includes the if/else statement.
    Args:
        feature: Feature dictionary

    Returns: Feature hash name

    """
    preproc = feature.get(PREPROCESSING, {})
    if isinstance(preproc, dict):
        preproc_hash = hash_dict(preproc)
    else:
        preproc_hash = hash_dict(preproc.to_dict())
    return sanitize(feature[NAME]) + '_' + preproc_hash.decode('ascii')


TrainingSetMetadataDict = Dict[str, Any]


@DeveloperAPI
class Encoder(LudwigModule, ABC):

    @abstractmethod
    def forward(self, inputs, training=None, mask=None):
        raise NotImplementedError

    @property
    def name(self):
        return self.__class__.__name__

    @classmethod
    def get_fixed_preprocessing_params(cls, encoder_params: Dict[str, Any]) ->Dict[str, Any]:
        """Returns a dict of fixed preprocessing parameters for the encoder if required."""
        return {}

    @classmethod
    def is_pretrained(cls, encoder_params: Dict[str, Any]) ->bool:
        return False


DEFAULT_KEYS = ['None', 'none', 'null', None]


def get_encoder_cls(feature: str, name: str) ->Type[Encoder]:
    return get_encoder_registry()[feature][name]


ACTIVE = 'active'


TYPE = 'type'


@DeveloperAPI
class BaseFeatureContainer:
    """Base Feature container for input and output features."""

    def to_dict(self):
        """Method for getting a dictionary representation of the input features.

        Returns:
            Dictionary of input features specified.
        """
        return convert_submodules(self.__dict__)

    def to_list(self):
        """Method for getting a list representation of the input features.

        Returns:
            List of input features specified.
        """
        return list(convert_submodules(self.__dict__).values())

    def filter_features(self):
        """This function is intended to filter out the parameters on input/output features that we want to show in
        the config object repr."""
        return {key: {k: v for k, v in value.items() if k in {NAME, TYPE, ACTIVE}} for key, value in self.to_dict().items()}

    def get(self, feature_name):
        """Gets a feature by name.

        raises AttributeError if no feature with the specified name is present.
        """
        return getattr(self, feature_name)

    def __repr__(self):
        filtered_repr = self.filter_features()
        return yaml.dump(filtered_repr, sort_keys=True)


@DeveloperAPI
class InputFeaturesContainer(BaseFeatureContainer):
    """InputFeatures is a container for all input features."""
    pass


LOSS = 'loss'


FEATURE_NAME_SUFFIX = '__ludwig'


def get_module_dict_key_from_name(name: str, feature_name_suffix: str=FEATURE_NAME_SUFFIX) ->str:
    """Returns a key that's guaranteed to be compatible with torch."""
    key = name.replace('.', '__ludwig_punct_period__')
    return key + feature_name_suffix


FEATURE_NAME_SUFFIX_LENGTH = len(FEATURE_NAME_SUFFIX)


def get_name_from_module_dict_key(key: str, feature_name_suffix_length: int=FEATURE_NAME_SUFFIX_LENGTH) ->str:
    """Reverse of get_module_dict_key_from_name."""
    name = key.replace('__ludwig_punct_period__', '.')
    return name[:-feature_name_suffix_length]


class LudwigFeatureDict(torch.nn.Module):
    """Torch ModuleDict wrapper that permits keys with any name.

    Torch's ModuleDict implementation doesn't allow certain keys to be used if they conflict with existing class
    attributes, e.g.

    > torch.nn.ModuleDict({'type': torch.nn.Module()})  # Raises KeyError.

    This class is a simple wrapper around torch's ModuleDict that mitigates possible conflicts by using a key-suffixing
    protocol.

    This is also tracked in Pytorch: https://github.com/pytorch/pytorch/issues/71203.
    """

    def __init__(self):
        super().__init__()
        self.module_dict = torch.nn.ModuleDict()
        self.internal_key_to_original_name_map = {}

    def __getitem__(self, key) ->torch.nn.Module:
        return self.module_dict[get_module_dict_key_from_name(key)]

    def __setitem__(self, key: str, module: torch.nn.Module) ->None:
        module_dict_key_name = get_module_dict_key_from_name(key)
        self.internal_key_to_original_name_map[module_dict_key_name] = key
        self.module_dict[module_dict_key_name] = module

    def __len__(self) ->int:
        return len(self.module_dict)

    def __next__(self) ->None:
        return next(iter(self))

    def __iter__(self) ->None:
        return iter(self.keys())

    def keys(self) ->List[str]:
        return [get_name_from_module_dict_key(feature_name) for feature_name in self.internal_key_to_original_name_map.keys()]

    def values(self) ->List[torch.nn.Module]:
        return [module for _, module in self.module_dict.items()]

    def items(self) ->List[Tuple[str, torch.nn.Module]]:
        return [(get_name_from_module_dict_key(feature_name), module) for feature_name, module in self.module_dict.items()]

    def update(self, modules: Dict[str, torch.nn.Module]) ->None:
        for feature_name, module in modules.items():
            self.__setitem__(feature_name, module)


initializer_registry = {'uniform': nn.init.uniform_, 'normal': nn.init.normal_, 'constant': nn.init.constant_, 'ones': nn.init.ones_, 'zeros': nn.init.zeros_, 'eye': nn.init.eye_, 'dirac': nn.init.dirac_, 'xavier_uniform': nn.init.xavier_uniform_, 'xavier_normal': nn.init.xavier_normal_, 'kaiming_uniform': nn.init.kaiming_uniform_, 'kaiming_normal': nn.init.kaiming_normal_, 'orthogonal': nn.init.orthogonal_, 'sparse': nn.init.sparse_, 'identity': nn.init.eye_}


class FCLayer(LudwigModule):
    """A torch.nn.Linear wrapper that declares input and output shapes, and enables the customization of:

    1. how weights and biases are initialized
    2. normalization (layer and batch)
    3. activations
    4. dropout
    """

    @property
    def input_shape(self) ->torch.Size:
        return torch.Size([self.input_size])

    @property
    def output_shape(self) ->torch.Size:
        return torch.Size([self.output_size])

    def __init__(self, input_size: int, input_rank: int=2, output_size: int=256, use_bias: bool=True, weights_initializer: str='xavier_uniform', bias_initializer: str='zeros', norm: Optional[str]=None, norm_params: Optional[Dict]=None, activation: str='relu', dropout: float=0):
        super().__init__()
        self.layers = ModuleList()
        self.input_size = input_size
        self.output_size = output_size
        fc = Linear(in_features=input_size, out_features=output_size, bias=use_bias)
        self.layers.append(fc)
        weights_initializer = initializer_registry[weights_initializer]
        weights_initializer(fc.weight)
        if use_bias:
            bias_initializer = initializer_registry[bias_initializer]
            bias_initializer(fc.bias)
        if norm and norm_params is None:
            norm_params = {}
        if norm == 'batch':
            if input_rank == 2:
                self.layers.append(BatchNorm1d(output_size, **norm_params))
            elif input_rank == 3:
                self.layers.append(BatchNorm2d(output_size, **norm_params))
            else:
                ValueError(f'input_rank parameter expected to be either 2 or 3, however valued found to be {input_rank}.')
        elif norm == 'layer':
            self.layers.append(LayerNorm(output_size, **norm_params))
        self.layers.append(activations[activation]())
        if dropout > 0:
            self.layers.append(Dropout(dropout))

    def forward(self, inputs, mask=None):
        hidden = inputs
        for layer in self.layers:
            hidden = layer(hidden)
        return hidden


class FCStack(LudwigModule):
    """A stack of FCLayers.

    The specification of each FCLayer is specified by the `layers` dictionary parameter, whose keys correspond with an
    FCLayer's constructor arguments, i.e.

    [
        {"input_size": 2, "output_size": 4},
        {"output_size": 4, "use_bias": False},
    ]

    `default_*` parameters dictate default values to use for each FCLayer, if not specified by `layers`. If `layers` is
    `None`, then a stack of size `num_layers` of `FCLayer`s configured with all of the `default_*` parameters is used.

    If `layers` is None and `num_layers` is 0, then there are no fully connected layers and this module serves as a
    trivial passthrough.
    """

    def __init__(self, first_layer_input_size: int, layers: Optional[List[Dict]]=None, num_layers: int=1, default_input_rank: int=2, default_output_size: int=256, default_use_bias: bool=True, default_weights_initializer: str='xavier_uniform', default_bias_initializer: str='zeros', default_norm: Optional[str]=None, default_norm_params: Optional[Dict]=None, default_activation: str='relu', default_dropout: float=0, residual: bool=False, **kwargs):
        super().__init__()
        self.input_size = first_layer_input_size
        if layers is None:
            self.layers = []
            for i in range(num_layers):
                self.layers.append({})
        else:
            self.layers = deepcopy(layers)
        if len(self.layers) > 0 and 'input_size' not in self.layers[0]:
            self.layers[0]['input_size'] = first_layer_input_size
        for i, layer in enumerate(self.layers):
            if i != 0:
                layer['input_size'] = self.layers[i - 1]['output_size']
            if 'input_rank' not in layer:
                layer['input_rank'] = default_input_rank
            if 'output_size' not in layer:
                layer['output_size'] = default_output_size
            if 'use_bias' not in layer:
                layer['use_bias'] = default_use_bias
            if 'weights_initializer' not in layer:
                layer['weights_initializer'] = default_weights_initializer
            if 'bias_initializer' not in layer:
                layer['bias_initializer'] = default_bias_initializer
            if 'norm' not in layer:
                layer['norm'] = default_norm
            if 'norm_params' not in layer:
                layer['norm_params'] = default_norm_params
            if 'activation' not in layer:
                layer['activation'] = default_activation
            if 'dropout' not in layer:
                layer['dropout'] = default_dropout
        self.stack = ModuleList()
        for i, layer in enumerate(self.layers):
            self.stack.append(FCLayer(input_size=layer['input_size'], input_rank=layer['input_rank'], output_size=layer['output_size'], use_bias=layer['use_bias'], weights_initializer=layer['weights_initializer'], bias_initializer=layer['bias_initializer'], norm=layer['norm'], norm_params=layer['norm_params'], activation=layer['activation'], dropout=layer['dropout']))
        self.residual = residual

    def forward(self, inputs, mask=None):
        hidden = inputs
        prev_fc_layer_size = self.input_size
        for layer in self.stack:
            out = layer(hidden)
            if self.residual and layer.output_size == prev_fc_layer_size:
                hidden = hidden + out
            else:
                hidden = out
            prev_fc_layer_size = layer.layers[0].out_features
        return hidden

    @property
    def num_layers(self) ->int:
        return len(self.layers)

    @property
    def input_shape(self) ->torch.Size:
        return torch.Size([self.input_size])

    @property
    def output_shape(self) ->torch.Size:
        if len(self.stack) > 0:
            return self.stack[-1].output_shape
        return torch.Size([self.input_size])


LENGTHS = 'lengths'


def gather_all_tensors(result: torch.Tensor, group: Optional[Any]=None) ->List[torch.Tensor]:
    """Function to gather all tensors from several processes onto a list that is broadcast to all processes.

    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case
    tensors are padded, gathered and then trimmed to secure equal workload for all processes.

    :param result: the value to sync
    :param group: the process group to gather results from (not supported: always uses world)

    :return: list with size equal to the process group where gathered_result[i]
             corresponds to result tensor from process i
    """
    if group is not None:
        raise ValueError('Horovod does not support allgather using a subcommunicator at this time. Unset `group`.')
    if _HVD is None or not _HVD.is_initialized():
        return [result]
    if len(result.shape) == 0:
        result = result.reshape(1)
    is_bool = False
    if result.dtype == torch.bool:
        result = result.int()
        is_bool = True
    result = result.unsqueeze(0)
    gathered = _HVD.allgather(result)
    gathered_result = list(gathered.split(1, dim=0))
    if is_bool:
        gathered_result = [t.bool() for t in gathered_result]
    return gathered_result


def is_distributed_available() ->bool:
    return _HVD is not None and _HVD.is_initialized()


LOGITS = 'logits'


PREDICTIONS = 'predictions'


PROBABILITIES = 'probabilities'


class PredictModule(torch.nn.Module):
    """Base class for all modules that convert model outputs to predictions.

    Explicit member variables needed here for scripting, as Torchscript will not be able to recognize global variables
    during scripting.
    """

    def __init__(self):
        super().__init__()
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES
        self.logits_key = LOGITS


@DeveloperAPI
class Decoder(LudwigModule, ABC):

    @abstractmethod
    def forward(self, inputs, mask=None):
        raise NotImplementedError

    @property
    def name(self):
        return self.__class__.__name__


@DeveloperAPI
def get_decoder_cls(feature: str, name: str) ->Type[Decoder]:
    return get_decoder_registry()[feature][name]


def get_input_size_with_dependencies(combiner_output_size: int, dependencies: List[str], other_output_features):
    """Returns the input size for the first layer of this output feature's FC stack, accounting for dependencies on
    other output features.

    In the forward pass, the hidden states of any dependent output features get concatenated with the combiner's output.
    If this output feature depends on other output features, then the input size for this feature's FCStack is the sum
    of the output sizes of other output features + the combiner's output size.
    """
    input_size_with_dependencies = combiner_output_size
    for feature_name in dependencies:
        if other_output_features[feature_name].fc_stack.num_layers:
            input_size_with_dependencies += other_output_features[feature_name].fc_stack.output_shape[-1]
        else:
            input_size_with_dependencies += other_output_features[feature_name].input_size
    return input_size_with_dependencies


def get_loss_cls(feature: str, name: str):
    return loss_registry[feature][name]


def get_metric_classes(feature: str):
    return metric_feature_registry[feature]


def get_metric_cls(feature: str, name: str):
    return metric_feature_registry[feature][name]


@DeveloperAPI
class OutputFeaturesContainer(BaseFeatureContainer):
    """OutputFeatures is a container for all output features."""
    pass


TorchDevice = Union[str, torch.device]


AUDIO_FEATURE_KEYS = ['type', 'window_length_in_s', 'window_shift_in_s', 'num_fft_points', 'window_type', 'num_filter_bands']


FeatureConfigDict = Dict[str, Any]


FeatureMetadataDict = Dict[str, Any]


PreprocessingConfigDict = Dict[str, Any]


COLUMN = 'column'


PROC_COLUMN = 'proc_column'


SRC = 'dataset_src'


@DeveloperAPI
def calculate_mean(sum1, count):
    return sum1 / float(count)


@DeveloperAPI
def calculate_var(sum1, sum2, count):
    return (sum2 - sum1 * sum1 / float(count)) / float(count - 1) if count > 1 else 0.0


@DeveloperAPI
def has_remote_protocol(url):
    protocol, _ = split_protocol(url)
    return protocol and protocol != 'file'


@DeveloperAPI
def get_abs_path(src_path, file_path):
    if has_remote_protocol(file_path):
        return file_path
    elif src_path is not None:
        return os.path.join(src_path, file_path)
    else:
        return file_path


DEFAULT_AUDIO_TENSOR_LENGTH = 70000


TorchAudioTuple = Tuple[torch.Tensor, int]


@DeveloperAPI
def get_default_audio(audio_lst: List[TorchAudioTuple]) ->TorchAudioTuple:
    sampling_rates = [audio[1] for audio in audio_lst]
    tensor_list = [audio[0] for audio in audio_lst]
    for i, tensor in enumerate(tensor_list):
        if tensor.shape[1] > DEFAULT_AUDIO_TENSOR_LENGTH:
            tensor_list[i] = tensor[:, :DEFAULT_AUDIO_TENSOR_LENGTH]
        else:
            pad_size = DEFAULT_AUDIO_TENSOR_LENGTH - tensor.shape[1]
            tensor_list[i] = F.pad(tensor, (0, pad_size))
    default_audio_tensor = torch.mean(torch.stack(tensor_list), dim=0)
    default_sampling_rate = calculate_mean(sum(sampling_rates), len(sampling_rates))
    return default_audio_tensor, default_sampling_rate


def _convert_hz_to_mel(hz: int) ->float:
    return float(2595.0 * torch.log10(torch.tensor(1 + hz / 700.0)))


def _convert_mel_to_hz(mel):
    return 700.0 * (10 ** (mel / 2595.0) - 1)


def _create_triangular_filter(start_bin_freq: torch.Tensor, middle_bin_freq: torch.Tensor, end_bin_freq: torch.Tensor, num_ess_fft_points: int):
    filter_window = torch.zeros(num_ess_fft_points, dtype=torch.float32, device=start_bin_freq.device)
    filt_support_begin = middle_bin_freq - start_bin_freq
    filt_support_end = end_bin_freq - middle_bin_freq
    for freq in range(int(start_bin_freq), int(middle_bin_freq)):
        filter_window[freq] = (freq - start_bin_freq) / filt_support_begin
    for freq in range(int(middle_bin_freq), int(end_bin_freq)):
        filter_window[freq] = (end_bin_freq - freq) / filt_support_end
    return filter_window


@DeveloperAPI
def get_non_symmetric_length(symmetric_length: int) ->int:
    return int(symmetric_length / 2) + 1


def _get_mel_fbank_matrix(list_mel_points: torch.Tensor, num_filter_bands: int, num_fft_points: int, sampling_rate_in_hz: int) ->torch.Tensor:
    num_ess_fft_points = get_non_symmetric_length(num_fft_points)
    freq_scale = (num_fft_points + 1) / sampling_rate_in_hz
    freq_bins_on_mel_scale = torch.floor(freq_scale * _convert_mel_to_hz(list_mel_points))
    mel_scaled_fbank = torch.zeros((num_filter_bands, num_ess_fft_points), dtype=torch.float32, device=list_mel_points.device)
    for filt_idx in range(num_filter_bands):
        start_bin_freq = freq_bins_on_mel_scale[filt_idx]
        middle_bin_freq = freq_bins_on_mel_scale[filt_idx + 1]
        end_bin_freq = freq_bins_on_mel_scale[filt_idx + 2]
        mel_scaled_fbank[filt_idx] = _create_triangular_filter(start_bin_freq, middle_bin_freq, end_bin_freq, num_ess_fft_points)
    return mel_scaled_fbank


def _pre_emphasize_data(data: torch.Tensor, emphasize_value: float=0.97):
    filter_window = torch.tensor([1.0, -emphasize_value], dtype=torch.float64, device=data.device)
    a_coeffs = torch.tensor([1, 0], dtype=torch.float64, device=data.device)
    pre_emphasized_data = torchaudio.functional.lfilter(data.to(dtype=torch.float64), a_coeffs, filter_window, clamp=False)
    return pre_emphasized_data


@DeveloperAPI
def get_num_output_padded_to_fit_input(num_input: int, window_length_in_samp: int, window_shift_in_samp: int) ->int:
    num_output_valid = torch.tensor((num_input - window_length_in_samp) / window_shift_in_samp + 1)
    return int(torch.ceil(num_output_valid))


def _preprocess_to_padded_matrix(data: torch.Tensor, window_length_in_samp: int, window_shift_in_samp: int, zero_mean_offset: bool=False) ->torch.Tensor:
    num_input = data.shape[0]
    num_output = get_num_output_padded_to_fit_input(num_input, window_length_in_samp, window_shift_in_samp)
    zero_padded_matrix = torch.zeros((num_output, window_length_in_samp), dtype=torch.float32, device=data.device)
    for num_output_idx in range(num_output):
        start_idx = window_shift_in_samp * num_output_idx
        is_last_output = num_output_idx == num_output - 1
        end_idx = start_idx + window_length_in_samp if not is_last_output else num_input
        end_padded_idx = window_length_in_samp if not is_last_output else end_idx - start_idx
        window_data = data[start_idx:end_idx]
        if zero_mean_offset:
            window_data = window_data - torch.mean(window_data)
        zero_padded_matrix[num_output_idx, :end_padded_idx] = window_data
    return zero_padded_matrix


@DeveloperAPI
def get_window(window_type: str, window_length_in_samp: int, device: Optional[torch.device]=None) ->torch.Tensor:
    if window_type == 'bartlett':
        return torch.bartlett_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    elif window_type == 'blackman':
        return torch.blackman_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    elif window_type == 'hamming':
        return torch.hamming_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    elif window_type == 'hann':
        return torch.hann_window(window_length_in_samp, periodic=False, dtype=torch.float64, device=device)
    else:
        raise ValueError(f'Unknown window type: {window_type}')


def _weight_data_matrix(data_matrix: torch.Tensor, window_type: str, data_transformation: Optional[str]=None) ->torch.Tensor:
    window_length_in_samp = data_matrix[0].shape[0]
    window = get_window(window_type, window_length_in_samp, device=data_matrix.device)
    if data_transformation is not None and data_transformation == 'group_delay':
        window *= torch.arange(window_length_in_samp, device=data_matrix.device).float()
    return data_matrix * window


@DeveloperAPI
def get_length_in_samp(sampling_rate_in_hz: Union[float, int], length_in_s: Union[float, int]) ->int:
    return int(sampling_rate_in_hz * length_in_s)


def _short_time_fourier_transform(data: torch.Tensor, sampling_rate_in_hz: int, window_length_in_s: float, window_shift_in_s: float, num_fft_points: int, window_type: str, data_transformation: Optional[str]=None, zero_mean_offset: bool=False) ->torch.Tensor:
    window_length_in_samp: int = get_length_in_samp(window_length_in_s, sampling_rate_in_hz)
    window_shift_in_samp: int = get_length_in_samp(window_shift_in_s, sampling_rate_in_hz)
    preprocessed_data_matrix = _preprocess_to_padded_matrix(data[0], window_length_in_samp, window_shift_in_samp, zero_mean_offset=zero_mean_offset)
    weighted_data_matrix = _weight_data_matrix(preprocessed_data_matrix, window_type, data_transformation=data_transformation)
    fft = torch.fft.fft(weighted_data_matrix, n=num_fft_points)
    return fft


@DeveloperAPI
def get_non_symmetric_data(data: torch.Tensor) ->torch.Tensor:
    num_fft_points = data.shape[-1]
    num_ess_fft_points = get_non_symmetric_length(num_fft_points)
    return data[:, :num_ess_fft_points]


def _get_stft(raw_data: torch.Tensor, sampling_rate_in_hz: int, window_length_in_s: float, window_shift_in_s: float, num_fft_points: int, window_type: str, data_transformation: Optional[str]=None, zero_mean_offset: bool=False) ->torch.Tensor:
    pre_emphasized_data = _pre_emphasize_data(raw_data)
    stft = _short_time_fourier_transform(pre_emphasized_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type, data_transformation, zero_mean_offset)
    non_symmetric_stft = get_non_symmetric_data(stft)
    return non_symmetric_stft


@DeveloperAPI
def get_fbank(raw_data: torch.Tensor, sampling_rate_in_hz: int, window_length_in_s: float, window_shift_in_s: float, num_fft_points: int, window_type: str, num_filter_bands: int) ->torch.Tensor:
    stft = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type, zero_mean_offset=True)
    stft_power = torch.abs(stft) ** 2
    upper_limit_freq = int(sampling_rate_in_hz / 2)
    upper_limit_mel = _convert_hz_to_mel(upper_limit_freq)
    lower_limit_mel = 0
    list_mel_points = torch.linspace(lower_limit_mel, upper_limit_mel, num_filter_bands + 2, device=raw_data.device)
    mel_fbank_matrix = _get_mel_fbank_matrix(list_mel_points, num_filter_bands, num_fft_points, sampling_rate_in_hz)
    mel_fbank_feature = torch.matmul(stft_power, torch.transpose(mel_fbank_matrix, 0, 1))
    log_mel_fbank_feature = torch.log(mel_fbank_feature + 1e-10)
    return torch.transpose(log_mel_fbank_feature, 0, 1)


@DeveloperAPI
def get_group_delay(raw_data: torch.Tensor, sampling_rate_in_hz: int, window_length_in_s: float, window_shift_in_s: float, num_fft_points: int, window_type: str):
    X_stft_transform = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type)
    Y_stft_transform = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type, data_transformation='group_delay')
    X_stft_transform_real = torch.real(X_stft_transform)
    X_stft_transform_imag = torch.imag(X_stft_transform)
    Y_stft_transform_real = torch.real(Y_stft_transform)
    Y_stft_transform_imag = torch.imag(Y_stft_transform)
    nominator = torch.multiply(X_stft_transform_real, Y_stft_transform_real) + torch.multiply(X_stft_transform_imag, Y_stft_transform_imag)
    denominator = torch.square(torch.abs(X_stft_transform))
    group_delay = torch.divide(nominator, denominator + 1e-10)
    assert not torch.isnan(group_delay).any(), 'There are NaN values in group delay'
    return torch.transpose(group_delay, 0, 1)


@DeveloperAPI
def get_max_length_stft_based(length_in_samp, window_length_in_s, window_shift_in_s, sampling_rate_in_hz):
    window_length_in_samp = get_length_in_samp(window_length_in_s, sampling_rate_in_hz)
    window_shift_in_samp = get_length_in_samp(window_shift_in_s, sampling_rate_in_hz)
    return get_num_output_padded_to_fit_input(length_in_samp, window_length_in_samp, window_shift_in_samp)


@DeveloperAPI
def get_phase_stft_magnitude(raw_data: torch.Tensor, sampling_rate_in_hz: int, window_length_in_s: float, window_shift_in_s: float, num_fft_points: int, window_type: str) ->torch.Tensor:
    stft = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type)
    abs_stft = torch.abs(stft)
    phase = torch.angle(stft)
    stft_phase = torch.cat([phase, abs_stft], dim=1)
    return torch.transpose(stft_phase, 0, 1)


@DeveloperAPI
def get_stft_magnitude(raw_data: torch.Tensor, sampling_rate_in_hz: int, window_length_in_s: float, window_shift_in_s: float, num_fft_points: int, window_type: str):
    stft = _get_stft(raw_data, sampling_rate_in_hz, window_length_in_s, window_shift_in_s, num_fft_points, window_type=window_type)
    stft_magnitude = torch.abs(stft)
    return torch.transpose(stft_magnitude, 0, 1)


@DeveloperAPI
def is_torch_audio_tuple(audio: Any) ->bool:
    if isinstance(audio, tuple):
        if len(audio) == 2 and isinstance(audio[0], torch.Tensor) and isinstance(audio[1], int):
            return True
    return False


@DeveloperAPI
@functools.lru_cache(maxsize=32)
def read_audio_from_bytes_obj(bytes_obj: bytes) ->Optional[TorchAudioTuple]:
    try:
        f = BytesIO(bytes_obj)
        return torchaudio.backend.sox_io_backend.load(f)
    except Exception as e:
        logger.warning(e)
        return None


@DeveloperAPI
def set_default_value(dictionary, key, value):
    if key not in dictionary:
        dictionary[key] = value


def error(preds: Tensor, target: Tensor) ->Tensor:
    return target - preds


def get_encoder_classes(feature: str) ->Dict[str, Type[Encoder]]:
    return get_encoder_registry()[feature]


@DeveloperAPI
def get_encoder_conds(feature_type: str):
    """Returns a JSON schema of conditionals to validate against encoder types for specific feature types."""
    conds = []
    for encoder in get_encoder_classes(feature_type):
        encoder_cls = get_encoder_cls(feature_type, encoder)
        other_props = schema_utils.unload_jsonschema_from_marshmallow_class(encoder_cls)['properties']
        schema_utils.remove_duplicate_fields(other_props)
        encoder_cond = schema_utils.create_cond({'type': encoder}, other_props)
        conds.append(encoder_cond)
    return conds


@DeveloperAPI
def EncoderDataclassField(feature_type: str, default: str):
    """Custom dataclass field that when used inside a dataclass will allow the user to specify an encoder config.

    Returns: Initialized dataclass field that converts an untyped dict with params to an encoder config.
    """


    class EncoderMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict for a valid encoder config from the encoder_registry
        and creates a corresponding `oneOf` JSON schema for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if TYPE in value and value[TYPE] in get_encoder_classes(feature_type):
                    enc = get_encoder_cls(feature_type, value[TYPE])
                    try:
                        return enc.Schema().load(value)
                    except (TypeError, ValidationError) as error:
                        raise ValidationError(f'Invalid encoder params: {value}, see `{enc}` definition. Error: {error}')
                raise ValidationError(f'Invalid params for encoder: {value}, expect dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            encoder_classes = list(get_encoder_classes(feature_type).keys())
            return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': encoder_classes, 'default': default}}, 'title': 'encoder_options', 'allOf': get_encoder_conds(feature_type)}
    try:
        encoder = get_encoder_cls(feature_type, default)
        load_default = encoder.Schema().load({'type': default})
        dump_default = encoder.Schema().dump({'type': default})
        return field(metadata={'marshmallow_field': EncoderMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default)}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f'Unsupported encoder type: {default}. See encoder_registry. Details: {e}')


@DeveloperAPI
def PreprocessingDataclassField(feature_type: str):
    """Custom dataclass field that when used inside a dataclass will allow the user to specify a preprocessing
    config.

    Returns: Initialized dataclass field that converts an untyped dict with params to a preprocessing config.
    """


    class PreprocessingMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict for a valid preprocessing config from the
        preprocessing_registry and creates a corresponding JSON schema for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if feature_type in preprocessing_registry:
                    pre = preprocessing_registry[feature_type]
                    try:
                        return pre.Schema().load(value)
                    except (TypeError, ValidationError) as error:
                        raise ValidationError(f'Invalid preprocessing params: {value}, see `{pre}` definition. Error: {error}')
                raise ValidationError(f'Invalid params for preprocessor: {value}, expect dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            preprocessor_cls = preprocessing_registry[feature_type]
            props = schema_utils.unload_jsonschema_from_marshmallow_class(preprocessor_cls)['properties']
            return {'type': 'object', 'properties': props, 'title': 'preprocessing_options', 'additionalProperties': True}
    try:
        preprocessor = preprocessing_registry[feature_type]
        load_default = preprocessor.Schema().load({'feature_type': feature_type})
        dump_default = preprocessor.Schema().dump({'feature_type': feature_type})
        return field(metadata={'marshmallow_field': PreprocessingMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default)}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f'Unsupported preprocessing type: {feature_type}. See preprocessing_registry. Details: {e}')


PADDING_SYMBOL = '<PAD>'


PREDICTIONS_SHAPES_FILE_NAME = 'predictions.shapes.json'


@DeveloperAPI
def get_fs_and_path(url):
    protocol, path = split_protocol(url)
    path = unquote(urlparse(path).path)
    path = os.fspath(pathlib.PurePosixPath(path))
    fs = fsspec.filesystem(protocol)
    return fs, path


@DeveloperAPI
def load_json(data_fp):
    with open_file(data_fp, 'r') as input_file:
        data = json.load(input_file)
    return data


@DeveloperAPI
def save_json(data_fp, data, sort_keys=True, indent=4):
    with open_file(data_fp, 'w') as output_file:
        json.dump(data, output_file, cls=NumpyEncoder, sort_keys=sort_keys, indent=indent)


def cumsum(x: List[int]) ->List[int]:
    results = []
    j = 0
    for i in range(0, len(x)):
        j += x[i]
        results.append(j)
    return results


@DeveloperAPI
def split_by_slices(slices: List[Any], n: int, probabilities: List[float]) ->List[Any]:
    splits = []
    indices = cumsum([int(x * n) for x in probabilities])
    start = 0
    for end in indices:
        splits.append(slices[start:end])
        start = end
    return splits


UNKNOWN_SYMBOL = '<UNK>'


START_SYMBOL = '<SOS>'


STOP_SYMBOL = '<EOS>'


def _get_sequence_vector(sequence, tokenizer, tokenizer_type, format_dtype, unit_to_id, lowercase=True, unknown_symbol=UNKNOWN_SYMBOL) ->np.ndarray:
    unit_sequence = tokenizer(sequence.lower() if lowercase else sequence)
    unit_indices_vector = np.empty(len(unit_sequence), dtype=format_dtype)
    for i in range(len(unit_sequence)):
        curr_unit = unit_sequence[i]
        if tokenizer_type == 'hf_tokenizer':
            unit_indices_vector[i] = curr_unit
        elif curr_unit in unit_to_id:
            unit_indices_vector[i] = unit_to_id[curr_unit]
        else:
            unit_indices_vector[i] = unit_to_id[unknown_symbol]
    if tokenizer_type != 'hf_tokenizer':
        unit_indices_vector = np.append(unit_indices_vector, unit_to_id[STOP_SYMBOL])
        unit_indices_vector = np.insert(unit_indices_vector, 0, unit_to_id[START_SYMBOL])
    return unit_indices_vector


DICT_FORMATS = {'dict', 'dictionary', dict}


def convert_to_df(predictions, output_features, backend: Optional['Backend']=None):
    return predictions


DASK_MODULE_NAME = 'dask.dataframe'


@DeveloperAPI
def is_dask_lib(df_lib) ->bool:
    """Returns whether the dataframe library is dask."""
    return df_lib.__name__ == DASK_MODULE_NAME


@DeveloperAPI
def is_dask_backend(backend: Optional['Backend']) ->bool:
    """Returns whether the backend's dataframe is dask."""
    return backend is not None and is_dask_lib(backend.df_engine.df_lib)


TORCHTEXT_0_12_0_TOKENIZERS = {'sentencepiece', 'clip', 'gpt2bpe'}


TORCHTEXT_0_13_0_TOKENIZERS = {'bert'}


class BaseTokenizer:

    @abstractmethod
    def __init__(self, **kwargs):
        pass

    @abstractmethod
    def __call__(self, text: str):
        pass


class CharactersToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return [char for char in text]


language_module_registry = {'en': 'en_core_web_sm', 'it': 'it_core_news_sm', 'es': 'es_core_news_sm', 'de': 'de_core_news_sm', 'fr': 'fr_core_news_sm', 'pt': 'pt_core_news_sm', 'nl': 'nl_core_news_sm', 'el': 'el_core_news_sm', 'nb': 'nb_core_news_sm', 'lt': 'lt_core_news_sm', 'da': 'da_core_news_sm', 'pl': 'pl_core_news_sm', 'ro': 'ro_core_news_sm', 'ja': 'ja_core_news_sm', 'zh': 'zh_core_web_sm', 'xx': 'xx_ent_wiki_sm'}


punctuation = {'.', ',', '@', '$', '%', '/', ':', ';', '+', '='}


def pass_filters(token, filter_numbers=False, filter_punctuation=False, filter_short_tokens=False, filter_stopwords=False):
    passes_filters = True
    if filter_numbers:
        passes_filters = not token.like_num
    if passes_filters and filter_punctuation:
        passes_filters = not bool(set(token.orth_) & punctuation)
    if passes_filters and filter_short_tokens:
        passes_filters = len(token) > 2
    if passes_filters and filter_stopwords:
        passes_filters = not token.is_stop
    return passes_filters


def process_text(text, nlp_pipeline, return_lemma=False, filter_numbers=False, filter_punctuation=False, filter_short_tokens=False, filter_stopwords=False):
    doc = nlp_pipeline(text)
    return [(token.lemma_ if return_lemma else token.text) for token in doc if pass_filters(token, filter_numbers, filter_punctuation, filter_short_tokens, filter_stopwords)]


class ChineseFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ChineseLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ChineseLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), return_lemma=True, filter_stopwords=True)


class ChineseLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), return_lemma=True)


class ChineseRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'), filter_stopwords=True)


class ChineseTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('zh'))


COMMA_REGEX = re.compile('\\s*,\\s*')


class CommaStringToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return COMMA_REGEX.split(text.strip())


class DanishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DanishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DanishLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), return_lemma=True, filter_stopwords=True)


class DanishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), return_lemma=True)


class DanishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'), filter_stopwords=True)


class DanishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('da'))


class DutchFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DutchLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class DutchLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), return_lemma=True, filter_stopwords=True)


class DutchLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), return_lemma=True)


class DutchRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'), filter_stopwords=True)


class DutchTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nl'))


class EnglishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class EnglishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class EnglishLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), return_lemma=True, filter_stopwords=True)


class EnglishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        process_text(text, load_nlp_pipeline('en'), return_lemma=True)


class EnglishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'), filter_stopwords=True)


class EnglishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('en'))


class FrenchFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class FrenchLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class FrenchLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), return_lemma=True, filter_stopwords=True)


class FrenchLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), return_lemma=True)


class FrenchRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'), filter_stopwords=True)


class FrenchTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('fr'))


class GermanFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GermanLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GermanLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), return_lemma=True, filter_stopwords=True)


class GermanLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), return_lemma=True)


class GermanRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'), filter_stopwords=True)


class GermanTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('de'))


class GreekFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GreekLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class GreekLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), return_lemma=True, filter_stopwords=True)


class GreekLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), return_lemma=True)


class GreekRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'), filter_stopwords=True)


class GreekTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('el'))


class ItalianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ItalianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class ItalianLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), return_lemma=True, filter_stopwords=True)


class ItalianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), return_lemma=True)


class ItalianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'), filter_stopwords=True)


class ItalianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('it'))


class JapaneseFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class JapaneseLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class JapaneseLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), return_lemma=True, filter_stopwords=True)


class JapaneseLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), return_lemma=True)


class JapaneseRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'), filter_stopwords=True)


class JapaneseTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('jp'))


class LithuanianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class LithuanianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class LithuanianLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), return_lemma=True, filter_stopwords=True)


class LithuanianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), return_lemma=True)


class LithuanianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'), filter_stopwords=True)


class LithuanianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('lt'))


class MultiFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class MultiLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class MultiLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), return_lemma=True, filter_stopwords=True)


class MultiLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), return_lemma=True)


class MultiRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), filter_stopwords=True)


class MultiTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('xx'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class SpaceStringToListTokenizer(torch.nn.Module):
    """Implements torchscript-compatible whitespace tokenization."""

    def __init__(self, **kwargs):
        super().__init__()

    def forward(self, v: Union[str, List[str], torch.Tensor]) ->Any:
        if isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        inputs: List[str] = []
        if isinstance(v, str):
            inputs.append(v)
        else:
            inputs.extend(v)
        tokens: List[List[str]] = []
        for sequence in inputs:
            split_sequence = sequence.strip().split(' ')
            token_sequence: List[str] = []
            for token in self.get_tokens(split_sequence):
                if len(token) > 0:
                    token_sequence.append(token)
            tokens.append(token_sequence)
        return tokens[0] if isinstance(v, str) else tokens

    def get_tokens(self, tokens: List[str]) ->List[str]:
        return tokens


class NgramTokenizer(SpaceStringToListTokenizer):
    """Implements torchscript-compatible n-gram tokenization."""

    def __init__(self, ngram_size: int=2, **kwargs):
        super().__init__()
        self.n = ngram_size or 2

    def get_tokens(self, tokens: List[str]) ->List[str]:
        from torchtext.data.utils import ngrams_iterator
        return list(ngrams_iterator(tokens, ngrams=self.n))


class NorwegianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class NorwegianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class NorwegianLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), return_lemma=True, filter_stopwords=True)


class NorwegianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), return_lemma=True)


class NorwegianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'), filter_stopwords=True)


class NorwegianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('nb'))


class PolishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PolishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PolishLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), return_lemma=True, filter_stopwords=True)


class PolishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), return_lemma=True)


class PolishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'), filter_stopwords=True)


class PolishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pl'))


class PortugueseFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PortugueseLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class PortugueseLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), return_lemma=True, filter_stopwords=True)


class PortugueseLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), return_lemma=True)


class PortugueseRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'), filter_stopwords=True)


class PortugueseTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('pt'))


class RomanianFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class RomanianLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class RomanianLemmatizeRemoveStopwordsFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), return_lemma=True, filter_stopwords=True)


class RomanianLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), return_lemma=True)


class RomanianRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'), filter_stopwords=True)


class RomanianTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('ro'))


class SpacePunctuationStringToListTokenizer(torch.nn.Module):
    """Implements torchscript-compatible space_punct tokenization."""

    def __init__(self, **kwargs):
        super().__init__()

    def is_regex_w(self, c: str) ->bool:
        return c.isalnum() or c == '_'

    def forward(self, v: Union[str, List[str], torch.Tensor]) ->Any:
        if isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        inputs: List[str] = []
        if isinstance(v, str):
            inputs.append(v)
        else:
            inputs.extend(v)
        tokens: List[List[str]] = []
        for sequence in inputs:
            token_sequence: List[str] = []
            word: List[str] = []
            for c in sequence:
                if self.is_regex_w(c):
                    word.append(c)
                elif len(word) > 0:
                    token_sequence.append(''.join(word))
                    word.clear()
                if not self.is_regex_w(c) and not c.isspace():
                    token_sequence.append(c)
            if len(word) > 0:
                token_sequence.append(''.join(word))
            tokens.append(token_sequence)
        return tokens[0] if isinstance(v, str) else tokens


class SpanishFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class SpanishLemmatizeFilterTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), return_lemma=True, filter_numbers=True, filter_punctuation=True, filter_short_tokens=True)


class SpanishLemmatizeRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), return_lemma=True, filter_stopwords=True)


class SpanishLemmatizeTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), return_lemma=True)


class SpanishRemoveStopwordsTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'), filter_stopwords=True)


class SpanishTokenizer(BaseTokenizer):

    def __call__(self, text):
        return process_text(text, load_nlp_pipeline('es'))


class StrippedStringToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return [text.strip()]


UNDERSCORE_REGEX = re.compile('\\s*_\\s*')


class UnderscoreStringToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return UNDERSCORE_REGEX.split(text.strip())


class UntokenizedStringToListTokenizer(BaseTokenizer):

    def __call__(self, text):
        return [text]


tokenizer_registry = {'space': SpaceStringToListTokenizer, 'space_punct': SpacePunctuationStringToListTokenizer, 'ngram': NgramTokenizer, 'characters': CharactersToListTokenizer, 'underscore': UnderscoreStringToListTokenizer, 'comma': CommaStringToListTokenizer, 'untokenized': UntokenizedStringToListTokenizer, 'stripped': StrippedStringToListTokenizer, 'english_tokenize': EnglishTokenizer, 'english_tokenize_filter': EnglishFilterTokenizer, 'english_tokenize_remove_stopwords': EnglishRemoveStopwordsTokenizer, 'english_lemmatize': EnglishLemmatizeTokenizer, 'english_lemmatize_filter': EnglishLemmatizeFilterTokenizer, 'english_lemmatize_remove_stopwords': EnglishLemmatizeRemoveStopwordsTokenizer, 'italian_tokenize': ItalianTokenizer, 'italian_tokenize_filter': ItalianFilterTokenizer, 'italian_tokenize_remove_stopwords': ItalianRemoveStopwordsTokenizer, 'italian_lemmatize': ItalianLemmatizeTokenizer, 'italian_lemmatize_filter': ItalianLemmatizeFilterTokenizer, 'italian_lemmatize_remove_stopwords': ItalianLemmatizeRemoveStopwordsTokenizer, 'spanish_tokenize': SpanishTokenizer, 'spanish_tokenize_filter': SpanishFilterTokenizer, 'spanish_tokenize_remove_stopwords': SpanishRemoveStopwordsTokenizer, 'spanish_lemmatize': SpanishLemmatizeTokenizer, 'spanish_lemmatize_filter': SpanishLemmatizeFilterTokenizer, 'spanish_lemmatize_remove_stopwords': SpanishLemmatizeRemoveStopwordsTokenizer, 'german_tokenize': GermanTokenizer, 'german_tokenize_filter': GermanFilterTokenizer, 'german_tokenize_remove_stopwords': GermanRemoveStopwordsTokenizer, 'german_lemmatize': GermanLemmatizeTokenizer, 'german_lemmatize_filter': GermanLemmatizeFilterTokenizer, 'german_lemmatize_remove_stopwords': GermanLemmatizeRemoveStopwordsTokenizer, 'french_tokenize': FrenchTokenizer, 'french_tokenize_filter': FrenchFilterTokenizer, 'french_tokenize_remove_stopwords': FrenchRemoveStopwordsTokenizer, 'french_lemmatize': FrenchLemmatizeTokenizer, 'french_lemmatize_filter': FrenchLemmatizeFilterTokenizer, 'french_lemmatize_remove_stopwords': FrenchLemmatizeRemoveStopwordsTokenizer, 'portuguese_tokenize': PortugueseTokenizer, 'portuguese_tokenize_filter': PortugueseFilterTokenizer, 'portuguese_tokenize_remove_stopwords': PortugueseRemoveStopwordsTokenizer, 'portuguese_lemmatize': PortugueseLemmatizeTokenizer, 'portuguese_lemmatize_filter': PortugueseLemmatizeFilterTokenizer, 'portuguese_lemmatize_remove_stopwords': PortugueseLemmatizeRemoveStopwordsTokenizer, 'dutch_tokenize': DutchTokenizer, 'dutch_tokenize_filter': DutchFilterTokenizer, 'dutch_tokenize_remove_stopwords': DutchRemoveStopwordsTokenizer, 'dutch_lemmatize': DutchLemmatizeTokenizer, 'dutch_lemmatize_filter': DutchLemmatizeFilterTokenizer, 'dutch_lemmatize_remove_stopwords': DutchLemmatizeRemoveStopwordsTokenizer, 'greek_tokenize': GreekTokenizer, 'greek_tokenize_filter': GreekFilterTokenizer, 'greek_tokenize_remove_stopwords': GreekRemoveStopwordsTokenizer, 'greek_lemmatize': GreekLemmatizeTokenizer, 'greek_lemmatize_filter': GreekLemmatizeFilterTokenizer, 'greek_lemmatize_remove_stopwords': GreekLemmatizeRemoveStopwordsFilterTokenizer, 'norwegian_tokenize': NorwegianTokenizer, 'norwegian_tokenize_filter': NorwegianFilterTokenizer, 'norwegian_tokenize_remove_stopwords': NorwegianRemoveStopwordsTokenizer, 'norwegian_lemmatize': NorwegianLemmatizeTokenizer, 'norwegian_lemmatize_filter': NorwegianLemmatizeFilterTokenizer, 'norwegian_lemmatize_remove_stopwords': NorwegianLemmatizeRemoveStopwordsFilterTokenizer, 'lithuanian_tokenize': LithuanianTokenizer, 'lithuanian_tokenize_filter': LithuanianFilterTokenizer, 'lithuanian_tokenize_remove_stopwords': LithuanianRemoveStopwordsTokenizer, 'lithuanian_lemmatize': LithuanianLemmatizeTokenizer, 'lithuanian_lemmatize_filter': LithuanianLemmatizeFilterTokenizer, 'lithuanian_lemmatize_remove_stopwords': LithuanianLemmatizeRemoveStopwordsFilterTokenizer, 'danish_tokenize': DanishTokenizer, 'danish_tokenize_filter': DanishFilterTokenizer, 'danish_tokenize_remove_stopwords': DanishRemoveStopwordsTokenizer, 'danish_lemmatize': DanishLemmatizeTokenizer, 'danish_lemmatize_filter': DanishLemmatizeFilterTokenizer, 'danish_lemmatize_remove_stopwords': DanishLemmatizeRemoveStopwordsFilterTokenizer, 'polish_tokenize': PolishTokenizer, 'polish_tokenize_filter': PolishFilterTokenizer, 'polish_tokenize_remove_stopwords': PolishRemoveStopwordsTokenizer, 'polish_lemmatize': PolishLemmatizeTokenizer, 'polish_lemmatize_filter': PolishLemmatizeFilterTokenizer, 'polish_lemmatize_remove_stopwords': PolishLemmatizeRemoveStopwordsFilterTokenizer, 'romanian_tokenize': RomanianTokenizer, 'romanian_tokenize_filter': RomanianFilterTokenizer, 'romanian_tokenize_remove_stopwords': RomanianRemoveStopwordsTokenizer, 'romanian_lemmatize': RomanianLemmatizeTokenizer, 'romanian_lemmatize_filter': RomanianLemmatizeFilterTokenizer, 'romanian_lemmatize_remove_stopwords': RomanianLemmatizeRemoveStopwordsFilterTokenizer, 'japanese_tokenize': JapaneseTokenizer, 'japanese_tokenize_filter': JapaneseFilterTokenizer, 'japanese_tokenize_remove_stopwords': JapaneseRemoveStopwordsTokenizer, 'japanese_lemmatize': JapaneseLemmatizeTokenizer, 'japanese_lemmatize_filter': JapaneseLemmatizeFilterTokenizer, 'japanese_lemmatize_remove_stopwords': JapaneseLemmatizeRemoveStopwordsFilterTokenizer, 'chinese_tokenize': ChineseTokenizer, 'chinese_tokenize_filter': ChineseFilterTokenizer, 'chinese_tokenize_remove_stopwords': ChineseRemoveStopwordsTokenizer, 'chinese_lemmatize': ChineseLemmatizeTokenizer, 'chinese_lemmatize_filter': ChineseLemmatizeFilterTokenizer, 'chinese_lemmatize_remove_stopwords': ChineseLemmatizeRemoveStopwordsFilterTokenizer, 'multi_tokenize': MultiTokenizer, 'multi_tokenize_filter': MultiFilterTokenizer, 'multi_tokenize_remove_stopwords': MultiRemoveStopwordsTokenizer, 'multi_lemmatize': MultiLemmatizeTokenizer, 'multi_lemmatize_filter': MultiLemmatizeFilterTokenizer, 'multi_lemmatize_remove_stopwords': MultiLemmatizeRemoveStopwordsTokenizer}


def get_tokenizer_from_registry(tokenizer_name: str) ->torch.nn.Module:
    """Returns the appropriate tokenizer from the tokenizer registry.

    Raises a KeyError if a tokenizer that does not exist in the registry is requested, with additional help text if the
    requested tokenizer would be available for a different version of torchtext.
    """
    if tokenizer_name in tokenizer_registry:
        return tokenizer_registry[tokenizer_name]
    if torch.torch_version.TorchVersion(torchtext.__version__) < (0, 12, 0) and tokenizer_name in TORCHTEXT_0_12_0_TOKENIZERS:
        raise KeyError(f"torchtext>=0.12.0 is not installed, so '{tokenizer_name}' and the following tokenizers are not available: {TORCHTEXT_0_12_0_TOKENIZERS}")
    if torch.torch_version.TorchVersion(torchtext.__version__) < (0, 13, 0) and tokenizer_name in TORCHTEXT_0_13_0_TOKENIZERS:
        raise KeyError(f"torchtext>=0.13.0 is not installed, so '{tokenizer_name}' and the following tokenizers are not available: {TORCHTEXT_0_13_0_TOKENIZERS}")
    raise KeyError(f"Invalid tokenizer name: '{tokenizer_name}'. Available tokenizers: {tokenizer_registry.keys()}")


def int_type(number):
    if number <= np.iinfo(np.int8).max:
        return np.int8
    elif number <= np.iinfo(np.int16).max:
        return np.int16
    elif number <= np.iinfo(np.int32).max:
        return np.int32
    else:
        return np.int64


def add_or_move_symbol(vocab_list: List[str], vocab_set: Set[str], symbol: str, index: int):
    """Inserts or moves the symbol to the specified index."""
    if symbol in vocab_set:
        vocab_list.remove(symbol)
    vocab_list.insert(index, symbol)


def load_vocabulary(vocab_file):
    with open_file(vocab_file, 'r', encoding='utf-8') as f:
        vocabulary = []
        for line in f:
            line = line.strip()
            if ' ' in line:
                line = line.split(' ')[0]
            vocabulary.append(line)
        return vocabulary


TorchscriptPreprocessingInput = Union[List[str], List[torch.Tensor], List[TorchAudioTuple], torch.Tensor]


class _SequencePreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by SequenceFeatureMixin.add_feature_data."""

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.lowercase = metadata['preprocessing']['lowercase']
        self.tokenizer_type = metadata['preprocessing']['tokenizer']
        self.tokenizer = get_tokenizer_from_registry(self.tokenizer_type)(pretrained_model_name_or_path=metadata['preprocessing'].get('pretrained_model_name_or_path', None))
        if not isinstance(self.tokenizer, torch.nn.Module):
            raise ValueError(f'tokenizer must be a torch.nn.Module, got {self.tokenizer}')
        self.padding_symbol = metadata['preprocessing']['padding_symbol']
        self.unknown_symbol = metadata['preprocessing']['unknown_symbol']
        self.start_symbol = START_SYMBOL
        self.stop_symbol = STOP_SYMBOL
        self.max_sequence_length = int(metadata['max_sequence_length'])
        self.unit_to_id = metadata['str2idx']
        self.computed_fill_value = metadata['preprocessing']['computed_fill_value']

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        """Takes a list of strings and returns a tensor of token ids."""
        if not torch.jit.isinstance(v, List[str]):
            raise ValueError(f'Unsupported input: {v}')
        futures: List[torch.jit.Future[torch.Tensor]] = []
        for sequence in v:
            futures.append(torch.jit.fork(self._process_sequence, sequence))
        sequence_matrix = []
        for future in futures:
            sequence_matrix.append(torch.jit.wait(future))
        return torch.stack(sequence_matrix)

    def _process_sequence(self, sequence: str) ->torch.Tensor:
        sequence = self.computed_fill_value if sequence == 'nan' else sequence
        if self.lowercase and self.tokenizer_type != 'hf_tokenizer':
            sequence_str: str = sequence.lower()
        else:
            sequence_str: str = sequence
        sequence_vector = torch.full([self.max_sequence_length], self.unit_to_id[self.padding_symbol])
        if self.tokenizer_type == 'hf_tokenizer':
            unit_sequence = self.tokenizer(sequence)
            assert torch.jit.isinstance(unit_sequence, List[int])
            sequence_length = min(len(unit_sequence), self.max_sequence_length)
            sequence_vector[:sequence_length] = torch.tensor(unit_sequence)[:sequence_length]
            return sequence_vector
        unit_sequence = self.tokenizer(sequence_str)
        assert torch.jit.isinstance(unit_sequence, List[str])
        sequence_vector[0] = self.unit_to_id[self.start_symbol]
        if len(unit_sequence) + 1 < self.max_sequence_length:
            sequence_length = len(unit_sequence)
            sequence_vector[len(unit_sequence) + 1] = self.unit_to_id[self.stop_symbol]
        else:
            sequence_length = self.max_sequence_length - 1
        for i in range(sequence_length):
            curr_unit = unit_sequence[i]
            if curr_unit in self.unit_to_id:
                curr_id = self.unit_to_id[curr_unit]
            else:
                curr_id = self.unit_to_id[self.unknown_symbol]
            sequence_vector[i + 1] = curr_id
        return sequence_vector


class _AudioPreprocessing(torch.nn.Module):
    audio_feature_dict: Dict[str, Union[float, int, str]]

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.audio_feature_dict = {key: value for key, value in metadata['preprocessing'].items() if key in AUDIO_FEATURE_KEYS and value is not None}
        self.feature_dim = metadata['feature_dim']
        self.max_length = metadata['max_length']
        self.padding_value = metadata['preprocessing']['padding_value']
        self.normalization_type = metadata['preprocessing']['norm']

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if not torch.jit.isinstance(v, List[Tuple[torch.Tensor, int]]):
            raise ValueError(f'Unsupported input: {v}')
        processed_audio_matrix = []
        for audio, sampling_rate_in_hz in v:
            processed_audio = AudioFeatureMixin._transform_to_feature(audio, sampling_rate_in_hz, self.audio_feature_dict, self.feature_dim, self.max_length, self.padding_value, self.normalization_type)
            processed_audio_matrix.append(processed_audio)
        return torch.stack(processed_audio_matrix)


def set_str_to_idx(set_string, feature_dict, tokenizer_name):
    try:
        tokenizer = get_tokenizer_from_registry(tokenizer_name)()
    except ValueError:
        raise Exception(f'Tokenizer {tokenizer_name} not supported')
    out = [feature_dict.get(item, feature_dict[UNKNOWN_SYMBOL]) for item in tokenizer(set_string)]
    return np.array(out, dtype=np.int32)


TORCHSCRIPT_COMPATIBLE_TOKENIZERS = {'space', 'space_punct'}


class _SetPreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by SetFeatureMixin.add_feature_data.

    If is_bag is true, forward returns a vector for each sample indicating counts of each token. Else, forward returns a
    multi-hot vector for each sample indicating presence of each token.
    """

    def __init__(self, metadata: TrainingSetMetadataDict, is_bag: bool=False):
        super().__init__()
        if metadata['preprocessing']['tokenizer'] not in TORCHSCRIPT_COMPATIBLE_TOKENIZERS:
            raise ValueError(f"{metadata['preprocessing']['tokenizer']} is not supported by torchscript. Please use one of {TORCHSCRIPT_COMPATIBLE_TOKENIZERS}.")
        self.lowercase = metadata['preprocessing']['lowercase']
        self.tokenizer = get_tokenizer_from_registry(metadata['preprocessing']['tokenizer'])()
        self.vocab_size = metadata['vocab_size']
        self.unknown_symbol = UNKNOWN_SYMBOL
        self.unit_to_id = metadata['str2idx']
        self.is_bag = is_bag

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        """Takes a list of strings and returns a tensor of counts for each token."""
        if not torch.jit.isinstance(v, List[str]):
            raise ValueError(f'Unsupported input: {v}')
        if self.lowercase:
            sequences = [sequence.lower() for sequence in v]
        else:
            sequences = v
        unit_sequences = self.tokenizer(sequences)
        assert torch.jit.isinstance(unit_sequences, List[List[str]]), 'unit_sequences is not a list of lists.'
        set_matrix = torch.zeros(len(unit_sequences), self.vocab_size, dtype=torch.float32)
        for sample_idx, unit_sequence in enumerate(unit_sequences):
            sequence_length = len(unit_sequence)
            for i in range(sequence_length):
                curr_unit = unit_sequence[i]
                if curr_unit in self.unit_to_id:
                    curr_id = self.unit_to_id[curr_unit]
                else:
                    curr_id = self.unit_to_id[self.unknown_symbol]
                if self.is_bag:
                    set_matrix[sample_idx][curr_id] += 1
                else:
                    set_matrix[sample_idx][curr_id] = 1
        return set_matrix


def PublicAPI(*args, **kwargs):
    """Annotation for documenting public APIs. Public APIs are classes and methods exposed to end users of Ludwig.

    If stability="stable", the APIs will remain backwards compatible across minor Ludwig releases
    (e.g., Ludwig 0.6 -> Ludwig 0.7).

    If stability="experimental", the APIs can be used by advanced users who are tolerant to and expect
    breaking changes. This will likely be seen in the case of incremental new feature development.

    Args:
        stability: One of {"stable", "experimental"}

    Examples:
        >>> from api_annotations import PublicAPI
        >>> @PublicAPI
        ... def func1(x):
        ...     return x
        >>> @PublicAPI(stability="experimental")
        ... def func2(y):
        ...     return y
    """
    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):
        return PublicAPI(stability='stable')(args[0])
    if 'stability' in kwargs:
        stability = kwargs['stability']
        assert stability in ['stable', 'experimental'], stability
    elif kwargs:
        raise ValueError(f'Unknown kwargs: {kwargs.keys()}')
    else:
        stability = 'stable'

    def wrap(obj):
        if stability == 'experimental':
            message = f'PublicAPI ({stability}): This API is {stability} and may change before becoming stable.'
        else:
            message = 'PublicAPI: This API is stable across Ludwig releases.'
        _append_doc(obj, message=message)
        _mark_annotated(obj)
        return obj
    return wrap


@PublicAPI
class LudwigError(Exception):
    """Base class for all custom exceptions raised by the Ludwig framework."""
    pass


@PublicAPI
class InputDataError(LudwigError, ValueError):
    """Exception raised for errors in the input data.

    Appropriate for data which is not convertible to the input feature type, columns with all missing values,
    categorical columns with only one category, etc...

    Attributes:
        column - The name of the input column which caused the error
        feature_type - The Ludwig feature type which caused the error (number, binary, category...).
        message - An error message describing the situation.
    """

    def __init__(self, column_name: str, feature_type: str, message: str):
        self.column_name = column_name
        self.feature_type = feature_type
        self.message = message
        super().__init__(message)

    def __str__(self):
        return f'Column "{self.column_name}" as {self.feature_type} feature: {self.message}'

    def __reduce__(self):
        return type(self), (self.column_name, self.feature_type, self.message)


class _BinaryPreprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        str2bool = metadata.get('str2bool')
        self.str2bool = str2bool or {v: (True) for v in strings_utils.BOOL_TRUE_STRS}
        self.should_lower = str2bool is None

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if torch.jit.isinstance(v, List[Tuple[torch.Tensor, int]]):
            raise ValueError(f'Unsupported input: {v}')
        if torch.jit.isinstance(v, List[torch.Tensor]):
            v = torch.stack(v)
        if torch.jit.isinstance(v, torch.Tensor):
            return v
        v = [s.strip() for s in v]
        if self.should_lower:
            v = [s.lower() for s in v]
        indices = [self.str2bool.get(s, False) for s in v]
        return torch.tensor(indices, dtype=torch.float32)


class _CategoryPreprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.str2idx = metadata['str2idx']
        if UNKNOWN_SYMBOL in self.str2idx:
            self.unk = self.str2idx[UNKNOWN_SYMBOL]
        else:
            self.unk = 0

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if not torch.jit.isinstance(v, List[str]):
            raise ValueError(f'Unsupported input: {v}')
        indices = [self.str2idx.get(s.strip(), self.unk) for s in v]
        return torch.tensor(indices, dtype=torch.int32)


DATE_VECTOR_LENGTH = 9


@DeveloperAPI
def create_vector_from_datetime_obj(datetime_obj):
    yearday = datetime_obj.toordinal() - date(datetime_obj.year, 1, 1).toordinal() + 1
    midnight = datetime_obj.replace(hour=0, minute=0, second=0, microsecond=0)
    second_of_day = (datetime_obj - midnight).seconds
    return [datetime_obj.year, datetime_obj.month, datetime_obj.day, datetime_obj.weekday(), yearday, datetime_obj.hour, datetime_obj.minute, datetime_obj.second, second_of_day]


class _DatePreprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if torch.jit.isinstance(v, List[torch.Tensor]):
            v = torch.stack(v)
        if torch.jit.isinstance(v, torch.Tensor):
            return v
        else:
            raise ValueError(f'Unsupported input: {v}')


H3_PADDING_VALUE = 7


MAX_H3_RESOLUTION = 15


def bitslice(x: int, start_bit: int, slice_length: int) ->int:
    ones_mask: int = int(2 ** slice_length - 1)
    return (x & ones_mask << start_bit) >> start_bit


def h3_base_cell(h3_long: int) ->int:
    return bitslice(h3_long, 64 - 19, 7)


def h3_component(h3_long: int, i: int) ->int:
    return bitslice(h3_long, 64 - 19 - 3 * i, 3)


def h3_resolution(h3_long: int) ->int:
    return bitslice(h3_long, 64 - 12, 4)


def h3_components(h3_long: int) ->List[int]:
    return [h3_component(h3_long, i) for i in range(1, h3_resolution(h3_long) + 1)]


def h3_edge(h3_long: int) ->int:
    return bitslice(h3_long, 64 - 8, 3)


def h3_index_mode(h3_long: int) ->int:
    return bitslice(h3_long, 64 - 5, 4)


H3_VECTOR_LENGTH = MAX_H3_RESOLUTION + 4


class _H3Preprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.max_h3_resolution = MAX_H3_RESOLUTION
        self.h3_padding_value = H3_PADDING_VALUE
        self.computed_fill_value = float(metadata['preprocessing']['computed_fill_value'])

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if torch.jit.isinstance(v, List[torch.Tensor]):
            v = torch.stack(v)
        if not torch.jit.isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        v = torch.nan_to_num(v, nan=self.computed_fill_value)
        v = v.long()
        outputs: List[torch.Tensor] = []
        for v_i in v:
            components = h3_to_components(v_i)
            header: List[int] = [components.mode, components.edge, components.resolution, components.base_cell]
            cells_padding: List[int] = [self.h3_padding_value] * (self.max_h3_resolution - len(components.cells))
            output = torch.tensor(header + components.cells + cells_padding, dtype=torch.uint8, device=v.device)
            outputs.append(output)
        return torch.stack(outputs)


CHECKSUM = 'checksum'


ENCODER = 'encoder'


HEIGHT = 'height'


INFER_IMAGE_DIMENSIONS = 'infer_image_dimensions'


INFER_IMAGE_MAX_HEIGHT = 'infer_image_max_height'


INFER_IMAGE_MAX_WIDTH = 'infer_image_max_width'


INFER_IMAGE_SAMPLE_SIZE = 'infer_image_sample_size'


NUM_CHANNELS = 'num_channels'


REQUIRES_EQUAL_DIMENSIONS = 'requires_equal_dimensions'


TRAINING = 'training'


WIDTH = 'width'


@DeveloperAPI
def get_gray_default_image(num_channels: int, height: int, width: int) ->np.ndarray:
    return np.full((num_channels, height, width), 128, dtype=np.uint8)


@DeveloperAPI
def grayscale(img: torch.Tensor) ->torch.Tensor:
    """Grayscales RGB image."""
    return F.rgb_to_grayscale(img)


@DeveloperAPI
def num_channels_in_image(img: torch.Tensor):
    """Returns number of channels in image."""
    if img is None or img.ndim < 2:
        raise ValueError('Invalid image data')
    if img.ndim == 2:
        return 1
    else:
        return img.shape[0]


@DeveloperAPI
def get_image_read_mode_from_num_channels(num_channels: int) ->ImageReadMode:
    """Returns the torchvision.io.ImageReadMode corresponding to the number of channels.

    If num_channels is not recognized, returns ImageReadMode.UNCHANGED.
    """
    mode = ImageReadMode.UNCHANGED
    if num_channels == 1:
        mode = ImageReadMode.GRAY
    elif num_channels == 2:
        mode = ImageReadMode.GRAY_ALPHA
    elif num_channels == 3:
        mode = ImageReadMode.RGB
    elif num_channels == 4:
        mode = ImageReadMode.RGB_ALPHA
    return mode


@DeveloperAPI
def read_image_as_numpy(bytes_obj: Optional[bytes]=None) ->Optional[torch.Tensor]:
    """Reads image from bytes object from a numpy file."""
    try:
        with BytesIO(bytes_obj) as buffer:
            image = np.load(buffer)
            return torch.from_numpy(image)
    except Exception as e:
        warnings.warn(f'Failed to read image from numpy file. Original exception: {e}')
        return None


@DeveloperAPI
def read_image_as_png(bytes_obj: Optional[bytes]=None, mode: ImageReadMode=ImageReadMode.UNCHANGED) ->Optional[torch.Tensor]:
    """Reads image from bytes object from a PNG file."""
    try:
        with BytesIO(bytes_obj) as buffer:
            buffer_view = buffer.getbuffer()
            if len(buffer_view) == 0:
                del buffer_view
                raise Exception('Bytes object is empty. This could be due to a failed load from storage.')
            image = decode_image(torch.frombuffer(buffer_view, dtype=torch.uint8), mode=mode)
            del buffer_view
            return image
    except Exception as e:
        warnings.warn(f'Failed to read image from PNG file. Original exception: {e}')
        return None


@DeveloperAPI
def read_image_from_bytes_obj(bytes_obj: Optional[bytes]=None, num_channels: Optional[int]=None) ->Optional[torch.Tensor]:
    """Tries to read image as a tensor from the path.

    If the path is not decodable as a PNG, attempts to read as a numpy file. If neither of these work, returns None.
    """
    mode = get_image_read_mode_from_num_channels(num_channels)
    image = read_image_as_png(bytes_obj, mode)
    if image is None:
        image = read_image_as_numpy(bytes_obj)
    if image is None:
        warnings.warn('Unable to read image from bytes object.')
    return image


@DeveloperAPI
def upgrade_http(urlpath):
    protocol, url = split_protocol(urlpath)
    if protocol == 'http':
        return 'https://' + url
    return None


@DeveloperAPI
@functools.lru_cache(maxsize=32)
def get_bytes_obj_from_http_path(path: str) ->bytes:
    resp = stream_http_get_request(path)
    if resp.status == 404:
        upgraded = upgrade_http(path)
        if upgraded:
            logger.info(f'reading url {path} failed. upgrading to https and retrying')
            return get_bytes_obj_from_http_path(upgraded)
        else:
            raise urllib3.exceptions.HTTPError(f'reading url {path} failed and cannot be upgraded to https')
    data = b''
    for chunk in resp.stream(1024):
        data += chunk
    return data


@DeveloperAPI
def is_http(urlpath):
    protocol, _ = split_protocol(urlpath)
    return protocol == 'http' or protocol == 'https'


@DeveloperAPI
@functools.lru_cache(maxsize=32)
def get_bytes_obj_from_path(path: str) ->Optional[bytes]:
    if is_http(path):
        try:
            return get_bytes_obj_from_http_path(path)
        except Exception as e:
            logger.warning(e)
            return None
    else:
        try:
            with open_file(path) as f:
                return f.read()
        except OSError as e:
            logger.warning(e)
            return None


@DeveloperAPI
def read_image_from_path(path: str, num_channels: Optional[int]=None, return_num_bytes=False) ->Union[Optional[torch.Tensor], Tuple[Optional[torch.Tensor], int]]:
    """Reads image from path.

    Useful for reading from a small number of paths. For more intensive reads, use backend.read_binary_files instead. If
    `return_num_bytes` is True, returns a tuple of (image, num_bytes).
    """
    bytes_obj = get_bytes_obj_from_path(path)
    image = read_image_from_bytes_obj(bytes_obj, num_channels)
    if return_num_bytes:
        if bytes_obj is not None:
            num_bytes = len(bytes_obj)
        else:
            num_bytes = None
        return image, num_bytes
    else:
        return image


CROP_OR_PAD = 'crop_or_pad'


INTERPOLATE = 'interpolate'


@DeveloperAPI
def to_tuple(v: Union[int, Tuple[int, int]]) ->Tuple[int, int]:
    """Converts int or tuple to tuple of ints."""
    if torch.jit.isinstance(v, int):
        return v, v
    else:
        return v


@DeveloperAPI
def crop(img: torch.Tensor, new_size: Union[int, Tuple[int, int]]) ->torch.Tensor:
    """torchscript-compatible implementation of crop.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to crop
        size (Union[int, Tuple[int, int]]): size to crop to. If int, crops to square image of that size.

    Returns:
        torch.Tensor: cropped image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    return F.center_crop(img, output_size=new_size)


@DeveloperAPI
def pad(img: torch.Tensor, new_size: Union[int, Tuple[int, int]]) ->torch.Tensor:
    """torchscript-compatible implementation of pad.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to pad
        new_size (Union[int, Tuple[int, int]]): size to pad to. If int, resizes to square image of that size.

    Returns:
        torch.Tensor: padded image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    old_size = img.shape[-2:]
    pad_size = (torch.tensor(new_size) - torch.tensor(old_size)) / 2
    padding = torch.cat((torch.floor(pad_size), torch.ceil(pad_size)))
    padding[padding < 0] = 0
    padding = [int(x) for x in padding]
    return F.pad(img, padding=padding, padding_mode='edge')


@DeveloperAPI
def crop_or_pad(img: torch.Tensor, new_size: Union[int, Tuple[int, int]]):
    """torchscript-compatible implementation of resize using constants.CROP_OR_PAD.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to resize
        new_size (Union[int, Tuple[int, int]]): size to resize to. If int, resizes to square image of that size.

    Returns:
        torch.Tensor: resized image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    if list(new_size) == list(img.shape[-2:]):
        return img
    img = pad(img, new_size)
    img = crop(img, new_size)
    return img


@DeveloperAPI
def resize_image(img: torch.Tensor, new_size: Union[int, Tuple[int, int]], resize_method: str, crop_or_pad_constant: str=CROP_OR_PAD, interpolate_constant: str=INTERPOLATE) ->torch.Tensor:
    """torchscript-compatible implementation of resize.

    Args:
        img (torch.Tensor): image with shape [..., height, width] to resize
        new_size (Union[int, Tuple[int, int]]): size to resize to. If int, resizes to square image of that size.
        resize_method (str): method to use for resizing. Either constants.CROP_OR_PAD or constants.INTERPOLATE.

    Returns:
        torch.Tensor: resized image of size [..., size[0], size[1]] or [..., size, size] if size is int.
    """
    new_size = to_tuple(new_size)
    if list(img.shape[-2:]) != list(new_size):
        if resize_method == crop_or_pad_constant:
            return crop_or_pad(img, new_size)
        elif resize_method == interpolate_constant:
            return F.resize(img, new_size)
        raise ValueError(f'Invalid image resize method: {resize_method}')
    return img


@DeveloperAPI
def path_exists(url):
    fs, path = get_fs_and_path(url)
    return fs.exists(path)


def alphanum(v):
    """Filters a string to only its alphanumeric characters."""
    return re.sub('\\W+', '', v)


class _ImagePreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by ImageFeatureMixin.add_feature_data."""

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.height = metadata['preprocessing']['height']
        self.width = metadata['preprocessing']['width']
        self.num_channels = metadata['preprocessing']['num_channels']
        self.resize_method = metadata['preprocessing']['resize_method']

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        """Takes a list of images and adjusts the size and number of channels as specified in the metadata.

        If `v` is already a torch.Tensor, we assume that the images are already preprocessed to be the same size.
        """
        if not torch.jit.isinstance(v, List[torch.Tensor]):
            if not torch.jit.isinstance(v, torch.Tensor):
                raise ValueError(f'Unsupported input: {v}')
        if torch.jit.isinstance(v, List[torch.Tensor]):
            imgs = [resize_image(img, (self.height, self.width), self.resize_method) for img in v]
            imgs_stacked = torch.stack(imgs)
        else:
            imgs_stacked = v
        _, num_channels, height, width = imgs_stacked.shape
        if height != self.height or width != self.width:
            imgs_stacked = resize_image(imgs_stacked, (self.height, self.width), self.resize_method)
        if num_channels != self.num_channels:
            if self.num_channels == 1:
                imgs_stacked = grayscale(imgs_stacked)
            elif num_channels < self.num_channels:
                extra_channels = self.num_channels - num_channels
                imgs_stacked = torch.nn.functional.pad(imgs_stacked, [0, 0, 0, 0, 0, extra_channels])
            else:
                raise ValueError(f'Number of channels cannot be reconciled. metadata.num_channels = {self.num_channels}, but imgs.shape[1] = {num_channels}')
        return imgs_stacked


class IdentityTransformer(nn.Module):

    def __init__(self, **kwargs):
        super().__init__()

    def transform(self, x: np.ndarray) ->np.ndarray:
        return x

    def inverse_transform(self, x: np.ndarray) ->np.ndarray:
        return x

    def transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return x

    def inverse_transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return x

    @staticmethod
    def fit_transform_params(column: np.ndarray, backend: 'Backend') ->dict:
        return {}


class InterQuartileTransformer(nn.Module):

    def __init__(self, q1: float=None, q2: float=None, q3: float=None, **kwargs: dict):
        super().__init__()
        self.q1 = float(q1) if q1 is not None else q1
        self.q2 = float(q2) if q2 is not None else q2
        self.q3 = float(q3) if q3 is not None else q3
        if self.q1 is None or self.q3 is None:
            self.interquartile_range = None
        else:
            self.interquartile_range = self.q3 - self.q1
        self.feature_name = kwargs.get(NAME, '')
        if self.interquartile_range == 0:
            raise RuntimeError(f'Cannot apply InterQuartileNormalization to `{self.feature_name}` sincethe interquartile range is 0, which will result in a ZeroDivisionError.')

    def transform(self, x: np.ndarray) ->np.ndarray:
        return (x - self.q2) / self.interquartile_range

    def inverse_transform(self, x: np.ndarray) ->np.ndarray:
        return x * self.interquartile_range + self.q2

    def transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return (x - self.q2) / self.interquartile_range

    def inverse_transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return x * self.interquartile_range + self.q2

    @staticmethod
    def fit_transform_params(column: np.ndarray, backend: 'Backend') ->dict:
        compute = backend.df_engine.compute
        return {'q1': compute(np.percentile(column.astype(np.float32), 25)), 'q2': compute(np.percentile(column.astype(np.float32), 50)), 'q3': compute(np.percentile(column.astype(np.float32), 75))}


class Log1pTransformer(nn.Module):

    def __init__(self, **kwargs: dict):
        super().__init__()

    def transform(self, x: np.ndarray) ->np.ndarray:
        if np.any(x <= 0):
            raise ValueError('One or more values are non-positive.  log1p normalization is defined only for positive values.')
        return np.log1p(x)

    def inverse_transform(self, x: np.ndarray) ->np.ndarray:
        return np.expm1(x)

    def transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return torch.log1p(x)

    def inverse_transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return torch.expm1(x)

    @staticmethod
    def fit_transform_params(column: np.ndarray, backend: 'Backend') ->dict:
        return {}


class MinMaxTransformer(nn.Module):

    def __init__(self, min: float=None, max: float=None, **kwargs: dict):
        super().__init__()
        self.min_value = float(min) if min is not None else min
        self.max_value = float(max) if max is not None else max
        if self.min_value is None or self.max_value is None:
            self.range = None
        else:
            self.range = self.max_value - self.min_value

    def transform(self, x: np.ndarray) ->np.ndarray:
        return (x - self.min_value) / self.range

    def inverse_transform(self, x: np.ndarray) ->np.ndarray:
        if self.range is None:
            raise ValueError('Numeric transformer needs to be instantiated with min and max values.')
        return x * self.range + self.min_value

    def transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return (x - self.min_value) / self.range

    def inverse_transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        if self.range is None:
            raise ValueError('Numeric transformer needs to be instantiated with min and max values.')
        return x * self.range + self.min_value

    @staticmethod
    def fit_transform_params(column: np.ndarray, backend: 'Backend') ->dict:
        compute = backend.df_engine.compute
        return {'min': compute(column.astype(np.float32).min()), 'max': compute(column.astype(np.float32).max())}


class ZScoreTransformer(nn.Module):

    def __init__(self, mean: float=None, std: float=None, **kwargs: dict):
        super().__init__()
        self.mu = float(mean) if mean is not None else mean
        self.sigma = float(std) if std is not None else std
        self.feature_name = kwargs.get(NAME, '')
        if self.sigma == 0:
            raise RuntimeError(f'Cannot apply zscore normalization to `{self.feature_name}` since it has a standard deviation of 0. This is most likely because `{self.feature_name}` has a constant value of {self.mu} for all rows in the dataset. Consider removing this feature from your Ludwig config since it is not useful for your machine learning model.')

    def transform(self, x: np.ndarray) ->np.ndarray:
        return (x - self.mu) / self.sigma

    def inverse_transform(self, x: np.ndarray) ->np.ndarray:
        return x * self.sigma + self.mu

    def transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return (x - self.mu) / self.sigma

    def inverse_transform_inference(self, x: torch.Tensor) ->torch.Tensor:
        return x * self.sigma + self.mu

    @staticmethod
    def fit_transform_params(column: np.ndarray, backend: 'Backend') ->dict:
        compute = backend.df_engine.compute
        return {'mean': compute(column.astype(np.float32).mean()), 'std': compute(column.astype(np.float32).std())}


numeric_transformation_registry = {'minmax': MinMaxTransformer, 'zscore': ZScoreTransformer, 'log1p': Log1pTransformer, 'iq': InterQuartileTransformer, None: IdentityTransformer}


def get_transformer(metadata, preprocessing_parameters):
    return get_from_registry(preprocessing_parameters.get('normalization', None), numeric_transformation_registry)(**metadata)


class _NumberPreprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.computed_fill_value = float(metadata['preprocessing']['computed_fill_value'])
        self.numeric_transformer = get_transformer(metadata, metadata['preprocessing'])

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if not torch.jit.isinstance(v, torch.Tensor):
            raise ValueError(f'Unsupported input: {v}')
        v = torch.nan_to_num(v, nan=self.computed_fill_value)
        v = v
        return self.numeric_transformer.transform_inference(v)


class _TimeseriesPreprocessing(torch.nn.Module):
    """Torchscript-enabled version of preprocessing done by TimeseriesFeatureMixin.add_feature_data."""

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        if metadata['preprocessing']['tokenizer'] not in TORCHSCRIPT_COMPATIBLE_TOKENIZERS:
            raise ValueError(f"{metadata['preprocessing']['tokenizer']} is not supported by torchscript. Please use one of {TORCHSCRIPT_COMPATIBLE_TOKENIZERS}.")
        self.tokenizer = get_tokenizer_from_registry(metadata['preprocessing']['tokenizer'])()
        self.padding = metadata['preprocessing']['padding']
        self.padding_value = float(metadata['preprocessing']['padding_value'])
        self.max_timeseries_length = int(metadata['max_timeseries_length'])
        self.computed_fill_value = metadata['preprocessing']['computed_fill_value']

    def _process_str_sequence(self, sequence: List[str], limit: int) ->torch.Tensor:
        float_sequence = [float(s) for s in sequence[:limit]]
        return torch.tensor(float_sequence)

    def _nan_to_fill_value(self, v: torch.Tensor) ->torch.Tensor:
        if v.isnan().any():
            tokenized_fill_value = self.tokenizer(self.computed_fill_value)
            assert torch.jit.isinstance(tokenized_fill_value, List[str])
            return self._process_str_sequence(tokenized_fill_value, self.max_timeseries_length)
        return v

    def forward_list_of_tensors(self, v: List[torch.Tensor]) ->torch.Tensor:
        v = [self._nan_to_fill_value(v_i) for v_i in v]
        if self.padding == 'right':
            timeseries_matrix = torch.nn.utils.rnn.pad_sequence(v, batch_first=True, padding_value=self.padding_value)
            timeseries_matrix = timeseries_matrix[:, :self.max_timeseries_length]
        else:
            reversed_timeseries = [torch.flip(v_i[:self.max_timeseries_length], dims=(0,)) for v_i in v]
            reversed_timeseries_padded = torch.nn.utils.rnn.pad_sequence(reversed_timeseries, batch_first=True, padding_value=self.padding_value)
            timeseries_matrix = torch.flip(reversed_timeseries_padded, dims=(1,))
        return timeseries_matrix

    def forward_list_of_strs(self, v: List[str]) ->torch.Tensor:
        v = [(self.computed_fill_value if s == 'nan' else s) for s in v]
        sequences = self.tokenizer(v)
        assert torch.jit.isinstance(sequences, List[List[str]]), 'sequences is not a list of lists.'
        timeseries_matrix = torch.full([len(sequences), self.max_timeseries_length], self.padding_value, dtype=torch.float32)
        for sample_idx, str_sequence in enumerate(sequences):
            limit = min(len(str_sequence), self.max_timeseries_length)
            float_sequence = self._process_str_sequence(str_sequence, limit)
            if self.padding == 'right':
                timeseries_matrix[sample_idx][:limit] = float_sequence
            else:
                timeseries_matrix[sample_idx][self.max_timeseries_length - limit:] = float_sequence
        return timeseries_matrix

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        """Takes a list of float values and creates a padded torch.Tensor."""
        if torch.jit.isinstance(v, List[torch.Tensor]):
            return self.forward_list_of_tensors(v)
        if torch.jit.isinstance(v, List[str]):
            return self.forward_list_of_strs(v)
        raise ValueError(f'Unsupported input: {v}')


class VectorFeatureMixin:

    @staticmethod
    def type():
        return VECTOR

    @staticmethod
    def cast_column(column, backend):
        return column

    @staticmethod
    def get_feature_meta(column, preprocessing_parameters: PreprocessingConfigDict, backend) ->FeatureMetadataDict:
        return {'preprocessing': preprocessing_parameters}

    @staticmethod
    def add_feature_data(feature_config, input_df, proc_df, metadata, preprocessing_parameters: PreprocessingConfigDict, backend, skip_save_processed_input):
        """Expects all the vectors to be of the same size.

        The vectors need to be whitespace delimited strings. Missing values are not handled.
        """
        if len(input_df[feature_config[COLUMN]]) == 0:
            raise ValueError('There are no vectors in the dataset provided')
        try:
            proc_df[feature_config[PROC_COLUMN]] = backend.df_engine.map_objects(input_df[feature_config[COLUMN]], lambda x: np.array(x.split(), dtype=np.float32))
        except ValueError:
            logger.error('Unable to read the vector data. Make sure that all the vectors are of the same size and do not have missing/null values.')
            raise
        vector_size = backend.df_engine.compute(proc_df[feature_config[PROC_COLUMN]].map(len).max())
        if 'vector_size' in preprocessing_parameters:
            if vector_size != preprocessing_parameters['vector_size']:
                raise ValueError('The user provided value for vector size ({}) does not match the value observed in the data: {}'.format(preprocessing_parameters, vector_size))
        else:
            logger.debug(f'Observed vector size: {vector_size}')
        metadata[feature_config[NAME]]['vector_size'] = vector_size
        return proc_df


class _VectorPreprocessing(torch.nn.Module):

    def forward(self, v: TorchscriptPreprocessingInput) ->torch.Tensor:
        if torch.jit.isinstance(v, torch.Tensor):
            out = v
        elif torch.jit.isinstance(v, List[torch.Tensor]):
            out = torch.stack(v)
        elif torch.jit.isinstance(v, List[str]):
            vectors = []
            for sample in v:
                vector = torch.tensor([float(x) for x in sample.split()], dtype=torch.float32)
                vectors.append(vector)
            out = torch.stack(vectors)
        else:
            raise ValueError(f'Unsupported input: {v}')
        if out.isnan().any():
            raise ValueError('Scripted NaN handling not implemented for Vector feature')
        return out


@DeveloperAPI
def get_input_type_registry() ->Dict:
    return {TEXT: TextInputFeature, NUMBER: NumberInputFeature, BINARY: BinaryInputFeature, CATEGORY: CategoryInputFeature, SET: SetInputFeature, SEQUENCE: SequenceInputFeature, IMAGE: ImageInputFeature, AUDIO: AudioInputFeature, TIMESERIES: TimeseriesInputFeature, BAG: BagInputFeature, H3: H3InputFeature, DATE: DateInputFeature, VECTOR: VectorInputFeature}


ACCURACY = 'accuracy'


BINARY_WEIGHTED_CROSS_ENTROPY = 'binary_weighted_cross_entropy'


@DeveloperAPI
def get_decoder_classes(feature: str) ->Dict[str, Type[Decoder]]:
    return get_decoder_registry()[feature]


@DeveloperAPI
def get_decoder_conds(feature_type: str):
    """Returns a JSON schema of conditionals to validate against decoder types for specific feature types."""
    conds = []
    for decoder in get_decoder_classes(feature_type):
        decoder_cls = get_decoder_cls(feature_type, decoder)
        other_props = schema_utils.unload_jsonschema_from_marshmallow_class(decoder_cls)['properties']
        schema_utils.remove_duplicate_fields(other_props)
        decoder_cond = schema_utils.create_cond({'type': decoder}, other_props)
        conds.append(decoder_cond)
    return conds


@DeveloperAPI
def DecoderDataclassField(feature_type: str, default: str):
    """Custom dataclass field that when used inside a dataclass will allow the user to specify a decoder config.

    Returns: Initialized dataclass field that converts an untyped dict with params to a decoder config.
    """


    class DecoderMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict for a valid decoder config from the decoder_registry
        and creates a corresponding `oneOf` JSON schema for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if TYPE in value and value[TYPE] in get_decoder_classes(feature_type):
                    dec = get_decoder_cls(feature_type, value[TYPE])
                    try:
                        return dec.Schema().load(value)
                    except (TypeError, ValidationError) as error:
                        raise ValidationError(f'Invalid decoder params: {value}, see `{dec}` definition. Error: {error}')
                raise ValidationError(f'Invalid params for decoder: {value}, expect dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            decoder_classes = list(get_decoder_classes(feature_type).keys())
            return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': decoder_classes, 'default': default}}, 'title': 'decoder_options', 'allOf': get_decoder_conds(feature_type)}
    try:
        decoder = get_decoder_cls(feature_type, default)
        load_default = decoder.Schema().load({'type': default})
        dump_default = decoder.Schema().dump({'type': default})
        return field(metadata={'marshmallow_field': DecoderMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default)}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f'Unsupported decoder type: {default}. See decoder_registry. Details: {e}')


def get_loss_classes(feature: str):
    return loss_registry[feature]


@DeveloperAPI
def get_loss_conds(feature_type: str):
    """Returns a JSON schema of conditionals to validate against loss types for specific feature types."""
    conds = []
    for loss in get_loss_classes(feature_type):
        loss_cls = get_loss_cls(feature_type, loss).get_schema_cls()
        other_props = schema_utils.unload_jsonschema_from_marshmallow_class(loss_cls)['properties']
        schema_utils.remove_duplicate_fields(other_props)
        loss_cond = schema_utils.create_cond({'type': loss}, other_props)
        conds.append(loss_cond)
    return conds


@DeveloperAPI
def LossDataclassField(feature_type: str, default: str):
    """Custom dataclass field that when used inside a dataclass will allow the user to specify a loss config for
    the decoder of an output feature.

    Returns: Initialized dataclass field that converts an untyped dict with params to a loss config.
    """


    class LossMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict for a valid loss config from the
        preprocessing_registry and creates a corresponding JSON schema for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if TYPE in value and value[TYPE] in get_loss_classes(feature_type):
                    loss_config = get_loss_cls(feature_type, value[TYPE]).get_schema_cls()
                    try:
                        return loss_config.Schema().load(value)
                    except (TypeError, ValidationError) as error:
                        raise ValidationError(f'Invalid loss params: {value}, see `{loss_config}` definition. Error: {error}')
                raise ValidationError(f'Invalid params for loss: {value}, expect dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            loss_classes = list(get_loss_classes(feature_type).keys())
            return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': loss_classes, 'default': default}}, 'title': 'loss_options', 'allOf': get_loss_conds(feature_type)}
    try:
        loss = get_loss_cls(feature_type, default).get_schema_cls()
        load_default = loss.Schema().load({'type': default})
        dump_default = loss.Schema().dump({'type': default})
        return field(metadata={'marshmallow_field': LossMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default)}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f'Unsupported loss type: {default}. See loss_registry. Details: {e}')


ROC_AUC = 'roc_auc'


def _validate_output_feature_name_from_test_stats(output_feature_name, test_stats_per_model):
    """Validate prediction output_feature_name from model test stats and return it as list.

    :param output_feature_name: output_feature_name containing ground truth
    :param test_stats_per_model: list of per model test stats
    :return output_feature_names: list of output_feature_name(s) containing ground truth
    """
    output_feature_names_set = set()
    for ls in test_stats_per_model:
        for key in ls:
            output_feature_names_set.add(key)
    try:
        if output_feature_name in output_feature_names_set:
            return [output_feature_name]
        else:
            return output_feature_names_set
    except TypeError:
        return output_feature_names_set


@DeveloperAPI
def convert_to_list(item):
    """If item is not list class instance or None put inside a list.

    :param item: object to be checked and converted
    :return: original item if it is a list instance or list containing the item.
    """
    return item if item is None or isinstance(item, list) else [item]


@DeveloperAPI
def generate_filename_template_path(output_dir, filename_template):
    """Ensure path to template file can be constructed given an output dir.

    Create output directory if yet does exist.
    :param output_dir: Directory that will contain the filename_template file
    :param filename_template: name of the file template to be appended to the
            filename template path
    :return: path to filename template inside the output dir or None if the
             output dir is None
    """
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        return os.path.join(output_dir, filename_template)
    return None


@DeveloperAPI
def confusion_matrix(test_stats_per_model: List[dict], metadata: dict, output_feature_name: Union[str, None], top_n_classes: List[int], normalize: bool, model_names: Union[str, List[str]]=None, output_directory: str=None, file_format: str='pdf', **kwargs) ->None:
    """Show confusion matrix in the models predictions for each `output_feature_name`.

    For each model (in the aligned lists of test_statistics and model_names)
    it  produces a heatmap of the confusion matrix in the predictions for
    each  output_feature_name that has a confusion matrix in test_statistics.
    The value of `top_n_classes` limits the heatmap to the n most frequent
    classes.

    # Inputs

    :param test_stats_per_model: (List[dict]) dictionary containing evaluation
      performance statistics.
    :param metadata: (dict) intermediate preprocess structure created during
        training containing the mappings of the input dataset.
    :param output_feature_name: (Union[str, `None`]) name of the output feature
        to use for the visualization.  If `None`, use all output features.
    :param top_n_classes: (List[int]) number of top classes or list
        containing the number of top classes to plot.
    :param normalize: (bool) flag to normalize rows in confusion matrix.
    :param model_names: (Union[str, List[str]], default: `None`) model name or
        list of the model names to use as labels.
    :param output_directory: (str, default: `None`) directory where to save
        plots. If not specified, plots will be displayed in a window
    :param file_format: (str, default: `'pdf'`) file format of output plots -
        `'pdf'` or `'png'`.

    # Return

    :return: (None)
    """
    test_stats_per_model_list = test_stats_per_model
    model_names_list = convert_to_list(model_names)
    filename_template = 'confusion_matrix_{}_{}_{}.' + file_format
    filename_template_path = generate_filename_template_path(output_directory, filename_template)
    output_feature_names = _validate_output_feature_name_from_test_stats(output_feature_name, test_stats_per_model_list)
    confusion_matrix_found = False
    for i, test_statistics in enumerate(test_stats_per_model_list):
        for output_feature_name in output_feature_names:
            if 'confusion_matrix' in test_statistics[output_feature_name]:
                confusion_matrix_found = True
                _confusion_matrix = np.array(test_statistics[output_feature_name]['confusion_matrix'])
                model_name_name = model_names_list[i] if model_names_list is not None and i < len(model_names_list) else ''
                if metadata is not None and output_feature_name in metadata and 'idx2str' in metadata[output_feature_name]:
                    labels = metadata[output_feature_name]['idx2str']
                else:
                    labels = list(range(len(_confusion_matrix)))
                for k in top_n_classes:
                    k = min(k, _confusion_matrix.shape[0]) if k > 0 else _confusion_matrix.shape[0]
                    cm = _confusion_matrix[:k, :k]
                    if normalize:
                        with np.errstate(divide='ignore', invalid='ignore'):
                            cm_norm = np.true_divide(cm, cm.sum(1)[:, np.newaxis])
                            cm_norm[cm_norm == np.inf] = 0
                            cm_norm = np.nan_to_num(cm_norm)
                        cm = cm_norm
                    filename = None
                    if output_directory:
                        os.makedirs(output_directory, exist_ok=True)
                        filename = filename_template_path.format(model_name_name, output_feature_name, 'top' + str(k))
                    visualization_utils.confusion_matrix_plot(cm, labels[:k], output_feature_name=output_feature_name, filename=filename)
                    entropies = []
                    for row in cm:
                        if np.count_nonzero(row) > 0:
                            entropies.append(entropy(row))
                        else:
                            entropies.append(0)
                    class_entropy = np.array(entropies)
                    class_desc_entropy = np.argsort(class_entropy)[::-1]
                    desc_entropy = class_entropy[class_desc_entropy]
                    filename = None
                    if output_directory:
                        filename = filename_template_path.format('entropy_' + model_name_name, output_feature_name, 'top' + str(k))
                    visualization_utils.bar_plot(class_desc_entropy, desc_entropy, labels=[labels[i] for i in class_desc_entropy], title='Classes ranked by entropy of Confusion Matrix row', filename=filename)
    if not confusion_matrix_found:
        logger.error('Cannot find confusion_matrix in evaluation data')
        raise FileNotFoundError('Cannot find confusion_matrix in evaluation data')


class ConfusionMatrix:

    def __init__(self, conditions, predictions, labels=None, sample_weight=None):
        min_length = min(len(predictions), len(conditions))
        self.predictions = predictions[:min_length]
        self.conditions = conditions[:min_length]
        if labels is not None:
            self.label2idx = {label: idx for idx, label in enumerate(labels)}
            self.idx2label = {idx: label for idx, label in enumerate(labels)}
            labels = list(range(len(labels)))
        else:
            self.label2idx = {str(label): idx for idx, label in enumerate(np.unique([self.predictions, self.conditions]))}
            self.idx2label = {idx: str(label) for idx, label in enumerate(np.unique([self.predictions, self.conditions]))}
        self.cm = confusion_matrix(self.conditions, self.predictions, labels=labels, sample_weight=sample_weight)
        self.sum_predictions = np.sum(self.cm, axis=0)
        self.sum_conditions = np.sum(self.cm, axis=1)
        self.all = np.sum(self.cm)

    def label_to_idx(self, label):
        return self.label2idx[label]

    def true_positives(self, idx):
        return self.cm[idx, idx]

    def true_negatives(self, idx):
        return self.all - self.sum_predictions[idx] - self.sum_conditions[idx] + self.true_positives(idx)

    def false_positives(self, idx):
        return self.sum_predictions[idx] - self.true_positives(idx)

    def false_negatives(self, idx):
        return self.sum_conditions[idx] - self.true_positives(idx)

    def true_positive_rate(self, idx):
        nom = self.true_positives(idx)
        den = self.sum_conditions[idx]
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def true_negative_rate(self, idx):
        nom = tn = self.true_negatives(idx)
        den = tn + self.false_positives(idx)
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def positive_predictive_value(self, idx):
        nom = self.true_positives(idx)
        den = self.sum_predictions[idx]
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def negative_predictive_value(self, idx):
        nom = tn = self.true_negatives(idx)
        den = tn + self.false_negatives(idx)
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def false_negative_rate(self, idx):
        return 1.0 - self.true_positive_rate(idx)

    def false_positive_rate(self, idx):
        return 1.0 - self.true_negative_rate(idx)

    def false_discovery_rate(self, idx):
        return 1.0 - self.positive_predictive_value(idx)

    def false_omission_rate(self, idx):
        return 1.0 - self.negative_predictive_value(idx)

    def accuracy(self, idx):
        nom = self.true_positives(idx) + self.true_negatives(idx)
        den = self.all
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def precision(self, idx):
        return self.positive_predictive_value(idx)

    def recall(self, idx):
        return self.true_positive_rate(idx)

    def fbeta_score(self, beta, idx):
        beta_2 = np.power(beta, 2)
        precision = self.precision(idx)
        recall = self.recall(idx)
        nom = (1 + beta_2) * precision * recall
        den = beta_2 * precision + recall
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def f1_score(self, idx):
        return self.fbeta_score(1, idx)

    def sensitivity(self, idx):
        return self.true_positive_rate(idx)

    def specificity(self, idx):
        return self.true_negative_rate(idx)

    def hit_rate(self, idx):
        return self.true_positive_rate(idx)

    def miss_rate(self, idx):
        return self.false_negative_rate(idx)

    def fall_out(self, idx):
        return self.false_positive_rate(idx)

    def matthews_correlation_coefficient(self, idx):
        tp = self.true_positives(idx)
        tn = self.true_negatives(idx)
        fp = self.false_positives(idx)
        fn = self.false_negatives(idx)
        nom = tp * tn - fp * fn
        den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))
        if den == 0 or den == np.nan:
            return 0
        else:
            return nom / den

    def informedness(self, idx):
        return self.true_positive_rate(idx) + self.true_negative_rate(idx) - 1

    def markedness(self, idx):
        return self.positive_predictive_value(idx) + self.negative_predictive_value(idx) - 1

    def token_accuracy(self):
        return metrics.accuracy_score(self.conditions, self.predictions)

    def avg_precision(self, average='macro'):
        return metrics.precision_score(self.conditions, self.predictions, average=average)

    def avg_recall(self, average='macro'):
        return metrics.recall_score(self.conditions, self.predictions, average=average)

    def avg_f1_score(self, average='macro'):
        return metrics.f1_score(self.conditions, self.predictions, average=average)

    def avg_fbeta_score(self, beta, average='macro'):
        return metrics.fbeta_score(self.conditions, self.predictions, beta=beta, average=average)

    def kappa_score(self):
        return metrics.cohen_kappa_score(self.conditions, self.predictions)

    def class_stats(self, idx):
        return {'true_positives': self.true_positives(idx), 'true_negatives': self.true_negatives(idx), 'false_positives': self.false_positives(idx), 'false_negatives': self.false_negatives(idx), 'true_positive_rate': self.true_positive_rate(idx), 'true_negative_rate': self.true_negative_rate(idx), 'positive_predictive_value': self.positive_predictive_value(idx), 'negative_predictive_value': self.negative_predictive_value(idx), 'false_negative_rate': self.false_negative_rate(idx), 'false_positive_rate': self.false_positive_rate(idx), 'false_discovery_rate': self.false_discovery_rate(idx), 'false_omission_rate': self.false_omission_rate(idx), 'accuracy': self.accuracy(idx), 'precision': self.precision(idx), 'recall': self.recall(idx), 'f1_score': self.f1_score(idx), 'sensitivity': self.sensitivity(idx), 'specificity': self.specificity(idx), 'hit_rate': self.hit_rate(idx), 'miss_rate': self.miss_rate(idx), 'fall_out': self.fall_out(idx), 'matthews_correlation_coefficient': self.matthews_correlation_coefficient(idx), 'informedness': self.informedness(idx), 'markedness': self.markedness(idx)}

    def per_class_stats(self):
        stats = OrderedDict()
        for idx in sorted(self.idx2label.keys()):
            stats[self.idx2label[idx]] = self.class_stats(idx)
        return stats

    def stats(self):
        return {'token_accuracy': self.token_accuracy(), 'avg_precision_macro': self.avg_precision(average='macro'), 'avg_recall_macro': self.avg_recall(average='macro'), 'avg_f1_score_macro': self.avg_f1_score(average='macro'), 'avg_precision_micro': self.avg_precision(average='micro'), 'avg_recall_micro': self.avg_recall(average='micro'), 'avg_f1_score_micro': self.avg_f1_score(average='micro'), 'avg_precision_weighted': self.avg_precision(average='micro'), 'avg_recall_weighted': self.avg_recall(average='micro'), 'avg_f1_score_weighted': self.avg_f1_score(average='weighted'), 'kappa_score': self.kappa_score()}


PROBABILITY = 'probability'


FeaturePostProcessingOutputDict = Dict[str, Any]


class _BinaryPostprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        bool2str = metadata.get('bool2str')
        self.bool2str = {i: v for i, v in enumerate(bool2str)} if bool2str is not None else None
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES

    def forward(self, preds: Dict[str, torch.Tensor], feature_name: str) ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        if self.bool2str is not None:
            predictions = predictions
            predictions = [self.bool2str.get(pred, self.bool2str[0]) for pred in predictions]
        probabilities = torch.stack([1 - probabilities, probabilities], dim=-1)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities}


class _BinaryPredict(PredictModule):

    def __init__(self, threshold, calibration_module=None):
        super().__init__()
        self.threshold = threshold
        self.calibration_module = calibration_module

    def forward(self, inputs: Dict[str, torch.Tensor], feature_name: str) ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        if self.calibration_module is not None:
            probabilities = self.calibration_module(logits)
        else:
            probabilities = torch.sigmoid(logits)
        predictions = probabilities >= self.threshold
        return {self.probabilities_key: probabilities, self.predictions_key: predictions, self.logits_key: logits}


def average_precision_score(conditions, prediction_scores, average='micro', sample_weight=None):
    return metrics.average_precision_score(conditions, prediction_scores, average=average, sample_weight=sample_weight)


def precision_recall_curve(conditions, prediction_scores, pos_label=None, sample_weight=None):
    return metrics.precision_recall_curve(conditions, prediction_scores, pos_label=pos_label, sample_weight=sample_weight)


def roc_auc_score(conditions, prediction_scores, average='micro', sample_weight=None):
    try:
        return metrics.roc_auc_score(conditions, prediction_scores, average=average, sample_weight=sample_weight)
    except ValueError as ve:
        logger.info(ve)


def roc_curve(conditions, prediction_scores, pos_label=None, sample_weight=None):
    return metrics.roc_curve(conditions, prediction_scores, pos_label=pos_label, sample_weight=sample_weight)


SOFTMAX_CROSS_ENTROPY = 'softmax_cross_entropy'


HITS_AT_K = 'hits_at_k'


PROJECTION_INPUT = 'projection_input'


class _CategoryPostprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.idx2str = {i: v for i, v in enumerate(metadata['idx2str'])}
        self.unk = UNKNOWN_SYMBOL
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES

    def forward(self, preds: Dict[str, torch.Tensor], feature_name: str) ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        inv_preds = [self.idx2str.get(pred, self.unk) for pred in predictions]
        return {self.predictions_key: inv_preds, self.probabilities_key: probabilities}


class _CategoryPredict(PredictModule):

    def __init__(self, calibration_module=None):
        super().__init__()
        self.calibration_module = calibration_module

    def forward(self, inputs: Dict[str, torch.Tensor], feature_name: str) ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        if self.calibration_module is not None:
            probabilities = self.calibration_module(logits)
        else:
            probabilities = torch.softmax(logits, -1)
        predictions = torch.argmax(probabilities, -1)
        predictions = predictions.long()
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.logits_key: logits}


def softmax(x, temperature=1.0):
    e_x = np.exp((x - np.max(x)) / temperature)
    return e_x / e_x.sum()


MEAN_ABSOLUTE_ERROR = 'mean_absolute_error'


MEAN_SQUARED_ERROR = 'mean_squared_error'


R2 = 'r2'


ROOT_MEAN_SQUARED_ERROR = 'root_mean_squared_error'


ROOT_MEAN_SQUARED_PERCENTAGE_ERROR = 'root_mean_squared_percentage_error'


class _NumberPostprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.numeric_transformer = get_transformer(metadata, metadata['preprocessing'])
        self.predictions_key = PREDICTIONS

    def forward(self, preds: Dict[str, torch.Tensor], feature_name: str) ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        return {self.predictions_key: self.numeric_transformer.inverse_transform_inference(predictions)}


class _NumberPredict(PredictModule):

    def __init__(self, clip):
        super().__init__()
        self.clip = clip

    def forward(self, inputs: Dict[str, torch.Tensor], feature_name: str) ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        predictions = logits
        if self.clip is not None:
            predictions = torch.clamp(logits, self.clip[0], self.clip[1])
            logger.debug(f'  clipped_predictions: {predictions}')
        return {self.predictions_key: predictions, self.logits_key: logits}


LAST_PREDICTIONS = 'last_predictions'


EDIT_DISTANCE = 'edit_distance'


LAST_ACCURACY = 'last_accuracy'


PERPLEXITY = 'perplexity'


SEQUENCE_ACCURACY = 'sequence_accuracy'


SEQUENCE_SOFTMAX_CROSS_ENTROPY = 'sequence_softmax_cross_entropy'


TOKEN_ACCURACY = 'token_accuracy'


class _SequencePostprocessing(torch.nn.Module):

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.max_sequence_length = int(metadata['max_sequence_length'])
        self.idx2str = metadata['idx2str']
        self.unknown_symbol = UNKNOWN_SYMBOL
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES
        self.probability_key = PROBABILITY

    def forward(self, preds: Dict[str, torch.Tensor], feature_name: str) ->FeaturePostProcessingOutputDict:
        pred_predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        pred_probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        predictions: List[List[str]] = []
        for sequence in pred_predictions:
            sequence_predictions: List[str] = []
            for i in range(self.max_sequence_length):
                unit_id = int(sequence[i].item())
                if unit_id < len(self.idx2str):
                    unit_prediction = self.idx2str[unit_id]
                else:
                    unit_prediction = self.unknown_symbol
                sequence_predictions.append(unit_prediction)
            predictions.append(sequence_predictions)
        probabilities, _ = torch.max(pred_probabilities, dim=-1)
        probability = torch.sum(torch.log(probabilities), dim=-1)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.probability_key: probability}


class _SequencePredict(PredictModule):

    def forward(self, inputs: Dict[str, torch.Tensor], feature_name: str) ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        probabilities = torch.softmax(logits, -1)
        predictions = torch.argmax(logits, -1)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.logits_key: logits}


def compute_sequence_probability(sequence_probabilities: np.ndarray, max_sequence_length: Optional[int]=None, return_log_prob: bool=True) ->float:
    """Computes the sequence level probability.

    Args:
        sequence_probabilities: An iterable of iterables or np.ndarray with shape (sequence_length,)
        max_sequence_length: The maximum sequence length to use. If None, uses the first dim of `sequence_probabilities`
        return_log_prob: Whether to return the log probability. Defaults to True.
    """
    if max_sequence_length is None:
        max_sequence_length = sequence_probabilities.shape[0]
    sequence_probabilities = sequence_probabilities[:max_sequence_length]
    if return_log_prob:
        return np.sum(np.log(sequence_probabilities))
    else:
        return np.prod(sequence_probabilities)


def compute_token_probabilities(probabilities: Union[list, tuple, np.ndarray]) ->np.ndarray:
    """Gets the maximum probability per timestep.

    Args:
        probabilities: An iterable of iterables or np.ndarray with shape (sequence_length, num_classes)
            where each inner iterable or np.ndarray is the probability distribution for a single timestep.
    Returns:
        An np.ndarray with shape (sequence_length,) containing the maximum probability for each timestep.
    """
    if isinstance(probabilities, (list, tuple)):
        max_probs = []
        for timestep_probs in probabilities:
            max_probs.append(np.max(timestep_probs))
        max_probs = np.array(max_probs)
    elif isinstance(probabilities, np.ndarray):
        max_probs = np.max(probabilities, axis=-1)
    else:
        raise ValueError(f'probabilities type must be in [list, tuple, np.ndarray]. Got {type(probabilities)}')
    return max_probs


JACCARD = 'jaccard'


SIGMOID_CROSS_ENTROPY = 'sigmoid_cross_entropy'


class _SetPostprocessing(torch.nn.Module):
    """Torchscript-enabled version of postprocessing done by SetFeatureMixin.add_feature_data."""

    def __init__(self, metadata: TrainingSetMetadataDict):
        super().__init__()
        self.idx2str = {i: v for i, v in enumerate(metadata['idx2str'])}
        self.predictions_key = PREDICTIONS
        self.probabilities_key = PROBABILITIES
        self.unk = UNKNOWN_SYMBOL

    def forward(self, preds: Dict[str, torch.Tensor], feature_name: str) ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        probabilities = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.probabilities_key)
        inv_preds: List[List[str]] = []
        filtered_probs: List[torch.Tensor] = []
        for sample_idx, sample in enumerate(predictions):
            sample_preds: List[str] = []
            pos_sample_idxs: List[int] = []
            pos_class_idxs: List[int] = []
            for class_idx, is_positive in enumerate(sample):
                if is_positive == 1:
                    sample_preds.append(self.idx2str.get(class_idx, self.unk))
                    pos_sample_idxs.append(sample_idx)
                    pos_class_idxs.append(class_idx)
            inv_preds.append(sample_preds)
            filtered_probs.append(probabilities[pos_sample_idxs, pos_class_idxs])
        return {self.predictions_key: inv_preds, self.probabilities_key: filtered_probs}


class _SetPredict(PredictModule):

    def __init__(self, threshold):
        super().__init__()
        self.threshold = threshold

    def forward(self, inputs: Dict[str, torch.Tensor], feature_name: str) ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        probabilities = torch.sigmoid(logits)
        predictions = torch.greater_equal(probabilities, self.threshold)
        predictions = predictions.type(torch.int64)
        return {self.predictions_key: predictions, self.probabilities_key: probabilities, self.logits_key: logits}


ERROR = 'error'


class _VectorPostprocessing(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.predictions_key = PREDICTIONS
        self.logits_key = LOGITS

    def forward(self, preds: Dict[str, torch.Tensor], feature_name: str) ->FeaturePostProcessingOutputDict:
        predictions = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.predictions_key)
        logits = output_feature_utils.get_output_feature_tensor(preds, feature_name, self.logits_key)
        return {self.predictions_key: predictions, self.logits_key: logits}


class _VectorPredict(PredictModule):

    def forward(self, inputs: Dict[str, torch.Tensor], feature_name: str) ->Dict[str, torch.Tensor]:
        logits = output_feature_utils.get_output_feature_tensor(inputs, feature_name, self.logits_key)
        return {self.predictions_key: logits, self.logits_key: logits}


@DeveloperAPI
def get_output_type_registry() ->Dict:
    return {CATEGORY: CategoryOutputFeature, BINARY: BinaryOutputFeature, NUMBER: NumberOutputFeature, SEQUENCE: SequenceOutputFeature, SET: SetOutputFeature, TEXT: TextOutputFeature, VECTOR: VectorOutputFeature}


@DeveloperAPI
def reg_loss(model: nn.Module, regularizer: str, l1: float=0.01, l2: float=0.01):
    """Computes the regularization loss for a given model.

    Parameters:
        model: torch.nn.Module object to compute regularization loss for.
        regularizer: regularizer to use (currently l1, l2 and l1_l2 supported).
        l1: L1 regularization coefficient.
        l2: L2 regularization coefficient.

    Returns:
        Regularization loss for the model (float).
    """
    if regularizer == 'l1':
        l1_reg = l1 * sum(torch.abs(p).sum() for p in model.parameters())
        return l1_reg
    if regularizer == 'l2':
        l2_reg = l2 * sum(torch.square(p).sum() for p in model.parameters())
        return l2_reg
    if regularizer == 'l1_l2':
        l1_reg = l1 * sum(torch.abs(p).sum() for p in model.parameters())
        l2_reg = l2 * sum(torch.square(p).sum() for p in model.parameters())
        return l1_reg + l2_reg


TIED = 'tied'


def topological_sort(graph_unsorted):
    """Repeatedly go through all of the nodes in the graph, moving each of the nodes that has all its edges
    resolved, onto a sequence that forms our sorted graph.

    A node has all of its edges resolved and can be moved once all the nodes its edges point to, have been moved from
    the unsorted graph onto the sorted one.
    """
    graph_sorted = []
    graph_unsorted = dict(graph_unsorted)
    while graph_unsorted:
        acyclic = False
        for node, edges in list(graph_unsorted.items()):
            if edges is None:
                edges = []
            for edge in edges:
                if edge in graph_unsorted:
                    break
            else:
                acyclic = True
                del graph_unsorted[node]
                graph_sorted.append((node, edges))
        if not acyclic:
            raise RuntimeError('A cyclic dependency occurred')
    return graph_sorted


def topological_sort_feature_dependencies(features):
    dependencies_graph = {}
    output_features_dict = {}
    for feature in features:
        dependencies = []
        if 'dependencies' in feature:
            dependencies.extend(feature['dependencies'])
        if TIED in feature:
            dependencies.append(feature[TIED])
        dependencies_graph[feature['name']] = dependencies
        output_features_dict[feature['name']] = feature
    return [output_features_dict[node[0]] for node in topological_sort(dependencies_graph)]


MODEL_ECD = 'ecd'


MODEL_WEIGHTS_FILE_NAME = 'model_weights'


@DeveloperAPI
def register_encoder_config(name: str, features: Union[str, List[str]]):
    if isinstance(features, str):
        features = [features]

    def wrap(cls):
        for feature in features:
            feature_registry = encoder_config_registry.get(feature, {})
            feature_registry[name] = cls
            encoder_config_registry[feature] = feature_registry
        return cls
    return wrap


COMBINER = 'combiner'


DECODER = 'decoder'


DEFAULTS = 'defaults'


@DeveloperAPI
def DefaultsDataclassField(feature_type: str):
    """Custom dataclass field that when used inside a dataclass will allow the user to specify a nested default
    config for a specific feature type.

    Returns: Initialized dataclass field that converts an untyped dict with params to a defaults config.
    """


    class DefaultMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict for a valid defaults config from the feature_registry
        and creates a corresponding JSON schema for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                input_feature_class = input_mixin_registry[feature_type]
                output_feature_class = output_mixin_registry.get(feature_type, None)
                try:
                    input_schema = input_feature_class.Schema().load(value)
                    if output_feature_class:
                        output_schema = output_feature_class.Schema().load(value)
                        combined = input_schema + output_schema
                    else:
                        combined = input_schema
                    return combined
                except (TypeError, ValidationError) as error:
                    raise ValidationError(f'Invalid params: {value}, see `{attr}` definition. Error: {error}')
            raise ValidationError(f'Invalid params: {value}')

        @staticmethod
        def _jsonschema_type_mapping():
            input_feature_cls = input_mixin_registry.get(feature_type)
            output_feature_cls = output_mixin_registry.get(feature_type, None)
            input_props = schema_utils.unload_jsonschema_from_marshmallow_class(input_feature_cls)['properties']
            if output_feature_cls:
                output_props = schema_utils.unload_jsonschema_from_marshmallow_class(output_feature_cls)['properties']
                combined_props = {**output_props, **input_props}
            else:
                combined_props = input_props
            return {'type': 'object', 'properties': combined_props, 'additionalProperties': False, 'title': 'defaults_options'}
    try:
        input_cls = input_mixin_registry[feature_type]
        output_cls = output_mixin_registry.get(feature_type, None)
        dump_default = input_cls.Schema().dump({'type': feature_type})
        if output_cls:
            output_dump = output_cls.Schema().dump({'type': feature_type})
            dump_default = {**output_dump, **dump_default}
        load_default = input_cls.Schema().load({'type': feature_type})
        if output_cls:
            output_load = output_cls.Schema().load({'type': feature_type})
            for k in dump_default.keys():
                if getattr(load_default, k, -1) == -1:
                    setattr(load_default, k, getattr(output_load, k))
        return field(metadata={'marshmallow_field': DefaultMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default)}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f'Unsupported feature type: {feature_type}. See input_type_registry. Details: {e}')


DEFAULT_BATCH_SIZE = 128


@DeveloperAPI
def GradientClippingDataclassField(description: str, default: Dict={}):
    """Returns custom dataclass field for `ludwig.modules.optimization_modules.GradientClippingConfig`. Allows
    `None` by default.

    :param description: Description of the gradient dataclass field
    :param default: dict that specifies clipping param values that will be loaded by its schema class (default: {}).
    """
    allow_none = True


    class GradientClippingMarshmallowField(fields.Field):
        """Custom marshmallow field class for gradient clipping.

        Deserializes a dict to a valid instance of `ludwig.modules.optimization_modules.GradientClippingConfig` and
        creates a corresponding JSON schema for external usage.
        """

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return value
            if isinstance(value, dict):
                try:
                    return GradientClippingConfig.Schema().load(value)
                except (TypeError, ValidationError):
                    raise ValidationError(f'Invalid params for gradient clipping: {value}, see GradientClippingConfig class.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            return {'oneOf': [{'type': 'null', 'title': 'disabled', 'description': 'Disable gradient clipping.'}, {**schema_utils.unload_jsonschema_from_marshmallow_class(GradientClippingConfig), 'title': 'enabled_options'}], 'title': 'gradient_clipping_options', 'description': description}
    if not isinstance(default, dict):
        raise ValidationError(f'Invalid default: `{default}`')
    load_default = GradientClippingConfig.Schema().load(default)
    dump_default = GradientClippingConfig.Schema().dump(default)
    return field(metadata={'marshmallow_field': GradientClippingMarshmallowField(allow_none=allow_none, load_default=load_default, dump_default=dump_default, metadata={'description': description, 'parameter_metadata': convert_metadata_to_json(TRAINER_METADATA['gradient_clipping'])})}, default_factory=lambda : load_default)


MAX_POSSIBLE_BATCH_SIZE = 1099511627776


@DeveloperAPI
def get_optimizer_conds():
    """Returns a JSON schema of conditionals to validate against optimizer types defined in
    `ludwig.modules.optimization_modules.optimizer_registry`."""
    conds = []
    for optimizer in optimizer_registry:
        optimizer_cls = optimizer_registry[optimizer][1]
        other_props = schema_utils.unload_jsonschema_from_marshmallow_class(optimizer_cls)['properties']
        schema_utils.remove_duplicate_fields(other_props)
        preproc_cond = schema_utils.create_cond({'type': optimizer}, other_props)
        conds.append(preproc_cond)
    return conds


@DeveloperAPI
def OptimizerDataclassField(default={'type': 'adam'}, description='TODO'):
    """Custom dataclass field that when used inside of a dataclass will allow any optimizer in
    `ludwig.modules.optimization_modules.optimizer_registry`.

    Sets default optimizer to 'adam'.

    :param default: Dict specifying an optimizer with a `type` field and its associated parameters. Will attempt to use
           `type` to load optimizer from registry with given params. (default: {"type": "adam"}).
    :return: Initialized dataclass field that converts untyped dicts with params to optimizer dataclass instances.
    """


    class OptimizerMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict to a valid optimizer from
        `ludwig.modules.optimization_modules.optimizer_registry` and creates a corresponding `oneOf` JSON schema
        for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if 'type' in value and value['type'] in optimizer_registry:
                    opt = optimizer_registry[value['type'].lower()][1]
                    try:
                        return opt.Schema().load(value)
                    except (TypeError, ValidationError) as e:
                        raise ValidationError(f'Invalid params for optimizer: {value}, see `{opt}` definition. Error: {e}')
                raise ValidationError(f'Invalid params for optimizer: {value}, expect dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': list(optimizer_registry.keys()), 'default': default['type'], 'description': 'The type of optimizer to use during the learning process'}}, 'title': 'optimizer_options', 'allOf': get_optimizer_conds(), 'required': ['type'], 'description': description}
    if not isinstance(default, dict) or 'type' not in default or default['type'] not in optimizer_registry:
        raise ValidationError(f'Invalid default: `{default}`')
    try:
        opt = optimizer_registry[default['type'].lower()][1]
        load_default = opt.Schema()
        load_default = load_default.load(default)
        dump_default = opt.Schema().dump(default)
        return field(metadata={'marshmallow_field': OptimizerMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default, metadata={'description': description})}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f"Unsupported optimizer type: {default['type']}. See optimizer_registry. Details: {e}")


@DeveloperAPI
def register_trainer_schema(model_type: str):

    def wrap(trainer_config: BaseTrainerConfig):
        trainer_schema_registry[model_type] = trainer_config
        return trainer_config
    return wrap


EXECUTOR = 'executor'


MODEL_GBM = 'gbm'


HYPEROPT = 'hyperopt'


INPUT_FEATURES = [{'name': 'num_1', 'type': 'number'}, {'name': 'num_2', 'type': 'number'}]


MODEL_TYPE = 'model_type'


ModelConfigDict = Dict[str, Any]


OPTIMIZER = 'optimizer'


OUTPUT_FEATURES = [{'name': 'y', 'type': 'number'}]


RANDOM = 'random'


@DeveloperAPI
def get_split_conds():
    """Returns a JSON schema of conditionals to validate against optimizer types defined in
    `ludwig.modules.optimization_modules.optimizer_registry`."""
    conds = []
    for splitter in split_config_registry.data:
        splitter_cls = split_config_registry.data[splitter]
        other_props = schema_utils.unload_jsonschema_from_marshmallow_class(splitter_cls)['properties']
        schema_utils.remove_duplicate_fields(other_props, TYPE)
        splitter_cond = schema_utils.create_cond({'type': splitter}, other_props)
        conds.append(splitter_cond)
    return conds


@DeveloperAPI
def SplitDataclassField(default: str):
    """Custom dataclass field that when used inside a dataclass will allow the user to specify a nested split
    config.

    Returns: Initialized dataclass field that converts an untyped dict with params to a split config.
    """


    class SplitMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict for a valid split config from the split_registry and
        creates a corresponding JSON schema for external usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if TYPE in value and value[TYPE] in split_config_registry.data:
                    split_class = split_config_registry.data[value[TYPE]]
                    try:
                        return split_class.get_schema_cls().Schema().load(value)
                    except (TypeError, ValidationError) as error:
                        raise ValidationError(f'Invalid split params: {value}, see `{split_class}` definition. Error: {error}')
                raise ValidationError(f'Invalid params for splitter: {value}, expected dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': list(split_config_registry.data.keys()), 'default': default}}, 'title': 'split_options', 'allOf': get_split_conds()}
    try:
        splitter = split_config_registry.data[default]
        load_default = splitter.Schema().load({'type': default})
        dump_default = splitter.Schema().dump({'type': default})
        return field(metadata={'marshmallow_field': SplitMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default)}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f'Unsupported splitter type: {default}. See split_registry. Details: {e}')


RAY = 'ray'


RECURSION_STOP_ENUM = {'weights_initializer', 'bias_initializer', 'norm_params'}


SPLIT = 'split'


TRAINER = 'trainer'


def get_feature_to_metric_names_map(output_features: List[FeatureConfigDict]) ->Dict[str, List[str]]:
    """Returns a dict of output_feature_name -> list of metric names."""
    metrics_names = {}
    for output_feature in output_features:
        output_feature_name = output_feature[NAME]
        output_feature_type = output_feature[TYPE]
        metrics_names[output_feature_name] = get_output_type_registry()[output_feature_type].metric_functions
    metrics_names[COMBINED] = [LOSS]
    return metrics_names


@DeveloperAPI
def get_input_feature_cls(name: str):
    return input_config_registry[name]


@DeveloperAPI
def get_optimizer_cls(name: str):
    """Get the optimizer schema class from the optimizer schema class registry."""
    return optimizer_registry[name][1]


@DeveloperAPI
def get_output_feature_cls(name: str):
    return output_config_registry[name]


@DeveloperAPI
def get_split_cls(name: str):
    return split_config_registry[name]


LUDWIG_VERSION = 'ludwig_version'


@DeveloperAPI
def upgrade_config_dict_to_latest_version(config: ModelConfigDict) ->ModelConfigDict:
    """Updates config from an older version of Ludwig to the current version. If config does not have a
    "ludwig_version" key, all updates are applied.

    Args:
        config: A config saved by an older version of Ludwig.

    Returns A new copy of config, upgraded to the current Ludwig version. Returns config if config has no
            "ludwig_version".
    """
    return config_transformation_registry.update_config(config, from_version=config.get('ludwig_version', '0.0'), to_version=LUDWIG_VERSION)


@DeveloperAPI
def get_combiner_conds():
    """Returns a list of if-then JSON clauses for each combiner type in `combiner_registry` and its properties'
    constraints."""
    combiner_types = sorted(list(combiner_registry.keys()))
    conds = []
    for combiner_type in combiner_types:
        combiner_cls = combiner_registry[combiner_type]
        schema_cls = combiner_cls.get_schema_cls()
        combiner_schema = schema_utils.unload_jsonschema_from_marshmallow_class(schema_cls)
        combiner_props = combiner_schema['properties']
        schema_utils.remove_duplicate_fields(combiner_props)
        combiner_cond = schema_utils.create_cond({'type': combiner_type}, combiner_props)
        conds.append(combiner_cond)
    return conds


@DeveloperAPI
def get_combiner_jsonschema():
    """Returns a JSON schema structured to only require a `type` key and then conditionally apply a corresponding
    combiner's field constraints."""
    combiner_types = sorted(list(combiner_registry.keys()))
    parameter_metadata = convert_metadata_to_json(COMBINER_METADATA[TYPE])
    return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': combiner_types, 'default': 'concat', 'title': 'combiner_options', 'description': 'Select the combiner type.', 'parameter_metadata': parameter_metadata}}, 'allOf': get_combiner_conds(), 'required': ['type']}


@DeveloperAPI
def get_defaults_jsonschema():
    """Returns a JSON schema structured to only require a `type` key and then conditionally apply a corresponding
    combiner's field constraints."""
    preproc_schema = schema_utils.unload_jsonschema_from_marshmallow_class(DefaultsConfig)
    props = preproc_schema['properties']
    return {'type': 'object', 'properties': props, 'additionalProperties': False, 'title': 'global_defaults_options', 'description': 'Set global defaults for input and output features'}


@DeveloperAPI
def get_scheduler_conds():
    """Returns a JSON schema of conditionals to validate against scheduler types defined in
    `ludwig.schema.hyperopt.scheduler_registry`."""
    conds = []
    for scheduler_config in scheduler_config_registry:
        scheduler_cls = scheduler_config_registry[scheduler_config]
        other_props = schema_utils.unload_jsonschema_from_marshmallow_class(scheduler_cls)['properties']
        schema_utils.remove_duplicate_fields(other_props)
        preproc_cond = schema_utils.create_cond({'type': scheduler_config}, other_props)
        conds.append(preproc_cond)
    return conds


@DeveloperAPI
def SchedulerDataclassField(default={'type': 'fifo'}, description='Hyperopt scheduler settings.'):
    """Custom dataclass field that when used inside of a dataclass will allow any scheduler in
    `ludwig.schema.hyperopt.scheduler.scheduler_registry`. Sets default scheduler to 'fifo'.

    :param default: Dict specifying a scheduler with a `type` field and its associated parameters. Will attempt to use
           `type` to load scheduler from registry with given params. (default: {"type": "fifo"}).
    :return: Initialized dataclass field that converts untyped dicts with params to scheduler dataclass instances.
    """


    class SchedulerMarshmallowField(fields.Field):
        """Custom marshmallow field that deserializes a dict to a valid scheduler from
        `ludwig.schema.hyperopt.scheduler_registry` and creates a corresponding `oneOf` JSON schema for external
        usage."""

        def _deserialize(self, value, attr, data, **kwargs):
            if value is None:
                return None
            if isinstance(value, dict):
                if 'type' in value and value['type'] in scheduler_config_registry:
                    scheduler_config_cls = scheduler_config_registry[value['type'].lower()]
                    try:
                        return scheduler_config_cls.Schema().load(value)
                    except (TypeError, ValidationError) as e:
                        raise ValidationError(f'Invalid params for scheduler: {value}, see `{opt}` definition. Error: {e}')
                raise ValidationError(f'Invalid params for scheduler: {value}, expect dict with at least a valid `type` attribute.')
            raise ValidationError('Field should be None or dict')

        @staticmethod
        def _jsonschema_type_mapping():
            return {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': list(scheduler_config_registry.keys()), 'default': default['type'], 'description': 'The type of scheduler to use during hyperopt'}}, 'title': 'scheduler_options', 'allOf': get_scheduler_conds(), 'required': ['type'], 'description': description}
    if not isinstance(default, dict) or 'type' not in default or default['type'] not in scheduler_config_registry:
        raise ValidationError(f'Invalid default: `{default}`')
    try:
        opt = scheduler_config_registry[default['type'].lower()]
        load_default = opt.Schema().load(default)
        dump_default = opt.Schema().dump(default)
        return field(metadata={'marshmallow_field': SchedulerMarshmallowField(allow_none=False, dump_default=dump_default, load_default=load_default, metadata={'description': description})}, default_factory=lambda : load_default)
    except Exception as e:
        raise ValidationError(f"Unsupported scheduler type: {default['type']}. See scheduler_config_registry. Details: {e}")


@DeveloperAPI
def ExecutorDataclassField(description: str, default: Dict={}):


    class ExecutorMarshmallowField(fields.Field):

        def _deserialize(self, value, attr, data, **kwargs):
            if isinstance(value, dict):
                try:
                    return ExecutorConfig.Schema().load(value)
                except (TypeError, ValidationError):
                    raise ValidationError(f'Invalid params for executor: {value}, see ExecutorConfig class.')
            raise ValidationError('Field should be dict')

        def _jsonschema_type_mapping(self):
            return {**schema_utils.unload_jsonschema_from_marshmallow_class(ExecutorConfig), 'title': 'executor', 'description': description}
    if not isinstance(default, dict):
        raise ValidationError(f'Invalid default: `{default}`')
    load_default = ExecutorConfig.Schema().load(default)
    dump_default = ExecutorConfig.Schema().dump(default)
    return field(metadata={'marshmallow_field': ExecutorMarshmallowField(allow_none=False, load_default=load_default, dump_default=dump_default, metadata={'description': description, 'parameter_metadata': None})}, default_factory=lambda : load_default)


@DeveloperAPI
def SearchAlgorithmDataclassField(description: str='', default: Dict={'type': 'variant_generator'}):


    class SearchAlgorithmMarshmallowField(fields.Field):

        def _deserialize(self, value, attr, data, **kwargs):
            if isinstance(value, dict):
                try:
                    return BaseSearchAlgorithmConfig.Schema().load(value)
                except (TypeError, ValidationError):
                    raise ValidationError(f'Invalid params for scheduler: {value}, see SearchAlgorithmConfig class.')
            raise ValidationError('Field should be dict')

        def _jsonschema_type_mapping(self):
            return {**schema_utils.unload_jsonschema_from_marshmallow_class(BaseSearchAlgorithmConfig), 'title': 'scheduler', 'description': description}
    if not isinstance(default, dict):
        raise ValidationError(f'Invalid default: `{default}`')
    load_default = BaseSearchAlgorithmConfig.Schema().load(default)
    dump_default = BaseSearchAlgorithmConfig.Schema().dump(default)
    return field(metadata={'marshmallow_field': SearchAlgorithmMarshmallowField(allow_none=False, load_default=load_default, dump_default=dump_default, metadata={'description': description, 'parameter_metadata': None})}, default_factory=lambda : load_default)


TEST = 'test'


TRAIN = 'train'


VALIDATION = 'validation'


@DeveloperAPI
def get_hyperopt_jsonschema():
    props = schema_utils.unload_jsonschema_from_marshmallow_class(HyperoptConfig)['properties']
    return {'type': 'object', 'properties': props, 'title': 'hyperopt_options', 'description': 'Settings for hyperopt'}


@DeveloperAPI
def get_input_feature_conds():
    """This function returns a list of if-then JSON clauses for each input feature type along with their properties
    and constraints.

    Returns: List of JSON clauses
    """
    input_feature_types = sorted(list(input_config_registry.keys()))
    conds = []
    for feature_type in input_feature_types:
        schema_cls = get_input_feature_cls(feature_type)
        feature_schema = schema_utils.unload_jsonschema_from_marshmallow_class(schema_cls)
        feature_props = feature_schema['properties']
        schema_utils.remove_duplicate_fields(feature_props)
        feature_cond = schema_utils.create_cond({'type': feature_type}, feature_props)
        conds.append(feature_cond)
    return conds


def prune_gbm_features(schema: Dict):
    """Removes unsupported feature types from the given JSON schema.

    Designed for use with `get_{input/output}_feature_jsonschema`.
    """
    gbm_feature_types = ['binary', 'category', 'number']
    pruned_all_of = []
    for cond in schema['items']['allOf']:
        if_type = cond['if']['properties']['type']['const']
        if if_type in gbm_feature_types:
            pruned_all_of += [cond]
    schema['items']['allOf'] = pruned_all_of


@DeveloperAPI
def get_input_feature_jsonschema(model_type: str):
    """This function returns a JSON schema structured to only requires a `type` key and then conditionally applies
    a corresponding input feature's field constraints.

    Returns: JSON Schema
    """
    input_feature_types = sorted(list(input_config_registry.keys()))
    schema = {'type': 'array', 'minItems': 1, 'items': {'type': 'object', 'properties': {'name': {'type': 'string', 'title': 'name', 'description': 'Name of the input feature.'}, 'type': {'type': 'string', 'enum': input_feature_types, 'title': 'type', 'description': 'Type of the input feature'}, 'column': {'type': 'string', 'title': 'column', 'description': 'Name of the column.'}}, 'additionalProperties': True, 'allOf': get_input_feature_conds(), 'required': ['name', 'type'], 'title': 'input_features'}, 'uniqueItemProperties': ['name']}
    if model_type == MODEL_GBM:
        prune_gbm_features(schema)
    return schema


def get_ludwig_version_jsonschema():
    return {'type': 'string', 'title': 'ludwig_version', 'description': 'Current Ludwig model schema version.'}


@DeveloperAPI
def get_model_type_jsonschema(model_type: str=MODEL_ECD):
    enum = [MODEL_ECD, 'ecd_ray_legacy']
    if model_type == MODEL_GBM:
        enum = [MODEL_GBM]
    return {'type': 'string', 'enum': enum, 'default': MODEL_ECD, 'title': 'model_type', 'description': 'Select the model type.'}


@DeveloperAPI
def get_output_feature_conds():
    """This function returns a list of if-then JSON clauses for each output feature type along with their
    properties and constraints.

    Returns: List of JSON clauses
    """
    output_feature_types = sorted(list(output_config_registry.keys()))
    conds = []
    for feature_type in output_feature_types:
        schema_cls = get_output_feature_cls(feature_type)
        feature_schema = schema_utils.unload_jsonschema_from_marshmallow_class(schema_cls)
        feature_props = feature_schema['properties']
        schema_utils.remove_duplicate_fields(feature_props)
        feature_cond = schema_utils.create_cond({'type': feature_type}, feature_props)
        conds.append(feature_cond)
    return conds


@DeveloperAPI
def get_output_feature_jsonschema(model_type: str):
    """This function returns a JSON schema structured to only requires a `type` key and then conditionally applies
    a corresponding output feature's field constraints.

    Returns: JSON Schema
    """
    output_feature_types = sorted(list(output_config_registry.keys()))
    schema = {'type': 'array', 'minItems': 1, 'items': {'type': 'object', 'properties': {'name': {'type': 'string', 'title': 'name', 'description': 'Name of the output feature.'}, 'type': {'type': 'string', 'enum': output_feature_types, 'title': 'type', 'description': 'Type of the output feature'}, 'column': {'type': 'string', 'title': 'column', 'description': 'Name of the column.'}}, 'additionalProperties': True, 'allOf': get_output_feature_conds(), 'required': ['name', 'type'], 'title': 'output_features'}}
    if model_type == MODEL_GBM:
        prune_gbm_features(schema)
        schema['maxItems'] = 1
    return schema


@DeveloperAPI
def get_preprocessing_jsonschema():
    """Returns a JSON schema structured to only require a `type` key and then conditionally apply a corresponding
    combiner's field constraints."""
    preproc_schema = schema_utils.unload_jsonschema_from_marshmallow_class(PreprocessingConfig)
    props = preproc_schema['properties']
    return {'type': 'object', 'properties': props, 'additionalProperties': True, 'title': 'global_preprocessing_options', 'description': 'Select the preprocessing type.'}


@DeveloperAPI
def get_trainer_jsonschema(model_type: str):
    trainer_cls = trainer_schema_registry[model_type]
    props = schema_utils.unload_jsonschema_from_marshmallow_class(trainer_cls)['properties']
    return {'type': 'object', 'properties': props, 'title': 'trainer_options', 'additionalProperties': False, 'description': 'Schema for trainer determined by Model Type'}


@DeveloperAPI
@lru_cache(maxsize=2)
def get_schema(model_type: str=MODEL_ECD):
    schema = {'type': 'object', 'properties': {MODEL_TYPE: get_model_type_jsonschema(model_type), INPUT_FEATURES: get_input_feature_jsonschema(model_type), OUTPUT_FEATURES: get_output_feature_jsonschema(model_type), TRAINER: get_trainer_jsonschema(model_type), PREPROCESSING: get_preprocessing_jsonschema(), HYPEROPT: get_hyperopt_jsonschema(), DEFAULTS: get_defaults_jsonschema(), LUDWIG_VERSION: get_ludwig_version_jsonschema()}, 'definitions': {}, 'required': [INPUT_FEATURES, OUTPUT_FEATURES], 'additionalProperties': False}
    if model_type == MODEL_ECD:
        schema['properties'][COMBINER] = get_combiner_jsonschema()
    return schema


@DeveloperAPI
@lru_cache(maxsize=2)
def get_validator():

    def custom_is_array(checker, instance):
        return isinstance(instance, list) or isinstance(instance, tuple)
    type_checker = Draft7Validator.TYPE_CHECKER.redefine('array', custom_is_array)
    return extend(Draft7Validator, type_checker=type_checker)


@DeveloperAPI
def validate_config(config):
    updated_config = upgrade_config_dict_to_latest_version(config)
    model_type = updated_config.get(MODEL_TYPE, MODEL_ECD)
    splitter = get_splitter(**updated_config.get(PREPROCESSING, {}).get(SPLIT, {}))
    splitter.validate(updated_config)
    with VALIDATION_LOCK:
        validate(instance=updated_config, schema=get_schema(model_type=model_type), cls=get_validator())


@DeveloperAPI
@functools.lru_cache(1)
def load_glove(file_path: str, return_embedding_size: bool=False) ->Dict[str, np.ndarray]:
    """Loads Glove embeddings for each word.

    Returns:
        Mapping between word and numpy array of size embedding_size as set by
        first line of file.
    """
    logger.info(f'  Loading Glove format file {file_path}')
    embeddings = {}
    embedding_size = 0
    with open_file(file_path, 'r', encoding='utf-8') as f:
        found_line = False
        while not found_line:
            line = f.readline()
            if line:
                embedding_size = len(line.split()) - 1
                found_line = True
    with open_file(file_path, 'r', encoding='utf-8') as f:
        for line_number, line in enumerate(f):
            if line:
                try:
                    split = line.split()
                    if len(split) != embedding_size + 1:
                        raise ValueError(f'Line {line_number} is of length {len(split)}, while expected length is {embedding_size + 1}.')
                    word = split[0]
                    embedding = np.array([float(val) for val in split[-embedding_size:]])
                    embeddings[word] = embedding
                except ValueError:
                    logger.warning('Line {} in the GloVe file {} is malformed, skipping it'.format(line_number, file_path))
    logger.info(f'  {len(embeddings)} embeddings loaded')
    if return_embedding_size:
        return embeddings, embedding_size
    return embeddings


@DeveloperAPI
def clear_data_cache():
    """Clears any cached data objects (e.g., embeddings)"""
    load_glove.cache_clear()


def get_combiner_class(combiner_type: str):
    """Returns the corresponding combiner class from `ludwig.combiners.combiners.combiner_registry`.

    :param combiner_type: identifier that should correspond to a registered combiner
    :type combiner_type: str
    """
    return get_from_registry(combiner_type, combiner_registry)


def get_pred_col(preds, target):
    t = target.lower()
    for c in preds.keys():
        if c.lower() == t:
            if 'probabilities' in preds[c]:
                return preds[c]['probabilities']
            else:
                return preds[c]['predictions']
    raise ValueError(f'Unable to find target column {t} in {preds.keys()}')


MODEL_HYPERPARAMETERS_FILE_NAME = 'model_hyperparameters.json'


POSTPROCESSOR = 'postprocessor'


PREDICTOR = 'predictor'


PREPROCESSOR = 'preprocessor'


TRAIN_SET_METADATA_FILE_NAME = 'training_set_metadata.json'


def get_filename_from_stage(stage: str, device: TorchDevice) ->str:
    """Returns the filename for a stage of inference."""
    if stage not in [PREPROCESSOR, PREDICTOR, POSTPROCESSOR]:
        raise ValueError(f'Invalid stage: {stage}.')
    if stage == PREDICTOR:
        return f'inference_{stage}-{device}.pt'
    else:
        return f'inference_{stage}.pt'


def _init_inference_stages_from_directory(directory: str, device: TorchDevice) ->Dict[str, torch.nn.Module]:
    """Initializes inference stage modules from directory."""
    stage_to_filenames = {stage: get_filename_from_stage(stage, device) for stage in [PREPROCESSOR, PREDICTOR, POSTPROCESSOR]}
    stage_to_module = {}
    for stage in [PREPROCESSOR, PREDICTOR, POSTPROCESSOR]:
        stage_to_module[stage] = torch.jit.load(os.path.join(directory, stage_to_filenames[stage]))
        None
    return stage_to_module


class _InferencePostprocessor(nn.Module):
    """Wraps postprocessing modules into a single nn.Module.

    The forward call of this module returns a flattened dictionary in order to support Triton input/output.

    TODO(geoffrey): Implement torchscript-compatible feature_utils.LudwigFeatureDict to replace
    get_module_dict_key_from_name and get_name_from_module_dict_key usage.
    """

    def __init__(self, model: 'BaseModel', training_set_metadata: TrainingSetMetadataDict):
        super().__init__()
        self.postproc_modules = nn.ModuleDict()
        for feature_name, feature in model.output_features.items():
            module_dict_key = get_module_dict_key_from_name(feature_name)
            self.postproc_modules[module_dict_key] = feature.create_postproc_module(training_set_metadata[feature_name])

    def forward(self, predictions_flattened: Dict[str, torch.Tensor]) ->Dict[str, Any]:
        postproc_outputs_flattened: Dict[str, Any] = {}
        for module_dict_key, postproc in self.postproc_modules.items():
            feature_name = get_name_from_module_dict_key(module_dict_key)
            feature_postproc_outputs = postproc(predictions_flattened, feature_name)
            for postproc_key, tensor_values in feature_postproc_outputs.items():
                postproc_concat_key = output_feature_utils.get_feature_concat_name(feature_name, postproc_key)
                postproc_outputs_flattened[postproc_concat_key] = tensor_values
        return postproc_outputs_flattened


class _InferencePredictor(nn.Module):
    """Wraps model forward pass + predictions into a single nn.Module.

    The forward call of this module returns a flattened dictionary in order to support Triton input/output.

    TODO(geoffrey): Implement torchscript-compatible feature_utils.LudwigFeatureDict to replace
    get_module_dict_key_from_name and get_name_from_module_dict_key usage.
    """

    def __init__(self, model: 'BaseModel', device: TorchDevice):
        super().__init__()
        self.device = torch.device(device)
        self.model = model.to_torchscript(self.device)
        self.predict_modules = nn.ModuleDict()
        for feature_name, feature in model.output_features.items():
            module_dict_key = get_module_dict_key_from_name(feature_name)
            self.predict_modules[module_dict_key] = feature.prediction_module

    def forward(self, preproc_inputs: Dict[str, torch.Tensor]) ->Dict[str, torch.Tensor]:
        model_outputs = self.model(preproc_inputs)
        predictions_flattened: Dict[str, torch.Tensor] = {}
        for module_dict_key, predict in self.predict_modules.items():
            feature_name = get_name_from_module_dict_key(module_dict_key)
            feature_predictions = predict(model_outputs, feature_name)
            for predict_key, tensor_values in feature_predictions.items():
                predict_concat_key = output_feature_utils.get_feature_concat_name(feature_name, predict_key)
                predictions_flattened[predict_concat_key] = tensor_values
        return predictions_flattened


class _InferencePreprocessor(nn.Module):
    """Wraps preprocessing modules into a single nn.Module.

    TODO(geoffrey): Implement torchscript-compatible feature_utils.LudwigFeatureDict to replace
    get_module_dict_key_from_name and get_name_from_module_dict_key usage.
    """

    def __init__(self, config: ModelConfigDict, training_set_metadata: TrainingSetMetadataDict):
        super().__init__()
        self.preproc_modules = nn.ModuleDict()
        for feature_config in config['input_features']:
            feature_name = feature_config[NAME]
            feature = get_from_registry(feature_config[TYPE], get_input_type_registry())
            module_dict_key = get_module_dict_key_from_name(feature_name)
            self.preproc_modules[module_dict_key] = feature.create_preproc_module(training_set_metadata[feature_name])

    def forward(self, inputs: Dict[str, TorchscriptPreprocessingInput]) ->Dict[str, torch.Tensor]:
        preproc_inputs = {}
        for module_dict_key, preproc in self.preproc_modules.items():
            feature_name = get_name_from_module_dict_key(module_dict_key)
            preproc_inputs[feature_name] = preproc(inputs[feature_name])
        return preproc_inputs


def _init_inference_stages_from_ludwig_model(model: 'BaseModel', config: ModelConfigDict, training_set_metadata: TrainingSetMetadataDict, device: TorchDevice, scripted: bool=True) ->Dict[str, torch.nn.Module]:
    """Initializes inference stage modules from a LudwigModel (a BaseModel model, config, and
    training_set_metadata)."""
    preprocessor = _InferencePreprocessor(config, training_set_metadata)
    predictor = _InferencePredictor(model, device=device)
    postprocessor = _InferencePostprocessor(model, training_set_metadata)
    stage_to_module = {PREPROCESSOR: preprocessor, PREDICTOR: predictor, POSTPROCESSOR: postprocessor}
    if scripted:
        stage_to_module = {stage: torch.jit.script(module) for stage, module in stage_to_module.items()}
    return stage_to_module


def get_feature_name_from_concat_name(concat_name: str) ->str:
    return '::'.join(concat_name.split('::')[:-1])


def get_tensor_name_from_concat_name(concat_name: str) ->str:
    return concat_name.split('::')[-1]


def _unflatten_dict_by_feature_name(flattened_dict: Dict[str, Any]) ->Dict[str, Dict[str, Any]]:
    """Convert a flattened dictionary of objects to a nested dictionary of outputs per feature name."""
    outputs: Dict[str, Dict[str, Any]] = {}
    for concat_key, tensor_values in flattened_dict.items():
        feature_name = get_feature_name_from_concat_name(concat_key)
        tensor_name = get_tensor_name_from_concat_name(concat_key)
        feature_outputs: Dict[str, Any] = {}
        if feature_name not in outputs:
            outputs[feature_name] = feature_outputs
        else:
            feature_outputs = outputs[feature_name]
        feature_outputs[tensor_name] = tensor_values
    return outputs


def convert_dict_to_df(predictions: Dict[str, Dict[str, Union[List[Any], torch.Tensor, np.array]]]) ->pd.DataFrame:
    """Converts a dictionary of predictions into a pandas DataFrame.

    Example format of predictions dictionary:

    {
        "binary_C82EB": {
            "predictions": torch.tensor([True, True, True, False]),
            "probabilities": torch.tensor([[0.4777, 0.5223], [0.4482, 0.5518], [0.4380, 0.5620], [0.5059, 0.4941]]),
        },
        "category_1491D": {
            "predictions": ["NkNUG", "NkNUG", "NkNUG", "NkNUG"],
            "probabilities": torch.tensor(
                [
                    [0.1058, 0.4366, 0.1939, 0.2637],
                    [0.0816, 0.4807, 0.1978, 0.2399],
                    [0.0907, 0.4957, 0.1829, 0.2308],
                    [0.0728, 0.5015, 0.1900, 0.2357],
                ]
            ),
        },
        "num_7B25F": {"predictions": torch.tensor([2.0436, 2.1158, 2.1222, 2.1964])},
    }
    """
    output = {}
    for of_name, preds_dict in predictions.items():
        for key, value in preds_dict.items():
            output_key = f'{of_name}_{key}'
            if not isinstance(value, list):
                value = value.tolist()
            output[output_key] = value
    return pd.DataFrame.from_dict(output)


MISSING_VALUE_STRATEGY = 'missing_value_strategy'


def _is_old_missing_value_strategy(feature_config: FeatureConfigDict):
    if PREPROCESSING not in feature_config:
        return False
    missing_value_strategy = feature_config.get(PREPROCESSING).get(MISSING_VALUE_STRATEGY, None)
    if not missing_value_strategy or missing_value_strategy not in ('backfill', 'pad'):
        return False
    return True


def _update_old_missing_value_strategy(feature_config: FeatureConfigDict):
    missing_value_strategy = feature_config.get(PREPROCESSING).get(MISSING_VALUE_STRATEGY)
    replacement_strategy = 'bfill' if missing_value_strategy == 'backfill' else 'ffill'
    feature_name = feature_config.get(NAME)
    warnings.warn(f'Using `{replacement_strategy}` instead of `{missing_value_strategy}` as the missing value strategy for `{feature_name}`. These are identical. `{missing_value_strategy}` will be removed in v0.8', DeprecationWarning)
    feature_config[PREPROCESSING].update({MISSING_VALUE_STRATEGY: replacement_strategy})


def _upgrade_metadata_missing_values(metadata: TrainingSetMetadataDict):
    for k, v in metadata.items():
        if isinstance(v, dict) and _is_old_missing_value_strategy(v):
            _update_old_missing_value_strategy(v)


def upgrade_metadata(metadata: TrainingSetMetadataDict) ->TrainingSetMetadataDict:
    metadata = copy.deepcopy(metadata)
    _upgrade_metadata_missing_values(metadata)
    return metadata


def load_metadata(metadata_file_path: str) ->TrainingSetMetadataDict:
    logger.info(f'Loading metadata from: {metadata_file_path}')
    training_set_metadata = data_utils.load_json(metadata_file_path)
    training_set_metadata = upgrade_metadata(training_set_metadata)
    return training_set_metadata


@DeveloperAPI
def place_on_device(x, device):
    """Recursively places the input on the specified device."""
    if isinstance(x, list):
        return [place_on_device(xi, device) for xi in x]
    elif isinstance(x, dict):
        return {k: place_on_device(v, device) for k, v in x.items()}
    elif isinstance(x, set):
        return {place_on_device(xi, device) for xi in x}
    elif isinstance(x, tuple):
        return tuple(place_on_device(xi, device) for xi in x)
    elif isinstance(x, torch.Tensor):
        return x
    else:
        return x


FEATURES_TO_CAST_AS_STRINGS = {BINARY, CATEGORY, BAG, SET, TEXT, SEQUENCE, TIMESERIES, VECTOR}


@DeveloperAPI
def read_audio_from_path(path: str) ->Optional[TorchAudioTuple]:
    """Reads audio from path.

    Useful for reading from a small number of paths. For more intensive reads, use backend.read_binary_files instead.
    """
    bytes_obj = get_bytes_obj_from_path(path)
    return read_audio_from_bytes_obj(bytes_obj)


def to_inference_model_input_from_series(s: pd.Series, feature_type: str, load_paths: bool=False, feature_config: Optional[FeatureConfigDict]=None) ->TorchscriptPreprocessingInput:
    """Converts a pandas Series to be compatible with a torchscripted InferenceModule forward pass."""
    if feature_type == IMAGE:
        if load_paths:
            return [(read_image_from_path(v) if isinstance(v, str) else v) for v in s]
    elif feature_type == AUDIO:
        if load_paths:
            return [(read_audio_from_path(v) if isinstance(v, str) else v) for v in s]
    elif feature_type == DATE:
        if feature_config is None:
            raise ValueError('"date" feature type requires the associated feature config to be provided.')
        datetime_format = feature_config['preprocessing']['datetime_format']
        return [torch.tensor(create_vector_from_datetime_obj(datetime.strptime(v, datetime_format))) for v in s]
    elif feature_type in FEATURES_TO_CAST_AS_STRINGS:
        return s.astype(str).to_list()
    return torch.from_numpy(s.to_numpy())


def to_inference_module_input_from_dataframe(dataset: pd.DataFrame, config: ModelConfigDict, load_paths: bool=False, device: Optional[torch.device]=None) ->Dict[str, TorchscriptPreprocessingInput]:
    """Converts a pandas DataFrame to be compatible with a torchscripted InferenceModule forward pass."""
    inputs = {}
    for if_config in config['input_features']:
        feature_inputs = to_inference_model_input_from_series(dataset[if_config[COLUMN]], if_config[TYPE], load_paths=load_paths, feature_config=if_config)
        feature_inputs = place_on_device(feature_inputs, device)
        inputs[if_config[NAME]] = feature_inputs
    return inputs


class InferenceModule(nn.Module):
    """A nn.Module subclass that wraps the inference preprocessor, predictor, and postprocessor."""

    def __init__(self, preprocessor: torch.jit.ScriptModule, predictor: torch.jit.ScriptModule, postprocessor: torch.jit.ScriptModule, config: Optional[ModelConfigDict]=None, training_set_metadata: Optional[TrainingSetMetadataDict]=None):
        super().__init__()
        self.preprocessor = preprocessor
        self.predictor = predictor
        self.postprocessor = postprocessor
        self.config = config
        self.training_set_metadata = training_set_metadata

    def preprocessor_forward(self, inputs: Dict[str, TorchscriptPreprocessingInput]) ->Dict[str, torch.Tensor]:
        """Forward pass through the preprocessor."""
        return self.preprocessor(inputs)

    def predictor_forward(self, preproc_inputs: Dict[str, torch.Tensor]) ->Dict[str, torch.Tensor]:
        """Forward pass through the predictor.

        Ensures that the inputs are on the correct device. The outputs are on the same device as self.predictor.
        """
        for k, v in preproc_inputs.items():
            preproc_inputs[k] = v
        with torch.no_grad():
            predictions_flattened = self.predictor(preproc_inputs)
            return predictions_flattened

    def postprocessor_forward(self, predictions_flattened: Dict[str, torch.Tensor]) ->Dict[str, Dict[str, Any]]:
        """Forward pass through the postprocessor."""
        postproc_outputs_flattened: Dict[str, Any] = self.postprocessor(predictions_flattened)
        postproc_outputs: Dict[str, Dict[str, Any]] = _unflatten_dict_by_feature_name(postproc_outputs_flattened)
        return postproc_outputs

    def forward(self, inputs: Dict[str, TorchscriptPreprocessingInput]) ->Dict[str, Dict[str, Any]]:
        preproc_inputs: Dict[str, torch.Tensor] = self.preprocessor_forward(inputs)
        predictions_flattened: Dict[str, torch.Tensor] = self.predictor_forward(preproc_inputs)
        postproc_outputs: Dict[str, Dict[str, Any]] = self.postprocessor_forward(predictions_flattened)
        return postproc_outputs

    @torch.jit.unused
    def predict(self, dataset: pd.DataFrame, return_type: Union[dict, pd.DataFrame]=pd.DataFrame) ->Union[pd.DataFrame, dict]:
        """Predict on a batch of data with an interface similar to LudwigModel.predict."""
        inputs = to_inference_module_input_from_dataframe(dataset, self.config, load_paths=True)
        preds = self(inputs)
        if return_type == pd.DataFrame:
            preds = convert_dict_to_df(preds)
        return preds, None

    @torch.jit.unused
    @classmethod
    def from_ludwig_model(cls: 'InferenceModule', model: 'BaseModel', config: ModelConfigDict, training_set_metadata: TrainingSetMetadataDict, device: Optional[TorchDevice]=None):
        """Create an InferenceModule from a trained LudwigModel."""
        if device is None:
            logger.info(f'No device specified. Loading using device "{DEVICE}".')
            device = DEVICE
        stage_to_module = _init_inference_stages_from_ludwig_model(model, config, training_set_metadata, device=device, scripted=True)
        return cls(stage_to_module[PREPROCESSOR], stage_to_module[PREDICTOR], stage_to_module[POSTPROCESSOR], config=config, training_set_metadata=training_set_metadata)

    @torch.jit.unused
    @classmethod
    def from_directory(cls: 'InferenceModule', directory: str, device: Optional[TorchDevice]=None):
        """Create an InferenceModule from a directory containing a model, config, and training set metadata."""
        if device is None:
            logger.info(f'No device specified. Loading using device "{DEVICE}".')
            device = DEVICE
        stage_to_module = _init_inference_stages_from_directory(directory, device=device)
        config_path = os.path.join(directory, MODEL_HYPERPARAMETERS_FILE_NAME)
        config = load_json(config_path) if os.path.exists(config_path) else None
        metadata_path = os.path.join(directory, TRAIN_SET_METADATA_FILE_NAME)
        training_set_metadata = load_metadata(metadata_path) if os.path.exists(metadata_path) else None
        return cls(stage_to_module[PREPROCESSOR], stage_to_module[PREDICTOR], stage_to_module[POSTPROCESSOR], config=config, training_set_metadata=training_set_metadata)


class LogitsInputsMixin:

    @classmethod
    def get_loss_inputs(cls):
        """Maps loss to the desired predicted input type."""
        return LOGITS


def register_loss(name: str, features: Union[str, List[str]]):
    if isinstance(features, str):
        features = [features]

    def wrap(cls):
        for feature in features:
            feature_registry = loss_registry.get(feature, {})
            feature_registry[name] = cls
            loss_registry[feature] = feature_registry
        return cls
    return wrap


class MSELoss(_MSELoss, LogitsInputsMixin):
    """Mean squared error."""

    def __init__(self, **kwargs):
        super().__init__()

    @staticmethod
    def get_schema_cls():
        return MSELossConfig


class MAELoss(L1Loss, LogitsInputsMixin):
    """Mean absolute error."""

    def __init__(self, **kwargs):
        super().__init__()

    @staticmethod
    def get_schema_cls():
        return MAELossConfig


class RMSELoss(nn.Module, LogitsInputsMixin):
    """Root mean square error."""

    def __init__(self, **kwargs):
        super().__init__()
        self.mse = nn.MSELoss(**kwargs)

    def forward(self, preds: Tensor, target: Tensor) ->Tensor:
        return torch.sqrt(self.mse(preds, target))

    @staticmethod
    def get_schema_cls():
        return RMSELossConfig


class RMSPELoss(nn.Module, LogitsInputsMixin):
    """Root mean square percentage error."""

    def __init__(self, **kwargs):
        super().__init__()

    def forward(self, preds: Tensor, target: Tensor) ->Tensor:
        loss = utils.rmspe_loss(target, preds)
        return loss

    @staticmethod
    def get_schema_cls():
        return RMSPELossConfig


class BWCEWLoss(nn.Module, LogitsInputsMixin):
    """Binary weighted cross entropy loss."""

    def __init__(self, positive_class_weight: Optional[Union[Tensor, int]]=None, robust_lambda: int=0, confidence_penalty: int=0, **kwargs):
        super().__init__()
        if positive_class_weight:
            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor([positive_class_weight]), **kwargs)
        else:
            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=positive_class_weight, **kwargs)
        self.robust_lambda = robust_lambda
        self.confidence_penalty = confidence_penalty

    def forward(self, preds: torch.Tensor, target: torch.Tensor):
        train_loss = self.loss_fn(preds, target.float())
        if self.robust_lambda > 0:
            train_loss = (1 - self.robust_lambda) * train_loss + self.robust_lambda / 2
        train_mean_loss = torch.mean(train_loss)
        if self.confidence_penalty > 0:
            probabilities = torch.sigmoid(preds)
            mean_penalty = utils.mean_confidence_penalty(probabilities, 2)
            train_mean_loss += self.confidence_penalty * mean_penalty
        return train_mean_loss

    @staticmethod
    def get_schema_cls():
        return BWCEWLossConfig


class SoftmaxCrossEntropyLoss(nn.Module, LogitsInputsMixin):

    def __init__(self, class_weights: Optional[Union[Tensor, List]]=None, **kwargs):
        """
        Params:
            class_weights: List or 1D tensor of length equal to number of classes.
        """
        super().__init__()
        if class_weights:
            self.loss_fn = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights))
        else:
            self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, preds: Tensor, target: Tensor) ->Tensor:
        """
        Params:
            preds: Tensor of shape [batch x num_classes]
            target: Tensor of shape [batch], where each element is integral
                between 0 and num_classes.
        """
        target = target.long()
        return self.loss_fn(preds, target)

    @staticmethod
    def get_schema_cls():
        return SoftmaxCrossEntropyLossConfig


class SequenceSoftmaxCrossEntropyLoss(nn.Module, LogitsInputsMixin):

    def __init__(self, **kwargs):
        """
        Params:
            class_weights: List or 1D tensor of length equal to number of classes.
        """
        super().__init__()
        self.loss_fn = nn.CrossEntropyLoss(ignore_index=strings_utils.SpecialSymbol.PADDING.value)

    def forward(self, preds: Tensor, target: Tensor) ->Tensor:
        """
        Params:
            preds: Tensor of shape [batch x sequence_length x vocab_size]
            target: Tensor of shape [batch x sequence_length], where each element is integral between 0 and vocab_size.
        """
        target = target.long()
        return self.loss_fn(preds[1:].view(-1, preds.size(-1)), target[1:].view(-1))

    @staticmethod
    def get_schema_cls():
        return SequenceSoftmaxCrossEntropyLossConfig


class SigmoidCrossEntropyLoss(nn.Module, LogitsInputsMixin):

    def __init__(self, class_weights: Optional[Union[Tensor, List]]=None, **kwargs):
        """
        Params:
            class_weights: List or 1D tensor of length equal to number of classes.
        """
        super().__init__()
        if class_weights:
            self.loss_fn = nn.BCEWithLogitsLoss(pos_weight=torch.Tensor(class_weights))
        else:
            self.loss_fn = nn.BCEWithLogitsLoss()

    def forward(self, preds: Tensor, target: Tensor) ->Tensor:
        if preds.ndim != 2:
            raise RuntimeError('SigmoidCrossEntropyLoss currently supported for 2D tensors.')
        return self.loss_fn(preds.type(torch.float32), target.type(torch.float32))

    @staticmethod
    def get_schema_cls():
        return SigmoidCrossEntropyLossConfig


@DeveloperAPI
class ECELoss(nn.Module):
    """Calculates the Expected Calibration Error of a model.

    The input to this loss is the logits of a model, NOT the softmax scores.
    This divides the confidence outputs into equally-sized interval bins.
    In each bin, we compute the confidence gap:

        bin_gap = | avg_confidence_in_bin - accuracy_in_bin |

    We then return an average of the gaps, weighted by the number of samples in each bin.

    References:
        Naeini, Mahdi Pakdaman, Gregory F. Cooper, and Milos Hauskrecht
        "Obtaining Well Calibrated Probabilities Using Bayesian Binning." AAAI. 2015.

        Chuan Guo, Geoff Pleiss, Yu Sun, Kilian Q. Weinberger
        "On Calibration of Modern Neural Networks." PMLR 2017.
    """

    def __init__(self, n_bins: int=15):
        """n_bins (int): number of confidence interval bins."""
        super().__init__()
        bin_boundaries = torch.linspace(0, 1, n_bins + 1)
        self.bin_lowers = bin_boundaries[:-1]
        self.bin_uppers = bin_boundaries[1:]

    def forward(self, logits: torch.Tensor, one_hot_labels: torch.Tensor) ->torch.Tensor:
        softmaxes = nn.functional.softmax(logits, dim=1)
        confidences, predictions = torch.max(softmaxes, 1)
        labels = torch.argmax(one_hot_labels, 1)
        accuracies = predictions.eq(labels)
        ece = torch.zeros(1, device=logits.device)
        for bin_lower, bin_upper in zip(self.bin_lowers, self.bin_uppers):
            in_bin = confidences.gt(bin_lower.item()) * confidences.le(bin_upper.item())
            prop_in_bin = in_bin.float().mean()
            if prop_in_bin.item() > 0:
                accuracy_in_bin = accuracies[in_bin].float().mean()
                avg_confidence_in_bin = confidences[in_bin].mean()
                ece += torch.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin
        return ece


@DeveloperAPI
def register_calibration(name: str, features: Union[str, List[str]], default=False):
    """Registers a calibration implementation for a list of features."""
    if isinstance(features, str):
        features = [features]

    def wrap(cls):
        for feature in features:
            feature_registry = calibration_registry.get(feature, {})
            feature_registry[name] = cls
            if default:
                for key in DEFAULT_KEYS:
                    feature_registry[key] = cls
            calibration_registry[feature] = feature_registry
        return cls
    return wrap


def _make_ix_like(X, dim):
    d = X.size(dim)
    rho = torch.arange(1, d + 1, device=X.device, dtype=X.dtype)
    view = [1] * X.dim()
    view[0] = -1
    return rho.view(view).transpose(0, dim)


def _roll_last(X, dim):
    if dim == -1:
        return X
    elif dim < 0:
        dim = X.dim() - dim
    perm = [i for i in range(X.dim()) if i != dim] + [dim]
    return X.permute(perm)


def _sparsemax_threshold_and_support(X, dim=-1, k=None):
    """Core computation for sparsemax: optimal threshold and support size.

    Parameters
    ----------
    X : torch.Tensor
        The input tensor to compute thresholds over.

    dim : int
        The dimension along which to apply sparsemax.

    k : int or None
        number of largest elements to partial-sort over. For optimal
        performance, should be slightly bigger than the expected number of
        nonzeros in the solution. If the solution is more than k-sparse,
        this function is recursively called with a 2*k schedule.
        If `None`, full sorting is performed from the beginning.

    Returns
    -------
    tau : torch.Tensor like `X`, with all but the `dim` dimension intact
        the threshold value for each vector
    support_size : torch LongTensor, shape like `tau`
        the number of nonzeros in each vector.
    """
    if k is None or k >= X.shape[dim]:
        topk, _ = torch.sort(X, dim=dim, descending=True)
    else:
        topk, _ = torch.topk(X, k=k, dim=dim)
    topk_cumsum = topk.cumsum(dim) - 1
    rhos = _make_ix_like(topk, dim)
    support = rhos * topk > topk_cumsum
    support_size = support.sum(dim=dim).unsqueeze(dim)
    tau = topk_cumsum.gather(dim, support_size - 1)
    tau /= support_size
    if k is not None and k < X.shape[dim]:
        unsolved = (support_size == k).squeeze(dim)
        if torch.any(unsolved):
            in_ = _roll_last(X, dim)[unsolved]
            tau_, ss_ = _sparsemax_threshold_and_support(in_, dim=-1, k=2 * k)
            _roll_last(tau, dim)[unsolved] = tau_
            _roll_last(support_size, dim)[unsolved] = ss_
    return tau, support_size


def _sparsemax_forward(X, dim, k):
    max_val, _ = X.max(dim=dim, keepdim=True)
    X = X - max_val
    tau, supp_size = _sparsemax_threshold_and_support(X, dim=dim, k=k)
    output = torch.clamp(X - tau, min=0)
    return output, {'supp_size': supp_size}


class SparsemaxFunction(Function):

    @classmethod
    def forward(cls, ctx, X, dim=-1, k=None):
        ctx.dim = dim
        output, backwards_kwargs = _sparsemax_forward(X, dim, k)
        ctx.save_for_backward(backwards_kwargs['supp_size'], output)
        return output

    @classmethod
    def backward(cls, ctx, grad_output):
        supp_size, output = ctx.saved_tensors
        dim = ctx.dim
        grad_input = grad_output.clone()
        grad_input[output == 0] = 0
        v_hat = grad_input.sum(dim=dim) / supp_size.squeeze(dim)
        v_hat = v_hat.unsqueeze(dim)
        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)
        return grad_input, None, None


def sparsemax(X, dim=-1, k=None, training=True):
    """sparsemax: normalizing sparse transform (a la softmax).

    Solves the projection:

        min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.

    Parameters
    ----------
    X : torch.Tensor
        The input tensor.

    dim : int
        The dimension along which to apply sparsemax.

    k : int or None
        number of largest elements to partial-sort over. For optimal
        performance, should be slightly bigger than the expected number of
        nonzeros in the solution. If the solution is more than k-sparse,
        this function is recursively called with a 2*k schedule.
        If `None`, full sorting is performed from the beginning.

    Returns
    -------
    P : torch tensor, same shape as X
        The projection result, such that P.sum(dim=dim) == 1 elementwise.
    """
    if not training:
        output, _ = _sparsemax_forward(X, dim, k)
        return output
    return SparsemaxFunction.apply(X, dim, k)


class Sparsemax(nn.Module):

    def __init__(self, dim=-1, k=None):
        """sparsemax: normalizing sparse transform (a la softmax).

        Solves the projection:

            min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.

        Parameters
        ----------
        dim : int
            The dimension along which to apply sparsemax.

        k : int or None
            number of largest elements to partial-sort over. For optimal
            performance, should be slightly bigger than the expected number of
            nonzeros in the solution. If the solution is more than k-sparse,
            this function is recursively called with a 2*k schedule.
            If `None`, full sorting is performed from the beginning.
        """
        self.dim = dim
        self.k = k
        super().__init__()

    def forward(self, X):
        return sparsemax(X, dim=self.dim, k=self.k, training=self.training)


def _entmax_threshold_and_support(X, dim=-1, k=None):
    """Core computation for 1.5-entmax: optimal threshold and support size.

    Parameters
    ----------
    X : torch.Tensor
        The input tensor to compute thresholds over.

    dim : int
        The dimension along which to apply 1.5-entmax.

    k : int or None
        number of largest elements to partial-sort over. For optimal
        performance, should be slightly bigger than the expected number of
        nonzeros in the solution. If the solution is more than k-sparse,
        this function is recursively called with a 2*k schedule.
        If `None`, full sorting is performed from the beginning.

    Returns
    -------
    tau : torch.Tensor like `X`, with all but the `dim` dimension intact
        the threshold value for each vector
    support_size : torch LongTensor, shape like `tau`
        the number of nonzeros in each vector.
    """
    if k is None or k >= X.shape[dim]:
        Xsrt, _ = torch.sort(X, dim=dim, descending=True)
    else:
        Xsrt, _ = torch.topk(X, k=k, dim=dim)
    rho = _make_ix_like(Xsrt, dim)
    mean = Xsrt.cumsum(dim) / rho
    mean_sq = (Xsrt ** 2).cumsum(dim) / rho
    ss = rho * (mean_sq - mean ** 2)
    delta = (1 - ss) / rho
    delta_nz = torch.clamp(delta, 0)
    tau = mean - torch.sqrt(delta_nz)
    support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)
    tau_star = tau.gather(dim, support_size - 1)
    if k is not None and k < X.shape[dim]:
        unsolved = (support_size == k).squeeze(dim)
        if torch.any(unsolved):
            X_ = _roll_last(X, dim)[unsolved]
            tau_, ss_ = _entmax_threshold_and_support(X_, dim=-1, k=2 * k)
            _roll_last(tau_star, dim)[unsolved] = tau_
            _roll_last(support_size, dim)[unsolved] = ss_
    return tau_star, support_size


def _entmax15_forward(X, dim, k):
    max_val, _ = X.max(dim=dim, keepdim=True)
    X = X - max_val
    X = X / 2
    tau_star, _ = _entmax_threshold_and_support(X, dim=dim, k=k)
    Y = torch.clamp(X - tau_star, min=0) ** 2
    return Y, {}


class Entmax15Function(Function):

    @classmethod
    def forward(cls, ctx, X, dim=0, k=None):
        ctx.dim = dim
        Y, _ = _entmax15_forward(X, dim, k)
        ctx.save_for_backward(Y)
        return Y

    @classmethod
    def backward(cls, ctx, dY):
        Y, = ctx.saved_tensors
        gppr = Y.sqrt()
        dX = dY * gppr
        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)
        q = q.unsqueeze(ctx.dim)
        dX -= q * gppr
        return dX, None, None


def entmax15(X, dim=-1, k=None, training=True):
    """1.5-entmax: normalizing sparse transform (a la softmax).

    Solves the optimization problem:

        max_p <x, p> - H_1.5(p)    s.t.    p >= 0, sum(p) == 1.

    where H_1.5(p) is the Tsallis alpha-entropy with alpha=1.5.

    Parameters
    ----------
    X : torch.Tensor
        The input tensor.

    dim : int
        The dimension along which to apply 1.5-entmax.

    k : int or None
        number of largest elements to partial-sort over. For optimal
        performance, should be slightly bigger than the expected number of
        nonzeros in the solution. If the solution is more than k-sparse,
        this function is recursively called with a 2*k schedule.
        If `None`, full sorting is performed from the beginning.

    Returns
    -------
    P : torch tensor, same shape as X
        The projection result, such that P.sum(dim=dim) == 1 elementwise.
    """
    if not training:
        output, _ = _entmax15_forward(X, dim, k)
        return output
    return Entmax15Function.apply(X, dim, k)


class Entmax15(nn.Module):

    def __init__(self, dim=-1, k=None):
        """1.5-entmax: normalizing sparse transform (a la softmax).

        Solves the optimization problem:

            max_p <x, p> - H_1.5(p)    s.t.    p >= 0, sum(p) == 1.

        where H_1.5(p) is the Tsallis alpha-entropy with alpha=1.5.

        Parameters
        ----------
        dim : int
            The dimension along which to apply 1.5-entmax.

        k : int or None
            number of largest elements to partial-sort over. For optimal
            performance, should be slightly bigger than the expected number of
            nonzeros in the solution. If the solution is more than k-sparse,
            this function is recursively called with a 2*k schedule.
            If `None`, full sorting is performed from the beginning.
        """
        self.dim = dim
        self.k = k
        super().__init__()

    def forward(self, X):
        return entmax15(X, dim=self.dim, k=self.k, training=self.training)


class _GenericLoss(nn.Module):

    def __init__(self, ignore_index=-100, reduction='elementwise_mean'):
        assert reduction in ['elementwise_mean', 'sum', 'none']
        self.reduction = reduction
        self.ignore_index = ignore_index
        super().__init__()

    def forward(self, X, target):
        loss = self.loss(X, target)
        if self.ignore_index >= 0:
            ignored_positions = target == self.ignore_index
            size = (target.size(0) - ignored_positions.sum()).item()
            loss.masked_fill_(ignored_positions, 0.0)
        else:
            size = target.size(0)
        if self.reduction == 'sum':
            loss = loss.sum()
        elif self.reduction == 'elementwise_mean':
            if size == 0:
                loss = loss.sum() * 0.0
            else:
                loss = loss.sum() / float(size)
        return loss


class _GenericLossFunction(Function):

    @classmethod
    def forward(cls, ctx, X, target, alpha, proj_args):
        """X (FloatTensor): n x num_classes target (LongTensor): n, the indices of the target classes."""
        assert X.shape[0] == target.shape[0]
        p_star = cls.project(X, alpha, **proj_args)
        loss = cls.omega(p_star, alpha)
        p_star.scatter_add_(1, target.unsqueeze(1), torch.full_like(p_star, -1))
        loss += torch.einsum('ij,ij->i', p_star, X)
        ctx.save_for_backward(p_star)
        return loss

    @classmethod
    def backward(cls, ctx, grad_output):
        p_star, = ctx.saved_tensors
        grad = grad_output.unsqueeze(1) * p_star
        ret = grad,
        return ret + (None,) * (1 + cls.n_fwd_args)


class EntmaxBisectFunction(Function):

    @classmethod
    def _gp(cls, x, alpha):
        return x ** (alpha - 1)

    @classmethod
    def _gp_inv(cls, y, alpha):
        return y ** (1 / (alpha - 1))

    @classmethod
    def _p(cls, X, alpha):
        return cls._gp_inv(torch.clamp(X, min=0), alpha)

    @classmethod
    def forward(cls, ctx, X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True):
        p_m, backward_kwargs = _entmax_bisect_forward(X, alpha, dim, n_iter, ensure_sum_one, cls)
        ctx.alpha = backward_kwargs['alpha']
        ctx.dim = backward_kwargs['dim']
        ctx.save_for_backward(p_m)
        return p_m

    @classmethod
    def backward(cls, ctx, dY):
        Y, = ctx.saved_tensors
        gppr = torch.where(Y > 0, Y ** (2 - ctx.alpha), Y.new_zeros(1))
        dX = dY * gppr
        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)
        q = q.unsqueeze(ctx.dim)
        dX -= q * gppr
        d_alpha = None
        if ctx.needs_input_grad[1]:
            S = torch.where(Y > 0, Y * torch.log(Y), Y.new_zeros(1))
            ent = S.sum(ctx.dim).unsqueeze(ctx.dim)
            Y_skewed = gppr / gppr.sum(ctx.dim).unsqueeze(ctx.dim)
            d_alpha = dY * (Y - Y_skewed) / (ctx.alpha - 1) ** 2
            d_alpha -= dY * (S - Y_skewed * ent) / (ctx.alpha - 1)
            d_alpha = d_alpha.sum(ctx.dim).unsqueeze(ctx.dim)
        return dX, d_alpha, None, None, None


def _sparsemax_bisect_forward(X, dim, n_iter, ensure_sum_one):
    return _entmax_bisect_forward(X, alpha=2, dim=dim, n_iter=50, ensure_sum_one=True, cls=SparsemaxBisectFunction)


class SparsemaxBisectFunction(EntmaxBisectFunction):

    @classmethod
    def _gp(cls, x, alpha):
        return x

    @classmethod
    def _gp_inv(cls, y, alpha):
        return y

    @classmethod
    def _p(cls, x, alpha):
        return torch.clamp(x, min=0)

    @classmethod
    def forward(cls, ctx, X, dim=-1, n_iter=50, ensure_sum_one=True):
        p_m, backward_kwargs = _sparsemax_bisect_forward(X, dim, n_iter, ensure_sum_one)
        ctx.alpha = backward_kwargs['alpha']
        ctx.dim = backward_kwargs['dim']
        ctx.save_for_backward(p_m)
        return p_m

    @classmethod
    def backward(cls, ctx, dY):
        Y, = ctx.saved_tensors
        gppr = Y > 0
        dX = dY * gppr
        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)
        q = q.unsqueeze(ctx.dim)
        dX -= q * gppr
        return dX, None, None, None


def sparsemax_bisect(X, dim=-1, n_iter=50, ensure_sum_one=True, training=True):
    """sparsemax: normalizing sparse transform (a la softmax), via bisection.

    Solves the projection:

        min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.

    Parameters
    ----------
    X : torch.Tensor
        The input tensor.

    dim : int
        The dimension along which to apply sparsemax.

    n_iter : int
        Number of bisection iterations. For float32, 24 iterations should
        suffice for machine precision.

    ensure_sum_one : bool,
        Whether to divide the result by its sum. If false, the result might
        sum to close but not exactly 1, which might cause downstream problems.

    Note: This function does not yet support normalizing along anything except
    the last dimension. Please use transposing and views to achieve more
    general behavior.

    Returns
    -------
    P : torch tensor, same shape as X
        The projection result, such that P.sum(dim=dim) == 1 elementwise.
    """
    if not training:
        output, _ = _sparsemax_bisect_forward(X, dim, n_iter, ensure_sum_one)
        return output
    return SparsemaxBisectFunction.apply(X, dim, n_iter, ensure_sum_one)


class SparsemaxBisectLossFunction(_GenericLossFunction):
    n_fwd_args = 1

    @classmethod
    def project(cls, X, alpha, n_iter):
        return sparsemax_bisect(X, n_iter=n_iter)

    @classmethod
    def omega(cls, p_star, alpha):
        return (1 - (p_star ** 2).sum(dim=1)) / 2

    @classmethod
    def forward(cls, ctx, X, target, n_iter=50):
        return super().forward(ctx, X, target, alpha=2, proj_args=dict(n_iter=n_iter))


def sparsemax_bisect_loss(X, target, n_iter=50):
    """sparsemax loss: sparse alternative to cross-entropy.

    Computed using bisection.

    Parameters
    ----------
    X : torch.Tensor, shape=(n_samples, n_classes)
        The input 2D tensor of predicted scores

    target : torch.LongTensor, shape=(n_samples,)
        The ground truth labels, 0 <= target < n_classes.

    n_iter : int
        Number of bisection iterations. For float32, 24 iterations should
        suffice for machine precision.

    Returns
    -------
    losses, torch.Tensor, shape=(n_samples,)
        The loss incurred at each sample.
    """
    return SparsemaxBisectLossFunction.apply(X, target, n_iter)


class SparsemaxBisectLoss(_GenericLoss):

    def __init__(self, n_iter=50, ignore_index=-100, reduction='elementwise_mean'):
        self.n_iter = n_iter
        super().__init__(ignore_index, reduction)

    def loss(self, X, target):
        return sparsemax_bisect_loss(X, target, self.n_iter)


class SparsemaxLossFunction(_GenericLossFunction):
    n_fwd_args = 1

    @classmethod
    def project(cls, X, alpha, k):
        return sparsemax(X, dim=-1, k=k)

    @classmethod
    def omega(cls, p_star, alpha):
        return (1 - (p_star ** 2).sum(dim=1)) / 2

    @classmethod
    def forward(cls, ctx, X, target, k=None):
        return super().forward(ctx, X, target, alpha=2, proj_args=dict(k=k))


def sparsemax_loss(X, target, k=None):
    """sparsemax loss: sparse alternative to cross-entropy.

    Computed using a partial sorting strategy.

    Parameters
    ----------
    X : torch.Tensor, shape=(n_samples, n_classes)
        The input 2D tensor of predicted scores

    target : torch.LongTensor, shape=(n_samples,)
        The ground truth labels, 0 <= target < n_classes.

    k : int or None
        number of largest elements to partial-sort over. For optimal
        performance, should be slightly bigger than the expected number of
        nonzeros in the solution. If the solution is more than k-sparse,
        this function is recursively called with a 2*k schedule.
        If `None`, full sorting is performed from the beginning.

    Returns
    -------
    losses, torch.Tensor, shape=(n_samples,)
        The loss incurred at each sample.
    """
    return SparsemaxLossFunction.apply(X, target, k)


class SparsemaxLoss(_GenericLoss):

    def __init__(self, k=None, ignore_index=-100, reduction='elementwise_mean'):
        self.k = k
        super().__init__(ignore_index, reduction)

    def loss(self, X, target):
        return sparsemax_loss(X, target, self.k)


def entmax_bisect(X, alpha=1.5, dim=-1, n_iter=50, ensure_sum_one=True, training=True):
    """alpha-entmax: normalizing sparse transform (a la softmax).

    Solves the optimization problem:

        max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.

    where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,
    using a bisection (root finding, binary search) algorithm.

    This function is differentiable with respect to both X and alpha.

    Parameters
    ----------
    X : torch.Tensor
        The input tensor.

    alpha : float or torch.Tensor
        Tensor of alpha parameters (> 1) to use. If scalar
        or python float, the same value is used for all rows, otherwise,
        it must have shape (or be expandable to)
        alpha.shape[j] == (X.shape[j] if j != dim else 1)
        A value of alpha=2 corresponds to sparsemax, and alpha=1 would in theory recover
        softmax. For numeric reasons, this algorithm does not work with `alpha=1`: if you
        want softmax, we recommend `torch.nn.softmax`.

    dim : int
        The dimension along which to apply alpha-entmax.

    n_iter : int
        Number of bisection iterations. For float32, 24 iterations should
        suffice for machine precision.

    ensure_sum_one : bool,
        Whether to divide the result by its sum. If false, the result might
        sum to close but not exactly 1, which might cause downstream problems.

    Returns
    -------
    P : torch tensor, same shape as X
        The projection result, such that P.sum(dim=dim) == 1 elementwise.
    """
    if not training:
        output, _ = _entmax_bisect_forward(X, alpha, dim, n_iter, ensure_sum_one)
        return output
    return EntmaxBisectFunction.apply(X, alpha, dim, n_iter, ensure_sum_one)


class EntmaxBisectLossFunction(_GenericLossFunction):
    n_fwd_args = 2

    @classmethod
    def project(cls, X, alpha, n_iter):
        return entmax_bisect(X, alpha=alpha, n_iter=n_iter, ensure_sum_one=True)

    @classmethod
    def omega(cls, p_star, alpha):
        return (1 - (p_star ** alpha).sum(dim=1)) / (alpha * (alpha - 1))

    @classmethod
    def forward(cls, ctx, X, target, alpha=1.5, n_iter=50):
        return super().forward(ctx, X, target, alpha, proj_args=dict(n_iter=n_iter))


def entmax_bisect_loss(X, target, alpha=1.5, n_iter=50):
    """alpha-entmax loss: sparse alternative to cross-entropy.

    Computed using bisection, supporting arbitrary alpha > 1.

    Parameters
    ----------
    X : torch.Tensor, shape=(n_samples, n_classes)
        The input 2D tensor of predicted scores

    target : torch.LongTensor, shape=(n_samples,)
        The ground truth labels, 0 <= target < n_classes.

    alpha : float or torch.Tensor
        Tensor of alpha parameters (> 1) to use for each row of X. If scalar
        or python float, the same value is used for all rows. A value of
        alpha=2 corresponds to sparsemax, and alpha=1 would in theory recover
        softmax. For numeric reasons, this algorithm does not work with `alpha=1`:
        if you want softmax, we recommend `torch.nn.softmax`

    n_iter : int
        Number of bisection iterations. For float32, 24 iterations should
        suffice for machine precision.

    Returns
    -------
    losses, torch.Tensor, shape=(n_samples,)
        The loss incurred at each sample.
    """
    return EntmaxBisectLossFunction.apply(X, target, alpha, n_iter)


class EntmaxBisectLoss(_GenericLoss):

    def __init__(self, alpha=1.5, n_iter=50, ignore_index=-100, reduction='elementwise_mean'):
        self.alpha = alpha
        self.n_iter = n_iter
        super().__init__(ignore_index, reduction)

    def loss(self, X, target):
        return entmax_bisect_loss(X, target, self.alpha, self.n_iter)


class Entmax15LossFunction(_GenericLossFunction):
    n_fwd_args = 1

    @classmethod
    def project(cls, X, alpha, k=None):
        return entmax15(X, dim=-1, k=k)

    @classmethod
    def omega(cls, p_star, alpha):
        return (1 - (p_star * torch.sqrt(p_star)).sum(dim=1)) / 0.75

    @classmethod
    def forward(cls, ctx, X, target, k=None):
        return super().forward(ctx, X, target, alpha=1.5, proj_args=dict(k=k))


def entmax15_loss(X, target, k=None):
    """1.5-entmax loss: sparse alternative to cross-entropy

    Computed using a partial sorting strategy.

    Parameters
    ----------
    X : torch.Tensor, shape=(n_samples, n_classes)
        The input 2D tensor of predicted scores

    target : torch.LongTensor, shape=(n_samples,)
        The ground truth labels, 0 <= target < n_classes.

    k : int or None
        number of largest elements to partial-sort over. For optimal
        performance, should be slightly bigger than the expected number of
        nonzeros in the solution. If the solution is more than k-sparse,
        this function is recursively called with a 2*k schedule.
        If `None`, full sorting is performed from the beginning.

    Returns
    -------
    losses, torch.Tensor, shape=(n_samples,)
        The loss incurred at each sample.
    """
    return Entmax15LossFunction.apply(X, target, k)


class Entmax15Loss(_GenericLoss):

    def __init__(self, k=100, ignore_index=-100, reduction='elementwise_mean'):
        self.k = k
        super().__init__(ignore_index, reduction)

    def loss(self, X, target):
        return entmax15_loss(X, target, self.k)


class SparsemaxBisect(nn.Module):

    def __init__(self, dim=-1, n_iter=None):
        """sparsemax: normalizing sparse transform (a la softmax) via bisection

        Solves the projection:

            min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.

        Parameters
        ----------
        dim : int
            The dimension along which to apply sparsemax.

        n_iter : int
            Number of bisection iterations. For float32, 24 iterations should
            suffice for machine precision.
        """
        self.dim = dim
        self.n_iter = n_iter
        super().__init__()

    def forward(self, X):
        return sparsemax_bisect(X, dim=self.dim, n_iter=self.n_iter, training=self.training)


class EntmaxBisect(nn.Module):

    def __init__(self, alpha=1.5, dim=-1, n_iter=50):
        """alpha-entmax: normalizing sparse map (a la softmax) via bisection.

        Solves the optimization problem:

            max_p <x, p> - H_a(p)    s.t.    p >= 0, sum(p) == 1.

        where H_a(p) is the Tsallis alpha-entropy with custom alpha >= 1,
        using a bisection (root finding, binary search) algorithm.

        Parameters
        ----------
        alpha : float or torch.Tensor
            Tensor of alpha parameters (> 1) to use. If scalar
            or python float, the same value is used for all rows, otherwise,
            it must have shape (or be expandable to)
            alpha.shape[j] == (X.shape[j] if j != dim else 1)
            A value of alpha=2 corresponds to sparsemax; and alpha=1 would in theory recover
            softmax. For numeric reasons, this algorithm does not work with `alpha=1`; if you
            want softmax, we recommend `torch.nn.softmax`.

        dim : int
            The dimension along which to apply alpha-entmax.

        n_iter : int
            Number of bisection iterations. For float32, 24 iterations should
            suffice for machine precision.

        """
        self.dim = dim
        self.n_iter = n_iter
        self.alpha = alpha
        super().__init__()

    def forward(self, X):
        return entmax_bisect(X, alpha=self.alpha, dim=self.dim, n_iter=self.n_iter, training=self.training)


def freeze_parameters(module: nn.Module):
    """Freezes the parameters of a torch module."""
    for p in module.parameters():
        p.requires_grad = False


@DeveloperAPI
class FreezeModule(nn.Module):

    def __init__(self, module: nn.Module, frozen: bool):
        super().__init__()
        if frozen:
            freeze_parameters(module)
            module.eval()
        else:
            module.train()
        self.module = module
        self.frozen = frozen

    def train(self, mode: bool=True):
        if self.frozen:
            return self
        return super().train(mode)


@DeveloperAPI
class Dense(LudwigModule):

    def __init__(self, input_size, output_size, use_bias=True, weights_initializer='xavier_uniform', bias_initializer='zeros'):
        super().__init__()
        self.dense = nn.Linear(in_features=input_size, out_features=output_size, bias=use_bias)
        weights_initializer = initializer_registry[weights_initializer]
        weights_initializer(self.dense.weight)
        if use_bias:
            bias_initializer = initializer_registry[bias_initializer]
            bias_initializer(self.dense.bias)

    @property
    def input_shape(self) ->torch.Size:
        return self.dense.input_shape

    def forward(self, input: torch.Tensor) ->torch.Tensor:
        output = torch.squeeze(self.dense(input), dim=-1)
        return output


class CustomLoss(nn.Module, LogitsInputsMixin):

    def __init__(self, **kwargs):
        super().__init__()

    def forward(self, preds: Tensor, target: Tensor) ->Tensor:
        return torch.mean(torch.square(preds - target))

    @staticmethod
    def get_schema_cls():
        return CustomLossConfig


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (BWCEWLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (CustomLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Dense,
     lambda: ([], {'input_size': 4, 'output_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ECELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Entmax15,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FCLayer,
     lambda: ([], {'input_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MAELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (MSELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (RMSELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReduceConcat,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReduceLast,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReduceMax,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReduceMean,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReduceNone,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReduceSum,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SequenceReducer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SigmoidCrossEntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (Sparsemax,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (_DatePreprocessing,
     lambda: ([], {'metadata': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (_VectorPreprocessing,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_ludwig_ai_ludwig(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

