import sys
_module = sys.modules[__name__]
del sys
conf = _module
examples = _module
air = _module
main = _module
modules = _module
viz = _module
baseball = _module
cjs = _module
contrib = _module
mixture = _module
scoping_mixture = _module
tree_data = _module
synthetic = _module
regional = _module
sir = _module
bart = _module
funsor = _module
hmm = _module
FactorMuE = _module
ProfileHMM = _module
ab_test = _module
gp_bayes_opt = _module
gp_models = _module
cvae = _module
baseline = _module
cvae = _module
main = _module
mnist = _module
util = _module
dmm = _module
data = _module
mcmc = _module
svi = _module
einsum = _module
hmm = _module
inclined_plane = _module
lda = _module
lkj = _module
minipyro = _module
mixed_hmm = _module
experiment = _module
model = _module
seal_data = _module
neutra = _module
generics = _module
hyperbole = _module
schelling = _module
schelling_false = _module
search_inference = _module
semantic_parsing = _module
scanvi = _module
scanvi = _module
sir_hmc = _module
smcfilter = _module
sparse_gamma_def = _module
sparse_regression = _module
svi_horovod = _module
toy_mixture_model_discrete_enumeration = _module
ss_vae_M2 = _module
utils = _module
custom_mlp = _module
mnist_cached = _module
vae_plots = _module
vae = _module
vae_comparison = _module
profiler = _module
distributions = _module
gaussianhmm = _module
profiling_utils = _module
pyro = _module
autoguide = _module
autoname = _module
named = _module
scoping = _module
bnn = _module
hidden_layer = _module
utils = _module
cevae = _module
conjugate = _module
infer = _module
easyguide = _module
easyguide = _module
epidemiology = _module
compartmental = _module
distributions = _module
models = _module
util = _module
bart = _module
finance = _module
multi_mnist = _module
nextstrain = _module
polyphonic_data_loader = _module
scanvi_data = _module
util = _module
forecast = _module
evaluate = _module
forecaster = _module
util = _module
funsor = _module
handlers = _module
enum_messenger = _module
named_messenger = _module
plate_messenger = _module
primitives = _module
replay_messenger = _module
runtime = _module
trace_messenger = _module
discrete = _module
elbo = _module
trace_elbo = _module
traceenum_elbo = _module
tracetmc_elbo = _module
gp = _module
kernels = _module
brownian = _module
coregionalize = _module
dot_product = _module
isotropic = _module
kernel = _module
periodic = _module
static = _module
likelihoods = _module
binary = _module
gaussian = _module
likelihood = _module
multi_class = _module
poisson = _module
gplvm = _module
gpr = _module
model = _module
sgpr = _module
vgp = _module
vsgp = _module
parameterized = _module
util = _module
minipyro = _module
mue = _module
dataloaders = _module
missingdatahmm = _module
models = _module
statearrangers = _module
oed = _module
eig = _module
glmm = _module
glmm = _module
guides = _module
search = _module
util = _module
randomvariable = _module
random_variable = _module
timeseries = _module
base = _module
gp = _module
lgssm = _module
lgssmgp = _module
tracking = _module
assignment = _module
distributions = _module
dynamic_models = _module
extended_kalman_filter = _module
hashing = _module
measurements = _module
util = _module
distributions = _module
affine_beta = _module
asymmetriclaplace = _module
avf_mvn = _module
coalescent = _module
conditional = _module
conjugate = _module
constraints = _module
delta = _module
diag_normal_mixture = _module
diag_normal_mixture_shared_cov = _module
distribution = _module
empirical = _module
extended = _module
folded = _module
gaussian_scale_mixture = _module
grouped_normal_normal = _module
hmm = _module
improper_uniform = _module
inverse_gamma = _module
kl = _module
lkj = _module
log_normal_negative_binomial = _module
logistic = _module
mixture = _module
multivariate_studentt = _module
nanmasked = _module
omt_mvn = _module
one_one_matching = _module
one_two_matching = _module
ordered_logistic = _module
polya_gamma = _module
projected_normal = _module
rejector = _module
relaxed_straight_through = _module
score_parts = _module
sine_bivariate_von_mises = _module
sine_skewed = _module
softlaplace = _module
spanning_tree = _module
stable = _module
testing = _module
fakes = _module
gof = _module
naive_dirichlet = _module
rejection_exponential = _module
rejection_gamma = _module
special = _module
torch_distribution = _module
torch_patch = _module
torch_transform = _module
transforms = _module
affine_autoregressive = _module
affine_coupling = _module
basic = _module
batchnorm = _module
block_autoregressive = _module
cholesky = _module
discrete_cosine = _module
generalized_channel_permute = _module
haar = _module
householder = _module
lower_cholesky_affine = _module
matrix_exponential = _module
neural_autoregressive = _module
normalize = _module
ordered = _module
permute = _module
planar = _module
polynomial = _module
power = _module
radial = _module
softplus = _module
spline = _module
spline_autoregressive = _module
spline_coupling = _module
sylvester = _module
unit_cholesky = _module
unit = _module
util = _module
von_mises_3d = _module
zero_inflated = _module
generic = _module
abstract_infer = _module
effect = _module
gaussian = _module
guides = _module
initialization = _module
structured = _module
csis = _module
discrete = _module
elbo = _module
energy_distance = _module
enum = _module
importance = _module
inspect = _module
adaptation = _module
api = _module
hmc = _module
logger = _module
mcmc_kernel = _module
nuts = _module
util = _module
predictive = _module
renyi_elbo = _module
reparam = _module
conjugate = _module
hmm = _module
loc_scale = _module
neutra = _module
projected_normal = _module
reparam = _module
softmax = _module
split = _module
stable = _module
strategies = _module
studentt = _module
transform = _module
unit_jacobian = _module
resampler = _module
rws = _module
smcfilter = _module
svgd = _module
svi = _module
trace_mean_field_elbo = _module
trace_mmd = _module
trace_tail_adaptive_elbo = _module
traceenum_elbo = _module
tracegraph_elbo = _module
tracetmc_elbo = _module
util = _module
nn = _module
auto_reg_nn = _module
dense_nn = _module
module = _module
ops = _module
arrowhead = _module
contract = _module
dual_averaging = _module
adjoint = _module
torch_log = _module
torch_map = _module
torch_marginal = _module
torch_sample = _module
util = _module
gamma_gaussian = _module
gaussian = _module
hessian = _module
indexing = _module
integrator = _module
jit = _module
linalg = _module
newton = _module
packed = _module
provenance = _module
rings = _module
special = _module
ssm_gp = _module
stats = _module
streaming = _module
tensor_utils = _module
welford = _module
optim = _module
adagrad_rmsprop = _module
clipped_adam = _module
dct_adam = _module
horovod = _module
lr_scheduler = _module
multi = _module
optim = _module
pytorch_optimizers = _module
params = _module
param_store = _module
poutine = _module
block_messenger = _module
broadcast_messenger = _module
collapse_messenger = _module
condition_messenger = _module
do_messenger = _module
enum_messenger = _module
escape_messenger = _module
guide = _module
handlers = _module
indep_messenger = _module
infer_config_messenger = _module
lift_messenger = _module
markov_messenger = _module
mask_messenger = _module
messenger = _module
reentrant_messenger = _module
reparam_messenger = _module
replay_messenger = _module
runtime = _module
scale_messenger = _module
seed_messenger = _module
subsample_messenger = _module
trace_messenger = _module
trace_struct = _module
uncondition_messenger = _module
primitives = _module
settings = _module
util = _module
update_headers = _module
update_version = _module
setup = _module
tests = _module
common = _module
conftest = _module
test_autoname = _module
test_named = _module
test_scoping = _module
test_hidden_layer = _module
test_cevae = _module
test_easyguide = _module
test_distributions = _module
test_models = _module
test_quant = _module
test_util = _module
test_evaluate = _module
test_forecaster = _module
test_util = _module
test_enum_funsor = _module
test_infer_discrete = _module
test_named_handlers = _module
test_pyroapi_funsor = _module
test_tmc = _module
test_valid_models_enum = _module
test_valid_models_plate = _module
test_valid_models_sequential_plate = _module
test_vectorized_markov = _module
test_conditional = _module
test_kernels = _module
test_likelihoods = _module
test_models = _module
test_parameterized = _module
test_dataloaders = _module
test_missingdatahmm = _module
test_models = _module
test_statearrangers = _module
test_ewma = _module
test_finite_spaces_eig = _module
test_glmm = _module
test_linear_models_eig = _module
test_xexpx = _module
test_random_variable = _module
test_hessian = _module
test_minipyro = _module
test_util = _module
test_gp = _module
test_lgssm = _module
test_assignment = _module
test_distributions = _module
test_dynamic_models = _module
test_ekf = _module
test_em = _module
test_hashing = _module
test_measurements = _module
dist_fixture = _module
test_binomial = _module
test_categorical = _module
test_coalescent = _module
test_conjugate = _module
test_conjugate_update = _module
test_constraints = _module
test_cuda = _module
test_delta = _module
test_distributions = _module
test_empirical = _module
test_extended = _module
test_gaussian_mixtures = _module
test_grouped_normal_normal = _module
test_haar = _module
test_hmm = _module
test_ig = _module
test_improper_uniform = _module
test_independent = _module
test_kl = _module
test_lkj = _module
test_log_normal_negative_binomial = _module
test_lowrank_mvn = _module
test_mask = _module
test_mixture = _module
test_mvn = _module
test_mvt = _module
test_nanmasked = _module
test_omt_mvn = _module
test_one_hot_categorical = _module
test_one_one_matching = _module
test_one_two_matching = _module
test_ordered_logistic = _module
test_pickle = _module
test_polya_gamma = _module
test_projected_normal = _module
test_rejector = _module
test_relaxed_straight_through = _module
test_reshape = _module
test_shapes = _module
test_sine_bivariate_von_mises = _module
test_sine_skewed = _module
test_spanning_tree = _module
test_stable = _module
test_tensor_type = _module
test_torch_patch = _module
test_transforms = _module
test_unit = _module
test_util = _module
test_von_mises = _module
test_zero_inflated = _module
test_gof = _module
test_special = _module
doctest_fixtures = _module
test_gaussian = _module
test_inference = _module
test_mean_field_entropy = _module
test_adaptation = _module
test_hmc = _module
test_mcmc_api = _module
test_mcmc_util = _module
test_nuts = _module
test_valid_models = _module
test_conjugate = _module
test_discrete_cosine = _module
test_haar = _module
test_hmm = _module
test_loc_scale = _module
test_neutra = _module
test_projected_normal = _module
test_softmax = _module
test_split = _module
test_stable = _module
test_strategies = _module
test_structured = _module
test_studentt = _module
test_transform = _module
test_unit_jacobian = _module
test_abstract_infer = _module
test_autoguide = _module
test_compute_downstream_costs = _module
test_conjugate_gradients = _module
test_csis = _module
test_discrete = _module
test_elbo_mapdata = _module
test_enum = _module
test_gradient = _module
test_inference = _module
test_initialization = _module
test_inspect = _module
test_jit = _module
test_multi_sample_elbos = _module
test_predictive = _module
test_resampler = _module
test_sampling = _module
test_smcfilter = _module
test_svgd = _module
test_tmc = _module
test_util = _module
test_valid_models = _module
integration_tests = _module
test_conjugate_gaussian_models = _module
test_tracegraph_elbo = _module
test_autoregressive = _module
test_module = _module
test_adjoint = _module
test_torch_log = _module
gamma_gaussian = _module
gaussian = _module
test_arrowhead = _module
test_contract = _module
test_gamma_gaussian = _module
test_gaussian = _module
test_indexing = _module
test_integrator = _module
test_jit = _module
test_linalg = _module
test_newton = _module
test_packed = _module
test_special = _module
test_ssm_gp = _module
test_stats = _module
test_streaming = _module
test_tensor_utils = _module
test_welford = _module
test_multi = _module
test_optim = _module
test_module = _module
test_param = _module
perf = _module
test_benchmark = _module
test_counterfactual = _module
test_mapdata = _module
test_nesting = _module
test_poutines = _module
test_properties = _module
test_runtime = _module
test_trace_struct = _module
test_pyroapi = _module
test_examples = _module
test_generic = _module
test_primitives = _module
test_settings = _module
test_util = _module
cleannb = _module
search_inference = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from collections import namedtuple


import torch


import torch.nn as nn


import torch.nn.functional as F


import math


import time


from functools import partial


import numpy as np


from torch.nn.functional import softplus


import logging


import pandas as pd


from torch.distributions import constraints


from torch.distributions import biject_to


import functools


from torchvision import transforms


import matplotlib.pyplot as plt


from torch.optim import Adam


import torch.autograd as autograd


import torch.optim as optim


from torch.distributions import transform_to


import copy


from torch.utils.data import DataLoader


from torch.utils.data import Dataset


from torchvision.transforms import Compose


from torchvision.transforms import functional


from torchvision.utils import make_grid


from torch.distributions import transforms


from torch.autograd import grad


from torch import nn


import uuid


from matplotlib.gridspec import GridSpec


from torch.distributions.utils import broadcast_all


import collections


import numbers


import queue


from matplotlib.patches import Patch


from torch.nn.functional import softmax


import re


from collections import OrderedDict


import torch.multiprocessing as mp


from torch.distributions.bernoulli import Bernoulli


from torch.distributions.beta import Beta


from inspect import isclass


from functools import reduce


import itertools


from abc import ABCMeta


from abc import abstractmethod


from torch.nn import functional


from torchvision.utils import save_image


from torch.autograd import Variable


from torch.distributions.utils import lazy_property


from torch.utils.data import TensorDataset


from collections import defaultdict


import warnings


from abc import ABC


from torch.nn.functional import pad


import numpy


from torch.nn.utils.rnn import pad_sequence


from scipy import sparse


import torchvision.datasets as datasets


from functools import singledispatch


from numbers import Number


from torch.nn import Parameter


import torch.distributions as torchdist


import random


from torch.distributions import Categorical


from torch.distributions import OneHotCategorical


from torch.distributions.transforms import AffineTransform


from torch.distributions.transforms import SigmoidTransform


from typing import Union


from torch import Tensor


from torch.distributions import MultivariateNormal


from torch.autograd import Function


from torch.autograd.function import once_differentiable


import torch.nn


from torch.distributions.constraints import *


from torch.distributions.constraints import Constraint


from torch.distributions.constraints import __all__ as torch_constraints


from torch.distributions.constraints import independent


from torch.distributions.constraints import lower_cholesky


from torch.distributions.constraints import positive


from torch.distributions.constraints import positive_definite


import inspect


from torch.distributions.transforms import AbsTransform


from torch.distributions.transforms import PowerTransform


from torch.distributions import Independent


from torch.distributions import Normal


from torch.distributions import kl_divergence


from torch.distributions import register_kl


from torch.nn.functional import logsigmoid


from torch.distributions.utils import clamp_probs


from math import pi


from torch.distributions import VonMises


from torch import broadcast_shapes


from torch.distributions import Uniform


from torch.distributions.kl import kl_divergence


from torch.distributions.kl import register_kl


from torch.distributions.transforms import *


from torch.distributions.transforms import ComposeTransform


from torch.distributions.transforms import ExpTransform


from torch.distributions.transforms import LowerCholeskyTransform


from torch.distributions.transforms import __all__ as torch_transforms


from torch.distributions.utils import _sum_rightmost


from torch.distributions.transforms import TanhTransform


from torch.distributions.transforms import Transform


from torch.distributions import Transform


from torch.distributions import Distribution


import torch.distributions as torch_dist


from torch import logsumexp


from torch.distributions.utils import logits_to_probs


from torch.distributions.utils import probs_to_logits


from typing import Callable


from typing import Optional


from typing import Tuple


from types import SimpleNamespace


from typing import Dict


from typing import Set


from typing import List


from itertools import product


from collections import Counter


from torch.nn import functional as F


from numpy.polynomial.hermite import hermgauss


from torch.fft import irfft


from torch.fft import rfft


from typing import Any


from typing import Hashable


from torch.optim.optimizer import Optimizer


from typing import ValuesView


from torch.optim import Optimizer


from typing import Iterable


from typing import Type


from torch.nn.utils import clip_grad_norm_


from torch.nn.utils import clip_grad_value_


from typing import Iterator


from itertools import zip_longest


import torch.cuda


from numpy.testing import assert_allclose


from queue import LifoQueue


import scipy.stats as sp


from torch.distributions import AffineTransform


from torch.distributions import Beta


from torch.distributions import TransformedDistribution


from torch import tensor


from torch.distributions import Gamma


from torch.distributions import StudentT


from torch.autograd.functional import jacobian


from scipy.special import binom


from torch.distributions import HalfNormal


from scipy.integrate.quadpack import IntegrationWarning


from scipy.stats import ks_2samp


from scipy.stats import kstest


from scipy.stats import levy_stable


from torch import optim


import torch.distributions as dist


import scipy.special as sc


import torch.optim


from torch import nn as nn


from scipy.special import iv


import scipy.fftpack as fftpack


from copy import copy


from queue import Queue


class Decoder(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc3 = nn.Linear(20, 400)
        self.fc4 = nn.Linear(400, 784)
        self.relu = nn.ReLU()

    def forward(self, z):
        h3 = self.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h3))


class Encoder(nn.Module):

    def __init__(self, data_length, alphabet_length, z_dim):
        super().__init__()
        self.input_size = data_length * alphabet_length
        self.f1_mn = nn.Linear(self.input_size, z_dim)
        self.f1_sd = nn.Linear(self.input_size, z_dim)

    def forward(self, data):
        data = data.reshape(-1, self.input_size)
        z_loc = self.f1_mn(data)
        z_scale = softplus(self.f1_sd(data))
        return z_loc, z_scale


GuideState = namedtuple('GuideState', ['h', 'c', 'bl_h', 'bl_c', 'z_pres', 'z_where', 'z_what'])


class Identity(nn.Module):

    def __init__(self):
        super().__init__()

    def forward(self, x):
        return x


def broadcast_shape(*shapes, **kwargs):
    """
    Similar to ``np.broadcast()`` but for shapes.
    Equivalent to ``np.broadcast(*map(np.empty, shapes)).shape``.

    :param tuple shapes: shapes of tensors.
    :param bool strict: whether to use extend-but-not-resize broadcasting.
    :returns: broadcasted shape
    :rtype: tuple
    :raises: ValueError
    """
    strict = kwargs.pop('strict', False)
    reversed_shape = []
    for shape in shapes:
        for i, size in enumerate(reversed(shape)):
            if i >= len(reversed_shape):
                reversed_shape.append(size)
            elif reversed_shape[i] == 1 and not strict:
                reversed_shape[i] = size
            elif reversed_shape[i] != size and (size != 1 or strict):
                raise ValueError('shape mismatch: objects cannot be broadcast to a single shape: {}'.format(' vs '.join(map(str, shapes))))
    return tuple(reversed(reversed_shape))


class ConcatModule(nn.Module):
    """
    a custom module for concatenation of tensors
    """

    def __init__(self, allow_broadcast=False):
        self.allow_broadcast = allow_broadcast
        super().__init__()

    def forward(self, *input_args):
        if len(input_args) == 1:
            input_args = input_args[0]
        if torch.is_tensor(input_args):
            return input_args
        else:
            if self.allow_broadcast:
                shape = broadcast_shape(*[s.shape[:-1] for s in input_args]) + (-1,)
                input_args = [s.expand(shape) for s in input_args]
            return torch.cat(input_args, dim=-1)


class ListOutModule(nn.ModuleList):
    """
    a custom module for outputting a list of tensors from a list of nn modules
    """

    def __init__(self, modules):
        super().__init__(modules)

    def forward(self, *args, **kwargs):
        return [mm.forward(*args, **kwargs) for mm in self]


def call_nn_op(op):
    """
    a helper function that adds appropriate parameters when calling
    an nn module representing an operation like Softmax

    :param op: the nn.Module operation to instantiate
    :return: instantiation of the op module with appropriate parameters
    """
    if op in [nn.Softmax, nn.LogSoftmax]:
        return op(dim=1)
    else:
        return op()


class MLP(nn.Module):

    def __init__(self, mlp_sizes, activation=nn.ReLU, output_activation=None, post_layer_fct=lambda layer_ix, total_layers, layer: None, post_act_fct=lambda layer_ix, total_layers, layer: None, allow_broadcast=False, use_cuda=False):
        super().__init__()
        assert len(mlp_sizes) >= 2, 'Must have input and output layer sizes defined'
        input_size, hidden_sizes, output_size = mlp_sizes[0], mlp_sizes[1:-1], mlp_sizes[-1]
        assert isinstance(input_size, (int, list, tuple)), 'input_size must be int, list, tuple'
        last_layer_size = input_size if type(input_size) == int else sum(input_size)
        all_modules = [ConcatModule(allow_broadcast)]
        for layer_ix, layer_size in enumerate(hidden_sizes):
            assert type(layer_size) == int, 'Hidden layer sizes must be ints'
            cur_linear_layer = nn.Linear(last_layer_size, layer_size)
            cur_linear_layer.weight.data.normal_(0, 0.001)
            cur_linear_layer.bias.data.normal_(0, 0.001)
            if use_cuda:
                cur_linear_layer = nn.DataParallel(cur_linear_layer)
            all_modules.append(cur_linear_layer)
            post_linear = post_layer_fct(layer_ix + 1, len(hidden_sizes), all_modules[-1])
            if post_linear is not None:
                all_modules.append(post_linear)
            all_modules.append(activation())
            post_activation = post_act_fct(layer_ix + 1, len(hidden_sizes), all_modules[-1])
            if post_activation is not None:
                all_modules.append(post_activation)
            last_layer_size = layer_size
        assert isinstance(output_size, (int, list, tuple)), 'output_size must be int, list, tuple'
        if type(output_size) == int:
            all_modules.append(nn.Linear(last_layer_size, output_size))
            if output_activation is not None:
                all_modules.append(call_nn_op(output_activation) if isclass(output_activation) else output_activation)
        else:
            out_layers = []
            for out_ix, out_size in enumerate(output_size):
                split_layer = []
                split_layer.append(nn.Linear(last_layer_size, out_size))
                act_out_fct = output_activation if not isinstance(output_activation, (list, tuple)) else output_activation[out_ix]
                if act_out_fct:
                    split_layer.append(call_nn_op(act_out_fct) if isclass(act_out_fct) else act_out_fct)
                out_layers.append(nn.Sequential(*split_layer))
            all_modules.append(ListOutModule(out_layers))
        self.sequential_mlp = nn.Sequential(*all_modules)

    def forward(self, *args, **kwargs):
        return self.sequential_mlp.forward(*args, **kwargs)


ModelState = namedtuple('ModelState', ['x', 'z_pres', 'z_where'])


class Predict(nn.Module):

    def __init__(self, input_size, h_sizes, z_pres_size, z_where_size, non_linear_layer):
        super().__init__()
        self.z_pres_size = z_pres_size
        self.z_where_size = z_where_size
        output_size = z_pres_size + 2 * z_where_size
        self.mlp = MLP(input_size, h_sizes + [output_size], non_linear_layer)

    def forward(self, h):
        out = self.mlp(h)
        z_pres_p = torch.sigmoid(out[:, 0:self.z_pres_size])
        z_where_loc = out[:, self.z_pres_size:self.z_pres_size + self.z_where_size]
        z_where_scale = softplus(out[:, self.z_pres_size + self.z_where_size:])
        return z_pres_p, z_where_loc, z_where_scale


def default_z_pres_prior_p(t):
    return 0.5


expansion_indices = torch.LongTensor([1, 0, 2, 0, 1, 3])


def expand_z_where(z_where):
    n = z_where.size(0)
    out = torch.cat((z_where.new_zeros(n, 1), z_where), 1)
    ix = expansion_indices
    if z_where.is_cuda:
        ix = ix
    out = torch.index_select(out, 1, ix)
    out = out.view(n, 2, 3)
    return out


def z_where_inv(z_where):
    n = z_where.size(0)
    out = torch.cat((z_where.new_ones(n, 1), -z_where[:, 1:]), 1)
    out = out / z_where[:, 0:1]
    return out


def image_to_window(z_where, window_size, image_size, images):
    n = images.size(0)
    assert images.size(1) == images.size(2) == image_size, 'Size mismatch.'
    theta_inv = expand_z_where(z_where_inv(z_where))
    grid = F.affine_grid(theta_inv, torch.Size((n, 1, window_size, window_size)))
    out = F.grid_sample(images.view(n, 1, image_size, image_size), grid)
    return out.view(n, -1)


def window_to_image(z_where, window_size, image_size, windows):
    n = windows.size(0)
    assert windows.size(1) == window_size ** 2, 'Size mismatch.'
    theta = expand_z_where(z_where)
    grid = F.affine_grid(theta, torch.Size((n, 1, image_size, image_size)))
    out = F.grid_sample(windows.view(n, 1, window_size, window_size), grid)
    return out.view(n, image_size, image_size)


class AIR(nn.Module):

    def __init__(self, num_steps, x_size, window_size, z_what_size, rnn_hidden_size, encoder_net=[], decoder_net=[], predict_net=[], embed_net=None, bl_predict_net=[], non_linearity='ReLU', decoder_output_bias=None, decoder_output_use_sigmoid=False, use_masking=True, use_baselines=True, baseline_scalar=None, scale_prior_mean=3.0, scale_prior_sd=0.1, pos_prior_mean=0.0, pos_prior_sd=1.0, likelihood_sd=0.3, use_cuda=False):
        super().__init__()
        self.num_steps = num_steps
        self.x_size = x_size
        self.window_size = window_size
        self.z_what_size = z_what_size
        self.rnn_hidden_size = rnn_hidden_size
        self.use_masking = use_masking
        self.use_baselines = use_baselines
        self.baseline_scalar = baseline_scalar
        self.likelihood_sd = likelihood_sd
        self.use_cuda = use_cuda
        prototype = torch.tensor(0.0) if use_cuda else torch.tensor(0.0)
        self.options = dict(dtype=prototype.dtype, device=prototype.device)
        self.z_pres_size = 1
        self.z_where_size = 3
        self.z_where_loc_prior = nn.Parameter(torch.FloatTensor([scale_prior_mean, pos_prior_mean, pos_prior_mean]), requires_grad=False)
        self.z_where_scale_prior = nn.Parameter(torch.FloatTensor([scale_prior_sd, pos_prior_sd, pos_prior_sd]), requires_grad=False)
        rnn_input_size = x_size ** 2 if embed_net is None else embed_net[-1]
        rnn_input_size += self.z_where_size + z_what_size + self.z_pres_size
        nl = getattr(nn, non_linearity)
        self.rnn = nn.LSTMCell(rnn_input_size, rnn_hidden_size)
        self.encode = Encoder(window_size ** 2, encoder_net, z_what_size, nl)
        self.decode = Decoder(window_size ** 2, decoder_net, z_what_size, decoder_output_bias, decoder_output_use_sigmoid, nl)
        self.predict = Predict(rnn_hidden_size, predict_net, self.z_pres_size, self.z_where_size, nl)
        self.embed = Identity() if embed_net is None else MLP(x_size ** 2, embed_net, nl, True)
        self.bl_rnn = nn.LSTMCell(rnn_input_size, rnn_hidden_size)
        self.bl_predict = MLP(rnn_hidden_size, bl_predict_net + [1], nl)
        self.bl_embed = Identity() if embed_net is None else MLP(x_size ** 2, embed_net, nl, True)
        self.h_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))
        self.c_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))
        self.bl_h_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))
        self.bl_c_init = nn.Parameter(torch.zeros(1, rnn_hidden_size))
        self.z_where_init = nn.Parameter(torch.zeros(1, self.z_where_size))
        self.z_what_init = nn.Parameter(torch.zeros(1, self.z_what_size))
        if use_cuda:
            self

    def prior(self, n, **kwargs):
        state = ModelState(x=torch.zeros(n, self.x_size, self.x_size, **self.options), z_pres=torch.ones(n, self.z_pres_size, **self.options), z_where=None)
        z_pres = []
        z_where = []
        for t in range(self.num_steps):
            state = self.prior_step(t, n, state, **kwargs)
            z_where.append(state.z_where)
            z_pres.append(state.z_pres)
        return (z_where, z_pres), state.x

    def prior_step(self, t, n, prev, z_pres_prior_p=default_z_pres_prior_p):
        z_pres = pyro.sample('z_pres_{}'.format(t), dist.Bernoulli(z_pres_prior_p(t) * prev.z_pres).to_event(1))
        sample_mask = z_pres if self.use_masking else torch.tensor(1.0)
        z_where = pyro.sample('z_where_{}'.format(t), dist.Normal(self.z_where_loc_prior.expand(n, self.z_where_size), self.z_where_scale_prior.expand(n, self.z_where_size)).mask(sample_mask).to_event(1))
        z_what = pyro.sample('z_what_{}'.format(t), dist.Normal(torch.zeros(n, self.z_what_size, **self.options), torch.ones(n, self.z_what_size, **self.options)).mask(sample_mask).to_event(1))
        y_att = self.decode(z_what)
        y = window_to_image(z_where, self.window_size, self.x_size, y_att)
        x = prev.x + y * z_pres.view(-1, 1, 1)
        return ModelState(x=x, z_pres=z_pres, z_where=z_where)

    def model(self, data, batch_size, **kwargs):
        pyro.module('decode', self.decode)
        with pyro.plate('data', data.size(0), device=data.device) as ix:
            batch = data[ix]
            n = batch.size(0)
            (z_where, z_pres), x = self.prior(n, **kwargs)
            pyro.sample('obs', dist.Normal(x.view(n, -1), self.likelihood_sd * torch.ones(n, self.x_size ** 2, **self.options)).to_event(1), obs=batch.view(n, -1))

    def guide(self, data, batch_size, **kwargs):
        pyro.module('rnn', self.rnn),
        pyro.module('predict', self.predict),
        pyro.module('encode', self.encode),
        pyro.module('embed', self.embed),
        pyro.module('bl_rnn', self.bl_rnn),
        pyro.module('bl_predict', self.bl_predict),
        pyro.module('bl_embed', self.bl_embed)
        pyro.param('h_init', self.h_init)
        pyro.param('c_init', self.c_init)
        pyro.param('z_where_init', self.z_where_init)
        pyro.param('z_what_init', self.z_what_init)
        pyro.param('bl_h_init', self.bl_h_init)
        pyro.param('bl_c_init', self.bl_c_init)
        with pyro.plate('data', data.size(0), subsample_size=batch_size, device=data.device) as ix:
            batch = data[ix]
            n = batch.size(0)
            flattened_batch = batch.view(n, -1)
            inputs = {'raw': batch, 'embed': self.embed(flattened_batch), 'bl_embed': self.bl_embed(flattened_batch)}
            state = GuideState(h=self.h_init.expand(n, -1), c=self.c_init.expand(n, -1), bl_h=self.bl_h_init.expand(n, -1), bl_c=self.bl_c_init.expand(n, -1), z_pres=torch.ones(n, self.z_pres_size, **self.options), z_where=self.z_where_init.expand(n, -1), z_what=self.z_what_init.expand(n, -1))
            z_pres = []
            z_where = []
            for t in range(self.num_steps):
                state = self.guide_step(t, n, state, inputs)
                z_where.append(state.z_where)
                z_pres.append(state.z_pres)
            return z_where, z_pres

    def guide_step(self, t, n, prev, inputs):
        rnn_input = torch.cat((inputs['embed'], prev.z_where, prev.z_what, prev.z_pres), 1)
        h, c = self.rnn(rnn_input, (prev.h, prev.c))
        z_pres_p, z_where_loc, z_where_scale = self.predict(h)
        infer_dict, bl_h, bl_c = self.baseline_step(prev, inputs)
        z_pres = pyro.sample('z_pres_{}'.format(t), dist.Bernoulli(z_pres_p * prev.z_pres).to_event(1), infer=infer_dict)
        sample_mask = z_pres if self.use_masking else torch.tensor(1.0)
        z_where = pyro.sample('z_where_{}'.format(t), dist.Normal(z_where_loc + self.z_where_loc_prior, z_where_scale * self.z_where_scale_prior).mask(sample_mask).to_event(1))
        x_att = image_to_window(z_where, self.window_size, self.x_size, inputs['raw'])
        z_what_loc, z_what_scale = self.encode(x_att)
        z_what = pyro.sample('z_what_{}'.format(t), dist.Normal(z_what_loc, z_what_scale).mask(sample_mask).to_event(1))
        return GuideState(h=h, c=c, bl_h=bl_h, bl_c=bl_c, z_pres=z_pres, z_where=z_where, z_what=z_what)

    def baseline_step(self, prev, inputs):
        if not self.use_baselines:
            return dict(), None, None
        rnn_input = torch.cat((inputs['bl_embed'], prev.z_where.detach(), prev.z_what.detach(), prev.z_pres.detach()), 1)
        bl_h, bl_c = self.bl_rnn(rnn_input, (prev.bl_h, prev.bl_c))
        bl_value = self.bl_predict(bl_h)
        if self.use_masking:
            bl_value = bl_value * prev.z_pres
        if self.baseline_scalar is not None:
            bl_value = bl_value * self.baseline_scalar
        infer_dict = dict(baseline=dict(baseline_value=bl_value.squeeze(-1)))
        return infer_dict, bl_h, bl_c


class TonesGenerator(nn.Module):

    def __init__(self, args, data_dim):
        self.args = args
        self.data_dim = data_dim
        super().__init__()
        self.x_to_hidden = nn.Linear(args.hidden_dim, args.nn_dim)
        self.y_to_hidden = nn.Linear(args.nn_channels * data_dim, args.nn_dim)
        self.conv = nn.Conv1d(1, args.nn_channels, 3, padding=1)
        self.hidden_to_logits = nn.Linear(args.nn_dim, data_dim)
        self.relu = nn.ReLU()

    def forward(self, x, y):
        x_onehot = y.new_zeros(x.shape[:-1] + (self.args.hidden_dim,)).scatter_(-1, x, 1)
        y_conv = self.relu(self.conv(y.reshape(-1, 1, self.data_dim))).reshape(y.shape[:-1] + (-1,))
        h = self.relu(self.x_to_hidden(x_onehot) + self.y_to_hidden(y_conv))
        return self.hidden_to_logits(h)


class CNN(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.fc1 = nn.Linear(320, 50)
        self.fc2 = nn.Linear(50, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


class BaselineNet(nn.Module):

    def __init__(self, hidden_1, hidden_2):
        super().__init__()
        self.fc1 = nn.Linear(784, hidden_1)
        self.fc2 = nn.Linear(hidden_1, hidden_2)
        self.fc3 = nn.Linear(hidden_2, 784)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = x.view(-1, 784)
        hidden = self.relu(self.fc1(x))
        hidden = self.relu(self.fc2(hidden))
        y = torch.sigmoid(self.fc3(hidden))
        return y


class MaskedBCELoss(nn.Module):

    def __init__(self, masked_with=-1):
        super().__init__()
        self.masked_with = masked_with

    def forward(self, input, target):
        target = target.view(input.shape)
        loss = F.binary_cross_entropy(input, target, reduction='none')
        loss[target == self.masked_with] = 0
        return loss.sum()


class CVAE(nn.Module):

    def __init__(self, z_dim, hidden_1, hidden_2, pre_trained_baseline_net):
        super().__init__()
        self.baseline_net = pre_trained_baseline_net
        self.prior_net = Encoder(z_dim, hidden_1, hidden_2)
        self.generation_net = Decoder(z_dim, hidden_1, hidden_2)
        self.recognition_net = Encoder(z_dim, hidden_1, hidden_2)

    def model(self, xs, ys=None):
        pyro.module('generation_net', self)
        batch_size = xs.shape[0]
        with pyro.plate('data'):
            with torch.no_grad():
                y_hat = self.baseline_net(xs).view(xs.shape)
            prior_loc, prior_scale = self.prior_net(xs, y_hat)
            zs = pyro.sample('z', dist.Normal(prior_loc, prior_scale).to_event(1))
            loc = self.generation_net(zs)
            if ys is not None:
                mask_loc = loc[(xs == -1).view(-1, 784)].view(batch_size, -1)
                mask_ys = ys[xs == -1].view(batch_size, -1)
                pyro.sample('y', dist.Bernoulli(mask_loc, validate_args=False).to_event(1), obs=mask_ys)
            else:
                pyro.deterministic('y', loc.detach())
            return loc

    def guide(self, xs, ys=None):
        with pyro.plate('data'):
            if ys is None:
                y_hat = self.baseline_net(xs).view(xs.shape)
                loc, scale = self.prior_net(xs, y_hat)
            else:
                loc, scale = self.recognition_net(xs, ys)
            pyro.sample('z', dist.Normal(loc, scale).to_event(1))


class Emitter(nn.Module):
    """
    Parameterizes the bernoulli observation likelihood `p(x_t | z_t)`
    """

    def __init__(self, input_dim, z_dim, emission_dim):
        super().__init__()
        self.lin_z_to_hidden = nn.Linear(z_dim, emission_dim)
        self.lin_hidden_to_hidden = nn.Linear(emission_dim, emission_dim)
        self.lin_hidden_to_input = nn.Linear(emission_dim, input_dim)
        self.relu = nn.ReLU()

    def forward(self, z_t):
        """
        Given the latent z at a particular time step t we return the vector of
        probabilities `ps` that parameterizes the bernoulli distribution `p(x_t|z_t)`
        """
        h1 = self.relu(self.lin_z_to_hidden(z_t))
        h2 = self.relu(self.lin_hidden_to_hidden(h1))
        ps = torch.sigmoid(self.lin_hidden_to_input(h2))
        return ps


class GatedTransition(nn.Module):
    """
    Parameterizes the gaussian latent transition probability `p(z_t | z_{t-1})`
    See section 5 in the reference for comparison.
    """

    def __init__(self, z_dim, transition_dim):
        super().__init__()
        self.lin_gate_z_to_hidden = nn.Linear(z_dim, transition_dim)
        self.lin_gate_hidden_to_z = nn.Linear(transition_dim, z_dim)
        self.lin_proposed_mean_z_to_hidden = nn.Linear(z_dim, transition_dim)
        self.lin_proposed_mean_hidden_to_z = nn.Linear(transition_dim, z_dim)
        self.lin_sig = nn.Linear(z_dim, z_dim)
        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)
        self.lin_z_to_loc.weight.data = torch.eye(z_dim)
        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)
        self.relu = nn.ReLU()
        self.softplus = nn.Softplus()

    def forward(self, z_t_1):
        """
        Given the latent `z_{t-1}` corresponding to the time step t-1
        we return the mean and scale vectors that parameterize the
        (diagonal) gaussian distribution `p(z_t | z_{t-1})`
        """
        _gate = self.relu(self.lin_gate_z_to_hidden(z_t_1))
        gate = torch.sigmoid(self.lin_gate_hidden_to_z(_gate))
        _proposed_mean = self.relu(self.lin_proposed_mean_z_to_hidden(z_t_1))
        proposed_mean = self.lin_proposed_mean_hidden_to_z(_proposed_mean)
        loc = (1 - gate) * self.lin_z_to_loc(z_t_1) + gate * proposed_mean
        scale = self.softplus(self.lin_sig(self.relu(proposed_mean)))
        return loc, scale


class Combiner(nn.Module):
    """
    Parameterizes `q(z_t | z_{t-1}, x_{t:T})`, which is the basic building block
    of the guide (i.e. the variational distribution). The dependence on `x_{t:T}` is
    through the hidden state of the RNN (see the PyTorch module `rnn` below)
    """

    def __init__(self, z_dim, rnn_dim):
        super().__init__()
        self.lin_z_to_hidden = nn.Linear(z_dim, rnn_dim)
        self.lin_hidden_to_loc = nn.Linear(rnn_dim, z_dim)
        self.lin_hidden_to_scale = nn.Linear(rnn_dim, z_dim)
        self.tanh = nn.Tanh()
        self.softplus = nn.Softplus()

    def forward(self, z_t_1, h_rnn):
        """
        Given the latent z at at a particular time step t-1 as well as the hidden
        state of the RNN `h(x_{t:T})` we return the mean and scale vectors that
        parameterize the (diagonal) gaussian distribution `q(z_t | z_{t-1}, x_{t:T})`
        """
        h_combined = 0.5 * (self.tanh(self.lin_z_to_hidden(z_t_1)) + h_rnn)
        loc = self.lin_hidden_to_loc(h_combined)
        scale = self.softplus(self.lin_hidden_to_scale(h_combined))
        return loc, scale


class TransformModule(torch.distributions.Transform, torch.nn.Module):
    """
    Transforms with learnable parameters such as normalizing flows should inherit from this class rather
    than `Transform` so they are also a subclass of `nn.Module` and inherit all the useful methods of that class.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def __hash__(self):
        return super(torch.nn.Module, self).__hash__()


def clamp_preserve_gradients(x, min, max):
    return x + (x.clamp(min, max) - x).detach()


def copy_docs_from(source_class, full_text=False):
    """
    Decorator to copy class and method docs from source to destin class.
    """

    def decorator(destin_class):
        for name in dir(destin_class):
            if name.startswith('_'):
                continue
            destin_attr = getattr(destin_class, name)
            destin_attr = getattr(destin_attr, '__func__', destin_attr)
            source_attr = getattr(source_class, name, None)
            source_doc = getattr(source_attr, '__doc__', None)
            if source_doc and not getattr(destin_attr, '__doc__', None):
                if full_text or source_doc.startswith('See '):
                    destin_doc = source_doc
                else:
                    destin_doc = 'See :meth:`{}.{}.{}`'.format(source_class.__module__, source_class.__name__, name)
                if isinstance(destin_attr, property):
                    updated_property = property(destin_attr.fget, destin_attr.fset, destin_attr.fdel, destin_doc)
                    setattr(destin_class, name, updated_property)
                else:
                    destin_attr.__doc__ = destin_doc
        return destin_class
    return decorator


@copy_docs_from(TransformModule)
class AffineAutoregressive(TransformModule):
    """
    An implementation of the bijective transform of Inverse Autoregressive Flow
    (IAF), using by default Eq (10) from Kingma Et Al., 2016,

        :math:`\\mathbf{y} = \\mu_t + \\sigma_t\\odot\\mathbf{x}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    :math:`\\mu_t,\\sigma_t` are calculated from an autoregressive network on
    :math:`\\mathbf{x}`, and :math:`\\sigma_t>0`.

    If the stable keyword argument is set to True then the transformation used is,

        :math:`\\mathbf{y} = \\sigma_t\\odot\\mathbf{x} + (1-\\sigma_t)\\odot\\mu_t`

    where :math:`\\sigma_t` is restricted to :math:`(0,1)`. This variant of IAF is
    claimed by the authors to be more numerically stable than one using Eq (10),
    although in practice it leads to a restriction on the distributions that can be
    represented, presumably since the input is restricted to rescaling by a number
    on :math:`(0,1)`.

    Together with :class:`~pyro.distributions.TransformedDistribution` this provides
    a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn import AutoRegressiveNN
    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = AffineAutoregressive(AutoRegressiveNN(10, [40]))
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    The inverse of the Bijector is required when, e.g., scoring the log density of a
    sample with :class:`~pyro.distributions.TransformedDistribution`. This
    implementation caches the inverse of the Bijector when its forward operation is
    called, e.g., when sampling from
    :class:`~pyro.distributions.TransformedDistribution`. However, if the cached
    value isn't available, either because it was overwritten during sampling a new
    value or an arbitrary value is being scored, it will calculate it manually. Note
    that this is an operation that scales as O(D) where D is the input dimension,
    and so should be avoided for large dimensional uses. So in general, it is cheap
    to sample from IAF and score a value that was sampled by IAF, but expensive to
    score an arbitrary value.

    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns a real-valued mean and logit-scale as a tuple
    :type autoregressive_nn: callable
    :param log_scale_min_clip: The minimum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_min_clip: float
    :param log_scale_max_clip: The maximum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_max_clip: float
    :param sigmoid_bias: A term to add the logit of the input when using the stable
        tranform.
    :type sigmoid_bias: float
    :param stable: When true, uses the alternative "stable" version of the transform
        (see above).
    :type stable: bool

    References:

    [1] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever,
    Max Welling. Improving Variational Inference with Inverse Autoregressive Flow.
    [arXiv:1606.04934]

    [2] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with
    Normalizing Flows. [arXiv:1505.05770]

    [3] Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle. MADE: Masked
    Autoencoder for Distribution Estimation. [arXiv:1502.03509]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    sign = +1
    autoregressive = True

    def __init__(self, autoregressive_nn, log_scale_min_clip=-5.0, log_scale_max_clip=3.0, sigmoid_bias=2.0, stable=False):
        super().__init__(cache_size=1)
        self.arn = autoregressive_nn
        self._cached_log_scale = None
        self.log_scale_min_clip = log_scale_min_clip
        self.log_scale_max_clip = log_scale_max_clip
        self.sigmoid = nn.Sigmoid()
        self.logsigmoid = nn.LogSigmoid()
        self.sigmoid_bias = sigmoid_bias
        self.stable = stable
        if stable:
            self._call = self._call_stable
            self._inverse = self._inverse_stable

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        mean, log_scale = self.arn(x)
        log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)
        self._cached_log_scale = log_scale
        scale = torch.exp(log_scale)
        y = scale * x + mean
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. Uses a previously cached inverse if available, otherwise
        performs the inversion afresh.
        """
        x_size = y.size()[:-1]
        perm = self.arn.permutation
        input_dim = y.size(-1)
        x = [torch.zeros(x_size, device=y.device)] * input_dim
        for idx in perm:
            mean, log_scale = self.arn(torch.stack(x, dim=-1))
            inverse_scale = torch.exp(-clamp_preserve_gradients(log_scale[..., idx], min=self.log_scale_min_clip, max=self.log_scale_max_clip))
            mean = mean[..., idx]
            x[idx] = (y[..., idx] - mean) * inverse_scale
        x = torch.stack(x, dim=-1)
        log_scale = clamp_preserve_gradients(log_scale, min=self.log_scale_min_clip, max=self.log_scale_max_clip)
        self._cached_log_scale = log_scale
        return x

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        if self._cached_log_scale is not None:
            log_scale = self._cached_log_scale
        elif not self.stable:
            _, log_scale = self.arn(x)
            log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)
        else:
            _, logit_scale = self.arn(x)
            log_scale = self.logsigmoid(logit_scale + self.sigmoid_bias)
        return log_scale.sum(-1)

    def _call_stable(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        mean, logit_scale = self.arn(x)
        logit_scale = logit_scale + self.sigmoid_bias
        scale = self.sigmoid(logit_scale)
        log_scale = self.logsigmoid(logit_scale)
        self._cached_log_scale = log_scale
        y = scale * x + (1 - scale) * mean
        return y

    def _inverse_stable(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x.
        """
        x_size = y.size()[:-1]
        perm = self.arn.permutation
        input_dim = y.size(-1)
        x = [torch.zeros(x_size, device=y.device)] * input_dim
        for idx in perm:
            mean, logit_scale = self.arn(torch.stack(x, dim=-1))
            inverse_scale = 1 + torch.exp(-logit_scale[..., idx] - self.sigmoid_bias)
            x[idx] = inverse_scale * y[..., idx] + (1 - inverse_scale) * mean[..., idx]
        self._cached_log_scale = self.logsigmoid(logit_scale + self.sigmoid_bias)
        x = torch.stack(x, dim=-1)
        return x


class MaskedLinear(nn.Linear):
    """
    A linear mapping with a given mask on the weights (arbitrary bias)

    :param in_features: the number of input features
    :type in_features: int
    :param out_features: the number of output features
    :type out_features: int
    :param mask: the mask to apply to the in_features x out_features weight matrix
    :type mask: torch.Tensor
    :param bias: whether or not `MaskedLinear` should include a bias term. defaults to `True`
    :type bias: bool
    """

    def __init__(self, in_features, out_features, mask, bias=True):
        super().__init__(in_features, out_features, bias)
        self.register_buffer('mask', mask.data)

    def forward(self, _input):
        masked_weight = self.weight * self.mask
        return F.linear(_input, masked_weight, self.bias)


def sample_mask_indices(input_dim, hidden_dim, simple=True):
    """
    Samples the indices assigned to hidden units during the construction of MADE masks

    :param input_dim: the dimensionality of the input variable
    :type input_dim: int
    :param hidden_dim: the dimensionality of the hidden layer
    :type hidden_dim: int
    :param simple: True to space fractional indices by rounding to nearest int, false round randomly
    :type simple: bool
    """
    indices = torch.linspace(1, input_dim, steps=hidden_dim, device='cpu')
    if simple:
        return torch.round(indices)
    else:
        ints = indices.floor()
        ints += torch.bernoulli(indices - ints)
        return ints


def create_mask(input_dim, context_dim, hidden_dims, permutation, output_dim_multiplier):
    """
    Creates MADE masks for a conditional distribution

    :param input_dim: the dimensionality of the input variable
    :type input_dim: int
    :param context_dim: the dimensionality of the variable that is conditioned on (for conditional densities)
    :type context_dim: int
    :param hidden_dims: the dimensionality of the hidden layers(s)
    :type hidden_dims: list[int]
    :param permutation: the order of the input variables
    :type permutation: torch.LongTensor
    :param output_dim_multiplier: tiles the output (e.g. for when a separate mean and scale parameter are desired)
    :type output_dim_multiplier: int
    """
    var_index = torch.empty(permutation.shape, dtype=torch.get_default_dtype())
    var_index[permutation] = torch.arange(input_dim, dtype=torch.get_default_dtype())
    input_indices = torch.cat((torch.zeros(context_dim), 1 + var_index))
    if context_dim > 0:
        hidden_indices = [(sample_mask_indices(input_dim, h) - 1) for h in hidden_dims]
    else:
        hidden_indices = [sample_mask_indices(input_dim - 1, h) for h in hidden_dims]
    output_indices = (var_index + 1).repeat(output_dim_multiplier)
    mask_skip = (output_indices.unsqueeze(-1) > input_indices.unsqueeze(0)).type_as(var_index)
    masks = [(hidden_indices[0].unsqueeze(-1) >= input_indices.unsqueeze(0)).type_as(var_index)]
    for i in range(1, len(hidden_dims)):
        masks.append((hidden_indices[i].unsqueeze(-1) >= hidden_indices[i - 1].unsqueeze(0)).type_as(var_index))
    masks.append((output_indices.unsqueeze(-1) > hidden_indices[-1].unsqueeze(0)).type_as(var_index))
    return masks, mask_skip


class ConditionalAutoRegressiveNN(nn.Module):
    """
    An implementation of a MADE-like auto-regressive neural network that can input an additional context variable.
    (See Reference [2] Section 3.3 for an explanation of how the conditional MADE architecture works.)

    Example usage:

    >>> x = torch.randn(100, 10)
    >>> y = torch.randn(100, 5)
    >>> arn = ConditionalAutoRegressiveNN(10, 5, [50], param_dims=[1])
    >>> p = arn(x, context=y)  # 1 parameters of size (100, 10)
    >>> arn = ConditionalAutoRegressiveNN(10, 5, [50], param_dims=[1, 1])
    >>> m, s = arn(x, context=y) # 2 parameters of size (100, 10)
    >>> arn = ConditionalAutoRegressiveNN(10, 5, [50], param_dims=[1, 5, 3])
    >>> a, b, c = arn(x, context=y) # 3 parameters of sizes, (100, 1, 10), (100, 5, 10), (100, 3, 10)

    :param input_dim: the dimensionality of the input variable
    :type input_dim: int
    :param context_dim: the dimensionality of the context variable
    :type context_dim: int
    :param hidden_dims: the dimensionality of the hidden units per layer
    :type hidden_dims: list[int]
    :param param_dims: shape the output into parameters of dimension (p_n, input_dim) for p_n in param_dims
        when p_n > 1 and dimension (input_dim) when p_n == 1. The default is [1, 1], i.e. output two parameters
        of dimension (input_dim), which is useful for inverse autoregressive flow.
    :type param_dims: list[int]
    :param permutation: an optional permutation that is applied to the inputs and controls the order of the
        autoregressive factorization. in particular for the identity permutation the autoregressive structure
        is such that the Jacobian is upper triangular. By default this is chosen at random.
    :type permutation: torch.LongTensor
    :param skip_connections: Whether to add skip connections from the input to the output.
    :type skip_connections: bool
    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
        nonlinearity is applied to the final network output, so the output is an unbounded real number.
    :type nonlinearity: torch.nn.module

    Reference:

    1. MADE: Masked Autoencoder for Distribution Estimation [arXiv:1502.03509]
    Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle

    2. Inference Networks for Sequential Monte Carlo in Graphical Models [arXiv:1602.06701]
    Brooks Paige, Frank Wood

    """

    def __init__(self, input_dim, context_dim, hidden_dims, param_dims=[1, 1], permutation=None, skip_connections=False, nonlinearity=nn.ReLU()):
        super().__init__()
        if input_dim == 1:
            warnings.warn('ConditionalAutoRegressiveNN input_dim = 1. Consider using an affine transformation instead.')
        self.input_dim = input_dim
        self.context_dim = context_dim
        self.hidden_dims = hidden_dims
        self.param_dims = param_dims
        self.count_params = len(param_dims)
        self.output_multiplier = sum(param_dims)
        self.all_ones = (torch.tensor(param_dims) == 1).all().item()
        ends = torch.cumsum(torch.tensor(param_dims), dim=0)
        starts = torch.cat((torch.zeros(1).type_as(ends), ends[:-1]))
        self.param_slices = [slice(s.item(), e.item()) for s, e in zip(starts, ends)]
        for h in hidden_dims:
            if h < input_dim:
                raise ValueError('Hidden dimension must not be less than input dimension.')
        if permutation is None:
            P = torch.randperm(input_dim, device='cpu')
        else:
            P = permutation.type(dtype=torch.int64)
        self.register_buffer('permutation', P)
        self.masks, self.mask_skip = create_mask(input_dim=input_dim, context_dim=context_dim, hidden_dims=hidden_dims, permutation=self.permutation, output_dim_multiplier=self.output_multiplier)
        layers = [MaskedLinear(input_dim + context_dim, hidden_dims[0], self.masks[0])]
        for i in range(1, len(hidden_dims)):
            layers.append(MaskedLinear(hidden_dims[i - 1], hidden_dims[i], self.masks[i]))
        layers.append(MaskedLinear(hidden_dims[-1], input_dim * self.output_multiplier, self.masks[-1]))
        self.layers = nn.ModuleList(layers)
        if skip_connections:
            self.skip_layer = MaskedLinear(input_dim + context_dim, input_dim * self.output_multiplier, self.mask_skip, bias=False)
        else:
            self.skip_layer = None
        self.f = nonlinearity

    def get_permutation(self):
        """
        Get the permutation applied to the inputs (by default this is chosen at random)
        """
        return self.permutation

    def forward(self, x, context=None):
        if context is None:
            context = self.context
        context = context.expand(x.size()[:-1] + (context.size(-1),))
        x = torch.cat([context, x], dim=-1)
        return self._forward(x)

    def _forward(self, x):
        h = x
        for layer in self.layers[:-1]:
            h = self.f(layer(h))
        h = self.layers[-1](h)
        if self.skip_layer is not None:
            h = h + self.skip_layer(x)
        if self.output_multiplier == 1:
            return h
        else:
            h = h.reshape(list(x.size()[:-1]) + [self.output_multiplier, self.input_dim])
            if self.count_params == 1:
                return h
            elif self.all_ones:
                return torch.unbind(h, dim=-2)
            else:
                return tuple([h[..., s, :] for s in self.param_slices])


class AutoRegressiveNN(ConditionalAutoRegressiveNN):
    """
    An implementation of a MADE-like auto-regressive neural network.

    Example usage:

    >>> x = torch.randn(100, 10)
    >>> arn = AutoRegressiveNN(10, [50], param_dims=[1])
    >>> p = arn(x)  # 1 parameters of size (100, 10)
    >>> arn = AutoRegressiveNN(10, [50], param_dims=[1, 1])
    >>> m, s = arn(x) # 2 parameters of size (100, 10)
    >>> arn = AutoRegressiveNN(10, [50], param_dims=[1, 5, 3])
    >>> a, b, c = arn(x) # 3 parameters of sizes, (100, 1, 10), (100, 5, 10), (100, 3, 10)

    :param input_dim: the dimensionality of the input variable
    :type input_dim: int
    :param hidden_dims: the dimensionality of the hidden units per layer
    :type hidden_dims: list[int]
    :param param_dims: shape the output into parameters of dimension (p_n, input_dim) for p_n in param_dims
        when p_n > 1 and dimension (input_dim) when p_n == 1. The default is [1, 1], i.e. output two parameters
        of dimension (input_dim), which is useful for inverse autoregressive flow.
    :type param_dims: list[int]
    :param permutation: an optional permutation that is applied to the inputs and controls the order of the
        autoregressive factorization. in particular for the identity permutation the autoregressive structure
        is such that the Jacobian is upper triangular. By default this is chosen at random.
    :type permutation: torch.LongTensor
    :param skip_connections: Whether to add skip connections from the input to the output.
    :type skip_connections: bool
    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
        nonlinearity is applied to the final network output, so the output is an unbounded real number.
    :type nonlinearity: torch.nn.module

    Reference:

    MADE: Masked Autoencoder for Distribution Estimation [arXiv:1502.03509]
    Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle

    """

    def __init__(self, input_dim, hidden_dims, param_dims=[1, 1], permutation=None, skip_connections=False, nonlinearity=nn.ReLU()):
        super(AutoRegressiveNN, self).__init__(input_dim, 0, hidden_dims, param_dims=param_dims, permutation=permutation, skip_connections=skip_connections, nonlinearity=nonlinearity)

    def forward(self, x):
        return self._forward(x)


def affine_autoregressive(input_dim, hidden_dims=None, **kwargs):
    """
    A helper function to create an
    :class:`~pyro.distributions.transforms.AffineAutoregressive` object that takes
    care of constructing an autoregressive network with the correct input/output
    dimensions.

    :param input_dim: Dimension of input variable
    :type input_dim: int
    :param hidden_dims: The desired hidden dimensions of the autoregressive network.
        Defaults to using [3*input_dim + 1]
    :type hidden_dims: list[int]
    :param log_scale_min_clip: The minimum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_min_clip: float
    :param log_scale_max_clip: The maximum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_max_clip: float
    :param sigmoid_bias: A term to add the logit of the input when using the stable
        tranform.
    :type sigmoid_bias: float
    :param stable: When true, uses the alternative "stable" version of the transform
        (see above).
    :type stable: bool

    """
    if hidden_dims is None:
        hidden_dims = [3 * input_dim + 1]
    arn = AutoRegressiveNN(input_dim, hidden_dims)
    return AffineAutoregressive(arn, **kwargs)


class DMM(nn.Module):
    """
    This PyTorch Module encapsulates the model as well as the
    variational distribution (the guide) for the Deep Markov Model
    """

    def __init__(self, input_dim=88, z_dim=100, emission_dim=100, transition_dim=200, rnn_dim=600, num_layers=1, rnn_dropout_rate=0.0, num_iafs=0, iaf_dim=50, use_cuda=False):
        super().__init__()
        self.emitter = Emitter(input_dim, z_dim, emission_dim)
        self.trans = GatedTransition(z_dim, transition_dim)
        self.combiner = Combiner(z_dim, rnn_dim)
        rnn_dropout_rate = 0.0 if num_layers == 1 else rnn_dropout_rate
        self.rnn = nn.RNN(input_size=input_dim, hidden_size=rnn_dim, nonlinearity='relu', batch_first=True, bidirectional=False, num_layers=num_layers, dropout=rnn_dropout_rate)
        self.iafs = [affine_autoregressive(z_dim, hidden_dims=[iaf_dim]) for _ in range(num_iafs)]
        self.iafs_modules = nn.ModuleList(self.iafs)
        self.z_0 = nn.Parameter(torch.zeros(z_dim))
        self.z_q_0 = nn.Parameter(torch.zeros(z_dim))
        self.h_0 = nn.Parameter(torch.zeros(1, 1, rnn_dim))
        self.use_cuda = use_cuda
        if use_cuda:
            self

    def model(self, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths, annealing_factor=1.0):
        T_max = mini_batch.size(1)
        pyro.module('dmm', self)
        z_prev = self.z_0.expand(mini_batch.size(0), self.z_0.size(0))
        with pyro.plate('z_minibatch', len(mini_batch)):
            for t in pyro.markov(range(1, T_max + 1)):
                z_loc, z_scale = self.trans(z_prev)
                with poutine.scale(scale=annealing_factor):
                    z_t = pyro.sample('z_%d' % t, dist.Normal(z_loc, z_scale).mask(mini_batch_mask[:, t - 1:t]).to_event(1))
                emission_probs_t = self.emitter(z_t)
                pyro.sample('obs_x_%d' % t, dist.Bernoulli(emission_probs_t).mask(mini_batch_mask[:, t - 1:t]).to_event(1), obs=mini_batch[:, t - 1, :])
                z_prev = z_t

    def guide(self, mini_batch, mini_batch_reversed, mini_batch_mask, mini_batch_seq_lengths, annealing_factor=1.0):
        T_max = mini_batch.size(1)
        pyro.module('dmm', self)
        h_0_contig = self.h_0.expand(1, mini_batch.size(0), self.rnn.hidden_size).contiguous()
        rnn_output, _ = self.rnn(mini_batch_reversed, h_0_contig)
        rnn_output = poly.pad_and_reverse(rnn_output, mini_batch_seq_lengths)
        z_prev = self.z_q_0.expand(mini_batch.size(0), self.z_q_0.size(0))
        with pyro.plate('z_minibatch', len(mini_batch)):
            for t in pyro.markov(range(1, T_max + 1)):
                z_loc, z_scale = self.combiner(z_prev, rnn_output[:, t - 1, :])
                if len(self.iafs) > 0:
                    z_dist = TransformedDistribution(dist.Normal(z_loc, z_scale), self.iafs)
                    assert z_dist.event_shape == (self.z_q_0.size(0),)
                    assert z_dist.batch_shape[-1:] == (len(mini_batch),)
                else:
                    z_dist = dist.Normal(z_loc, z_scale)
                    assert z_dist.event_shape == ()
                    assert z_dist.batch_shape[-2:] == (len(mini_batch), self.z_q_0.size(0))
                with pyro.poutine.scale(scale=annealing_factor):
                    if len(self.iafs) > 0:
                        z_t = pyro.sample('z_%d' % t, z_dist.mask(mini_batch_mask[:, t - 1]))
                    else:
                        z_t = pyro.sample('z_%d' % t, z_dist.mask(mini_batch_mask[:, t - 1:t]).to_event(1))
                z_prev = z_t


def make_fc(dims):
    layers = []
    for in_dim, out_dim in zip(dims, dims[1:]):
        layers.append(nn.Linear(in_dim, out_dim))
        layers.append(nn.BatchNorm1d(out_dim))
        layers.append(nn.ReLU())
    return nn.Sequential(*layers[:-1])


def split_in_half(t):
    return t.reshape(t.shape[:-1] + (2, -1)).unbind(-2)


class Z2Decoder(nn.Module):

    def __init__(self, z1_dim, y_dim, z2_dim, hidden_dims):
        super().__init__()
        dims = [z1_dim + y_dim] + hidden_dims + [2 * z2_dim]
        self.fc = make_fc(dims)

    def forward(self, z1, y):
        z1_y = torch.cat([z1, y], dim=-1)
        _z1_y = z1_y.reshape(-1, z1_y.size(-1))
        hidden = self.fc(_z1_y)
        hidden = hidden.reshape(z1_y.shape[:-1] + hidden.shape[-1:])
        loc, scale = split_in_half(hidden)
        scale = softplus(scale)
        return loc, scale


class XDecoder(nn.Module):

    def __init__(self, num_genes, z2_dim, hidden_dims):
        super().__init__()
        dims = [z2_dim] + hidden_dims + [2 * num_genes]
        self.fc = make_fc(dims)

    def forward(self, z2):
        gate_logits, mu = split_in_half(self.fc(z2))
        mu = softmax(mu, dim=-1)
        return gate_logits, mu


class Z2LEncoder(nn.Module):

    def __init__(self, num_genes, z2_dim, hidden_dims):
        super().__init__()
        dims = [num_genes] + hidden_dims + [2 * z2_dim + 2]
        self.fc = make_fc(dims)

    def forward(self, x):
        x = torch.log(1 + x)
        h1, h2 = split_in_half(self.fc(x))
        z2_loc, z2_scale = h1[..., :-1], softplus(h2[..., :-1])
        l_loc, l_scale = h1[..., -1:], softplus(h2[..., -1:])
        return z2_loc, z2_scale, l_loc, l_scale


def broadcast_inputs(input_args):
    shape = broadcast_shape(*[s.shape[:-1] for s in input_args]) + (-1,)
    input_args = [s.expand(shape) for s in input_args]
    return input_args


class Z1Encoder(nn.Module):

    def __init__(self, num_labels, z1_dim, z2_dim, hidden_dims):
        super().__init__()
        dims = [num_labels + z2_dim] + hidden_dims + [2 * z1_dim]
        self.fc = make_fc(dims)

    def forward(self, z2, y):
        z2_y = broadcast_inputs([z2, y])
        z2_y = torch.cat(z2_y, dim=-1)
        _z2_y = z2_y.reshape(-1, z2_y.size(-1))
        hidden = self.fc(_z2_y)
        hidden = hidden.reshape(z2_y.shape[:-1] + hidden.shape[-1:])
        loc, scale = split_in_half(hidden)
        scale = softplus(scale)
        return loc, scale


class Classifier(nn.Module):

    def __init__(self, z2_dim, hidden_dims, num_labels):
        super().__init__()
        dims = [z2_dim] + hidden_dims + [num_labels]
        self.fc = make_fc(dims)

    def forward(self, x):
        logits = self.fc(x)
        return logits


class SCANVI(nn.Module):

    def __init__(self, num_genes, num_labels, l_loc, l_scale, latent_dim=10, alpha=0.01, scale_factor=1.0):
        assert isinstance(num_genes, int)
        self.num_genes = num_genes
        assert isinstance(num_labels, int) and num_labels > 1
        self.num_labels = num_labels
        assert isinstance(latent_dim, int) and latent_dim > 0
        self.latent_dim = latent_dim
        assert isinstance(l_loc, float)
        self.l_loc = l_loc
        assert isinstance(l_scale, float) and l_scale > 0
        self.l_scale = l_scale
        assert isinstance(alpha, float) and alpha > 0
        self.alpha = alpha
        assert isinstance(scale_factor, float) and scale_factor > 0
        self.scale_factor = scale_factor
        super().__init__()
        self.z2_decoder = Z2Decoder(z1_dim=self.latent_dim, y_dim=self.num_labels, z2_dim=self.latent_dim, hidden_dims=[50])
        self.x_decoder = XDecoder(num_genes=num_genes, hidden_dims=[100], z2_dim=self.latent_dim)
        self.z2l_encoder = Z2LEncoder(num_genes=num_genes, z2_dim=self.latent_dim, hidden_dims=[100])
        self.classifier = Classifier(z2_dim=self.latent_dim, hidden_dims=[50], num_labels=num_labels)
        self.z1_encoder = Z1Encoder(num_labels=num_labels, z1_dim=self.latent_dim, z2_dim=self.latent_dim, hidden_dims=[50])
        self.epsilon = 0.005

    def model(self, x, y=None):
        pyro.module('scanvi', self)
        theta = pyro.param('inverse_dispersion', 10.0 * x.new_ones(self.num_genes), constraint=constraints.positive)
        with pyro.plate('batch', len(x)), poutine.scale(scale=self.scale_factor):
            z1 = pyro.sample('z1', dist.Normal(0, x.new_ones(self.latent_dim)).to_event(1))
            y = pyro.sample('y', dist.OneHotCategorical(logits=x.new_zeros(self.num_labels)), obs=y)
            z2_loc, z2_scale = self.z2_decoder(z1, y)
            z2 = pyro.sample('z2', dist.Normal(z2_loc, z2_scale).to_event(1))
            l_scale = self.l_scale * x.new_ones(1)
            l = pyro.sample('l', dist.LogNormal(self.l_loc, l_scale).to_event(1))
            gate_logits, mu = self.x_decoder(z2)
            nb_logits = (l * mu + self.epsilon).log() - (theta + self.epsilon).log()
            x_dist = dist.ZeroInflatedNegativeBinomial(gate_logits=gate_logits, total_count=theta, logits=nb_logits)
            pyro.sample('x', x_dist.to_event(1), obs=x)

    def guide(self, x, y=None):
        pyro.module('scanvi', self)
        with pyro.plate('batch', len(x)), poutine.scale(scale=self.scale_factor):
            z2_loc, z2_scale, l_loc, l_scale = self.z2l_encoder(x)
            pyro.sample('l', dist.LogNormal(l_loc, l_scale).to_event(1))
            z2 = pyro.sample('z2', dist.Normal(z2_loc, z2_scale).to_event(1))
            y_logits = self.classifier(z2)
            y_dist = dist.OneHotCategorical(logits=y_logits)
            if y is None:
                y = pyro.sample('y', y_dist)
            else:
                classification_loss = y_dist.log_prob(y)
                pyro.factor('classification_loss', -self.alpha * classification_loss, has_rsample=False)
            z1_loc, z1_scale = self.z1_encoder(z2, y)
            pyro.sample('z1', dist.Normal(z1_loc, z1_scale).to_event(1))


class Exp(nn.Module):
    """
    a custom module for exponentiation of tensors
    """

    def __init__(self):
        super().__init__()

    def forward(self, val):
        return torch.exp(val)


class SSVAE(nn.Module):
    """
    This class encapsulates the parameters (neural networks) and models & guides needed to train a
    semi-supervised variational auto-encoder on the MNIST image dataset

    :param output_size: size of the tensor representing the class label (10 for MNIST since
                        we represent the class labels as a one-hot vector with 10 components)
    :param input_size: size of the tensor representing the image (28*28 = 784 for our MNIST dataset
                       since we flatten the images and scale the pixels to be in [0,1])
    :param z_dim: size of the tensor representing the latent random variable z
                  (handwriting style for our MNIST dataset)
    :param hidden_layers: a tuple (or list) of MLP layers to be used in the neural networks
                          representing the parameters of the distributions in our model
    :param use_cuda: use GPUs for faster training
    :param aux_loss_multiplier: the multiplier to use with the auxiliary loss
    """

    def __init__(self, output_size=10, input_size=784, z_dim=50, hidden_layers=(500,), config_enum=None, use_cuda=False, aux_loss_multiplier=None):
        super().__init__()
        self.output_size = output_size
        self.input_size = input_size
        self.z_dim = z_dim
        self.hidden_layers = hidden_layers
        self.allow_broadcast = config_enum == 'parallel'
        self.use_cuda = use_cuda
        self.aux_loss_multiplier = aux_loss_multiplier
        self.setup_networks()

    def setup_networks(self):
        z_dim = self.z_dim
        hidden_sizes = self.hidden_layers
        self.encoder_y = MLP([self.input_size] + hidden_sizes + [self.output_size], activation=nn.Softplus, output_activation=nn.Softmax, allow_broadcast=self.allow_broadcast, use_cuda=self.use_cuda)
        self.encoder_z = MLP([self.input_size + self.output_size] + hidden_sizes + [[z_dim, z_dim]], activation=nn.Softplus, output_activation=[None, Exp], allow_broadcast=self.allow_broadcast, use_cuda=self.use_cuda)
        self.decoder = MLP([z_dim + self.output_size] + hidden_sizes + [self.input_size], activation=nn.Softplus, output_activation=nn.Sigmoid, allow_broadcast=self.allow_broadcast, use_cuda=self.use_cuda)
        if self.use_cuda:
            self

    def model(self, xs, ys=None):
        """
        The model corresponds to the following generative process:
        p(z) = normal(0,I)              # handwriting style (latent)
        p(y|x) = categorical(I/10.)     # which digit (semi-supervised)
        p(x|y,z) = bernoulli(loc(y,z))   # an image
        loc is given by a neural network  `decoder`

        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: (optional) a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """
        pyro.module('ss_vae', self)
        batch_size = xs.size(0)
        options = dict(dtype=xs.dtype, device=xs.device)
        with pyro.plate('data'):
            prior_loc = torch.zeros(batch_size, self.z_dim, **options)
            prior_scale = torch.ones(batch_size, self.z_dim, **options)
            zs = pyro.sample('z', dist.Normal(prior_loc, prior_scale).to_event(1))
            alpha_prior = torch.ones(batch_size, self.output_size, **options) / (1.0 * self.output_size)
            ys = pyro.sample('y', dist.OneHotCategorical(alpha_prior), obs=ys)
            loc = self.decoder.forward([zs, ys])
            pyro.sample('x', dist.Bernoulli(loc, validate_args=False).to_event(1), obs=xs)
            return loc

    def guide(self, xs, ys=None):
        """
        The guide corresponds to the following:
        q(y|x) = categorical(alpha(x))              # infer digit from an image
        q(z|x,y) = normal(loc(x,y),scale(x,y))       # infer handwriting style from an image and the digit
        loc, scale are given by a neural network `encoder_z`
        alpha is given by a neural network `encoder_y`

        :param xs: a batch of scaled vectors of pixels from an image
        :param ys: (optional) a batch of the class labels i.e.
                   the digit corresponding to the image(s)
        :return: None
        """
        with pyro.plate('data'):
            if ys is None:
                alpha = self.encoder_y.forward(xs)
                ys = pyro.sample('y', dist.OneHotCategorical(alpha))
            loc, scale = self.encoder_z.forward([xs, ys])
            pyro.sample('z', dist.Normal(loc, scale).to_event(1))

    def classifier(self, xs):
        """
        classify an image (or a batch of images)

        :param xs: a batch of scaled vectors of pixels from an image
        :return: a batch of the corresponding class labels (as one-hots)
        """
        alpha = self.encoder_y.forward(xs)
        res, ind = torch.topk(alpha, 1)
        ys = torch.zeros_like(alpha).scatter_(1, ind, 1.0)
        return ys

    def model_classify(self, xs, ys=None):
        """
        this model is used to add an auxiliary (supervised) loss as described in the
        Kingma et al., "Semi-Supervised Learning with Deep Generative Models".
        """
        pyro.module('ss_vae', self)
        with pyro.plate('data'):
            if ys is not None:
                alpha = self.encoder_y.forward(xs)
                with pyro.poutine.scale(scale=self.aux_loss_multiplier):
                    pyro.sample('y_aux', dist.OneHotCategorical(alpha), obs=ys)

    def guide_classify(self, xs, ys=None):
        """
        dummy guide function to accompany model_classify in inference
        """
        pass


TEST = 'test'


TRAIN = 'train'


class VAE(object, metaclass=ABCMeta):
    """
    Abstract class for the variational auto-encoder. The abstract method
    for training the network is implemented by subclasses.
    """

    def __init__(self, args, train_loader, test_loader):
        self.args = args
        self.vae_encoder = Encoder()
        self.vae_decoder = Decoder()
        self.train_loader = train_loader
        self.test_loader = test_loader
        self.mode = TRAIN

    def set_train(self, is_train=True):
        if is_train:
            self.mode = TRAIN
            self.vae_encoder.train()
            self.vae_decoder.train()
        else:
            self.mode = TEST
            self.vae_encoder.eval()
            self.vae_decoder.eval()

    @abstractmethod
    def compute_loss_and_gradient(self, x):
        """
        Given a batch of data `x`, run the optimizer (backpropagate the gradient),
        and return the computed loss.

        :param x: batch of data or a single datum (MNIST image).
        :return: loss computed on the data batch.
        """
        return

    def model_eval(self, x):
        """
        Given a batch of data `x`, run it through the trained VAE network to get
        the reconstructed image.

        :param x: batch of data or a single datum (MNIST image).
        :return: reconstructed image, and the latent z's mean and variance.
        """
        z_mean, z_var = self.vae_encoder(x)
        if self.mode == TRAIN:
            z = Normal(z_mean, z_var.sqrt()).rsample()
        else:
            z = z_mean
        return self.vae_decoder(z), z_mean, z_var

    def train(self, epoch):
        self.set_train(is_train=True)
        train_loss = 0
        for batch_idx, (x, _) in enumerate(self.train_loader):
            loss = self.compute_loss_and_gradient(x)
            train_loss += loss
        None

    def test(self, epoch):
        self.set_train(is_train=False)
        test_loss = 0
        for i, (x, _) in enumerate(self.test_loader):
            with torch.no_grad():
                recon_x = self.model_eval(x)[0]
                test_loss += self.compute_loss_and_gradient(x)
            if i == 0:
                n = min(x.size(0), 8)
                comparison = torch.cat([x[:n], recon_x.reshape(self.args.batch_size, 1, 28, 28)[:n]])
                save_image(comparison.detach().cpu(), os.path.join(OUTPUT_DIR, 'reconstruction_' + str(epoch) + '.png'), nrow=n)
        test_loss /= len(self.test_loader.dataset)
        None


class PyTorchVAEImpl(VAE):
    """
    Adapted from pytorch/examples.
    Source: https://github.com/pytorch/examples/tree/master/vae
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.optimizer = self.initialize_optimizer(lr=0.001)

    def compute_loss_and_gradient(self, x):
        self.optimizer.zero_grad()
        recon_x, z_mean, z_var = self.model_eval(x)
        binary_cross_entropy = functional.binary_cross_entropy(recon_x, x.reshape(-1, 784))
        kl_div = -0.5 * torch.sum(1 + z_var.log() - z_mean.pow(2) - z_var)
        kl_div /= self.args.batch_size * 784
        loss = binary_cross_entropy + kl_div
        if self.mode == TRAIN:
            loss.backward()
            self.optimizer.step()
        return loss.item()

    def initialize_optimizer(self, lr=0.001):
        model_params = itertools.chain(self.vae_encoder.parameters(), self.vae_decoder.parameters())
        return torch.optim.Adam(model_params, lr)


PYRO_STACK = []


class Messenger:

    def __init__(self, fn=None):
        self.fn = fn

    def __enter__(self):
        PYRO_STACK.append(self)

    def __exit__(self, *args, **kwargs):
        assert PYRO_STACK[-1] is self
        PYRO_STACK.pop()

    def process_message(self, msg):
        pass

    def postprocess_message(self, msg):
        pass

    def __call__(self, *args, **kwargs):
        with self:
            return self.fn(*args, **kwargs)


class block(Messenger):

    def __init__(self, fn=None, hide_fn=lambda msg: True):
        self.hide_fn = hide_fn
        super().__init__(fn)

    def process_message(self, msg):
        if self.hide_fn(msg):
            msg['stop'] = True


class replay(Messenger):

    def __init__(self, fn, guide_trace):
        self.guide_trace = guide_trace
        super().__init__(fn)

    def process_message(self, msg):
        if msg['name'] in self.guide_trace:
            msg['value'] = self.guide_trace[msg['name']]['value']


trace = None


def elbo(model, guide, *args, **kwargs):
    guide_trace = trace(guide).get_trace(*args, **kwargs)
    model_trace = trace(replay(model, guide_trace)).get_trace(*args, **kwargs)
    elbo = 0.0
    for site in model_trace.values():
        if site['type'] == 'sample':
            elbo = elbo + site['fn'].log_prob(site['value']).sum()
    for site in guide_trace.values():
        if site['type'] == 'sample':
            elbo = elbo - site['fn'].log_prob(site['value']).sum()
    return -elbo


def set(**kwargs) ->None:
    """
    Sets one or more settings.

    :param \\*\\*kwargs: alias=value pairs.
    """
    for alias, value in kwargs.items():
        module, deepname, validator = _REGISTRY[alias]
        if validator is not None:
            validator(value)
        destin = import_module(module)
        names = deepname.split('.')
        for name in names[:-1]:
            destin = getattr(destin, name)
        setattr(destin, names[-1], value)


_PYRO_STACK = []


def am_i_wrapped():
    """
    Checks whether the current computation is wrapped in a poutine.
    :returns: bool
    """
    return len(_PYRO_STACK) > 0


def default_process_message(msg):
    """
    Default method for processing messages in inference.

    :param msg: a message to be processed
    :returns: None
    """
    if msg['done'] or msg['is_observed'] or msg['value'] is not None:
        msg['done'] = True
        return msg
    msg['value'] = msg['fn'](*msg['args'], **msg['kwargs'])
    msg['done'] = True


def apply_stack(initial_msg):
    """
    Execute the effect stack at a single site according to the following scheme:

        1. For each ``Messenger`` in the stack from bottom to top,
           execute ``Messenger._process_message`` with the message;
           if the message field "stop" is True, stop;
           otherwise, continue
        2. Apply default behavior (``default_process_message``) to finish remaining site execution
        3. For each ``Messenger`` in the stack from top to bottom,
           execute ``_postprocess_message`` to update the message and internal messenger state with the site results
        4. If the message field "continuation" is not ``None``, call it with the message

    :param dict initial_msg: the starting version of the trace site
    :returns: ``None``
    """
    stack = _PYRO_STACK
    msg = initial_msg
    pointer = 0
    for frame in reversed(stack):
        pointer = pointer + 1
        frame._process_message(msg)
        if msg['stop']:
            break
    default_process_message(msg)
    for frame in stack[-pointer:]:
        frame._postprocess_message(msg)
    cont = msg['continuation']
    if cont is not None:
        cont(msg)
    return None


def effectful(fn=None, type=None):
    """
    :param fn: function or callable that performs an effectful computation
    :param str type: the type label of the operation, e.g. `"sample"`

    Wrapper for calling :func:`~pyro.poutine.runtime.apply_stack` to apply any active effects.
    """
    if fn is None:
        return functools.partial(effectful, type=type)
    if getattr(fn, '_is_effectful', None):
        return fn
    assert type is not None, 'must provide a type label for operation {}'.format(fn)
    assert type != 'message', "cannot use 'message' as keyword"

    @functools.wraps(fn)
    def _fn(*args, **kwargs):
        name = kwargs.pop('name', None)
        infer = kwargs.pop('infer', {})
        value = kwargs.pop('obs', None)
        is_observed = value is not None
        if not am_i_wrapped():
            return fn(*args, **kwargs)
        else:
            msg = {'type': type, 'name': name, 'fn': fn, 'is_observed': is_observed, 'args': args, 'kwargs': kwargs, 'value': value, 'scale': 1.0, 'mask': None, 'cond_indep_stack': (), 'done': False, 'stop': False, 'continuation': None, 'infer': infer}
            apply_stack(msg)
            return msg['value']
    _fn._is_effectful = True
    return _fn


def param(name, init_tensor=None, constraint=constraints.real, event_dim=None):
    """
    Saves the variable as a parameter in the param store.
    To interact with the param store or write to disk,
    see `Parameters <parameters.html>`_.

    :param str name: name of parameter
    :param init_tensor: initial tensor or lazy callable that returns a tensor.
        For large tensors, it may be cheaper to write e.g.
        ``lambda: torch.randn(100000)``, which will only be evaluated on the
        initial statement.
    :type init_tensor: torch.Tensor or callable
    :param constraint: torch constraint, defaults to ``constraints.real``.
    :type constraint: torch.distributions.constraints.Constraint
    :param int event_dim: (optional) number of rightmost dimensions unrelated
        to batching. Dimension to the left of this will be considered batch
        dimensions; if the param statement is inside a subsampled plate, then
        corresponding batch dimensions of the parameter will be correspondingly
        subsampled. If unspecified, all dimensions will be considered event
        dims and no subsampling will be performed.
    :returns: A constrained parameter. The underlying unconstrained parameter
        is accessible via ``pyro.param(...).unconstrained()``, where
        ``.unconstrained`` is a weakref attribute.
    :rtype: torch.Tensor
    """
    args = (name,) if init_tensor is None else (name, init_tensor)
    return _param(*args, constraint=constraint, event_dim=event_dim, name=name)


def enable_validation(is_validate=True):
    """
    Enable or disable validation checks in Pyro. Validation checks provide
    useful warnings and errors, e.g. NaN checks, validating distribution
    arguments and support values, detecting incorrect use of ELBO and MCMC.
    Since some of these checks may be expensive, you may want to disable
    validation of mature models to speed up inference.

    The default behavior mimics Python's ``assert`` statement: validation is on
    by default, but is disabled if Python is run in optimized mode (via
    ``python -O``). Equivalently, the default behavior depends on Python's
    global ``__debug__`` value via ``pyro.enable_validation(__debug__)``.

    Validation is temporarily disabled during jit compilation, for all
    inference algorithms that support the PyTorch jit. We recommend developing
    models with non-jitted inference algorithms to ease debugging, then
    optionally moving to jitted inference once a model is correct.

    :param bool is_validate: (optional; defaults to True) whether to
        enable validation checks.
    """
    dist.enable_validation(is_validate)
    infer.enable_validation(is_validate)
    poutine.enable_validation(is_validate)


class JitTrace_ELBO:

    def __init__(self, **kwargs):
        self.ignore_jit_warnings = kwargs.pop('ignore_jit_warnings', False)
        self._compiled = None
        self._param_trace = None

    def __call__(self, model, guide, *args):
        if self._param_trace is None:
            with block(), trace() as tr, block(hide_fn=lambda m: m['type'] != 'param'):
                elbo(model, guide, *args)
            self._param_trace = tr
        unconstrained_params = tuple(param(name).unconstrained() for name in self._param_trace)
        params_and_args = unconstrained_params + args
        if self._compiled is None:

            def compiled(*params_and_args):
                unconstrained_params = params_and_args[:len(self._param_trace)]
                args = params_and_args[len(self._param_trace):]
                for name, unconstrained_param in zip(self._param_trace, unconstrained_params):
                    constrained_param = param(name)
                    assert constrained_param.unconstrained() is unconstrained_param
                    self._param_trace[name]['value'] = constrained_param
                return replay(elbo, guide_trace=self._param_trace)(model, guide, *args)
            with validation_enabled(False), warnings.catch_warnings():
                if self.ignore_jit_warnings:
                    warnings.filterwarnings('ignore', category=torch.jit.TracerWarning)
                self._compiled = torch.jit.trace(compiled, params_and_args, check_trace=False)
        return self._compiled(*params_and_args)


class ELBOModule(torch.nn.Module):

    def __init__(self, model: torch.nn.Module, guide: torch.nn.Module, elbo: 'ELBO'):
        super().__init__()
        self.model = model
        self.guide = guide
        self.elbo = elbo

    def forward(self, *args, **kwargs):
        return self.elbo.differentiable_loss(self.model, self.guide, *args, **kwargs)


def check_site_shape(site, max_plate_nesting):
    actual_shape = list(site['log_prob'].shape)
    expected_shape = []
    for f in site['cond_indep_stack']:
        if f.dim is not None:
            assert f.dim < 0
            if len(expected_shape) < -f.dim:
                expected_shape = [None] * (-f.dim - len(expected_shape)) + expected_shape
            if expected_shape[f.dim] is not None:
                raise ValueError('\n  '.join(['at site "{}" within plate("{}", dim={}), dim collision'.format(site['name'], f.name, f.dim), 'Try setting dim arg in other plates.']))
            expected_shape[f.dim] = f.size
    expected_shape = [(-1 if e is None else e) for e in expected_shape]
    if len(expected_shape) > max_plate_nesting:
        raise ValueError('\n  '.join(['at site "{}", plate stack overflow'.format(site['name']), 'Try increasing max_plate_nesting to at least {}'.format(len(expected_shape))]))
    if max_plate_nesting < len(actual_shape):
        actual_shape = actual_shape[len(actual_shape) - max_plate_nesting:]
    for actual_size, expected_size in zip_longest(reversed(actual_shape), reversed(expected_shape), fillvalue=1):
        if expected_size != -1 and expected_size != actual_size:
            raise ValueError('\n  '.join(['at site "{}", invalid log_prob shape'.format(site['name']), 'Expected {}, actual {}'.format(expected_shape, actual_shape), 'Try one of the following fixes:', '- enclose the batched tensor in a with pyro.plate(...): context', '- .to_event(...) the distribution being sampled', '- .permute() data dimensions']))
    enum_dim = site['infer'].get('_enumerate_dim')
    if enum_dim is not None:
        if len(site['fn'].batch_shape) >= -enum_dim and site['fn'].batch_shape[enum_dim] != 1:
            raise ValueError('\n  '.join(['Enumeration dim conflict at site "{}"'.format(site['name']), 'Try increasing pyro.markov history size']))


_VALIDATION_ENABLED = __debug__


def is_validation_enabled():
    return _VALIDATION_ENABLED


def site_is_subsample(site):
    """
    Determines whether a trace site originated from a subsample statement inside an `plate`.
    """
    return site['type'] == 'sample' and type(site['fn']).__name__ == '_Subsample'


def prune_subsample_sites(trace):
    """
    Copies and removes all subsample sites from a trace.
    """
    trace = trace.copy()
    for name, site in list(trace.nodes.items()):
        if site_is_subsample(site):
            trace.remove_node(name)
    return trace


class ELBO(object, metaclass=ABCMeta):
    """
    :class:`ELBO` is the top-level interface for stochastic variational
    inference via optimization of the evidence lower bound.

    Most users will not interact with this base class :class:`ELBO` directly;
    instead they will create instances of derived classes:
    :class:`~pyro.infer.trace_elbo.Trace_ELBO`,
    :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO`, or
    :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO`.

    .. note:: Derived classes now provide a more idiomatic PyTorch interface via
        :meth:`__call__` for (model, guide) pairs that are :class:`~torch.nn.Module` s,
        which is useful for integrating Pyro's variational inference tooling with
        standard PyTorch interfaces like :class:`~torch.optim.Optimizer` s
        and the large ecosystem of libraries like PyTorch Lightning
        and the PyTorch JIT that work with these interfaces::

            model = Model()
            guide = pyro.infer.autoguide.AutoNormal(model)

            elbo_ = pyro.infer.Trace_ELBO(num_particles=10)

            # Fix the model/guide pair
            elbo = elbo_(model, guide)

            # perform any data-dependent initialization
            elbo(data)

            optim = torch.optim.Adam(elbo.parameters(), lr=0.001)

            for _ in range(100):
                optim.zero_grad()
                loss = elbo(data)
                loss.backward()
                optim.step()

        Note that Pyro's global parameter store may cause this new interface to
        behave unexpectedly relative to standard PyTorch when working with
        :class:`~pyro.nn.PyroModule` s.

        Users are therefore strongly encouraged to use this interface in conjunction
        with :func:`~pyro.enable_module_local_param` which will override the default
        implicit sharing of parameters across :class:`~pyro.nn.PyroModule` instances.

    :param num_particles: The number of particles/samples used to form the ELBO
        (gradient) estimators.
    :param int max_plate_nesting: Optional bound on max number of nested
        :func:`pyro.plate` contexts. This is only required when enumerating
        over sample sites in parallel, e.g. if a site sets
        ``infer={"enumerate": "parallel"}``. If omitted, ELBO may guess a valid
        value by running the (model,guide) pair once, however this guess may
        be incorrect if model or guide structure is dynamic.
    :param bool vectorize_particles: Whether to vectorize the ELBO computation
        over `num_particles`. Defaults to False. This requires static structure
        in model and guide.
    :param bool strict_enumeration_warning: Whether to warn about possible
        misuse of enumeration, i.e. that
        :class:`pyro.infer.traceenum_elbo.TraceEnum_ELBO` is used iff there
        are enumerated sample sites.
    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT
        tracer. When this is True, all :class:`torch.jit.TracerWarning` will
        be ignored. Defaults to False.
    :param bool jit_options: Optional dict of options to pass to
        :func:`torch.jit.trace` , e.g. ``{"check_trace": True}``.
    :param bool retain_graph: Whether to retain autograd graph during an SVI
        step. Defaults to None (False).
    :param float tail_adaptive_beta: Exponent beta with ``-1.0 <= beta < 0.0`` for
        use with `TraceTailAdaptive_ELBO`.

    References

    [1] `Automated Variational Inference in Probabilistic Programming`
    David Wingate, Theo Weber

    [2] `Black Box Variational Inference`,
    Rajesh Ranganath, Sean Gerrish, David M. Blei
    """

    def __init__(self, num_particles=1, max_plate_nesting=float('inf'), max_iarange_nesting=None, vectorize_particles=False, strict_enumeration_warning=True, ignore_jit_warnings=False, jit_options=None, retain_graph=None, tail_adaptive_beta=-1.0):
        if max_iarange_nesting is not None:
            warnings.warn('max_iarange_nesting is deprecated; use max_plate_nesting instead', DeprecationWarning)
            max_plate_nesting = max_iarange_nesting
        self.max_plate_nesting = max_plate_nesting
        self.num_particles = num_particles
        self.vectorize_particles = vectorize_particles
        self.retain_graph = retain_graph
        if self.vectorize_particles and self.num_particles > 1:
            self.max_plate_nesting += 1
        self.strict_enumeration_warning = strict_enumeration_warning
        self.ignore_jit_warnings = ignore_jit_warnings
        self.jit_options = jit_options
        self.tail_adaptive_beta = tail_adaptive_beta

    def __call__(self, model: torch.nn.Module, guide: torch.nn.Module) ->ELBOModule:
        """
        Given a model and guide, returns a :class:`~torch.nn.Module` which
        computes the ELBO loss when called with arguments to the model and guide.
        """
        return ELBOModule(model, guide, self)

    def _guess_max_plate_nesting(self, model, guide, args, kwargs):
        """
        Guesses max_plate_nesting by running the (model,guide) pair once
        without enumeration. This optimistically assumes static model
        structure.
        """
        with poutine.block():
            guide_trace = poutine.trace(guide).get_trace(*args, **kwargs)
            model_trace = poutine.trace(poutine.replay(model, trace=guide_trace)).get_trace(*args, **kwargs)
        guide_trace = prune_subsample_sites(guide_trace)
        model_trace = prune_subsample_sites(model_trace)
        sites = [site for trace in (model_trace, guide_trace) for site in trace.nodes.values() if site['type'] == 'sample']
        if is_validation_enabled():
            guide_trace.compute_log_prob()
            model_trace.compute_log_prob()
            for site in sites:
                check_site_shape(site, max_plate_nesting=float('inf'))
        dims = [frame.dim for site in sites for frame in site['cond_indep_stack'] if frame.vectorized]
        self.max_plate_nesting = -min(dims) if dims else 0
        if self.vectorize_particles and self.num_particles > 1:
            self.max_plate_nesting += 1
        logging.info('Guessed max_plate_nesting = {}'.format(self.max_plate_nesting))

    def _vectorized_num_particles(self, fn):
        """
        Wraps a callable inside an outermost :class:`~pyro.plate` to parallelize
        ELBO computation over `num_particles`, and to broadcast batch shapes of
        sample site functions in accordance with the `~pyro.plate` contexts
        within which they are embedded.

        :param fn: arbitrary callable containing Pyro primitives.
        :return: wrapped callable.
        """
        if self.num_particles == 1:
            return fn
        return pyro.plate('num_particles_vectorized', self.num_particles, dim=-self.max_plate_nesting)(fn)

    def _get_vectorized_trace(self, model, guide, args, kwargs):
        """
        Wraps the model and guide to vectorize ELBO computation over
        ``num_particles``, and returns a single trace from the wrapped model
        and guide.
        """
        return self._get_trace(self._vectorized_num_particles(model), self._vectorized_num_particles(guide), args, kwargs)

    @abstractmethod
    def _get_trace(self, model, guide, args, kwargs):
        """
        Returns a single trace from the guide, and the model that is run
        against it.
        """
        raise NotImplementedError

    def _get_traces(self, model, guide, args, kwargs):
        """
        Runs the guide and runs the model against the guide with
        the result packaged as a trace generator.
        """
        if self.vectorize_particles:
            if self.max_plate_nesting == float('inf'):
                self._guess_max_plate_nesting(model, guide, args, kwargs)
            yield self._get_vectorized_trace(model, guide, args, kwargs)
        else:
            for i in range(self.num_particles):
                yield self._get_trace(model, guide, args, kwargs)


class ScoreParts(namedtuple('ScoreParts', ['log_prob', 'score_function', 'entropy_term'])):
    """
    This data structure stores terms used in stochastic gradient estimators that
    combine the pathwise estimator and the score function estimator.
    """

    def scale_and_mask(self, scale=1.0, mask=None):
        """
        Scale and mask appropriate terms of a gradient estimator by a data multiplicity factor.
        Note that the `score_function` term should not be scaled or masked.

        :param scale: a positive scale
        :type scale: torch.Tensor or number
        :param mask: an optional masking tensor
        :type mask: torch.BoolTensor or None
        """
        log_prob = scale_and_mask(self.log_prob, scale, mask)
        score_function = self.score_function
        entropy_term = scale_and_mask(self.entropy_term, scale, mask)
        return ScoreParts(log_prob, score_function, entropy_term)


def is_identically_one(x):
    """
    Check if argument is exactly the number one. True for the number one;
    false for other numbers; false for :class:`~torch.Tensor`s.
    """
    if isinstance(x, numbers.Number):
        return x == 1
    if not torch._C._get_tracing_state():
        if isinstance(x, torch.Tensor) and x.dtype == torch.int64 and not x.shape:
            return x.item() == 1
    return False


def scale_and_mask(tensor, scale=1.0, mask=None):
    """
    Scale and mask a packed tensor, broadcasting and avoiding unnecessary ops.

    :param torch.Tensor tensor: a packed tensor
    :param scale: a positive scale
    :type scale: torch.Tensor or number
    :param mask: an optional packed tensor mask
    :type mask: torch.BoolTensor, bool, or None
    """
    if isinstance(scale, torch.Tensor) and scale.dim():
        raise NotImplementedError('non-scalar scale is not supported')
    if mask is None or mask is True:
        if is_identically_one(scale):
            return tensor
        result = tensor * scale
        result._pyro_dims = tensor._pyro_dims
        return result
    if mask is False:
        result = torch.zeros_like(tensor)
        result._pyro_dims = tensor._pyro_dims
        return result
    tensor, mask = broadcast_all(tensor, mask)
    result = torch.where(mask, tensor, tensor.new_zeros(()))
    result._pyro_dims = tensor._pyro_dims
    return result


class TorchDistributionMixin(Distribution):
    """
    Mixin to provide Pyro compatibility for PyTorch distributions.

    You should instead use `TorchDistribution` for new distribution classes.

    This is mainly useful for wrapping existing PyTorch distributions for
    use in Pyro.  Derived classes must first inherit from
    :class:`torch.distributions.distribution.Distribution` and then inherit
    from :class:`TorchDistributionMixin`.
    """

    def __call__(self, sample_shape=torch.Size()):
        """
        Samples a random value.

        This is reparameterized whenever possible, calling
        :meth:`~torch.distributions.distribution.Distribution.rsample` for
        reparameterized distributions and
        :meth:`~torch.distributions.distribution.Distribution.sample` for
        non-reparameterized distributions.

        :param sample_shape: the size of the iid batch to be drawn from the
            distribution.
        :type sample_shape: torch.Size
        :return: A random value or batch of random values (if parameters are
            batched). The shape of the result should be `self.shape()`.
        :rtype: torch.Tensor
        """
        return self.rsample(sample_shape) if self.has_rsample else self.sample(sample_shape)

    @property
    def event_dim(self):
        """
        :return: Number of dimensions of individual events.
        :rtype: int
        """
        return len(self.event_shape)

    def shape(self, sample_shape=torch.Size()):
        """
        The tensor shape of samples from this distribution.

        Samples are of shape::

          d.shape(sample_shape) == sample_shape + d.batch_shape + d.event_shape

        :param sample_shape: the size of the iid batch to be drawn from the
            distribution.
        :type sample_shape: torch.Size
        :return: Tensor shape of samples.
        :rtype: torch.Size
        """
        return sample_shape + self.batch_shape + self.event_shape

    @classmethod
    def infer_shapes(cls, **arg_shapes):
        """
        Infers ``batch_shape`` and ``event_shape`` given shapes of args to
        :meth:`__init__`.

        .. note:: This assumes distribution shape depends only on the shapes
            of tensor inputs, not in the data contained in those inputs.

        :param \\*\\*arg_shapes: Keywords mapping name of input arg to
            :class:`torch.Size` or tuple representing the sizes of each
            tensor input.
        :returns: A pair ``(batch_shape, event_shape)`` of the shapes of a
            distribution that would be created with input args of the given
            shapes.
        :rtype: tuple
        """
        if cls.support.event_dim > 0:
            raise NotImplementedError
        batch_shapes = []
        for name, shape in arg_shapes.items():
            event_dim = cls.arg_constraints.get(name, constraints.real).event_dim
            batch_shapes.append(shape[:len(shape) - event_dim])
        batch_shape = torch.Size(broadcast_shape(*batch_shapes))
        event_shape = torch.Size()
        return batch_shape, event_shape

    def expand(self, batch_shape, _instance=None):
        """
        Returns a new :class:`ExpandedDistribution` instance with batch
        dimensions expanded to `batch_shape`.

        :param tuple batch_shape: batch shape to expand to.
        :param _instance: unused argument for compatibility with
            :meth:`torch.distributions.Distribution.expand`
        :return: an instance of `ExpandedDistribution`.
        :rtype: :class:`ExpandedDistribution`
        """
        return ExpandedDistribution(self, batch_shape)

    def expand_by(self, sample_shape):
        """
        Expands a distribution by adding ``sample_shape`` to the left side of
        its :attr:`~torch.distributions.distribution.Distribution.batch_shape`.

        To expand internal dims of ``self.batch_shape`` from 1 to something
        larger, use :meth:`expand` instead.

        :param torch.Size sample_shape: The size of the iid batch to be drawn
            from the distribution.
        :return: An expanded version of this distribution.
        :rtype: :class:`ExpandedDistribution`
        """
        try:
            expanded_dist = self.expand(torch.Size(sample_shape) + self.batch_shape)
        except NotImplementedError:
            expanded_dist = TorchDistributionMixin.expand(self, torch.Size(sample_shape) + self.batch_shape)
        return expanded_dist

    def reshape(self, sample_shape=None, extra_event_dims=None):
        raise Exception("""
            .reshape(sample_shape=s, extra_event_dims=n) was renamed and split into
            .expand_by(sample_shape=s).to_event(reinterpreted_batch_ndims=n).""")

    def to_event(self, reinterpreted_batch_ndims=None):
        """
        Reinterprets the ``n`` rightmost dimensions of this distributions
        :attr:`~torch.distributions.distribution.Distribution.batch_shape`
        as event dims, adding them to the left side of
        :attr:`~torch.distributions.distribution.Distribution.event_shape`.

        Example:

            .. doctest::
               :hide:

               >>> d0 = dist.Normal(torch.zeros(2, 3, 4, 5), torch.ones(2, 3, 4, 5))
               >>> [d0.batch_shape, d0.event_shape]
               [torch.Size([2, 3, 4, 5]), torch.Size([])]
               >>> d1 = d0.to_event(2)

            >>> [d1.batch_shape, d1.event_shape]
            [torch.Size([2, 3]), torch.Size([4, 5])]
            >>> d2 = d1.to_event(1)
            >>> [d2.batch_shape, d2.event_shape]
            [torch.Size([2]), torch.Size([3, 4, 5])]
            >>> d3 = d1.to_event(2)
            >>> [d3.batch_shape, d3.event_shape]
            [torch.Size([]), torch.Size([2, 3, 4, 5])]

        :param int reinterpreted_batch_ndims: The number of batch dimensions to
            reinterpret as event dimensions. May be negative to remove
            dimensions from an :class:`pyro.distributions.torch.Independent` .
            If None, convert all dimensions to event dimensions.
        :return: A reshaped version of this distribution.
        :rtype: :class:`pyro.distributions.torch.Independent`
        """
        if reinterpreted_batch_ndims is None:
            reinterpreted_batch_ndims = len(self.batch_shape)
        base_dist = self
        while isinstance(base_dist, torch.distributions.Independent):
            reinterpreted_batch_ndims += base_dist.reinterpreted_batch_ndims
            base_dist = base_dist.base_dist
        if reinterpreted_batch_ndims == 0:
            return base_dist
        if reinterpreted_batch_ndims < 0:
            raise ValueError('Cannot remove event dimensions from {}'.format(type(self)))
        return pyro.distributions.torch.Independent(base_dist, reinterpreted_batch_ndims)

    def independent(self, reinterpreted_batch_ndims=None):
        warnings.warn('independent is deprecated; use to_event instead', DeprecationWarning)
        return self.to_event(reinterpreted_batch_ndims=reinterpreted_batch_ndims)

    def mask(self, mask):
        """
        Masks a distribution by a boolean or boolean-valued tensor that is
        broadcastable to the distributions
        :attr:`~torch.distributions.distribution.Distribution.batch_shape` .

        :param mask: A boolean or boolean valued tensor.
        :type mask: bool or torch.Tensor
        :return: A masked copy of this distribution.
        :rtype: :class:`MaskedDistribution`
        """
        return MaskedDistribution(self, mask)


class Marginals:
    """
    Holds the marginal distribution over one or more sites from the ``TracePosterior``'s
    model. This is a convenience container class, which can be extended by ``TracePosterior``
    subclasses. e.g. for implementing diagnostics.

    :param TracePosterior trace_posterior: a TracePosterior instance representing
        a Monte Carlo posterior.
    :param list sites: optional list of sites for which we need to generate
        the marginal distribution.
    """

    def __init__(self, trace_posterior, sites=None, validate_args=None):
        assert isinstance(trace_posterior, TracePosterior), 'trace_dist must be trace posterior distribution object'
        if sites is None:
            sites = ['_RETURN']
        elif isinstance(sites, str):
            sites = [sites]
        else:
            assert isinstance(sites, list)
        self.sites = sites
        self._marginals = OrderedDict()
        self._diagnostics = OrderedDict()
        self._trace_posterior = trace_posterior
        self._populate_traces(trace_posterior, validate_args)

    def _populate_traces(self, trace_posterior, validate):
        self._marginals = {site: EmpiricalMarginal(trace_posterior, site, validate) for site in self.sites}

    def support(self, flatten=False):
        """
        Gets support of this marginal distribution.

        :param bool flatten: A flag to decide if we want to flatten `batch_shape`
            when the marginal distribution is collected from the posterior with
            ``num_chains > 1``. Defaults to False.
        :returns: a dict with keys are sites' names and values are sites' supports.
        :rtype: :class:`OrderedDict`
        """
        support = OrderedDict([(site, value.enumerate_support()) for site, value in self._marginals.items()])
        if self._trace_posterior.num_chains > 1 and flatten:
            for site, samples in support.items():
                shape = samples.size()
                flattened_shape = torch.Size((shape[0] * shape[1],)) + shape[2:]
                support[site] = samples.reshape(flattened_shape)
        return support

    @property
    def empirical(self):
        """
        A dictionary of sites' names and their corresponding :class:`EmpiricalMarginal`
        distribution.

        :type: :class:`OrderedDict`
        """
        return self._marginals


def _weighted_mean(input, log_weights, dim=0, keepdim=False):
    dim = input.dim() + dim if dim < 0 else dim
    log_weights = log_weights.reshape([-1] + (input.dim() - dim - 1) * [1])
    max_log_weight = log_weights.max(dim=0)[0]
    relative_probs = (log_weights - max_log_weight).exp()
    return (input * relative_probs).sum(dim=dim, keepdim=keepdim) / relative_probs.sum()


def _weighted_variance(input, log_weights, dim=0, keepdim=False, unbiased=True):
    deviation_squared = (input - _weighted_mean(input, log_weights, dim, keepdim=True)).pow(2)
    correction = log_weights.size(0) / (log_weights.size(0) - 1.0) if unbiased else 1.0
    return _weighted_mean(deviation_squared, log_weights, dim, keepdim) * correction


def waic(input, log_weights=None, pointwise=False, dim=0):
    """
    Computes "Widely Applicable/Watanabe-Akaike Information Criterion" (WAIC) and
    its corresponding effective number of parameters.

    Reference:

    [1] `WAIC and cross-validation in Stan`,
    Aki Vehtari, Andrew Gelman

    :param torch.Tensor input: the input tensor, which is log likelihood of a model.
    :param torch.Tensor log_weights: weights of samples along ``dim``.
    :param int dim: the sample dimension of ``input``.
    :returns tuple: tuple of WAIC and effective number of parameters.
    """
    if log_weights is None:
        log_weights = torch.zeros(input.size(dim), dtype=input.dtype, device=input.device)
    dim = input.dim() + dim if dim < 0 else dim
    weighted_input = input + log_weights.reshape([-1] + (input.dim() - dim - 1) * [1])
    lpd = torch.logsumexp(weighted_input, dim=dim) - torch.logsumexp(log_weights, dim=0)
    p_waic = _weighted_variance(input, log_weights, dim)
    elpd = lpd - p_waic
    waic = -2 * elpd
    return (waic, p_waic) if pointwise else (waic.sum(), p_waic.sum())


class TracePosterior(object, metaclass=ABCMeta):
    """
    Abstract TracePosterior object from which posterior inference algorithms inherit.
    When run, collects a bag of execution traces from the approximate posterior.
    This is designed to be used by other utility classes like `EmpiricalMarginal`,
    that need access to the collected execution traces.
    """

    def __init__(self, num_chains=1):
        self.num_chains = num_chains
        self._reset()

    def _reset(self):
        self.log_weights = []
        self.exec_traces = []
        self.chain_ids = []
        self._idx_by_chain = [[] for _ in range(self.num_chains)]
        self._categorical = None

    def marginal(self, sites=None):
        """
        Generates the marginal distribution of this posterior.

        :param list sites: optional list of sites for which we need to generate
            the marginal distribution.
        :returns: A :class:`Marginals` class instance.
        :rtype: :class:`Marginals`
        """
        return Marginals(self, sites)

    @abstractmethod
    def _traces(self, *args, **kwargs):
        """
        Abstract method implemented by classes that inherit from `TracePosterior`.

        :return: Generator over ``(exec_trace, weight)`` or
        ``(exec_trace, weight, chain_id)``.
        """
        raise NotImplementedError('Inference algorithm must implement ``_traces``.')

    def __call__(self, *args, **kwargs):
        random_idx = self._categorical.sample().item()
        chain_idx, sample_idx = random_idx % self.num_chains, random_idx // self.num_chains
        sample_idx = self._idx_by_chain[chain_idx][sample_idx]
        trace = self.exec_traces[sample_idx].copy()
        for name in trace.observation_nodes:
            trace.remove_node(name)
        return trace

    def run(self, *args, **kwargs):
        """
        Calls `self._traces` to populate execution traces from a stochastic
        Pyro model.

        :param args: optional args taken by `self._traces`.
        :param kwargs: optional keywords args taken by `self._traces`.
        """
        self._reset()
        with poutine.block():
            for i, vals in enumerate(self._traces(*args, **kwargs)):
                if len(vals) == 2:
                    chain_id = 0
                    tr, logit = vals
                else:
                    tr, logit, chain_id = vals
                    assert chain_id < self.num_chains
                self.exec_traces.append(tr)
                self.log_weights.append(logit)
                self.chain_ids.append(chain_id)
                self._idx_by_chain[chain_id].append(i)
        self._categorical = Categorical(logits=torch.tensor(self.log_weights))
        return self

    def information_criterion(self, pointwise=False):
        """
        Computes information criterion of the model. Currently, returns only "Widely
        Applicable/Watanabe-Akaike Information Criterion" (WAIC) and the corresponding
        effective number of parameters.

        Reference:

        [1] `Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC`,
        Aki Vehtari, Andrew Gelman, and Jonah Gabry

        :param bool pointwise: a flag to decide if we want to get a vectorized WAIC or not. When
            ``pointwise=False``, returns the sum.
        :returns: a dictionary containing values of WAIC and its effective number of
            parameters.
        :rtype: :class:`OrderedDict`
        """
        if not self.exec_traces:
            return {}
        obs_node = None
        log_likelihoods = []
        for trace in self.exec_traces:
            obs_nodes = trace.observation_nodes
            if len(obs_nodes) > 1:
                raise ValueError('Infomation criterion calculation only works for models with one observation node.')
            if obs_node is None:
                obs_node = obs_nodes[0]
            elif obs_node != obs_nodes[0]:
                raise ValueError('Observation node has been changed, expected {} but got {}'.format(obs_node, obs_nodes[0]))
            log_likelihoods.append(trace.nodes[obs_node]['fn'].log_prob(trace.nodes[obs_node]['value']))
        ll = torch.stack(log_likelihoods, dim=0)
        waic_value, p_waic = waic(ll, torch.tensor(self.log_weights, device=ll.device), pointwise)
        return OrderedDict([('waic', waic_value), ('p_waic', p_waic)])


def torch_item(x):
    """
    Like ``x.item()`` for a :class:`~torch.Tensor`, but also works with numbers.
    """
    return x if isinstance(x, numbers.Number) else x.item()


class SVI(TracePosterior):
    """
    :param model: the model (callable containing Pyro primitives)
    :param guide: the guide (callable containing Pyro primitives)
    :param optim: a wrapper a for a PyTorch optimizer
    :type optim: ~pyro.optim.optim.PyroOptim
    :param loss: an instance of a subclass of :class:`~pyro.infer.elbo.ELBO`.
        Pyro provides three built-in losses:
        :class:`~pyro.infer.trace_elbo.Trace_ELBO`,
        :class:`~pyro.infer.tracegraph_elbo.TraceGraph_ELBO`, and
        :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO`.
        See the :class:`~pyro.infer.elbo.ELBO` docs to learn how to implement
        a custom loss.
    :type loss: pyro.infer.elbo.ELBO
    :param num_samples: (DEPRECATED) the number of samples for Monte Carlo posterior approximation
    :param num_steps: (DEPRECATED) the number of optimization steps to take in ``run()``

    A unified interface for stochastic variational inference in Pyro. The most
    commonly used loss is ``loss=Trace_ELBO()``. See the tutorial
    `SVI Part I <http://pyro.ai/examples/svi_part_i.html>`_ for a discussion.
    """

    def __init__(self, model, guide, optim, loss, loss_and_grads=None, num_samples=0, num_steps=0, **kwargs):
        if num_steps:
            warnings.warn('The `num_steps` argument to SVI is deprecated and will be removed in a future release. Use `SVI.step` directly to control the number of iterations.', FutureWarning)
        if num_samples:
            warnings.warn('The `num_samples` argument to SVI is deprecated and will be removed in a future release. Use `pyro.infer.Predictive` class to draw samples from the posterior.', FutureWarning)
        self.model = model
        self.guide = guide
        self.optim = optim
        self.num_steps = num_steps
        self.num_samples = num_samples
        super().__init__(**kwargs)
        if not isinstance(optim, pyro.optim.PyroOptim):
            raise ValueError('Optimizer should be an instance of pyro.optim.PyroOptim class.')
        if isinstance(loss, ELBO):
            self.loss = loss.loss
            self.loss_and_grads = loss.loss_and_grads
        else:
            if loss_and_grads is None:

                def _loss_and_grads(*args, **kwargs):
                    loss_val = loss(*args, **kwargs)
                    if getattr(loss_val, 'requires_grad', False):
                        loss_val.backward(retain_graph=True)
                    return loss_val
                loss_and_grads = _loss_and_grads
            self.loss = loss
            self.loss_and_grads = loss_and_grads

    def run(self, *args, **kwargs):
        """
        .. warning::
            This method is deprecated, and will be removed in a future release.
            For inference, use :meth:`step` directly, and for predictions,
            use the :class:`~pyro.infer.predictive.Predictive` class.
        """
        warnings.warn('The `SVI.run` method is deprecated and will be removed in a future release. For inference, use `SVI.step` directly, and for predictions, use the `pyro.infer.Predictive` class.', FutureWarning)
        if self.num_steps > 0:
            with poutine.block():
                for i in range(self.num_steps):
                    self.step(*args, **kwargs)
        return super().run(*args, **kwargs)

    def _traces(self, *args, **kwargs):
        for i in range(self.num_samples):
            guide_trace = poutine.trace(self.guide).get_trace(*args, **kwargs)
            model_trace = poutine.trace(poutine.replay(self.model, trace=guide_trace)).get_trace(*args, **kwargs)
            yield model_trace, 1.0

    def evaluate_loss(self, *args, **kwargs):
        """
        :returns: estimate of the loss
        :rtype: float

        Evaluate the loss function. Any args or kwargs are passed to the model and guide.
        """
        with torch.no_grad():
            loss = self.loss(self.model, self.guide, *args, **kwargs)
            if isinstance(loss, tuple):
                return type(loss)(map(torch_item, loss))
            else:
                return torch_item(loss)

    def step(self, *args, **kwargs):
        """
        :returns: estimate of the loss
        :rtype: float

        Take a gradient step on the loss function (and any auxiliary loss functions
        generated under the hood by `loss_and_grads`).
        Any args or kwargs are passed to the model and guide
        """
        with poutine.trace(param_only=True) as param_capture:
            loss = self.loss_and_grads(self.model, self.guide, *args, **kwargs)
        params = set(site['value'].unconstrained() for site in param_capture.trace.nodes.values())
        self.optim(params)
        pyro.infer.util.zero_grads(params)
        if isinstance(loss, tuple):
            return type(loss)(map(torch_item, loss))
        else:
            return torch_item(loss)


def Trace_ELBO(**kwargs):
    return elbo


class PyroVAEImpl(VAE):
    """
    Implementation of VAE using Pyro. Only the model and the guide specification
    is needed to run the optimizer (the objective function does not need to be
    specified as in the PyTorch implementation).
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.optimizer = self.initialize_optimizer(lr=0.001)

    def model(self, data):
        decoder = pyro.module('decoder', self.vae_decoder)
        z_mean, z_std = torch.zeros([data.size(0), 20]), torch.ones([data.size(0), 20])
        with pyro.plate('data', data.size(0)):
            z = pyro.sample('latent', Normal(z_mean, z_std).to_event(1))
            img = decoder.forward(z)
            pyro.sample('obs', Bernoulli(img, validate_args=False).to_event(1), obs=data.reshape(-1, 784))

    def guide(self, data):
        encoder = pyro.module('encoder', self.vae_encoder)
        with pyro.plate('data', data.size(0)):
            z_mean, z_var = encoder.forward(data)
            pyro.sample('latent', Normal(z_mean, z_var.sqrt()).to_event(1))

    def compute_loss_and_gradient(self, x):
        if self.mode == TRAIN:
            loss = self.optimizer.step(x)
        else:
            loss = self.optimizer.evaluate_loss(x)
        loss /= self.args.batch_size * 784
        return loss

    def initialize_optimizer(self, lr):
        optimizer = Adam({'lr': lr})
        elbo = JitTrace_ELBO() if self.args.jit else Trace_ELBO()
        return SVI(self.model, self.guide, optimizer, loss=elbo)


class FullyConnected(nn.Sequential):
    """
    Fully connected multi-layer network with ELU activations.
    """

    def __init__(self, sizes, final_activation=None):
        layers = []
        for in_size, out_size in zip(sizes, sizes[1:]):
            layers.append(nn.Linear(in_size, out_size))
            layers.append(nn.ELU())
        layers.pop(-1)
        if final_activation is not None:
            layers.append(final_activation)
        super().__init__(*layers)

    def append(self, layer):
        assert isinstance(layer, nn.Module)
        self.add_module(str(len(self)), layer)


class DistributionNet(nn.Module):
    """
    Base class for distribution nets.
    """

    @staticmethod
    def get_class(dtype):
        """
        Get a subclass by a prefix of its name, e.g.::

            assert DistributionNet.get_class("bernoulli") is BernoulliNet
        """
        for cls in DistributionNet.__subclasses__():
            if cls.__name__.lower() == dtype + 'net':
                return cls
        raise ValueError('dtype not supported: {}'.format(dtype))


class BernoulliNet(DistributionNet):
    """
    :class:`FullyConnected` network outputting a single ``logits`` value.

    This is used to represent a conditional probability distribution of a
    single Bernoulli random variable conditioned on a ``sizes[0]``-sized real
    value, for example::

        net = BernoulliNet([3, 4])
        z = torch.randn(3)
        logits, = net(z)
        t = net.make_dist(logits).sample()
    """

    def __init__(self, sizes):
        assert len(sizes) >= 1
        super().__init__()
        self.fc = FullyConnected(sizes + [1])

    def forward(self, x):
        logits = self.fc(x).squeeze(-1).clamp(min=-10, max=10)
        return logits,

    @staticmethod
    def make_dist(logits):
        return dist.Bernoulli(logits=logits)


class ExponentialNet(DistributionNet):
    """
    :class:`FullyConnected` network outputting a constrained ``rate``.

    This is used to represent a conditional probability distribution of a
    single Normal random variable conditioned on a ``sizes[0]``-size real
    value, for example::

        net = ExponentialNet([3, 4])
        x = torch.randn(3)
        rate, = net(x)
        y = net.make_dist(rate).sample()
    """

    def __init__(self, sizes):
        assert len(sizes) >= 1
        super().__init__()
        self.fc = FullyConnected(sizes + [1])

    def forward(self, x):
        scale = nn.functional.softplus(self.fc(x).squeeze(-1)).clamp(min=0.001, max=1000000.0)
        rate = scale.reciprocal()
        return rate,

    @staticmethod
    def make_dist(rate):
        return dist.Exponential(rate)


class LaplaceNet(DistributionNet):
    """
    :class:`FullyConnected` network outputting a constrained ``loc,scale``
    pair.

    This is used to represent a conditional probability distribution of a
    single Laplace random variable conditioned on a ``sizes[0]``-size real
    value, for example::

        net = LaplaceNet([3, 4])
        x = torch.randn(3)
        loc, scale = net(x)
        y = net.make_dist(loc, scale).sample()
    """

    def __init__(self, sizes):
        assert len(sizes) >= 1
        super().__init__()
        self.fc = FullyConnected(sizes + [2])

    def forward(self, x):
        loc_scale = self.fc(x)
        loc = loc_scale[..., 0].clamp(min=-1000000.0, max=1000000.0)
        scale = nn.functional.softplus(loc_scale[..., 1]).clamp(min=0.001, max=1000000.0)
        return loc, scale

    @staticmethod
    def make_dist(loc, scale):
        return dist.Laplace(loc, scale)


class NormalNet(DistributionNet):
    """
    :class:`FullyConnected` network outputting a constrained ``loc,scale``
    pair.

    This is used to represent a conditional probability distribution of a
    single Normal random variable conditioned on a ``sizes[0]``-size real
    value, for example::

        net = NormalNet([3, 4])
        x = torch.randn(3)
        loc, scale = net(x)
        y = net.make_dist(loc, scale).sample()
    """

    def __init__(self, sizes):
        assert len(sizes) >= 1
        super().__init__()
        self.fc = FullyConnected(sizes + [2])

    def forward(self, x):
        loc_scale = self.fc(x)
        loc = loc_scale[..., 0].clamp(min=-1000000.0, max=1000000.0)
        scale = nn.functional.softplus(loc_scale[..., 1]).clamp(min=0.001, max=1000000.0)
        return loc, scale

    @staticmethod
    def make_dist(loc, scale):
        return dist.Normal(loc, scale)


class StudentTNet(DistributionNet):
    """
    :class:`FullyConnected` network outputting a constrained ``df,loc,scale``
    triple, with shared ``df > 1``.

    This is used to represent a conditional probability distribution of a
    single Student's t random variable conditioned on a ``sizes[0]``-size real
    value, for example::

        net = StudentTNet([3, 4])
        x = torch.randn(3)
        df, loc, scale = net(x)
        y = net.make_dist(df, loc, scale).sample()
    """

    def __init__(self, sizes):
        assert len(sizes) >= 1
        super().__init__()
        self.fc = FullyConnected(sizes + [2])
        self.df_unconstrained = nn.Parameter(torch.tensor(0.0))

    def forward(self, x):
        loc_scale = self.fc(x)
        loc = loc_scale[..., 0].clamp(min=-1000000.0, max=1000000.0)
        scale = nn.functional.softplus(loc_scale[..., 1]).clamp(min=0.001, max=1000000.0)
        df = nn.functional.softplus(self.df_unconstrained).add(1).expand_as(loc)
        return df, loc, scale

    @staticmethod
    def make_dist(df, loc, scale):
        return dist.StudentT(df, loc, scale)


class DiagNormalNet(nn.Module):
    """
    :class:`FullyConnected` network outputting a constrained ``loc,scale``
    pair.

    This is used to represent a conditional probability distribution of a
    ``sizes[-1]``-sized diagonal Normal random variable conditioned on a
    ``sizes[0]``-size real value, for example::

        net = DiagNormalNet([3, 4, 5])
        z = torch.randn(3)
        loc, scale = net(z)
        x = dist.Normal(loc, scale).sample()

    This is intended for the latent ``z`` distribution and the prewhitened
    ``x`` features, and conservatively clips ``loc`` and ``scale`` values.
    """

    def __init__(self, sizes):
        assert len(sizes) >= 2
        self.dim = sizes[-1]
        super().__init__()
        self.fc = FullyConnected(sizes[:-1] + [self.dim * 2])

    def forward(self, x):
        loc_scale = self.fc(x)
        loc = loc_scale[..., :self.dim].clamp(min=-100.0, max=100.0)
        scale = nn.functional.softplus(loc_scale[..., self.dim:]).add(0.001).clamp(max=100.0)
        return loc, scale


class PreWhitener(nn.Module):
    """
    Data pre-whitener.
    """

    def __init__(self, data):
        super().__init__()
        with torch.no_grad():
            loc = data.mean(0)
            scale = data.std(0)
            scale[~(scale > 0)] = 1.0
            self.register_buffer('loc', loc)
            self.register_buffer('inv_scale', scale.reciprocal())

    def forward(self, data):
        return (data - self.loc) * self.inv_scale


def is_scheduler(optimizer) ->bool:
    """
    Helper method to determine whether a PyTorch object is either a PyTorch
    optimizer (return false) or a optimizer wrapped in an LRScheduler e.g. a
    ``ReduceLROnPlateau`` or subclasses of ``_LRScheduler`` (return true).
    """
    return hasattr(optimizer, 'optimizer')


def _get_state_dict(optimizer) ->dict:
    """
    Helper to get the state dict for either a raw optimizer or an optimizer
    wrapped in an LRScheduler.
    """
    if is_scheduler(optimizer):
        state = {'scheduler': optimizer.state_dict(), 'optimizer': optimizer.optimizer.state_dict()}
    else:
        state = optimizer.state_dict()
    return state


def _load_state_dict(optimizer, state: dict) ->None:
    """
    Helper to load the state dict into either a raw optimizer or an optimizer
    wrapped in an LRScheduler.
    """
    if is_scheduler(optimizer):
        optimizer.load_state_dict(state['scheduler'])
        optimizer.optimizer.load_state_dict(state['optimizer'])
    else:
        optimizer.load_state_dict(state)


_MODULE_NAMESPACE_DIVIDER = '$$$'


def module_from_param_with_module_name(param_name):
    return param_name.split(_MODULE_NAMESPACE_DIVIDER)[0]


def normalize_param_name(name):
    return name.replace(_MODULE_NAMESPACE_DIVIDER, '.')


def user_param_name(param_name):
    if _MODULE_NAMESPACE_DIVIDER in param_name:
        return param_name.split(_MODULE_NAMESPACE_DIVIDER)[1]
    return param_name


class PyroOptim:
    """
    A wrapper for torch.optim.Optimizer objects that helps with managing dynamically generated parameters.

    :param optim_constructor: a torch.optim.Optimizer
    :param optim_args: a dictionary of learning arguments for the optimizer or a callable that returns
        such dictionaries
    :param clip_args: a dictionary of clip_norm and/or clip_value args or a callable that returns
        such dictionaries
    """

    def __init__(self, optim_constructor: Union[Callable, Optimizer, Type[Optimizer]], optim_args: Union[Dict, Callable[..., Dict]], clip_args: Optional[Union[Dict, Callable[..., Dict]]]=None):
        self.pt_optim_constructor = optim_constructor
        assert callable(optim_args) or isinstance(optim_args, dict), 'optim_args must be function that returns defaults or a defaults dictionary'
        if clip_args is None:
            clip_args = {}
        assert callable(clip_args) or isinstance(clip_args, dict), 'clip_args must be function that returns defaults or a defaults dictionary'
        self.pt_optim_args = optim_args
        if callable(optim_args):
            self.pt_optim_args_argc = len(inspect.signature(optim_args).parameters)
        self.pt_clip_args = clip_args
        self.optim_objs: Dict = {}
        self.grad_clip: Dict = {}
        self._state_waiting_to_be_consumed: Dict = {}

    def __call__(self, params: Union[List, ValuesView], *args, **kwargs) ->None:
        """
        :param params: a list of parameters
        :type params: an iterable of strings

        Do an optimization step for each param in params. If a given param has never been seen before,
        initialize an optimizer for it.
        """
        for p in params:
            if p not in self.optim_objs:
                optimizer = self.optim_objs[p] = self._get_optim(p)
                self.grad_clip[p] = self._get_grad_clip(p)
                param_name = pyro.get_param_store().param_name(p)
                state = self._state_waiting_to_be_consumed.pop(param_name, None)
                if state is not None:
                    _load_state_dict(optimizer, state)
            if self.grad_clip[p] is not None:
                self.grad_clip[p](p)
            if isinstance(self.optim_objs[p], torch.optim.lr_scheduler._LRScheduler) or isinstance(self.optim_objs[p], torch.optim.lr_scheduler.ReduceLROnPlateau):
                self.optim_objs[p].optimizer.step(*args, **kwargs)
            else:
                self.optim_objs[p].step(*args, **kwargs)

    def get_state(self) ->Dict:
        """
        Get state associated with all the optimizers in the form of a dictionary with
        key-value pairs (parameter name, optim state dicts)
        """
        state_dict = {}
        for param in self.optim_objs:
            param_name = pyro.get_param_store().param_name(param)
            state_dict[param_name] = _get_state_dict(self.optim_objs[param])
        return state_dict

    def set_state(self, state_dict: Dict) ->None:
        """
        Set the state associated with all the optimizers using the state obtained
        from a previous call to get_state()
        """
        self._state_waiting_to_be_consumed.update(state_dict)

    def save(self, filename: str) ->None:
        """
        :param filename: file name to save to
        :type filename: str

        Save optimizer state to disk
        """
        with open(filename, 'wb') as output_file:
            torch.save(self.get_state(), output_file)

    def load(self, filename: str, map_location=None) ->None:
        """
        :param filename: file name to load from
        :type filename: str
        :param map_location: torch.load() map_location parameter
        :type map_location: function, torch.device, string or a dict

        Load optimizer state from disk
        """
        with open(filename, 'rb') as input_file:
            state = torch.load(input_file, map_location=map_location)
        self.set_state(state)

    def _get_optim(self, param: Union[Iterable[Tensor], Iterable[Dict[Any, Any]]]):
        return self.pt_optim_constructor([param], **self._get_optim_args(param))

    def _get_optim_args(self, param: Union[Iterable[Tensor], Iterable[Dict]]):
        if callable(self.pt_optim_args):
            param_name = pyro.get_param_store().param_name(param)
            if self.pt_optim_args_argc == 1:
                normal_name = normalize_param_name(param_name)
                opt_dict = self.pt_optim_args(normal_name)
            else:
                module_name = module_from_param_with_module_name(param_name)
                stripped_param_name = user_param_name(param_name)
                opt_dict = self.pt_optim_args(module_name, stripped_param_name)
            assert isinstance(opt_dict, dict), 'per-param optim arg must return defaults dictionary'
            return opt_dict
        else:
            return self.pt_optim_args

    def _get_grad_clip(self, param: str):
        grad_clip_args = self._get_grad_clip_args(param)
        if not grad_clip_args:
            return None

        def _clip_grad(params: Union[Tensor, Iterable[Tensor]]):
            self._clip_grad(params, **grad_clip_args)
        return _clip_grad

    def _get_grad_clip_args(self, param: str) ->Dict:
        if callable(self.pt_clip_args):
            param_name = pyro.get_param_store().param_name(param)
            module_name = module_from_param_with_module_name(param_name)
            stripped_param_name = user_param_name(param_name)
            clip_dict = self.pt_clip_args(module_name, stripped_param_name)
            assert isinstance(clip_dict, dict), 'per-param clip arg must return defaults dictionary'
            return clip_dict
        else:
            return self.pt_clip_args

    @staticmethod
    def _clip_grad(params: Union[Tensor, Iterable[Tensor]], clip_norm: Optional[Union[int, float]]=None, clip_value: Optional[Union[int, float]]=None) ->None:
        if clip_norm is not None:
            clip_grad_norm_(params, clip_norm)
        if clip_value is not None:
            clip_grad_value_(params, clip_value)


def ClippedAdam(optim_args: Dict) ->PyroOptim:
    """
    Wraps :class:`pyro.optim.clipped_adam.ClippedAdam` with :class:`~pyro.optim.optim.PyroOptim`.
    """
    return PyroOptim(pt_ClippedAdam, optim_args)


class Guide(nn.Module):

    def __init__(self):
        super().__init__()
        self.linear = torch.nn.Linear(1, 1, bias=False)
        self.std = torch.nn.Parameter(torch.tensor(1.0))

    def forward(self, observations={'y1': 0, 'y2': 0}):
        pyro.module('guide', self)
        summed_obs = observations['y1'] + observations['y2']
        mean = self.linear(summed_obs.view(1, 1))[0, 0]
        pyro.sample('x', dist.Normal(mean, self.std))


class Model(nn.Module):

    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(2, 1)

    def forward(self, x):
        return self.fc(x)


logger = logging.getLogger(__name__)


def torch_isnan(x):
    """
    A convenient function to check if a Tensor contains any nan; also works with numbers
    """
    if isinstance(x, numbers.Number):
        return x != x
    return torch.isnan(x).any()


class CEVAE(nn.Module):
    """
    Main class implementing a Causal Effect VAE [1]. This assumes a graphical model

    .. graphviz:: :graphviz_dot: neato

        digraph {
            Z [pos="1,2!",style=filled];
            X [pos="2,1!"];
            y [pos="1,0!"];
            t [pos="0,1!"];
            Z -> X;
            Z -> t;
            Z -> y;
            t -> y;
        }

    where `t` is a binary treatment variable, `y` is an outcome, `Z` is
    an unobserved confounder, and `X` is a noisy function of the hidden
    confounder `Z`.

    Example::

        cevae = CEVAE(feature_dim=5)
        cevae.fit(x_train, t_train, y_train)
        ite = cevae.ite(x_test)  # individual treatment effect
        ate = ite.mean()         # average treatment effect

    :ivar Model ~CEVAE.model: Generative model.
    :ivar Guide ~CEVAE.guide: Inference model.
    :param int feature_dim: Dimension of the feature space `x`.
    :param str outcome_dist: One of: "bernoulli" (default), "exponential", "laplace",
        "normal", "studentt".
    :param int latent_dim: Dimension of the latent variable `z`.
        Defaults to 20.
    :param int hidden_dim: Dimension of hidden layers of fully connected
        networks. Defaults to 200.
    :param int num_layers: Number of hidden layers in fully connected networks.
    :param int num_samples: Default number of samples for the :meth:`ite`
        method. Defaults to 100.
    """

    def __init__(self, feature_dim, outcome_dist='bernoulli', latent_dim=20, hidden_dim=200, num_layers=3, num_samples=100):
        config = dict(feature_dim=feature_dim, latent_dim=latent_dim, hidden_dim=hidden_dim, num_layers=num_layers, num_samples=num_samples)
        for name, size in config.items():
            if not (isinstance(size, int) and size > 0):
                raise ValueError('Expected {} > 0 but got {}'.format(name, size))
        config['outcome_dist'] = outcome_dist
        self.feature_dim = feature_dim
        self.num_samples = num_samples
        super().__init__()
        self.model = Model(config)
        self.guide = Guide(config)

    def fit(self, x, t, y, num_epochs=100, batch_size=100, learning_rate=0.001, learning_rate_decay=0.1, weight_decay=0.0001, log_every=100):
        """
        Train using :class:`~pyro.infer.svi.SVI` with the
        :class:`TraceCausalEffect_ELBO` loss.

        :param ~torch.Tensor x:
        :param ~torch.Tensor t:
        :param ~torch.Tensor y:
        :param int num_epochs: Number of training epochs. Defaults to 100.
        :param int batch_size: Batch size. Defaults to 100.
        :param float learning_rate: Learning rate. Defaults to 1e-3.
        :param float learning_rate_decay: Learning rate decay over all epochs;
            the per-step decay rate will depend on batch size and number of epochs
            such that the initial learning rate will be ``learning_rate`` and the final
            learning rate will be ``learning_rate * learning_rate_decay``.
            Defaults to 0.1.
        :param float weight_decay: Weight decay. Defaults to 1e-4.
        :param int log_every: Log loss each this-many steps. If zero,
            do not log loss. Defaults to 100.
        :return: list of epoch losses
        """
        assert x.dim() == 2 and x.size(-1) == self.feature_dim
        assert t.shape == x.shape[:1]
        assert y.shape == y.shape[:1]
        self.whiten = PreWhitener(x)
        dataset = TensorDataset(x, t, y)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        logger.info('Training with {} minibatches per epoch'.format(len(dataloader)))
        num_steps = num_epochs * len(dataloader)
        optim = ClippedAdam({'lr': learning_rate, 'weight_decay': weight_decay, 'lrd': learning_rate_decay ** (1 / num_steps)})
        svi = SVI(self.model, self.guide, optim, TraceCausalEffect_ELBO())
        losses = []
        for epoch in range(num_epochs):
            for x, t, y in dataloader:
                x = self.whiten(x)
                loss = svi.step(x, t, y, size=len(dataset)) / len(dataset)
                if log_every and len(losses) % log_every == 0:
                    logger.debug('step {: >5d} loss = {:0.6g}'.format(len(losses), loss))
                assert not torch_isnan(loss)
                losses.append(loss)
        return losses

    @torch.no_grad()
    def ite(self, x, num_samples=None, batch_size=None):
        """
        Computes Individual Treatment Effect for a batch of data ``x``.

        .. math::

            ITE(x) = \\mathbb E\\bigl[ \\mathbf y \\mid \\mathbf X=x, do(\\mathbf t=1) \\bigr]
                   - \\mathbb E\\bigl[ \\mathbf y \\mid \\mathbf X=x, do(\\mathbf t=0) \\bigr]

        This has complexity ``O(len(x) * num_samples ** 2)``.

        :param ~torch.Tensor x: A batch of data.
        :param int num_samples: The number of monte carlo samples.
            Defaults to ``self.num_samples`` which defaults to ``100``.
        :param int batch_size: Batch size. Defaults to ``len(x)``.
        :return: A ``len(x)``-sized tensor of estimated effects.
        :rtype: ~torch.Tensor
        """
        if num_samples is None:
            num_samples = self.num_samples
        if not torch._C._get_tracing_state():
            assert x.dim() == 2 and x.size(-1) == self.feature_dim
        dataloader = [x] if batch_size is None else DataLoader(x, batch_size=batch_size)
        logger.info('Evaluating {} minibatches'.format(len(dataloader)))
        result = []
        for x in dataloader:
            x = self.whiten(x)
            with pyro.plate('num_particles', num_samples, dim=-2):
                with poutine.trace() as tr, poutine.block(hide=['y', 't']):
                    self.guide(x)
                with poutine.do(data=dict(t=torch.zeros(()))):
                    y0 = poutine.replay(self.model.y_mean, tr.trace)(x)
                with poutine.do(data=dict(t=torch.ones(()))):
                    y1 = poutine.replay(self.model.y_mean, tr.trace)(x)
            ite = (y1 - y0).mean(0)
            if not torch._C._get_tracing_state():
                logger.debug('batch ate = {:0.6g}'.format(ite.mean()))
            result.append(ite)
        return torch.cat(result)

    def to_script_module(self):
        """
        Compile this module using :func:`torch.jit.trace_module` ,
        assuming self has already been fit to data.

        :return: A traced version of self with an :meth:`ite` method.
        :rtype: torch.jit.ScriptModule
        """
        self.train(False)
        fake_x = torch.randn(2, self.feature_dim)
        with pyro.validation_enabled(False):
            result = torch.jit.trace_module(self, {'ite': (fake_x,)}, check_trace=False)
        return result


class PyroParam(namedtuple('PyroParam', ('init_value', 'constraint', 'event_dim'))):
    """
    Declares a Pyro-managed learnable attribute of a :class:`PyroModule`,
    similar to :func:`pyro.param <pyro.primitives.param>`.

    This can be used either to set attributes of :class:`PyroModule`
    instances::

        assert isinstance(my_module, PyroModule)
        my_module.x = PyroParam(torch.zeros(4))                   # eager
        my_module.y = PyroParam(lambda: torch.randn(4))           # lazy
        my_module.z = PyroParam(torch.ones(4),                    # eager
                                constraint=constraints.positive,
                                event_dim=1)

    or EXPERIMENTALLY as a decorator on lazy initialization properties::

        class MyModule(PyroModule):
            @PyroParam
            def x(self):
                return torch.zeros(4)

            @PyroParam
            def y(self):
                return torch.randn(4)

            @PyroParam(constraint=constraints.real, event_dim=1)
            def z(self):
                return torch.ones(4)

            def forward(self):
                return self.x + self.y + self.z  # accessed like a @property

    :param init_value: Either a tensor for eager initialization, a callable for
        lazy initialization, or None for use as a decorator.
    :type init_value: torch.Tensor or callable returning a torch.Tensor or None
    :param constraint: torch constraint, defaults to ``constraints.real``.
    :type constraint: ~torch.distributions.constraints.Constraint
    :param int event_dim: (optional) number of rightmost dimensions unrelated
        to baching. Dimension to the left of this will be considered batch
        dimensions; if the param statement is inside a subsampled plate, then
        corresponding batch dimensions of the parameter will be correspondingly
        subsampled. If unspecified, all dimensions will be considered event
        dims and no subsampling will be performed.
    """

    def __get__(self, obj, obj_type):
        assert issubclass(obj_type, PyroModule)
        if obj is None:
            return self
        name = self.init_value.__name__
        if name not in obj.__dict__['_pyro_params']:
            init_value, constraint, event_dim = self
            init_value = functools.partial(init_value, obj)
            setattr(obj, name, PyroParam(init_value, constraint, event_dim))
        return obj.__getattr__(name)

    def __call__(self, init_value):
        assert self.init_value is None
        return PyroParam(init_value, self.constraint, self.event_dim)


class PyroSample(namedtuple('PyroSample', ('prior',))):
    """
    Declares a Pyro-managed random attribute of a :class:`PyroModule`, similar
    to :func:`pyro.sample <pyro.primitives.sample>`.

    This can be used either to set attributes of :class:`PyroModule`
    instances::

        assert isinstance(my_module, PyroModule)
        my_module.x = PyroSample(Normal(0, 1))                    # independent
        my_module.y = PyroSample(lambda self: Normal(self.x, 1))  # dependent

    or EXPERIMENTALLY as a decorator on lazy initialization methods::

        class MyModule(PyroModule):
            @PyroSample
            def x(self):
                return Normal(0, 1)       # independent

            @PyroSample
            def y(self):
                return Normal(self.x, 1)  # dependent

            def forward(self):
                return self.y             # accessed like a @property

    :param prior: distribution object or function that inputs the
        :class:`PyroModule` instance ``self`` and returns a distribution
        object.
    """

    def __init__(self, prior):
        super().__init__()
        if not hasattr(prior, 'sample'):
            assert 1 == sum(1 for p in inspect.signature(prior).parameters.values() if p.default is inspect.Parameter.empty), "prior should take the single argument 'self'"
            self.name = getattr(prior, '__name__', None)
            if self.name is not None:
                prior.__name__ = '_pyro_prior_' + prior.__name__
                qualname = prior.__qualname__.rsplit('.', 1)
                qualname[-1] = prior.__name__
                prior.__qualname__ = '.'.join(qualname)

    def __get__(self, obj, obj_type):
        assert issubclass(obj_type, PyroModule)
        if obj is None:
            return self
        if self.name is None:
            for name in dir(obj_type):
                if getattr(obj_type, name) is self:
                    self.name = name
                    break
        else:
            setattr(obj_type, self.prior.__name__, self.prior)
        obj.__dict__['_pyro_samples'].setdefault(self.name, self.prior)
        return obj.__getattr__(self.name)


def _is_module_local_param_enabled() ->bool:
    return pyro.settings.get('module_local_params')


class _Context:
    """
    Sometimes-active cache for ``PyroModule.__call__()`` contexts.
    """

    def __init__(self):
        self.active = 0
        self.cache = {}
        self.used = False
        if _is_module_local_param_enabled():
            self.param_state = {'params': {}, 'constraints': {}}

    def __enter__(self):
        if not self.active and _is_module_local_param_enabled():
            self._param_ctx = pyro.get_param_store().scope(state=self.param_state)
            self.param_state = self._param_ctx.__enter__()
        self.active += 1
        self.used = True

    def __exit__(self, type, value, traceback):
        self.active -= 1
        if not self.active:
            self.cache.clear()
            if _is_module_local_param_enabled():
                self._param_ctx.__exit__(type, value, traceback)
                del self._param_ctx

    def get(self, name):
        if self.active:
            return self.cache.get(name)

    def set(self, name, value):
        if self.active:
            self.cache[name] = value


class _PyroModuleMeta(type):
    _pyro_mixin_cache = {}


    class _New:

        def __init__(self, Module):
            self.__class__ = PyroModule[Module]

    def __getitem__(cls, Module):
        assert isinstance(Module, type)
        assert issubclass(Module, torch.nn.Module)
        if issubclass(Module, PyroModule):
            return Module
        if Module is torch.nn.Module:
            return PyroModule
        if Module in _PyroModuleMeta._pyro_mixin_cache:
            return _PyroModuleMeta._pyro_mixin_cache[Module]
        bases = [PyroModule[b] for b in Module.__bases__ if issubclass(b, torch.nn.Module)]


        class result(Module, *bases):

            def __reduce__(self):
                state = getattr(self, '__getstate__', self.__dict__.copy)()
                return _PyroModuleMeta._New, (Module,), state
        result.__name__ = 'Pyro' + Module.__name__
        _PyroModuleMeta._pyro_mixin_cache[Module] = result
        return result


def _get_pyro_params(module):
    for name in module._parameters:
        if name.endswith('_unconstrained'):
            constrained_name = name[:-len('_unconstrained')]
            if isinstance(module, PyroModule) and constrained_name in module._pyro_params:
                yield constrained_name, getattr(module, constrained_name)
                continue
        yield name, module._parameters[name]


def _make_name(prefix, name):
    return '{}.{}'.format(prefix, name) if prefix else name


def _unconstrain(constrained_value, constraint):
    with torch.no_grad():
        if callable(constrained_value):
            constrained_value = constrained_value()
        unconstrained_value = transform_to(constraint).inv(constrained_value.detach())
        return torch.nn.Parameter(unconstrained_value)


@singledispatch
def extract_provenance(x) ->Tuple[object, frozenset]:
    """
    Extracts the provenance of a data structure possibly containing
    :class:`torch.Tensor` s as leaves, and separates into a detached object and
    provenance.

    :param x: An input data structure.
    :returns: a tuple ``(detached_value, provenance)``
    :rtype: tuple
    """
    return x, frozenset()


def detach_provenance(x):
    """
    Blocks provenance tracking through a tensor, similar to :meth:`torch.Tensor.detach`.

    :param torch.Tensor tensor: An input tensor.
    :returns: A tensor sharing the same data but with no provenance.
    :rtype: torch.Tensor
    """
    value, _ = extract_provenance(x)
    return value


class PyroModule(torch.nn.Module, metaclass=_PyroModuleMeta):
    """
    Subclass of :class:`torch.nn.Module` whose attributes can be modified by
    Pyro effects. Attributes can be set using helpers :class:`PyroParam` and
    :class:`PyroSample` , and methods can be decorated by :func:`pyro_method` .

    **Parameters**

    To create a Pyro-managed parameter attribute, set that attribute using
    either :class:`torch.nn.Parameter` (for unconstrained parameters) or
    :class:`PyroParam` (for constrained parameters). Reading that attribute
    will then trigger a :func:`pyro.param <pyro.primitives.param>` statement.
    For example::

        # Create Pyro-managed parameter attributes.
        my_module = PyroModule()
        my_module.loc = nn.Parameter(torch.tensor(0.))
        my_module.scale = PyroParam(torch.tensor(1.),
                                    constraint=constraints.positive)
        # Read the attributes.
        loc = my_module.loc  # Triggers a pyro.param statement.
        scale = my_module.scale  # Triggers another pyro.param statement.

    Note that, unlike normal :class:`torch.nn.Module` s, :class:`PyroModule` s
    should not be registered with :func:`pyro.module <pyro.primitives.module>`
    statements.  :class:`PyroModule` s can contain other :class:`PyroModule` s
    and normal :class:`torch.nn.Module` s.  Accessing a normal
    :class:`torch.nn.Module` attribute of a :class:`PyroModule` triggers a
    :func:`pyro.module <pyro.primitives.module>` statement.  If multiple
    :class:`PyroModule` s appear in a single Pyro model or guide, they should
    be included in a single root :class:`PyroModule` for that model.

    :class:`PyroModule` s synchronize data with the param store at each
    ``setattr``, ``getattr``, and ``delattr`` event, based on the nested name
    of an attribute:

    -   Setting ``mod.x = x_init`` tries to read ``x`` from the param store. If a
        value is found in the param store, that value is copied into ``mod``
        and ``x_init`` is ignored; otherwise ``x_init`` is copied into both
        ``mod`` and the param store.
    -   Reading ``mod.x`` tries to read ``x`` from the param store. If a
        value is found in the param store, that value is copied into ``mod``;
        otherwise ``mod``'s value is copied into the param store. Finally
        ``mod`` and the param store agree on a single value to return.
    -   Deleting ``del mod.x`` removes a value from both ``mod`` and the param
        store.

    Note two :class:`PyroModule` of the same name will both synchronize with
    the global param store and thus contain the same data.  When creating a
    :class:`PyroModule`, then deleting it, then creating another with the same
    name, the latter will be populated with the former's data from the param
    store. To avoid this persistence, either ``pyro.clear_param_store()`` or
    call :func:`clear` before deleting a :class:`PyroModule` .

    :class:`PyroModule` s can be saved and loaded either directly using
    :func:`torch.save` / :func:`torch.load` or indirectly using the param
    store's :meth:`~pyro.params.param_store.ParamStoreDict.save` /
    :meth:`~pyro.params.param_store.ParamStoreDict.load` . Note that
    :func:`torch.load` will be overridden by any values in the param store, so
    it is safest to ``pyro.clear_param_store()`` before loading.

    **Samples**

    To create a Pyro-managed random attribute, set that attribute using the
    :class:`PyroSample` helper, specifying a prior distribution. Reading that
    attribute will then trigger a :func:`pyro.sample <pyro.primitives.sample>`
    statement. For example::

        # Create Pyro-managed random attributes.
        my_module.x = PyroSample(dist.Normal(0, 1))
        my_module.y = PyroSample(lambda self: dist.Normal(self.loc, self.scale))

        # Sample the attributes.
        x = my_module.x  # Triggers a pyro.sample statement.
        y = my_module.y  # Triggers one pyro.sample + two pyro.param statements.

    Sampling is cached within each invocation of ``.__call__()`` or method
    decorated by :func:`pyro_method` .  Because sample statements can appear
    only once in a Pyro trace, you should ensure that traced access to sample
    attributes is wrapped in a single invocation of ``.__call__()`` or method
    decorated by :func:`pyro_method` .

    To make an existing module probabilistic, you can create a subclass and
    overwrite some parameters with :class:`PyroSample` s::

        class RandomLinear(nn.Linear, PyroModule):  # used as a mixin
            def __init__(self, in_features, out_features):
                super().__init__(in_features, out_features)
                self.weight = PyroSample(
                    lambda self: dist.Normal(0, 1)
                                     .expand([self.out_features,
                                              self.in_features])
                                     .to_event(2))

    **Mixin classes**

    :class:`PyroModule` can be used as a mixin class, and supports simple
    syntax for dynamically creating mixins, for example the following are
    equivalent::

        # Version 1. create a named mixin class
        class PyroLinear(nn.Linear, PyroModule):
            pass

        m.linear = PyroLinear(m, n)

        # Version 2. create a dynamic mixin class
        m.linear = PyroModule[nn.Linear](m, n)

    This notation can be used recursively to create Bayesian modules, e.g.::

        model = PyroModule[nn.Sequential](
            PyroModule[nn.Linear](28 * 28, 100),
            PyroModule[nn.Sigmoid](),
            PyroModule[nn.Linear](100, 100),
            PyroModule[nn.Sigmoid](),
            PyroModule[nn.Linear](100, 10),
        )
        assert isinstance(model, nn.Sequential)
        assert isinstance(model, PyroModule)

        # Now we can be Bayesian about weights in the first layer.
        model[0].weight = PyroSample(
            prior=dist.Normal(0, 1).expand([28 * 28, 100]).to_event(2))
        guide = AutoDiagonalNormal(model)

    Note that ``PyroModule[...]`` does not recursively mix in
    :class:`PyroModule` to submodules of the input ``Module``; hence we needed
    to wrap each submodule of the ``nn.Sequential`` above.

    :param str name: Optional name for a root PyroModule. This is ignored in
        sub-PyroModules of another PyroModule.
    """

    def __init__(self, name=''):
        self._pyro_name = name
        self._pyro_context = _Context()
        self._pyro_params = OrderedDict()
        self._pyro_samples = OrderedDict()
        super().__init__()

    def add_module(self, name, module):
        """
        Adds a child module to the current module.
        """
        if isinstance(module, PyroModule):
            module._pyro_set_supermodule(_make_name(self._pyro_name, name), self._pyro_context)
        super().add_module(name, module)

    def named_pyro_params(self, prefix='', recurse=True):
        """
        Returns an iterator over PyroModule parameters, yielding both the
        name of the parameter as well as the parameter itself.

        :param str prefix: prefix to prepend to all parameter names.
        :param bool recurse: if True, then yields parameters of this module
            and all submodules. Otherwise, yields only parameters that
            are direct members of this module.
        :returns: a generator which yields tuples containing the name and parameter
        """
        gen = self._named_members(_get_pyro_params, prefix=prefix, recurse=recurse)
        for elem in gen:
            yield elem

    def _pyro_set_supermodule(self, name, context):
        if _is_module_local_param_enabled() and pyro.settings.get('validate_poutine'):
            self._check_module_local_param_usage()
        self._pyro_name = name
        self._pyro_context = context
        for key, value in self._modules.items():
            if isinstance(value, PyroModule):
                assert not value._pyro_context.used, 'submodule {} has executed outside of supermodule'.format(name)
                value._pyro_set_supermodule(_make_name(name, key), context)

    def _pyro_get_fullname(self, name):
        assert self.__dict__['_pyro_context'].used, 'fullname is not yet defined'
        return _make_name(self.__dict__['_pyro_name'], name)

    def __call__(self, *args, **kwargs):
        with self._pyro_context:
            result = super().__call__(*args, **kwargs)
        if pyro.settings.get('validate_poutine') and not self._pyro_context.active and _is_module_local_param_enabled():
            self._check_module_local_param_usage()
        return result

    def _check_module_local_param_usage(self) ->None:
        self_nn_params = set(id(p) for p in self.parameters())
        self_pyro_params = set(id(p if not hasattr(p, 'unconstrained') else p.unconstrained()) for p in self._pyro_context.param_state['params'].values())
        if not self_pyro_params <= self_nn_params:
            raise NotImplementedError('Support for global pyro.param statements in PyroModules with local param mode enabled is not yet implemented.')

    def __getattr__(self, name):
        if '_pyro_params' in self.__dict__:
            _pyro_params = self.__dict__['_pyro_params']
            if name in _pyro_params:
                constraint, event_dim = _pyro_params[name]
                unconstrained_value = getattr(self, name + '_unconstrained')
                if self._pyro_context.active:
                    fullname = self._pyro_get_fullname(name)
                    if fullname in _PYRO_PARAM_STORE:
                        if _PYRO_PARAM_STORE._params[fullname] is not unconstrained_value:
                            unconstrained_value = _PYRO_PARAM_STORE._params[fullname]
                            if not isinstance(unconstrained_value, torch.nn.Parameter):
                                unconstrained_value = torch.nn.Parameter(unconstrained_value)
                                _PYRO_PARAM_STORE._params[fullname] = unconstrained_value
                                _PYRO_PARAM_STORE._param_to_name[unconstrained_value] = fullname
                            super().__setattr__(name + '_unconstrained', unconstrained_value)
                    else:
                        _PYRO_PARAM_STORE._constraints[fullname] = constraint
                        _PYRO_PARAM_STORE._params[fullname] = unconstrained_value
                        _PYRO_PARAM_STORE._param_to_name[unconstrained_value] = fullname
                    return pyro.param(fullname, event_dim=event_dim)
                else:
                    return transform_to(constraint)(unconstrained_value)
        if '_pyro_samples' in self.__dict__:
            _pyro_samples = self.__dict__['_pyro_samples']
            if name in _pyro_samples:
                prior = _pyro_samples[name]
                context = self._pyro_context
                if context.active:
                    fullname = self._pyro_get_fullname(name)
                    value = context.get(fullname)
                    if value is None:
                        if not hasattr(prior, 'sample'):
                            prior = prior(self)
                        value = pyro.sample(fullname, prior)
                        context.set(fullname, value)
                    return value
                else:
                    if not hasattr(prior, 'sample'):
                        prior = prior(self)
                    return prior()
        result = super().__getattr__(name)
        if isinstance(result, torch.nn.Parameter) and not name.endswith('_unconstrained'):
            if self._pyro_context.active:
                pyro.param(self._pyro_get_fullname(name), result)
        if isinstance(result, torch.nn.Module):
            if isinstance(result, PyroModule):
                if not result._pyro_name:
                    result._pyro_set_supermodule(_make_name(self._pyro_name, name), self._pyro_context)
            elif self._pyro_context.active:
                pyro.module(self._pyro_get_fullname(name), result)
        return result

    def __setattr__(self, name, value):
        if isinstance(value, PyroModule):
            try:
                delattr(self, name)
            except AttributeError:
                pass
            self.add_module(name, value)
            return
        if isinstance(value, PyroParam):
            try:
                delattr(self, name)
            except AttributeError:
                pass
            constrained_value, constraint, event_dim = value
            self._pyro_params[name] = constraint, event_dim
            if self._pyro_context.active:
                fullname = self._pyro_get_fullname(name)
                pyro.param(fullname, constrained_value, constraint=constraint, event_dim=event_dim)
                constrained_value = detach_provenance(pyro.param(fullname))
                unconstrained_value = constrained_value.unconstrained()
                if not isinstance(unconstrained_value, torch.nn.Parameter):
                    unconstrained_value = torch.nn.Parameter(unconstrained_value)
                    _PYRO_PARAM_STORE._params[fullname] = unconstrained_value
                    _PYRO_PARAM_STORE._param_to_name[unconstrained_value] = fullname
            else:
                unconstrained_value = _unconstrain(constrained_value, constraint)
            super().__setattr__(name + '_unconstrained', unconstrained_value)
            return
        if isinstance(value, torch.nn.Parameter):
            try:
                delattr(self, name)
            except AttributeError:
                pass
            if self._pyro_context.active:
                fullname = self._pyro_get_fullname(name)
                value = pyro.param(fullname, value)
                if not isinstance(value, torch.nn.Parameter):
                    value = torch.nn.Parameter(detach_provenance(value))
                    _PYRO_PARAM_STORE._params[fullname] = value
                    _PYRO_PARAM_STORE._param_to_name[value] = fullname
            super().__setattr__(name, value)
            return
        if isinstance(value, torch.Tensor):
            if name in self._pyro_params:
                constraint, event_dim = self._pyro_params[name]
                unconstrained_value = getattr(self, name + '_unconstrained')
                with torch.no_grad():
                    unconstrained_value.data = transform_to(constraint).inv(value.detach())
                return
        if isinstance(value, PyroSample):
            try:
                delattr(self, name)
            except AttributeError:
                pass
            _pyro_samples = self.__dict__['_pyro_samples']
            _pyro_samples[name] = value.prior
            return
        super().__setattr__(name, value)

    def __delattr__(self, name):
        if name in self._parameters:
            del self._parameters[name]
            if self._pyro_context.used:
                fullname = self._pyro_get_fullname(name)
                if fullname in _PYRO_PARAM_STORE:
                    del _PYRO_PARAM_STORE[fullname]
            return
        if name in self._pyro_params:
            delattr(self, name + '_unconstrained')
            del self._pyro_params[name]
            if self._pyro_context.used:
                fullname = self._pyro_get_fullname(name)
                if fullname in _PYRO_PARAM_STORE:
                    del _PYRO_PARAM_STORE[fullname]
            return
        if name in self._pyro_samples:
            del self._pyro_samples[name]
            return
        if name in self._modules:
            del self._modules[name]
            if self._pyro_context.used:
                fullname = self._pyro_get_fullname(name)
                for p in list(_PYRO_PARAM_STORE.keys()):
                    if p.startswith(fullname):
                        del _PYRO_PARAM_STORE[p]
            return
        super().__delattr__(name)


def prototype_hide_fn(msg):
    return msg['type'] != 'sample' or msg['is_observed'] or site_is_subsample(msg)


class AutoGuide(PyroModule):
    """
    Base class for automatic guides.

    Derived classes must implement the :meth:`forward` method, with the
    same ``*args, **kwargs`` as the base ``model``.

    Auto guides can be used individually or combined in an
    :class:`AutoGuideList` object.

    :param callable model: A pyro model.
    :param callable create_plates: An optional function inputing the same
        ``*args,**kwargs`` as ``model()`` and returning a :class:`pyro.plate`
        or iterable of plates. Plates not returned will be created
        automatically as usual. This is useful for data subsampling.
    """

    def __init__(self, model, *, create_plates=None):
        super().__init__(name=type(self).__name__)
        self.master = None
        self._model = model,
        self.create_plates = create_plates
        self.prototype_trace = None
        self._prototype_frames = {}

    @property
    def model(self):
        return self._model[0]

    def __getstate__(self):
        self._model = None
        self.master = None
        return getattr(super(), '__getstate__', self.__dict__.copy)()

    def __setstate__(self, state):
        getattr(super(), '__setstate__', self.__dict__.update)(state)
        assert self.master is None
        master_ref = weakref.ref(self)
        for _, mod in self.named_modules():
            if mod is not self and isinstance(mod, AutoGuide):
                mod._update_master(master_ref)

    def _update_master(self, master_ref):
        self.master = master_ref
        for _, mod in self.named_modules():
            if mod is not self and isinstance(mod, AutoGuide):
                mod._update_master(master_ref)

    def call(self, *args, **kwargs):
        """
        Method that calls :meth:`forward` and returns parameter values of the
        guide as a `tuple` instead of a `dict`, which is a requirement for
        JIT tracing. Unlike :meth:`forward`, this method can be traced by
        :func:`torch.jit.trace_module`.

        .. warning::
            This method may be removed once PyTorch JIT tracer starts accepting
            `dict` as valid return types. See
            `issue <https://github.com/pytorch/pytorch/issues/27743>_`.
        """
        result = self(*args, **kwargs)
        return tuple(v for _, v in sorted(result.items()))

    def sample_latent(*args, **kwargs):
        """
        Samples an encoded latent given the same ``*args, **kwargs`` as the
        base ``model``.
        """
        pass

    def __setattr__(self, name, value):
        if isinstance(value, AutoGuide):
            master_ref = weakref.ref(self) if self.master is None else self.master
            value._update_master(master_ref)
        super().__setattr__(name, value)

    def _create_plates(self, *args, **kwargs):
        if self.master is None:
            if self.create_plates is None:
                self.plates = {}
            else:
                plates = self.create_plates(*args, **kwargs)
                if isinstance(plates, pyro.plate):
                    plates = [plates]
                assert all(isinstance(p, pyro.plate) for p in plates), 'create_plates() returned a non-plate'
                self.plates = {p.name: p for p in plates}
            for name, frame in sorted(self._prototype_frames.items()):
                if name not in self.plates:
                    full_size = getattr(frame, 'full_size', frame.size)
                    self.plates[name] = pyro.plate(name, full_size, dim=frame.dim, subsample_size=frame.size)
        else:
            assert self.create_plates is None, 'Cannot pass create_plates() to non-master guide'
            self.plates = self.master().plates
        return self.plates
    _prototype_hide_fn = staticmethod(prototype_hide_fn)

    def _setup_prototype(self, *args, **kwargs):
        model = poutine.block(self.model, self._prototype_hide_fn)
        self.prototype_trace = poutine.block(poutine.trace(model).get_trace)(*args, **kwargs)
        if self.master is not None:
            self.master()._check_prototype(self.prototype_trace)
        self._prototype_frames = {}
        for name, site in self.prototype_trace.iter_stochastic_nodes():
            for frame in site['cond_indep_stack']:
                if frame.vectorized:
                    self._prototype_frames[frame.name] = frame
                else:
                    raise NotImplementedError('AutoGuide does not support sequential pyro.plate')

    def median(self, *args, **kwargs):
        """
        Returns the posterior median value of each latent variable.

        :return: A dict mapping sample site name to median tensor.
        :rtype: dict
        """
        raise NotImplementedError


class InitMessenger(Messenger):
    """
    Initializes a site by replacing ``.sample()`` calls with values
    drawn from an initialization strategy. This is mainly for internal use by
    autoguide classes.

    :param callable init_fn: An initialization function.
    """

    def __init__(self, init_fn):
        self.init_fn = init_fn
        super().__init__()

    def _pyro_sample(self, msg):
        if msg['value'] is not None or type(msg['fn']).__name__ == '_Subsample':
            return
        with torch.no_grad(), helpful_support_errors(msg):
            value = self.init_fn(msg)
        if is_validation_enabled() and msg['value'] is not None:
            if not isinstance(value, type(msg['value'])):
                raise ValueError('{} provided invalid type for site {}:\nexpected {}\nactual {}'.format(self.init_fn, msg['name'], type(msg['value']), type(value)))
            if value.shape != msg['value'].shape:
                raise ValueError('{} provided invalid shape for site {}:\nexpected {}\nactual {}'.format(self.init_fn, msg['name'], msg['value'].shape, value.shape))
        msg['value'] = value

    def _pyro_get_init_messengers(self, msg):
        if msg['value'] is None:
            msg['value'] = []
        msg['value'].append(self)


def deep_getattr(obj, name):
    """
    Python getattr() for arbitrarily deep attributes
    Throws an AttributeError if bad attribute
    """
    return functools.reduce(getattr, name.split('.'), obj)


def deep_setattr(obj, key, val):
    """
    Set an attribute `key` on the object. If any of the prefix attributes do
    not exist, they are set to :class:`~pyro.nn.PyroModule`.
    """

    def _getattr(obj, attr):
        obj_next = getattr(obj, attr, None)
        if obj_next is not None:
            return obj_next
        setattr(obj, attr, PyroModule())
        return getattr(obj, attr)
    lpart, _, rpart = key.rpartition('.')
    if lpart:
        obj = functools.reduce(_getattr, [obj] + lpart.split('.'))
    setattr(obj, rpart, val)


def init_to_feasible(site=None):
    """
    Initialize to an arbitrary feasible point, ignoring distribution
    parameters.
    """
    if site is None:
        return init_to_feasible
    value = site['fn'].sample().detach()
    t = transform_to(site['fn'].support)
    value = t(torch.zeros_like(t.inv(value)))
    value._pyro_custom_init = False
    return value


def periodic_repeat(tensor, size, dim):
    """
    Repeat a ``period``-sized tensor up to given ``size``. For example::

        >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])
        >>> periodic_repeat(x, 4, 0)
        tensor([[1, 2, 3],
                [4, 5, 6],
                [1, 2, 3],
                [4, 5, 6]])
        >>> periodic_repeat(x, 4, 1)
        tensor([[1, 2, 3, 1],
                [4, 5, 6, 4]])

    This is useful for computing static seasonality in time series models.

    :param torch.Tensor tensor: A tensor of differences.
    :param int size: Desired size of the result along dimension ``dim``.
    :param int dim: The tensor dimension along which to repeat.
    """
    assert isinstance(size, int) and size >= 0
    assert isinstance(dim, int)
    if dim >= 0:
        dim -= tensor.dim()
    period = tensor.size(dim)
    repeats = [1] * tensor.dim()
    repeats[dim] = (size + period - 1) // period
    result = tensor.repeat(*repeats)
    result = result[(Ellipsis, slice(None, size)) + (slice(None),) * (-1 - dim)]
    return result


def sum_rightmost(value, dim):
    """
    Sum out ``dim`` many rightmost dimensions of a given tensor.

    If ``dim`` is 0, no dimensions are summed out.
    If ``dim`` is ``float('inf')``, then all dimensions are summed out.
    If ``dim`` is 1, the rightmost 1 dimension is summed out.
    If ``dim`` is 2, the rightmost two dimensions are summed out.
    If ``dim`` is -1, all but the leftmost 1 dimension is summed out.
    If ``dim`` is -2, all but the leftmost 2 dimensions are summed out.
    etc.

    :param torch.Tensor value: A tensor of ``.dim()`` at least ``dim``.
    :param int dim: The number of rightmost dims to sum out.
    """
    if isinstance(value, numbers.Number):
        return value
    if dim < 0:
        dim += value.dim()
    if dim == 0:
        return value
    if dim >= value.dim():
        return value.sum()
    return value.reshape(value.shape[:-dim] + (-1,)).sum(-1)


def DCTAdam(optim_args: Dict) ->PyroOptim:
    """
    Wraps :class:`pyro.optim.dct_adam.DCTAdam` with :class:`~pyro.optim.optim.PyroOptim`.
    """
    return PyroOptim(pt_DCTAdam, optim_args)


class MarkDCTParamMessenger(Messenger):
    """
    EXPERIMENTAL Messenger to mark DCT dimension of parameter, for use with
    :class:`pyro.optim.optim.DCTAdam`.

    :param str name: The name of the plate along which to apply discrete cosine
        transforms on gradients.
    """

    def __init__(self, name):
        super().__init__()
        self.name = name

    def _postprocess_message(self, msg):
        if msg['type'] != 'param':
            return
        event_dim = msg['kwargs'].get('event_dim')
        if event_dim is None:
            return
        for frame in msg['cond_indep_stack']:
            if frame.name == self.name:
                value = msg['value']
                event_dim += value.unconstrained().dim() - value.dim()
                value.unconstrained()._pyro_dct_dim = frame.dim - event_dim
                return


@singledispatch
def prefix_condition(d, data):
    """
    EXPERIMENTAL Given a distribution ``d`` of shape ``batch_shape + (t+f, d)``
    and data ``x`` of shape ``batch_shape + (t, d)``, compute a conditional
    distribution of shape ``batch_shape + (f, d)``. Typically ``t`` is the
    number of training time steps, ``f`` is the number of forecast time steps,
    and ``d`` is the data dimension.

    :param d: a distribution with ``len(d.shape()) >= 2``
    :type d: ~pyro.distributions.Distribution
    :param data: data of dimension at least 2.
    :type data: ~torch.Tensor
    """
    try:
        return d.prefix_condition(data)
    except AttributeError as e:
        raise NotImplementedError('prefix_condition() does not suport {}'.format(type(d))) from e


class PrefixConditionMessenger(Messenger):
    """
    EXPERIMENTAL Given a prefix of t-many observations, condition a (t+f)-long
    distribution on the observations, converting it to an f-long distribution.

    :param dict data: A dict mapping site name to tensors of observations.
    """

    def __init__(self, data):
        super().__init__()
        self.data = data

    def _pyro_sample(self, msg):
        if msg['name'] not in self.data:
            return
        assert msg['value'] is None
        data = self.data[msg['name']]
        msg['fn'] = prefix_condition(msg['fn'], data)


class PrefixReplayMessenger(Messenger):
    """
    EXPERIMENTAL Given a trace of training data, replay a model with batched
    sites extended to include both training and forecast time, using the guide
    trace for the training prefix and samples from the prior on the forecast
    postfix.

    :param trace: a guide trace.
    :type trace: ~pyro.poutine.trace_struct.Trace
    """

    def __init__(self, trace):
        super().__init__()
        self.trace = trace

    def _pyro_post_sample(self, msg):
        if site_is_subsample(msg):
            return
        name = msg['name']
        if name not in self.trace:
            return
        model_value = msg['value']
        guide_value = self.trace.nodes[name]['value']
        if model_value.shape == guide_value.shape:
            msg['value'] = guide_value
            return
        assert model_value.dim() == guide_value.dim()
        for dim in range(model_value.dim()):
            if model_value.size(dim) != guide_value.size(dim):
                break
        assert model_value.size(dim) > guide_value.size(dim)
        assert model_value.shape[dim + 1:] == guide_value.shape[dim + 1:]
        split = guide_value.size(dim)
        index = (slice(None),) * dim + (slice(split, None),)
        msg['value'] = torch.cat([guide_value, model_value[index]], dim=dim)


def get_param_store():
    """
    Returns the global :class:`~pyro.params.param_store.ParamStoreDict`.
    """
    return _PYRO_PARAM_STORE


class PrefixWarmStartMessenger(Messenger):
    """
    EXPERIMENTAL Assuming the global param store has been populated with params
    defined on a short time window, re-initialize by splicing old params with
    new initial params defined on a longer time window.
    """

    def _pyro_param(self, msg):
        store = get_param_store()
        name = msg['name']
        if name not in store:
            return
        if len(msg['args']) >= 2:
            new = msg['args'][1]
        elif 'init_tensor' in msg['kwargs']:
            new = msg['kwargs']['init_tensor']
        else:
            return
        if callable(new):
            new = new()
        old = store[name]
        assert new.dim() == old.dim()
        if new.shape == old.shape:
            return
        t = transform_to(store._constraints[name])
        new = t.inv(new)
        old = t.inv(old)
        for dim in range(new.dim()):
            if new.size(dim) != old.size(dim):
                break
        assert new.size(dim) > old.size(dim)
        assert new.shape[dim + 1:] == old.shape[dim + 1:]
        split = old.size(dim)
        index = (slice(None),) * dim + (slice(split, None),)
        new = torch.cat([old, new[index]], dim=dim)
        store[name] = t(new)


def init_to_sample(site=None):
    """
    Initialize to a random sample from the prior.
    """
    if site is None:
        return init_to_sample
    value = site['fn'].sample().detach()
    value._pyro_custom_init = False
    return value


_ROOT_TWO_INVERSE = 1.0 / math.sqrt(2.0)


def haar_transform(x):
    """
    Discrete Haar transform.

    Performs a Haar transform along the final dimension.
    This is the inverse of :func:`inverse_haar_transform`.

    :param Tensor x: The input signal.
    :rtype: Tensor
    """
    n = x.size(-1) // 2
    even, odd, end = x[..., 0:n + n:2], x[..., 1:n + n:2], x[..., n + n:]
    hi = _ROOT_TWO_INVERSE * (even - odd)
    lo = _ROOT_TWO_INVERSE * (even + odd)
    if n >= 2:
        lo = haar_transform(lo)
    x = torch.cat([lo, hi, end], dim=-1)
    return x


def inverse_haar_transform(x):
    """
    Performs an inverse Haar transform along the final dimension.
    This is the inverse of :func:`haar_transform`.

    :param Tensor x: The input signal.
    :rtype: Tensor
    """
    n = x.size(-1) // 2
    lo, hi, end = x[..., :n], x[..., n:n + n], x[..., n + n:]
    if n >= 2:
        lo = inverse_haar_transform(lo)
    even = _ROOT_TWO_INVERSE * (lo + hi)
    odd = _ROOT_TWO_INVERSE * (lo - hi)
    even_odd = torch.stack([even, odd], dim=-1).reshape(even.shape[:-1] + (-1,))
    x = torch.cat([even_odd, end], dim=-1)
    return x


class HaarTransform(Transform):
    """
    Discrete Haar transform.

    This uses :func:`~pyro.ops.tensor_utils.haar_transform` and
    :func:`~pyro.ops.tensor_utils.inverse_haar_transform` to compute
    (orthonormal) Haar and inverse Haar transforms. The jacobian is 1.
    For sequences with length `T` not a power of two, this implementation
    is equivalent to a block-structured Haar transform in which block
    sizes decrease by factors of one half from left to right.

    :param int dim: Dimension along which to transform. Must be negative.
        This is an absolute dim counting from the right.
    :param bool flip: Whether to flip the time axis before applying the
        Haar transform. Defaults to false.
    """
    bijective = True

    def __init__(self, dim=-1, flip=False, cache_size=0):
        assert isinstance(dim, int) and dim < 0
        self.dim = dim
        self.flip = flip
        super().__init__(cache_size=cache_size)

    def __hash__(self):
        return hash((type(self), self.event_dim, self.flip))

    def __eq__(self, other):
        return type(self) == type(other) and self.dim == other.dim and self.flip == other.flip

    @constraints.dependent_property(is_discrete=False)
    def domain(self):
        return constraints.independent(constraints.real, -self.dim)

    @constraints.dependent_property(is_discrete=False)
    def codomain(self):
        return constraints.independent(constraints.real, -self.dim)

    def _call(self, x):
        dim = self.dim
        if dim != -1:
            x = x.transpose(dim, -1)
        if self.flip:
            x = x.flip(-1)
        y = haar_transform(x)
        if dim != -1:
            y = y.transpose(dim, -1)
        return y

    def _inverse(self, y):
        dim = self.dim
        if dim != -1:
            y = y.transpose(dim, -1)
        x = inverse_haar_transform(y)
        if self.flip:
            x = x.flip(-1)
        if dim != -1:
            x = x.transpose(dim, -1)
        return x

    def log_abs_det_jacobian(self, x, y):
        return x.new_zeros(x.shape[:self.dim])

    def with_cache(self, cache_size=1):
        if self._cache_size == cache_size:
            return self
        return HaarTransform(self.dim, flip=self.flip, cache_size=cache_size)

    def forward_shape(self, shape):
        if len(shape) < self.event_dim:
            raise ValueError('Too few dimensions on input')
        return shape

    def inverse_shape(self, shape):
        if len(shape) < self.event_dim:
            raise ValueError('Too few dimensions on input')
        return shape


class PlateMessenger(Messenger):

    def __init__(self, fn, size, dim):
        assert dim < 0
        self.size = size
        self.dim = dim
        super().__init__(fn)

    def process_message(self, msg):
        if msg['type'] == 'sample':
            batch_shape = msg['fn'].batch_shape
            if len(batch_shape) < -self.dim or batch_shape[self.dim] != self.size:
                batch_shape = [1] * (-self.dim - len(batch_shape)) + list(batch_shape)
                batch_shape[self.dim] = self.size
                msg['fn'] = msg['fn'].expand(torch.Size(batch_shape))

    def __iter__(self):
        return range(self.size)


def time_reparam_dct(msg):
    """
    EXPERIMENTAL Configures ``poutine.reparam()`` to use a ``HaarReparam`` for
    all sites inside the ``time`` plate.
    """
    if msg['is_observed']:
        return
    for frame in msg['cond_indep_stack']:
        if frame.name == 'time':
            dim = frame.dim - msg['fn'].event_dim
            return HaarReparam(dim=dim, experimental_allow_batch=True)


def as_complex(x):
    """
    Similar to :func:`torch.view_as_complex` but copies data in case strides
    are not multiples of two.
    """
    if any(stride % 2 for stride in x.stride()[:-1]):
        x = x.squeeze().reshape(x.shape)
    if any(stride % 2 for stride in x.stride()[:-1]):
        x = x.clone()
    return torch.view_as_complex(x)


def dct(x, dim=-1):
    """
    Discrete cosine transform of type II, scaled to be orthonormal.

    This is the inverse of :func:`idct_ii` , and is equivalent to
    :func:`scipy.fftpack.dct` with ``norm="ortho"``.

    :param Tensor x: The input signal.
    :param int dim: Dimension along which to compute DCT.
    :rtype: Tensor
    """
    if dim >= 0:
        dim -= x.dim()
    if dim != -1:
        y = x.reshape(x.shape[:dim + 1] + (-1,)).transpose(-1, -2)
        return dct(y).transpose(-1, -2).reshape(x.shape)
    N = x.size(-1)
    y = torch.cat([x[..., ::2], x[..., 1::2].flip(-1)], dim=-1)
    Y = rfft(y, n=N)
    coef_real = torch.cos(torch.linspace(0, 0.5 * math.pi, N + 1, dtype=x.dtype, device=x.device))
    M = Y.size(-1)
    coef = torch.stack([coef_real[:M], -coef_real[-M:].flip(-1)], dim=-1)
    X = as_complex(coef) * Y
    X = torch.cat([X.real, -X.imag[..., 1:N - M + 1].flip(-1)], dim=-1)
    scale = torch.cat([x.new_tensor([math.sqrt(N)]), x.new_full((N - 1,), math.sqrt(0.5 * N))])
    return X / scale


def idct(x, dim=-1):
    """
    Inverse discrete cosine transform of type II, scaled to be orthonormal.

    This is the inverse of :func:`dct_ii` , and is equivalent to
    :func:`scipy.fftpack.idct` with ``norm="ortho"``.

    :param Tensor x: The input signal.
    :param int dim: Dimension along which to compute DCT.
    :rtype: Tensor
    """
    if dim >= 0:
        dim -= x.dim()
    if dim != -1:
        y = x.reshape(x.shape[:dim + 1] + (-1,)).transpose(-1, -2)
        return idct(y).transpose(-1, -2).reshape(x.shape)
    N = x.size(-1)
    scale = torch.cat([x.new_tensor([math.sqrt(N)]), x.new_full((N - 1,), math.sqrt(0.5 * N))])
    x = x * scale
    M = N // 2 + 1
    xi = torch.nn.functional.pad(-x[..., N - M + 1:], (0, 1)).flip(-1)
    X = torch.stack([x[..., :M], xi], dim=-1)
    coef_real = torch.cos(torch.linspace(0, 0.5 * math.pi, N + 1, dtype=x.dtype, device=x.device))
    coef = torch.stack([coef_real[:M], coef_real[-M:].flip(-1)], dim=-1)
    Y = as_complex(coef) * as_complex(X)
    y = irfft(Y, n=N)
    return torch.stack([y, y.flip(-1)], axis=-1).reshape(x.shape[:-1] + (-1,))[..., :N]


class DiscreteCosineTransform(Transform):
    """
    Discrete Cosine Transform of type-II.

    This uses :func:`~pyro.ops.tensor_utils.dct` and
    :func:`~pyro.ops.tensor_utils.idct` to compute
    orthonormal DCT and inverse DCT transforms. The jacobian is 1.

    :param int dim: Dimension along which to transform. Must be negative.
        This is an absolute dim counting from the right.
    :param float smooth: Smoothing parameter. When 0, this transforms white
        noise to white noise; when 1 this transforms Brownian noise to to white
        noise; when -1 this transforms violet noise to white noise; etc. Any
        real number is allowed. https://en.wikipedia.org/wiki/Colors_of_noise.
    """
    bijective = True

    def __init__(self, dim=-1, smooth=0.0, cache_size=0):
        assert isinstance(dim, int) and dim < 0
        self.dim = dim
        self.smooth = float(smooth)
        self._weight_cache = None
        super().__init__(cache_size=cache_size)

    def __hash__(self):
        return hash((type(self), self.dim, self.smooth))

    def __eq__(self, other):
        return type(self) == type(other) and self.dim == other.dim and self.smooth == other.smooth

    @constraints.dependent_property(is_discrete=False)
    def domain(self):
        return constraints.independent(constraints.real, -self.dim)

    @constraints.dependent_property(is_discrete=False)
    def codomain(self):
        return constraints.independent(constraints.real, -self.dim)

    @torch.no_grad()
    def _weight(self, y):
        size = y.size(-1)
        if self._weight_cache is None or self._weight_cache.size(-1) != size:
            freq = torch.linspace(0.5, size - 0.5, size, dtype=y.dtype, device=y.device)
            w = freq.pow_(self.smooth)
            w /= w.log().mean().exp()
            self._weight_cache = w
        return self._weight_cache

    def _call(self, x):
        dim = self.dim
        if dim != -1:
            x = x.transpose(dim, -1)
        y = dct(x)
        if self.smooth:
            y = y * self._weight(y)
        if dim != -1:
            y = y.transpose(dim, -1)
        return y

    def _inverse(self, y):
        dim = self.dim
        if dim != -1:
            y = y.transpose(dim, -1)
        if self.smooth:
            y = y / self._weight(y)
        x = idct(y)
        if dim != -1:
            x = x.transpose(dim, -1)
        return x

    def log_abs_det_jacobian(self, x, y):
        return x.new_zeros(x.shape[:self.dim])

    def with_cache(self, cache_size=1):
        if self._cache_size == cache_size:
            return self
        return DiscreteCosineTransform(self.dim, self.smooth, cache_size=cache_size)

    def forward_shape(self, shape):
        if len(shape) < self.event_dim:
            raise ValueError('Too few dimensions on input')
        return shape

    def inverse_shape(self, shape):
        if len(shape) < self.event_dim:
            raise ValueError('Too few dimensions on input')
        return shape


def time_reparam_haar(msg):
    """
    EXPERIMENTAL Configures ``poutine.reparam()`` to use a ``DiscreteCosineReparam`` for
    all sites inside the ``time`` plate.
    """
    if msg['is_observed']:
        return
    for frame in msg['cond_indep_stack']:
        if frame.name == 'time':
            dim = frame.dim - msg['fn'].event_dim
            return DiscreteCosineReparam(dim=dim, experimental_allow_batch=True)


class Forecaster(nn.Module):
    """
    Forecaster for a :class:`ForecastingModel` using variational inference.

    On initialization, this fits a distribution using variational inference
    over latent variables and exact inference over the noise distribution,
    typically a :class:`~pyro.distributions.GaussianHMM` or variant.

    After construction this can be called to generate sample forecasts.

    :ivar list losses: A list of losses recorded during training, typically
        used to debug convergence. Defined by ``loss = -elbo / data.numel()``.

    :param ForecastingModel model: A forecasting model subclass instance.
    :param data: A tensor dataset with time dimension -2.
    :type data: ~torch.Tensor
    :param covariates: A tensor of covariates with time dimension -2.
        For models not using covariates, pass a shaped empty tensor
        ``torch.empty(duration, 0)``.
    :type covariates: ~torch.Tensor

    :param guide: Optional guide instance. Defaults to a
        :class:`~pyro.infer.autoguide.AutoNormal`.
    :type guide: ~pyro.nn.module.PyroModule
    :param callable init_loc_fn: A per-site initialization function for the
        :class:`~pyro.infer.autoguide.AutoNormal` guide. Defaults to
        :func:`~pyro.infer.autoguide.initialization.init_to_sample`. See
        :ref:`autoguide-initialization` section for available functions.
    :param float init_scale: Initial uncertainty scale of the
        :class:`~pyro.infer.autoguide.AutoNormal` guide.
    :param callable create_plates: An optional function to create plates for
        subsampling with the :class:`~pyro.infer.autoguide.AutoNormal` guide.
    :param optim: An optional Pyro optimizer. Defaults to a freshly constructed
        :class:`~pyro.optim.optim.DCTAdam`.
    :type optim: ~pyro.optim.optim.PyroOptim
    :param float learning_rate: Learning rate used by
        :class:`~pyro.optim.optim.DCTAdam`.
    :param tuple betas: Coefficients for running averages used by
        :class:`~pyro.optim.optim.DCTAdam`.
    :param float learning_rate_decay: Learning rate decay used by
        :class:`~pyro.optim.optim.DCTAdam`. Note this is the total decay
        over all ``num_steps``, not the per-step decay factor.
    :param float clip_norm: Norm used for gradient clipping during
        optimization. Defaults to 10.0.
    :param str time_reparam: If not None (default), reparameterize all
        time-dependent variables via the Haar wavelet transform (if "haar") or
        the discrete cosine transform (if "dct").
    :param bool dct_gradients: Whether to discrete cosine transform gradients
        in :class:`~pyro.optim.optim.DCTAdam`. Defaults to False.
    :param bool subsample_aware: whether to update gradient statistics only
        for those elements that appear in a subsample. This is used
        by :class:`~pyro.optim.optim.DCTAdam`.
    :param int num_steps: Number of :class:`~pyro.infer.svi.SVI` steps.
    :param int num_particles: Number of particles used to compute the
        :class:`~pyro.infer.elbo.ELBO`.
    :param bool vectorize_particles: If ``num_particles > 1``, determines
        whether to vectorize computation of the :class:`~pyro.infer.elbo.ELBO`.
        Defaults to True. Set to False for models with dynamic control flow.
    :param bool warm_start: Whether to warm start parameters from a smaller
        time window. Note this may introduce statistical leakage; usage is
        recommended for model exploration purposes only and should be disabled
        when publishing metrics.
    :param int log_every: Number of training steps between logging messages.
    """

    def __init__(self, model, data, covariates, *, guide=None, init_loc_fn=init_to_sample, init_scale=0.1, create_plates=None, optim=None, learning_rate=0.01, betas=(0.9, 0.99), learning_rate_decay=0.1, clip_norm=10.0, time_reparam=None, dct_gradients=False, subsample_aware=False, num_steps=1001, num_particles=1, vectorize_particles=True, warm_start=False, log_every=100):
        assert data.size(-2) == covariates.size(-2)
        super().__init__()
        self.model = model
        if time_reparam == 'haar':
            model = poutine.reparam(model, time_reparam_haar)
        elif time_reparam == 'dct':
            model = poutine.reparam(model, time_reparam_dct)
        elif time_reparam is not None:
            raise ValueError('unknown time_reparam: {}'.format(time_reparam))
        if guide is None:
            guide = AutoNormal(model, init_loc_fn=init_loc_fn, init_scale=init_scale, create_plates=create_plates)
        self.guide = guide
        if warm_start:
            model = PrefixWarmStartMessenger()(model)
            guide = PrefixWarmStartMessenger()(guide)
        if dct_gradients:
            model = MarkDCTParamMessenger('time')(model)
            guide = MarkDCTParamMessenger('time')(guide)
        elbo = Trace_ELBO(num_particles=num_particles, vectorize_particles=vectorize_particles)
        elbo._guess_max_plate_nesting(model, guide, (data, covariates), {})
        elbo.max_plate_nesting = max(elbo.max_plate_nesting, 1)
        losses = []
        if num_steps:
            if optim is None:
                optim = DCTAdam({'lr': learning_rate, 'betas': betas, 'lrd': learning_rate_decay ** (1 / num_steps), 'clip_norm': clip_norm, 'subsample_aware': subsample_aware})
            svi = SVI(model, guide, optim, elbo)
            for step in range(num_steps):
                loss = svi.step(data, covariates) / data.numel()
                if log_every and step % log_every == 0:
                    logger.info('step {: >4d} loss = {:0.6g}'.format(step, loss))
                losses.append(loss)
        self.guide.create_plates = None
        self.max_plate_nesting = elbo.max_plate_nesting
        self.losses = losses

    def __call__(self, data, covariates, num_samples, batch_size=None):
        """
        Samples forecasted values of data for time steps in ``[t1,t2)``, where
        ``t1 = data.size(-2)`` is the duration of observed data and ``t2 =
        covariates.size(-2)`` is the extended duration of covariates. For
        example to forecast 7 days forward conditioned on 30 days of
        observations, set ``t1=30`` and ``t2=37``.

        :param data: A tensor dataset with time dimension -2.
        :type data: ~torch.Tensor
        :param covariates: A tensor of covariates with time dimension -2.
            For models not using covariates, pass a shaped empty tensor
            ``torch.empty(duration, 0)``.
        :type covariates: ~torch.Tensor
        :param int num_samples: The number of samples to generate.
        :param int batch_size: Optional batch size for sampling. This is useful
            for generating many samples from models with large memory
            footprint. Defaults to ``num_samples``.
        :returns: A batch of joint posterior samples of shape
            ``(num_samples,1,...,1) + data.shape[:-2] + (t2-t1,data.size(-1))``,
            where the ``1``'s are inserted to avoid conflict with model plates.
        :rtype: ~torch.Tensor
        """
        return super().__call__(data, covariates, num_samples, batch_size)

    def forward(self, data, covariates, num_samples, batch_size=None):
        assert data.size(-2) <= covariates.size(-2)
        assert isinstance(num_samples, int) and num_samples > 0
        if batch_size is not None:
            batches = []
            while num_samples > 0:
                batch = self.forward(data, covariates, min(num_samples, batch_size))
                batches.append(batch)
                num_samples -= batch_size
            return torch.cat(batches)
        assert self.max_plate_nesting >= 1
        dim = -1 - self.max_plate_nesting
        with torch.no_grad():
            with poutine.block(), poutine.trace() as tr:
                with pyro.plate('particles', num_samples, dim=dim):
                    self.guide(data, covariates)
            with ExitStack() as stack:
                if data.size(-2) < covariates.size(-2):
                    stack.enter_context(PrefixReplayMessenger(tr.trace))
                    stack.enter_context(PrefixConditionMessenger(self.model._prefix_condition_data))
                else:
                    stack.enter_context(poutine.replay(trace=tr.trace))
                with pyro.plate('particles', num_samples, dim=dim):
                    return self.model(data, covariates)


class MCMCKernel(object, metaclass=ABCMeta):

    def setup(self, warmup_steps, *args, **kwargs):
        """
        Optional method to set up any state required at the start of the
        simulation run.

        :param int warmup_steps: Number of warmup iterations.
        :param \\*args: Algorithm specific positional arguments.
        :param \\*\\*kwargs: Algorithm specific keyword arguments.
        """
        pass

    def cleanup(self):
        """
        Optional method to clean up any residual state on termination.
        """
        pass

    def logging(self):
        """
        Relevant logging information to be printed at regular intervals
        of the MCMC run. Returns `None` by default.

        :return: String containing the diagnostic summary. e.g. acceptance rate
        :rtype: string
        """
        return None

    def diagnostics(self):
        """
        Returns a dict of useful diagnostics after finishing sampling process.
        """
        return {}

    def end_warmup(self):
        """
        Optional method to tell kernel that warm-up phase has been finished.
        """
        pass

    @property
    def initial_params(self):
        """
        Returns a dict of initial params (by default, from the prior) to initiate the MCMC run.

        :return: dict of parameter values keyed by their name.
        """
        raise NotImplementedError

    @initial_params.setter
    def initial_params(self, params):
        """
        Sets the parameters to initiate the MCMC run. Note that the parameters must
        have unconstrained support.
        """
        raise NotImplementedError

    @abstractmethod
    def sample(self, params):
        """
        Samples parameters from the posterior distribution, when given existing parameters.

        :param dict params: Current parameter values.
        :param int time_step: Current time step.
        :return: New parameters from the posterior distribution.
        """
        raise NotImplementedError

    def __call__(self, params):
        """
        Alias for MCMCKernel.sample() method.
        """
        return self.sample(params)


class NonreparameterizedNormal(Normal):
    has_rsample = False


class WelfordCovariance:
    """
    Implements Welford's online scheme for estimating (co)variance (see :math:`[1]`).
    Useful for adapting diagonal and dense mass structures for HMC.

    **References**

    [1] `The Art of Computer Programming`,
    Donald E. Knuth
    """

    def __init__(self, diagonal=True):
        self.diagonal = diagonal
        self.reset()

    def reset(self):
        self._mean = 0.0
        self._m2 = 0.0
        self.n_samples = 0

    def update(self, sample):
        self.n_samples += 1
        delta_pre = sample - self._mean
        self._mean = self._mean + delta_pre / self.n_samples
        delta_post = sample - self._mean
        if self.diagonal:
            self._m2 += delta_pre * delta_post
        else:
            self._m2 += torch.ger(delta_post, delta_pre)

    def get_covariance(self, regularize=True):
        if self.n_samples < 2:
            raise RuntimeError('Insufficient samples to estimate covariance')
        cov = self._m2 / (self.n_samples - 1)
        if regularize:
            scaled_cov = self.n_samples / (self.n_samples + 5.0) * cov
            shrinkage = 0.001 * (5.0 / (self.n_samples + 5.0))
            if self.diagonal:
                cov = scaled_cov + shrinkage
            else:
                scaled_cov.view(-1)[::scaled_cov.size(0) + 1] += shrinkage
                cov = scaled_cov
        return cov


def _cholesky(x):
    return x.sqrt() if x.dim() == 1 else torch.linalg.cholesky(x)


def _matvecmul(x, y):
    return x.mul(y) if x.dim() == 1 else x.matmul(y)


def _transpose(x):
    return x if x.dim() == 1 else x.t()


def _triu_inverse(x):
    if x.dim() == 1:
        return x.reciprocal()
    else:
        identity = torch.eye(x.size(-1), dtype=x.dtype, device=x.device)
        return torch.linalg.solve_triangular(x, identity, upper=True)


class BlockMassMatrix:
    """
    EXPERIMENTAL This class is used to adapt (inverse) mass matrix and provide
    useful methods to calculate algebraic terms which involves the mass matrix.

    The mass matrix will have block structure, which can be specified by
    using the method :meth:`configure` with the corresponding structured
    `mass_matrix_shape` arg.

    :param float init_scale: initial scale to construct the initial mass matrix.
    """

    def __init__(self, init_scale=1.0):
        self._init_scale = init_scale
        self._adapt_scheme = {}
        self._inverse_mass_matrix = {}
        self._mass_matrix_sqrt = {}
        self._mass_matrix_sqrt_inverse = {}
        self._mass_matrix_size = {}

    @property
    def mass_matrix_size(self):
        """
        A dict that maps site names to the size of the corresponding mass matrix.
        """
        return self._mass_matrix_size

    @property
    def inverse_mass_matrix(self):
        return self._inverse_mass_matrix

    @inverse_mass_matrix.setter
    def inverse_mass_matrix(self, value):
        for site_names, inverse_mass_matrix in value.items():
            if site_names in self._adapt_scheme:
                self._adapt_scheme[site_names].reset()
            mass_matrix_sqrt_inverse = _transpose(_cholesky(inverse_mass_matrix))
            mass_matrix_sqrt = _triu_inverse(mass_matrix_sqrt_inverse)
            self._inverse_mass_matrix[site_names] = inverse_mass_matrix
            self._mass_matrix_sqrt[site_names] = mass_matrix_sqrt
            self._mass_matrix_sqrt_inverse[site_names] = mass_matrix_sqrt_inverse

    def configure(self, mass_matrix_shape, adapt_mass_matrix=True, options={}):
        """
        Sets up an initial mass matrix.

        :param dict mass_matrix_shape: a dict that maps tuples of site names to the shape of
            the corresponding mass matrix. Each tuple of site names corresponds to a block.
        :param bool adapt_mass_matrix: a flag to decide whether an adaptation scheme will be used.
        :param dict options: tensor options to construct the initial mass matrix.
        """
        inverse_mass_matrix = {}
        for site_names, shape in mass_matrix_shape.items():
            self._mass_matrix_size[site_names] = shape[0]
            diagonal = len(shape) == 1
            inverse_mass_matrix[site_names] = torch.full(shape, self._init_scale, **options) if diagonal else torch.eye(*shape, **options) * self._init_scale
            if adapt_mass_matrix:
                adapt_scheme = WelfordCovariance(diagonal=diagonal)
                self._adapt_scheme[site_names] = adapt_scheme
        self.inverse_mass_matrix = inverse_mass_matrix

    def update(self, z, z_grad):
        """
        Updates the adaptation scheme using the new sample `z` or its grad `z_grad`.

        :param dict z: the current value.
        :param dict z_grad: grad of the current value.
        """
        for site_names, adapt_scheme in self._adapt_scheme.items():
            z_flat = torch.cat([z[name].detach().reshape(-1) for name in site_names])
            adapt_scheme.update(z_flat)

    def end_adaptation(self):
        """
        Updates the current mass matrix using the adaptation scheme.
        """
        inverse_mass_matrix = {}
        for site_names, adapt_scheme in self._adapt_scheme.items():
            inverse_mass_matrix[site_names] = adapt_scheme.get_covariance(regularize=True)
        self.inverse_mass_matrix = inverse_mass_matrix

    def kinetic_grad(self, r):
        """
        Computes the gradient of kinetic energy w.r.t. the momentum `r`.
        It is equivalent to compute velocity given the momentum `r`.

        :param dict r: a dictionary maps site names to a tensor momentum.
        :returns: a dictionary maps site names to the corresponding gradient
        """
        v = {}
        for site_names, inverse_mass_matrix in self._inverse_mass_matrix.items():
            r_flat = torch.cat([r[site_name].reshape(-1) for site_name in site_names])
            v_flat = _matvecmul(inverse_mass_matrix, r_flat)
            pos = 0
            for site_name in site_names:
                next_pos = pos + r[site_name].numel()
                v[site_name] = v_flat[pos:next_pos].reshape(r[site_name].shape)
                pos = next_pos
        return v

    def scale(self, r_unscaled, r_prototype):
        """
        Computes `M^{1/2} @ r_unscaled`.

        Note that `r` is generated from a gaussian with scale `mass_matrix_sqrt`.
        This method will scale it.

        :param dict r_unscaled: a dictionary maps site names to a tensor momentum.
        :param dict r_prototype: a dictionary mapes site names to prototype momentum.
            Those prototype values are used to get shapes of the scaled version.
        :returns: a dictionary maps site names to the corresponding tensor
        """
        s = {}
        for site_names, mass_matrix_sqrt in self._mass_matrix_sqrt.items():
            r_flat = _matvecmul(mass_matrix_sqrt, r_unscaled[site_names])
            pos = 0
            for site_name in site_names:
                next_pos = pos + r_prototype[site_name].numel()
                s[site_name] = r_flat[pos:next_pos].reshape(r_prototype[site_name].shape)
                pos = next_pos
        return s

    def unscale(self, r):
        """
        Computes `inv(M^{1/2}) @ r`.

        Note that `r` is generated from a gaussian with scale `mass_matrix_sqrt`.
        This method will unscale it.

        :param dict r: a dictionary maps site names to a tensor momentum.
        :returns: a dictionary maps site names to the corresponding tensor
        """
        u = {}
        for site_names, mass_matrix_sqrt_inverse in self._mass_matrix_sqrt_inverse.items():
            r_flat = torch.cat([r[site_name].reshape(-1) for site_name in site_names])
            u[site_names] = _matvecmul(mass_matrix_sqrt_inverse, r_flat)
        return u


class DualAveraging:
    """
    Dual Averaging is a scheme to solve convex optimization problems. It belongs
    to a class of subgradient methods which uses subgradients to update parameters
    (in primal space) of a model. Under some conditions, the averages of generated
    parameters during the scheme are guaranteed to converge to an optimal value.
    However, a counter-intuitive aspect of traditional subgradient methods is
    "new subgradients enter the model with decreasing weights" (see :math:`[1]`).
    Dual Averaging scheme solves that phenomenon by updating parameters using
    weights equally for subgradients (which lie in a dual space), hence we have
    the name "dual averaging".

    This class implements a dual averaging scheme which is adapted for Markov chain
    Monte Carlo (MCMC) algorithms. To be more precise, we will replace subgradients
    by some statistics calculated during an MCMC trajectory. In addition,
    introducing some free parameters such as ``t0`` and ``kappa`` is helpful and
    still guarantees the convergence of the scheme.

    References

    [1] `Primal-dual subgradient methods for convex problems`,
    Yurii Nesterov

    [2] `The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo`,
    Matthew D. Hoffman, Andrew Gelman

    :param float prox_center: A "prox-center" parameter introduced in :math:`[1]`
        which pulls the primal sequence towards it.
    :param float t0: A free parameter introduced in :math:`[2]`
        that stabilizes the initial steps of the scheme.
    :param float kappa: A free parameter introduced in :math:`[2]`
        that controls the weights of steps of the scheme.
        For a small ``kappa``, the scheme will quickly forget states
        from early steps. This should be a number in :math:`(0.5, 1]`.
    :param float gamma: A free parameter which controls the speed
        of the convergence of the scheme.
    """

    def __init__(self, prox_center=0, t0=10, kappa=0.75, gamma=0.05):
        self.prox_center = prox_center
        self.t0 = t0
        self.kappa = kappa
        self.gamma = gamma
        self.reset()

    def reset(self):
        self._x_avg = 0
        self._g_avg = 0
        self._t = 0

    def step(self, g):
        """
        Updates states of the scheme given a new statistic/subgradient ``g``.

        :param float g: A statistic calculated during an MCMC trajectory or subgradient.
        """
        self._t += 1
        self._g_avg = (1 - 1 / (self._t + self.t0)) * self._g_avg + g / (self._t + self.t0)
        self._x_t = self.prox_center - self._t ** 0.5 / self.gamma * self._g_avg
        weight_t = self._t ** -self.kappa
        self._x_avg = (1 - weight_t) * self._x_avg + weight_t * self._x_t

    def get_state(self):
        """
        Returns the latest :math:`x_t` and average of
        :math:`\\left\\{x_i\\right\\}_{i=1}^t` in primal space.
        """
        return self._x_t, self._x_avg


adapt_window = namedtuple('adapt_window', ['start', 'end'])


class WarmupAdapter:
    """
    Adapts tunable parameters, namely step size and mass matrix, during the
    warmup phase. This class provides lookup properties to read the latest
    values of ``step_size`` and ``inverse_mass_matrix``. These values are
    periodically updated when adaptation is engaged.
    """

    def __init__(self, step_size=1, adapt_step_size=False, target_accept_prob=0.8, adapt_mass_matrix=False, dense_mass=False):
        self.adapt_step_size = adapt_step_size
        self.adapt_mass_matrix = adapt_mass_matrix
        self.target_accept_prob = target_accept_prob
        self.dense_mass = dense_mass
        self.step_size = 1 if step_size is None else step_size
        self._init_step_size = self.step_size
        self._adaptation_disabled = not (adapt_step_size or adapt_mass_matrix)
        if adapt_step_size:
            self._step_size_adapt_scheme = DualAveraging()
        self._mass_matrix_adapter = BlockMassMatrix()
        self._adapt_start_buffer = 75
        self._adapt_end_buffer = 50
        self._adapt_initial_window = 25
        self._warmup_steps = None
        self._adaptation_schedule = []

    def _build_adaptation_schedule(self):
        adaptation_schedule = []
        if self._warmup_steps < 20:
            adaptation_schedule.append(adapt_window(0, self._warmup_steps - 1))
            return adaptation_schedule
        start_buffer_size = self._adapt_start_buffer
        end_buffer_size = self._adapt_end_buffer
        init_window_size = self._adapt_initial_window
        if self._adapt_start_buffer + self._adapt_end_buffer + self._adapt_initial_window > self._warmup_steps:
            start_buffer_size = int(0.15 * self._warmup_steps)
            end_buffer_size = int(0.1 * self._warmup_steps)
            init_window_size = self._warmup_steps - start_buffer_size - end_buffer_size
        adaptation_schedule.append(adapt_window(start=0, end=start_buffer_size - 1))
        end_window_start = self._warmup_steps - end_buffer_size
        next_window_size = init_window_size
        next_window_start = start_buffer_size
        while next_window_start < end_window_start:
            cur_window_start, cur_window_size = next_window_start, next_window_size
            if 3 * cur_window_size <= end_window_start - cur_window_start:
                next_window_size = 2 * cur_window_size
            else:
                cur_window_size = end_window_start - cur_window_start
            next_window_start = cur_window_start + cur_window_size
            adaptation_schedule.append(adapt_window(cur_window_start, next_window_start - 1))
        adaptation_schedule.append(adapt_window(end_window_start, self._warmup_steps - 1))
        return adaptation_schedule

    def reset_step_size_adaptation(self, z):
        """
        Finds a reasonable step size and resets step size adaptation scheme.
        """
        if self._find_reasonable_step_size is not None:
            with pyro.validation_enabled(False):
                self.step_size = self._find_reasonable_step_size(z)
        self._step_size_adapt_scheme.prox_center = math.log(10 * self.step_size)
        self._step_size_adapt_scheme.reset()

    def _update_step_size(self, accept_prob):
        H = self.target_accept_prob - accept_prob
        self._step_size_adapt_scheme.step(H)
        log_step_size, _ = self._step_size_adapt_scheme.get_state()
        self.step_size = math.exp(log_step_size)

    def _end_adaptation(self):
        if self.adapt_step_size:
            _, log_step_size_avg = self._step_size_adapt_scheme.get_state()
            self.step_size = math.exp(log_step_size_avg)

    def configure(self, warmup_steps, initial_step_size=None, mass_matrix_shape=None, find_reasonable_step_size_fn=None, options={}):
        """
        Model specific properties that are specified when the HMC kernel is setup.

        :param warmup_steps: Number of warmup steps that the sampler is initialized with.
        :param initial_step_size: Step size to use to initialize the Dual Averaging scheme.
        :param mass_matrix_shape: Shape of the mass matrix.
        :param find_reasonable_step_size_fn: A callable to find reasonable step size when
            mass matrix is changed.
        :param dict options: A dict which maps `dtype`, `device` to the corresponding default
            tensor options. This is used to construct initial mass matrix in `mass_matrix_adapter`.
        """
        self._warmup_steps = warmup_steps
        self.step_size = initial_step_size if initial_step_size is not None else self._init_step_size
        if find_reasonable_step_size_fn is not None:
            self._find_reasonable_step_size = find_reasonable_step_size_fn
        if mass_matrix_shape is None or self.step_size is None:
            raise ValueError('Incomplete configuration - step size and inverse mass matrix need to be initialized.')
        self.mass_matrix_adapter.configure(mass_matrix_shape, self.adapt_mass_matrix, options=options)
        if not self._adaptation_disabled:
            self._adaptation_schedule = self._build_adaptation_schedule()
        self._current_window = 0
        if self.adapt_step_size:
            self._step_size_adapt_scheme.reset()

    def step(self, t, z, accept_prob, z_grad=None):
        """
        Called at each step during the warmup phase to learn tunable
        parameters.

        :param int t: time step, beginning at 0.
        :param dict z: latent variables.
        :param float accept_prob: acceptance probability of the proposal.
        """
        if t >= self._warmup_steps or self._adaptation_disabled:
            return
        window = self._adaptation_schedule[self._current_window]
        num_windows = len(self._adaptation_schedule)
        mass_matrix_adaptation_phase = self.adapt_mass_matrix and 0 < self._current_window < num_windows - 1
        if self.adapt_step_size:
            self._update_step_size(accept_prob.item())
        if mass_matrix_adaptation_phase:
            self.mass_matrix_adapter.update(z, z_grad)
        if t == window.end:
            if self._current_window == num_windows - 1:
                self._current_window += 1
                self._end_adaptation()
                return
            if self._current_window == 0:
                self._current_window += 1
                return
            if mass_matrix_adaptation_phase:
                self.mass_matrix_adapter.end_adaptation()
                if self.adapt_step_size:
                    self.reset_step_size_adaptation(z)
            self._current_window += 1

    @property
    def adaptation_schedule(self):
        return self._adaptation_schedule

    @property
    def mass_matrix_adapter(self):
        return self._mass_matrix_adapter

    @mass_matrix_adapter.setter
    def mass_matrix_adapter(self, value):
        self._mass_matrix_adapter = value


def init_to_uniform(site: Optional[dict]=None, radius: float=2.0):
    """
    Initialize to a random point in the area ``(-radius, radius)`` of
    unconstrained domain.

    :param float radius: specifies the range to draw an initial point in the
        unconstrained domain.
    """
    if site is None:
        return functools.partial(init_to_uniform, radius=radius)
    value = site['fn'].sample().detach()
    t = transform_to(site['fn'].support)
    value = t(torch.rand_like(t.inv(value)) * (2 * radius) - radius)
    value._pyro_custom_init = False
    return value


class Ring(object, metaclass=ABCMeta):
    """
    Abstract tensor ring class.

    Each tensor ring class has a notion of ``dims`` that can be sum-contracted
    out, and a notion of ``ordinal`` that represents a set of plate dimensions
    that can be broadcasted-up or product-contracted out.
    Implementations should cache intermediate results to be compatible with
    :func:`~opt_einsum.shared_intermediates`.

    Dims are characters (string or unicode).
    Ordinals are frozensets of characters.

    :param dict cache: an optional :func:`~opt_einsum.shared_intermediates`
        cache.
    """

    def __init__(self, cache=None):
        self._cache = {} if cache is None else cache

    def _hash_by_id(self, tensor):
        """
        Returns the id of a tensor and saves the tensor so that this id can be
        used as a key in the cache without risk of the id being recycled.
        """
        result = id(tensor)
        assert self._cache.setdefault(('tensor', result), tensor) is tensor
        return result

    @abstractmethod
    def sumproduct(self, terms, dims):
        """
        Multiply all ``terms`` together, then sum-contract out all ``dims``
        from the result.

        :param list terms: a list of tensors
        :param dims: an iterable of sum dims to contract
        """
        raise NotImplementedError

    @abstractmethod
    def product(self, term, ordinal):
        """
        Product-contract the given ``term`` along any plate dimensions
        present in given ``ordinal``.

        :param torch.Tensor term: the term to contract
        :param frozenset ordinal: an ordinal specifying plates to contract
        """
        raise NotImplementedError

    def broadcast(self, term, ordinal):
        """
        Broadcast the given ``term`` by expanding along any plate dimensions
        present in ``ordinal`` but not ``term``.

        :param torch.Tensor term: the term to expand
        :param frozenset ordinal: an ordinal specifying plates
        """
        dims = term._pyro_dims
        missing_dims = ''.join(sorted(set(ordinal) - set(dims)))
        if missing_dims:
            key = 'broadcast', self._hash_by_id(term), missing_dims
            if key in self._cache:
                term = self._cache[key]
            else:
                missing_shape = tuple(self._dim_to_size[dim] for dim in missing_dims)
                term = term.expand(missing_shape + term.shape)
                dims = missing_dims + dims
                self._cache[key] = term
                term._pyro_dims = dims
        return term

    @abstractmethod
    def inv(self, term):
        """
        Computes the reciprocal of a term, for use in inclusion-exclusion.

        :param torch.Tensor term: the term to invert
        """
        raise NotImplementedError

    def global_local(self, term, dims, ordinal):
        """
        Computes global and local terms for tensor message passing
        using inclusion-exclusion::

            term / sum(term, dims) * product(sum(term, dims), ordinal)
            \\____________________/   \\_______________________________/
                  local part                    global part

        :param torch.Tensor term: the term to contract
        :param dims: an iterable of sum dims to contract
        :param frozenset ordinal: an ordinal specifying plates to contract
        :return: a tuple ``(global_part, local_part)`` as defined above
        :rtype: tuple
        """
        assert dims, 'dims was empty, use .product() instead'
        key = 'global_local', self._hash_by_id(term), frozenset(dims), ordinal
        if key in self._cache:
            return self._cache[key]
        term_sum = self.sumproduct([term], dims)
        global_part = self.product(term_sum, ordinal)
        with ignore_jit_warnings():
            local_part = self.sumproduct([term, self.inv(term_sum)], set())
        assert sorted(local_part._pyro_dims) == sorted(term._pyro_dims)
        result = global_part, local_part
        self._cache[key] = result
        return result


_PATH_CACHE = {}


def contract_expression(equation, *shapes, **kwargs):
    """
    Wrapper around :func:`opt_einsum.contract_expression` that optionally uses
    Pyro's cheap optimizer and optionally caches contraction paths.

    :param bool cache_path: whether to cache the contraction path.
        Defaults to True.
    """
    cache_path = kwargs.pop('cache_path', True)
    if cache_path:
        kwargs_key = tuple(kwargs.items())
        key = equation, shapes, kwargs_key
        if key in _PATH_CACHE:
            return _PATH_CACHE[key]
    expr = opt_einsum.contract_expression(equation, *shapes, **kwargs)
    if cache_path:
        _PATH_CACHE[key] = expr
    return expr


def contract(equation, *operands, **kwargs):
    """
    Wrapper around :func:`opt_einsum.contract` that optionally uses Pyro's
    cheap optimizer and optionally caches contraction paths.

    :param bool cache_path: whether to cache the contraction path.
        Defaults to True.
    """
    backend = kwargs.pop('backend', 'numpy')
    out = kwargs.pop('out', None)
    shapes = [tuple(t.shape) for t in operands]
    with ignore_jit_warnings():
        expr = contract_expression(equation, *shapes)
        return expr(*operands, backend=backend, out=out)


class LogRing(Ring):
    """
    Ring of sum-product operations in log space.

    Tensor values are in log units, so ``sum`` is implemented as ``logsumexp``,
    and ``product`` is implemented as ``sum``.
    Tensor dimensions are packed; to read the name of a tensor, read the
    ``._pyro_dims`` attribute, which is a string of dimension names aligned
    with the tensor's shape.
    """
    _backend = 'pyro.ops.einsum.torch_log'

    def __init__(self, cache=None, dim_to_size=None):
        super().__init__(cache=cache)
        self._dim_to_size = {} if dim_to_size is None else dim_to_size

    def sumproduct(self, terms, dims):
        inputs = [term._pyro_dims for term in terms]
        output = ''.join(sorted(set(''.join(inputs)) - set(dims)))
        equation = ','.join(inputs) + '->' + output
        term = contract(equation, *terms, backend=self._backend)
        term._pyro_dims = output
        return term

    def product(self, term, ordinal):
        dims = term._pyro_dims
        for dim in sorted(ordinal, reverse=True):
            pos = dims.find(dim)
            if pos != -1:
                key = 'product', self._hash_by_id(term), dim
                if key in self._cache:
                    term = self._cache[key]
                else:
                    term = term.sum(pos)
                    dims = dims.replace(dim, '')
                    self._cache[key] = term
                    term._pyro_dims = dims
        return term

    def inv(self, term):
        key = 'inv', self._hash_by_id(term)
        if key in self._cache:
            return self._cache[key]
        result = -term
        result = result.clamp(max=torch.finfo(result.dtype).max)
        result._pyro_dims = term._pyro_dims
        self._cache[key] = result
        return result


def _check_plates_are_sensible(output_dims, nonoutput_ordinal):
    if output_dims and nonoutput_ordinal:
        raise ValueError("It is nonsensical to preserve a plated dim without preserving all of that dim's plates, but found '{}' without '{}'".format(output_dims, ','.join(nonoutput_ordinal)))


def _check_tree_structure(parent, leaf):
    if parent == leaf:
        raise NotImplementedError('Expected tree-structured plate nesting, but found dependencies on independent plates [{}]. Try converting one of the vectorized plates to a sequential plate (but beware exponential cost in the size of the sequence)'.format(', '.join(getattr(f, 'name', str(f)) for f in leaf)))


def _partition_terms(ring, terms, dims):
    """
    Given a list of terms and a set of contraction dims, partitions the terms
    up into sets that must be contracted together. By separating these
    components we avoid broadcasting.

    This function should be deterministic and free of side effects.
    """
    neighbors = OrderedDict([(t, []) for t in terms] + [(d, []) for d in sorted(dims)])
    for term in terms:
        for dim in term._pyro_dims:
            if dim in dims:
                neighbors[term].append(dim)
                neighbors[dim].append(term)
    components = []
    while neighbors:
        v, pending = neighbors.popitem()
        component = OrderedDict([(v, None)])
        for v in pending:
            component[v] = None
        while pending:
            v = pending.pop()
            for v in neighbors.pop(v):
                if v not in component:
                    component[v] = None
                    pending.append(v)
        component_terms = [v for v in component if isinstance(v, torch.Tensor)]
        if component_terms:
            component_dims = set(v for v in component if not isinstance(v, torch.Tensor))
            components.append((component_terms, component_dims))
    return components


def _contract_component(ring, tensor_tree, sum_dims, target_dims):
    """
    Contract out ``sum_dims - target_dims`` in a tree of tensors in-place, via
    message passing. This reduces all tensors down to a single tensor in the
    minimum plate context.

    This function should be deterministic.
    This function has side-effects: it modifies ``tensor_tree``.

    :param pyro.ops.rings.Ring ring: an algebraic ring defining tensor
        operations.
    :param OrderedDict tensor_tree: a dictionary mapping ordinals to lists of
        tensors. An ordinal is a frozenset of ``CondIndepStack`` frames.
    :param set sum_dims: the complete set of sum-contractions dimensions
        (indexed from the right). This is needed to distinguish sum-contraction
        dimensions from product-contraction dimensions.
    :param set target_dims: An subset of ``sum_dims`` that should be preserved
        in the result.
    :return: a pair ``(ordinal, tensor)``
    :rtype: tuple of frozenset and torch.Tensor
    """
    dim_to_ordinal = {}
    for t, terms in tensor_tree.items():
        for term in terms:
            for dim in sum_dims.intersection(term._pyro_dims):
                dim_to_ordinal[dim] = dim_to_ordinal.get(dim, t) & t
    dims_tree = defaultdict(set)
    for dim, t in dim_to_ordinal.items():
        dims_tree[t].add(dim)
    local_terms = []
    local_dims = target_dims.copy()
    local_ordinal = frozenset()
    min_ordinal = frozenset.intersection(*tensor_tree)
    while any(dims_tree.values()):
        leaf = max(tensor_tree, key=len)
        leaf_terms = tensor_tree.pop(leaf)
        leaf_dims = dims_tree.pop(leaf, set())
        for terms, dims in _partition_terms(ring, leaf_terms, leaf_dims):
            term = ring.sumproduct(terms, dims - local_dims)
            if leaf == min_ordinal:
                parent = leaf
            else:
                pending_dims = sum_dims.intersection(term._pyro_dims)
                parent = frozenset.union(*(t for t, d in dims_tree.items() if d & pending_dims))
                _check_tree_structure(parent, leaf)
                contract_frames = leaf - parent
                contract_dims = dims & local_dims
                if contract_dims:
                    term, local_term = ring.global_local(term, contract_dims, contract_frames)
                    local_terms.append(local_term)
                    local_dims |= sum_dims.intersection(local_term._pyro_dims)
                    local_ordinal |= leaf
                else:
                    term = ring.product(term, contract_frames)
            tensor_tree.setdefault(parent, []).append(term)
    assert len(tensor_tree) == 1
    ordinal, (term,) = tensor_tree.popitem()
    assert ordinal == min_ordinal
    if local_terms:
        assert target_dims
        local_terms.append(term)
        term = ring.sumproduct(local_terms, local_dims - target_dims)
        ordinal |= local_ordinal
    return ordinal, term


def contract_to_tensor(tensor_tree, sum_dims, target_ordinal=None, target_dims=None, cache=None, ring=None):
    """
    Contract out ``sum_dims`` in a tree of tensors, via message
    passing. This reduces all terms down to a single tensor in the plate
    context specified by ``target_ordinal``, optionally preserving sum
    dimensions ``target_dims``.

    This function should be deterministic and free of side effects.

    :param OrderedDict tensor_tree: a dictionary mapping ordinals to lists of
        tensors. An ordinal is a frozenset of ``CondIndepStack`` frames.
    :param set sum_dims: the complete set of sum-contractions dimensions
        (indexed from the right). This is needed to distinguish sum-contraction
        dimensions from product-contraction dimensions.
    :param frozenset target_ordinal: An optional ordinal to which the result
        will be contracted or broadcasted.
    :param set target_dims: An optional subset of ``sum_dims`` that should be
        preserved in the result.
    :param dict cache: an optional :func:`~opt_einsum.shared_intermediates`
        cache.
    :param pyro.ops.rings.Ring ring: an optional algebraic ring defining tensor
        operations.
    :returns: a single tensor
    :rtype: torch.Tensor
    """
    if target_ordinal is None:
        target_ordinal = frozenset()
    if target_dims is None:
        target_dims = set()
    assert isinstance(tensor_tree, OrderedDict)
    assert isinstance(sum_dims, set)
    assert isinstance(target_ordinal, frozenset)
    assert isinstance(target_dims, set) and target_dims <= sum_dims
    if ring is None:
        ring = LogRing(cache)
    ordinals = {term: t for t, terms in tensor_tree.items() for term in terms}
    all_terms = [term for terms in tensor_tree.values() for term in terms]
    contracted_terms = []
    modulo_total = bool(target_dims)
    for terms, dims in _partition_terms(ring, all_terms, sum_dims):
        if modulo_total and dims.isdisjoint(target_dims):
            continue
        component = OrderedDict()
        for term in terms:
            component.setdefault(ordinals[term], []).append(term)
        ordinal, term = _contract_component(ring, component, dims, target_dims & dims)
        _check_plates_are_sensible(target_dims.intersection(term._pyro_dims), ordinal - target_ordinal)
        contract_frames = ordinal - target_ordinal
        if contract_frames:
            assert not sum_dims.intersection(term._pyro_dims)
            term = ring.product(term, contract_frames)
        contracted_terms.append(term)
    term = ring.sumproduct(contracted_terms, set())
    assert sum_dims.intersection(term._pyro_dims) <= target_dims
    return ring.broadcast(term, target_ordinal)


class TraceEinsumEvaluator:
    """
    Computes the log probability density of a trace (of a model with
    tree structure) that possibly contains discrete sample sites
    enumerated in parallel. This uses optimized `einsum` operations
    to marginalize out the the enumerated dimensions in the trace
    via :class:`~pyro.ops.contract.contract_to_tensor`.

    :param model_trace: execution trace from a static model.
    :param bool has_enumerable_sites: whether the trace contains any
        discrete enumerable sites.
    :param int max_plate_nesting: Optional bound on max number of nested
        :func:`pyro.plate` contexts.
    """

    def __init__(self, model_trace, has_enumerable_sites=False, max_plate_nesting=None):
        self.has_enumerable_sites = has_enumerable_sites
        self.max_plate_nesting = max_plate_nesting
        self._enum_dims = set()
        self.ordering = {}
        self._populate_cache(model_trace)

    def _populate_cache(self, model_trace):
        """
        Populate the ordinals (set of ``CondIndepStack`` frames)
        and enum_dims for each sample site.
        """
        if not self.has_enumerable_sites:
            return
        if self.max_plate_nesting is None:
            raise ValueError('Finite value required for `max_plate_nesting` when model has discrete (enumerable) sites.')
        model_trace.compute_log_prob()
        model_trace.pack_tensors()
        for name, site in model_trace.nodes.items():
            if site['type'] == 'sample' and not isinstance(site['fn'], _Subsample):
                if is_validation_enabled():
                    check_site_shape(site, self.max_plate_nesting)
                self.ordering[name] = frozenset(model_trace.plate_to_symbol[f.name] for f in site['cond_indep_stack'] if f.vectorized)
        self._enum_dims = set(model_trace.symbol_to_dim) - set(model_trace.plate_to_symbol.values())

    def _get_log_factors(self, model_trace):
        """
        Aggregates the `log_prob` terms into a list for each
        ordinal.
        """
        model_trace.compute_log_prob()
        model_trace.pack_tensors()
        log_probs = OrderedDict()
        for name, site in model_trace.nodes.items():
            if site['type'] == 'sample' and not isinstance(site['fn'], _Subsample):
                if is_validation_enabled():
                    check_site_shape(site, self.max_plate_nesting)
                log_probs.setdefault(self.ordering[name], []).append(site['packed']['log_prob'])
        return log_probs

    def log_prob(self, model_trace):
        """
        Returns the log pdf of `model_trace` by appropriately handling
        enumerated log prob factors.

        :return: log pdf of the trace.
        """
        if not self.has_enumerable_sites:
            return model_trace.log_prob_sum()
        log_probs = self._get_log_factors(model_trace)
        with shared_intermediates() as cache:
            return contract_to_tensor(log_probs, self._enum_dims, cache=cache)


class _PEMaker:

    def __init__(self, model, model_args, model_kwargs, trace_prob_evaluator, transforms):
        self.model = model
        self.model_args = model_args
        self.model_kwargs = model_kwargs
        self.trace_prob_evaluator = trace_prob_evaluator
        self.transforms = transforms
        self._compiled_fn = None

    def _potential_fn(self, params):
        params_constrained = {k: self.transforms[k].inv(v) for k, v in params.items()}
        cond_model = poutine.condition(self.model, params_constrained)
        model_trace = poutine.trace(cond_model).get_trace(*self.model_args, **self.model_kwargs)
        log_joint = self.trace_prob_evaluator.log_prob(model_trace)
        for name, t in self.transforms.items():
            log_joint = log_joint - torch.sum(t.log_abs_det_jacobian(params_constrained[name], params[name]))
        return -log_joint

    def _potential_fn_jit(self, skip_jit_warnings, jit_options, params):
        if not params:
            return self._potential_fn(params)
        names, vals = zip(*sorted(params.items()))
        if self._compiled_fn:
            return self._compiled_fn(*vals)
        with pyro.validation_enabled(False):
            tmp = []
            for _, v in pyro.get_param_store().named_parameters():
                if v.requires_grad:
                    v.requires_grad_(False)
                    tmp.append(v)

            def _pe_jit(*zi):
                params = dict(zip(names, zi))
                return self._potential_fn(params)
            if skip_jit_warnings:
                _pe_jit = ignore_jit_warnings()(_pe_jit)
            self._compiled_fn = torch.jit.trace(_pe_jit, vals, **jit_options)
            result = self._compiled_fn(*vals)
            for v in tmp:
                v.requires_grad_(True)
            return result

    def get_potential_fn(self, jit_compile=False, skip_jit_warnings=True, jit_options=None):
        if jit_compile:
            jit_options = {'check_trace': False} if jit_options is None else jit_options
            return partial(self._potential_fn_jit, skip_jit_warnings, jit_options)
        return self._potential_fn


def potential_grad(potential_fn, z):
    """
    Gradient of `potential_fn` w.r.t. parameters z.

    :param potential_fn: python callable that takes in a dictionary of parameters
        and returns the potential energy.
    :param dict z: dictionary of parameter values keyed by site name.
    :return: tuple of `(z_grads, potential_energy)`, where `z_grads` is a dictionary
        with the same keys as `z` containing gradients and potential_energy is a
        torch scalar.
    """
    z_keys, z_nodes = zip(*z.items())
    for node in z_nodes:
        node.requires_grad_(True)
    try:
        potential_energy = potential_fn(z)
    except RuntimeError as e:
        if 'singular' in str(e) or 'input is not positive-definite' in str(e):
            grads = {k: v.new_zeros(v.shape) for k, v in z.items()}
            return grads, z_nodes[0].new_tensor(float('nan'))
        else:
            raise e
    grads = grad(potential_energy, z_nodes)
    for node in z_nodes:
        node.requires_grad_(False)
    return dict(zip(z_keys, grads)), potential_energy.detach()


def _find_valid_initial_params(model, model_args, model_kwargs, transforms, potential_fn, prototype_params, max_tries_initial_params=100, num_chains=1, init_strategy=init_to_uniform, trace=None):
    params = prototype_params
    if not params:
        return params
    params_per_chain = defaultdict(list)
    num_found = 0
    model = InitMessenger(init_strategy)(model)
    for attempt in range(num_chains * max_tries_initial_params):
        if trace is None:
            trace = poutine.trace(model).get_trace(*model_args, **model_kwargs)
        samples = {name: trace.nodes[name]['value'].detach() for name in params}
        params = {k: transforms[k](v) for k, v in samples.items()}
        pe_grad, pe = potential_grad(potential_fn, params)
        if torch.isfinite(pe) and all(map(torch.all, map(torch.isfinite, pe_grad.values()))):
            for k, v in params.items():
                params_per_chain[k].append(v)
            num_found += 1
            if num_found == num_chains:
                if num_chains == 1:
                    return {k: v[0] for k, v in params_per_chain.items()}
                else:
                    return {k: torch.stack(v) for k, v in params_per_chain.items()}
        trace = None
    raise ValueError('Model specification seems incorrect - cannot find valid initial params.')


def _guess_max_plate_nesting(model: callable) ->int:
    with torch.no_grad(), poutine.block(), poutine.mask(mask=False):
        trace = poutine.trace(model).get_trace()
    plate_nesting = {0}.union(-f.dim for site in trace.nodes.values() for f in site.get('cond_indep_stack', []) if f.vectorized)
    return max(plate_nesting)


def _config_fn(default, expand, num_samples, tmc, site):
    if site['type'] != 'sample' or site['is_observed']:
        return {}
    if type(site['fn']).__name__ == '_Subsample':
        return {}
    if num_samples is not None:
        return {'enumerate': site['infer'].get('enumerate', default), 'num_samples': site['infer'].get('num_samples', num_samples), 'expand': site['infer'].get('expand', expand), 'tmc': site['infer'].get('tmc', tmc)}
    if getattr(site['fn'], 'has_enumerate_support', False):
        return {'enumerate': site['infer'].get('enumerate', default), 'expand': site['infer'].get('expand', expand)}
    return {}


def _config_enumerate(default, expand, num_samples, tmc):
    return partial(_config_fn, default, expand, num_samples, tmc)


def config_enumerate(guide=None, default='parallel', expand=False, num_samples=None, tmc='diagonal'):
    """
    Configures enumeration for all relevant sites in a guide. This is mainly
    used in conjunction with :class:`~pyro.infer.traceenum_elbo.TraceEnum_ELBO`.

    When configuring for exhaustive enumeration of discrete variables, this
    configures all sample sites whose distribution satisfies
    ``.has_enumerate_support == True``.
    When configuring for local parallel Monte Carlo sampling via
    ``default="parallel", num_samples=n``, this configures all sample sites.
    This does not overwrite existing annotations ``infer={"enumerate": ...}``.

    This can be used as either a function::

        guide = config_enumerate(guide)

    or as a decorator::

        @config_enumerate
        def guide1(*args, **kwargs):
            ...

        @config_enumerate(default="sequential", expand=True)
        def guide2(*args, **kwargs):
            ...

    :param callable guide: a pyro model that will be used as a guide in
        :class:`~pyro.infer.svi.SVI`.
    :param str default: Which enumerate strategy to use, one of
        "sequential", "parallel", or None. Defaults to "parallel".
    :param bool expand: Whether to expand enumerated sample values. See
        :meth:`~pyro.distributions.Distribution.enumerate_support` for details.
        This only applies to exhaustive enumeration, where ``num_samples=None``.
        If ``num_samples`` is not ``None``, then this samples will always be
        expanded.
    :param num_samples: if not ``None``, use local Monte Carlo sampling rather
        than exhaustive enumeration. This makes sense for both continuous and
        discrete distributions.
    :type num_samples: int or None
    :param tmc: "mixture" or "diagonal" strategies to use in Tensor Monte Carlo
    :type tmc: string or None
    :return: an annotated guide
    :rtype: callable
    """
    if default not in ['sequential', 'parallel', 'flat', None]:
        raise ValueError("Invalid default value. Expected 'sequential', 'parallel', or None, but got {}".format(repr(default)))
    if expand not in [True, False]:
        raise ValueError('Invalid expand value. Expected True or False, but got {}'.format(repr(expand)))
    if num_samples is not None:
        if not (isinstance(num_samples, numbers.Number) and num_samples > 0):
            raise ValueError('Invalid num_samples, expected None or positive integer, but got {}'.format(repr(num_samples)))
        if default == 'sequential':
            raise ValueError('Local sampling does not support "sequential" sampling; use "parallel" sampling instead.')
    if tmc == 'full' and num_samples is not None and num_samples > 1:
        expand = True
    if guide is None:
        return lambda guide: config_enumerate(guide, default=default, expand=expand, num_samples=num_samples, tmc=tmc)
    return poutine.infer_config(guide, config_fn=_config_enumerate(default, expand, num_samples, tmc))


def initialize_model(model, model_args=(), model_kwargs={}, transforms=None, max_plate_nesting=None, jit_compile=False, jit_options=None, skip_jit_warnings=False, num_chains=1, init_strategy=init_to_uniform, initial_params=None):
    """
    Given a Python callable with Pyro primitives, generates the following model-specific
    properties needed for inference using HMC/NUTS kernels:

    - initial parameters to be sampled using a HMC kernel,
    - a potential function whose input is a dict of parameters in unconstrained space,
    - transforms to transform latent sites of `model` to unconstrained space,
    - a prototype trace to be used in MCMC to consume traces from sampled parameters.

    :param model: a Pyro model which contains Pyro primitives.
    :param tuple model_args: optional args taken by `model`.
    :param dict model_kwargs: optional kwargs taken by `model`.
    :param dict transforms: Optional dictionary that specifies a transform
        for a sample site with constrained support to unconstrained space. The
        transform should be invertible, and implement `log_abs_det_jacobian`.
        If not specified and the model has sites with constrained support,
        automatic transformations will be applied, as specified in
        :mod:`torch.distributions.constraint_registry`.
    :param int max_plate_nesting: Optional bound on max number of nested
        :func:`pyro.plate` contexts. This is required if model contains
        discrete sample sites that can be enumerated over in parallel.
    :param bool jit_compile: Optional parameter denoting whether to use
        the PyTorch JIT to trace the log density computation, and use this
        optimized executable trace in the integrator.
    :param dict jit_options: A dictionary contains optional arguments for
        :func:`torch.jit.trace` function.
    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT
        tracer when ``jit_compile=True``. Default is False.
    :param int num_chains: Number of parallel chains. If `num_chains > 1`,
        the returned `initial_params` will be a list with `num_chains` elements.
    :param callable init_strategy: A per-site initialization function.
        See :ref:`autoguide-initialization` section for available functions.
    :param dict initial_params: dict containing initial tensors in unconstrained
        space to initiate the markov chain.
    :returns: a tuple of (`initial_params`, `potential_fn`, `transforms`, `prototype_trace`)
    """
    if transforms is None:
        automatic_transform_enabled = True
        transforms = {}
    else:
        automatic_transform_enabled = False
    if max_plate_nesting is None:
        max_plate_nesting = _guess_max_plate_nesting(model, model_args, model_kwargs)
    model = poutine.enum(config_enumerate(model), first_available_dim=-1 - max_plate_nesting)
    prototype_model = poutine.trace(InitMessenger(init_strategy)(model))
    model_trace = prototype_model.get_trace(*model_args, **model_kwargs)
    has_enumerable_sites = False
    prototype_samples = {}
    for name, node in model_trace.iter_stochastic_nodes():
        fn = node['fn']
        if isinstance(fn, _Subsample):
            if fn.subsample_size is not None and fn.subsample_size < fn.size:
                raise NotImplementedError('HMC/NUTS does not support model with subsample sites.')
            continue
        if node['fn'].has_enumerate_support:
            has_enumerable_sites = True
            continue
        prototype_samples[name] = node['value'].detach()
        if automatic_transform_enabled:
            transforms[name] = biject_to(node['fn'].support).inv
    trace_prob_evaluator = TraceEinsumEvaluator(model_trace, has_enumerable_sites, max_plate_nesting)
    pe_maker = _PEMaker(model, model_args, model_kwargs, trace_prob_evaluator, transforms)
    if initial_params is None:
        prototype_params = {k: transforms[k](v) for k, v in prototype_samples.items()}
        initial_params = _find_valid_initial_params(model, model_args, model_kwargs, transforms, pe_maker.get_potential_fn(), prototype_params, num_chains=num_chains, init_strategy=init_strategy, trace=model_trace)
    potential_fn = pe_maker.get_potential_fn(jit_compile, skip_jit_warnings, jit_options)
    return initial_params, potential_fn, transforms, model_trace


class optional:
    """
    Optionally wrap inside `context_manager` if condition is `True`.
    """

    def __init__(self, context_manager, condition):
        self.context_manager = context_manager
        self.condition = condition

    def __enter__(self):
        if self.condition:
            return self.context_manager.__enter__()

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.condition:
            return self.context_manager.__exit__(exc_type, exc_val, exc_tb)


def scalar_like(prototype, fill_value):
    return torch.tensor(fill_value, dtype=prototype.dtype, device=prototype.device)


def _single_step_verlet(z, r, potential_fn, kinetic_grad, step_size, z_grads=None):
    """
    Single step velocity verlet that modifies the `z`, `r` dicts in place.
    """
    z_grads = potential_grad(potential_fn, z)[0] if z_grads is None else z_grads
    for site_name in r:
        r[site_name] = r[site_name] + 0.5 * step_size * -z_grads[site_name]
    r_grads = kinetic_grad(r)
    for site_name in z:
        z[site_name] = z[site_name] + step_size * r_grads[site_name]
    z_grads, potential_energy = potential_grad(potential_fn, z)
    for site_name in r:
        r[site_name] = r[site_name] + 0.5 * step_size * -z_grads[site_name]
    return z, r, z_grads, potential_energy


def velocity_verlet(z, r, potential_fn, kinetic_grad, step_size, num_steps=1, z_grads=None):
    """
    Second order symplectic integrator that uses the velocity verlet algorithm.

    :param dict z: dictionary of sample site names and their current values
        (type :class:`~torch.Tensor`).
    :param dict r: dictionary of sample site names and corresponding momenta
        (type :class:`~torch.Tensor`).
    :param callable potential_fn: function that returns potential energy given z
        for each sample site. The negative gradient of the function with respect
        to ``z`` determines the rate of change of the corresponding sites'
        momenta ``r``.
    :param callable kinetic_grad: a function calculating gradient of kinetic energy
        w.r.t. momentum variable.
    :param float step_size: step size for each time step iteration.
    :param int num_steps: number of discrete time steps over which to integrate.
    :param torch.Tensor z_grads: optional gradients of potential energy at current ``z``.
    :return tuple (z_next, r_next, z_grads, potential_energy): next position and momenta,
        together with the potential energy and its gradient w.r.t. ``z_next``.
    """
    z_next = z.copy()
    r_next = r.copy()
    for _ in range(num_steps):
        z_next, r_next, z_grads, potential_energy = _single_step_verlet(z_next, r_next, potential_fn, kinetic_grad, step_size, z_grads)
    return z_next, r_next, z_grads, potential_energy


class HMC(MCMCKernel):
    """
    Simple Hamiltonian Monte Carlo kernel, where ``step_size`` and ``num_steps``
    need to be explicitly specified by the user.

    **References**

    [1] `MCMC Using Hamiltonian Dynamics`,
    Radford M. Neal

    :param model: Python callable containing Pyro primitives.
    :param potential_fn: Python callable calculating potential energy with input
        is a dict of real support parameters.
    :param float step_size: Determines the size of a single step taken by the
        verlet integrator while computing the trajectory using Hamiltonian
        dynamics. If not specified, it will be set to 1.
    :param float trajectory_length: Length of a MCMC trajectory. If not
        specified, it will be set to ``step_size x num_steps``. In case
        ``num_steps`` is not specified, it will be set to :math:`2\\pi`.
    :param int num_steps: The number of discrete steps over which to simulate
        Hamiltonian dynamics. The state at the end of the trajectory is
        returned as the proposal. This value is always equal to
        ``int(trajectory_length / step_size)``.
    :param bool adapt_step_size: A flag to decide if we want to adapt step_size
        during warm-up phase using Dual Averaging scheme.
    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass
        matrix during warm-up phase using Welford scheme.
    :param bool full_mass: A flag to decide if mass matrix is dense or diagonal.
    :param dict transforms: Optional dictionary that specifies a transform
        for a sample site with constrained support to unconstrained space. The
        transform should be invertible, and implement `log_abs_det_jacobian`.
        If not specified and the model has sites with constrained support,
        automatic transformations will be applied, as specified in
        :mod:`torch.distributions.constraint_registry`.
    :param int max_plate_nesting: Optional bound on max number of nested
        :func:`pyro.plate` contexts. This is required if model contains
        discrete sample sites that can be enumerated over in parallel.
    :param bool jit_compile: Optional parameter denoting whether to use
        the PyTorch JIT to trace the log density computation, and use this
        optimized executable trace in the integrator.
    :param dict jit_options: A dictionary contains optional arguments for
        :func:`torch.jit.trace` function.
    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT
        tracer when ``jit_compile=True``. Default is False.
    :param float target_accept_prob: Increasing this value will lead to a smaller
        step size, hence the sampling will be slower and more robust. Default to 0.8.
    :param callable init_strategy: A per-site initialization function.
        See :ref:`autoguide-initialization` section for available functions.
    :param min_stepsize (float): Lower bound on stepsize in adaptation strategy.
    :param max_stepsize (float): Upper bound on stepsize in adaptation strategy.

    .. note:: Internally, the mass matrix will be ordered according to the order
        of the names of latent variables, not the order of their appearance in
        the model.

    Example:

        >>> true_coefs = torch.tensor([1., 2., 3.])
        >>> data = torch.randn(2000, 3)
        >>> dim = 3
        >>> labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()
        >>>
        >>> def model(data):
        ...     coefs_mean = torch.zeros(dim)
        ...     coefs = pyro.sample('beta', dist.Normal(coefs_mean, torch.ones(3)))
        ...     y = pyro.sample('y', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)
        ...     return y
        >>>
        >>> hmc_kernel = HMC(model, step_size=0.0855, num_steps=4)
        >>> mcmc = MCMC(hmc_kernel, num_samples=500, warmup_steps=100)
        >>> mcmc.run(data)
        >>> mcmc.get_samples()['beta'].mean(0)  # doctest: +SKIP
        tensor([ 0.9819,  1.9258,  2.9737])
    """

    def __init__(self, model=None, potential_fn=None, step_size=1, trajectory_length=None, num_steps=None, adapt_step_size=True, adapt_mass_matrix=True, full_mass=False, transforms=None, max_plate_nesting=None, jit_compile=False, jit_options=None, ignore_jit_warnings=False, target_accept_prob=0.8, init_strategy=init_to_uniform, *, min_stepsize: float=1e-10, max_stepsize: float=10000000000.0):
        if not (model is None) ^ (potential_fn is None):
            raise ValueError('Only one of `model` or `potential_fn` must be specified.')
        self.model = model
        self.transforms = transforms
        self._max_plate_nesting = max_plate_nesting
        self._jit_compile = jit_compile
        self._jit_options = jit_options
        self._ignore_jit_warnings = ignore_jit_warnings
        self._init_strategy = init_strategy
        self._min_stepsize = min_stepsize
        self._max_stepsize = max_stepsize
        self.potential_fn = potential_fn
        if trajectory_length is not None:
            self.trajectory_length = trajectory_length
        elif num_steps is not None:
            self.trajectory_length = step_size * num_steps
        else:
            self.trajectory_length = 2 * math.pi
        self._direction_threshold = math.log(0.8)
        self._max_sliced_energy = 1000
        self._reset()
        self._adapter = WarmupAdapter(step_size, adapt_step_size=adapt_step_size, adapt_mass_matrix=adapt_mass_matrix, target_accept_prob=target_accept_prob, dense_mass=full_mass)
        super().__init__()

    def _kinetic_energy(self, r_unscaled):
        energy = 0.0
        for site_names, value in r_unscaled.items():
            energy = energy + value.dot(value)
        return 0.5 * energy

    def _reset(self):
        self._t = 0
        self._accept_cnt = 0
        self._mean_accept_prob = 0.0
        self._divergences = []
        self._prototype_trace = None
        self._initial_params = None
        self._z_last = None
        self._potential_energy_last = None
        self._z_grads_last = None
        self._warmup_steps = None

    def _find_reasonable_step_size(self, z):
        step_size = self.step_size
        potential_energy = self.potential_fn(z)
        r, r_unscaled = self._sample_r(name='r_presample_0')
        energy_current = self._kinetic_energy(r_unscaled) + potential_energy
        z = {k: v.clone() for k, v in z.items()}
        z_new, r_new, z_grads_new, potential_energy_new = velocity_verlet(z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, step_size)
        r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)
        energy_new = self._kinetic_energy(r_new_unscaled) + potential_energy_new
        delta_energy = energy_new - energy_current
        direction = 1 if self._direction_threshold < -delta_energy else -1
        step_size_scale = 2 ** direction
        direction_new = direction
        t = 0
        while direction_new == direction and self._min_stepsize < step_size < self._max_stepsize:
            t += 1
            step_size = step_size_scale * step_size
            r, r_unscaled = self._sample_r(name='r_presample_{}'.format(t))
            energy_current = self._kinetic_energy(r_unscaled) + potential_energy
            z_new, r_new, z_grads_new, potential_energy_new = velocity_verlet(z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, step_size)
            r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)
            energy_new = self._kinetic_energy(r_new_unscaled) + potential_energy_new
            delta_energy = energy_new - energy_current
            direction_new = 1 if self._direction_threshold < -delta_energy else -1
        step_size = max(step_size, self._min_stepsize)
        step_size = min(step_size, self._max_stepsize)
        return step_size

    def _sample_r(self, name):
        r_unscaled = {}
        options = {'dtype': self._potential_energy_last.dtype, 'device': self._potential_energy_last.device}
        for site_names, size in self.mass_matrix_adapter.mass_matrix_size.items():
            r_unscaled[site_names] = pyro.sample('{}_{}'.format(name, site_names), NonreparameterizedNormal(torch.zeros(size, **options), torch.ones(size, **options)))
        r = self.mass_matrix_adapter.scale(r_unscaled, r_prototype=self.initial_params)
        return r, r_unscaled

    @property
    def mass_matrix_adapter(self):
        return self._adapter.mass_matrix_adapter

    @mass_matrix_adapter.setter
    def mass_matrix_adapter(self, value):
        self._adapter.mass_matrix_adapter = value

    @property
    def inverse_mass_matrix(self):
        return self.mass_matrix_adapter.inverse_mass_matrix

    @property
    def step_size(self):
        return self._adapter.step_size

    @property
    def num_steps(self):
        return max(1, int(self.trajectory_length / self.step_size))

    @property
    def initial_params(self):
        return self._initial_params

    @initial_params.setter
    def initial_params(self, params):
        self._initial_params = params

    def _initialize_model_properties(self, model_args, model_kwargs):
        init_params, potential_fn, transforms, trace = initialize_model(self.model, model_args, model_kwargs, transforms=self.transforms, max_plate_nesting=self._max_plate_nesting, jit_compile=self._jit_compile, jit_options=self._jit_options, skip_jit_warnings=self._ignore_jit_warnings, init_strategy=self._init_strategy, initial_params=self._initial_params)
        self.potential_fn = potential_fn
        self.transforms = transforms
        self._initial_params = init_params
        self._prototype_trace = trace

    def _initialize_adapter(self):
        if self._adapter.dense_mass is False:
            dense_sites_list = []
        elif self._adapter.dense_mass is True:
            dense_sites_list = [tuple(sorted(self.initial_params))]
        else:
            msg = 'full_mass should be a list of tuples of site names.'
            dense_sites_list = self._adapter.dense_mass
            assert isinstance(dense_sites_list, list), msg
            for dense_sites in dense_sites_list:
                assert dense_sites and isinstance(dense_sites, tuple), msg
                for name in dense_sites:
                    assert isinstance(name, str) and name in self.initial_params, msg
        dense_sites_set = set().union(*dense_sites_list)
        diag_sites = tuple(sorted([name for name in self.initial_params if name not in dense_sites_set]))
        assert len(diag_sites) + sum([len(sites) for sites in dense_sites_list]) == len(self.initial_params), 'Site names specified in full_mass are duplicated.'
        mass_matrix_shape = OrderedDict()
        for dense_sites in dense_sites_list:
            size = sum([self.initial_params[site].numel() for site in dense_sites])
            mass_matrix_shape[dense_sites] = size, size
        if diag_sites:
            size = sum([self.initial_params[site].numel() for site in diag_sites])
            mass_matrix_shape[diag_sites] = size,
        options = {'dtype': self._potential_energy_last.dtype, 'device': self._potential_energy_last.device}
        self._adapter.configure(self._warmup_steps, mass_matrix_shape=mass_matrix_shape, find_reasonable_step_size_fn=self._find_reasonable_step_size, options=options)
        if self._adapter.adapt_step_size:
            self._adapter.reset_step_size_adaptation(self._initial_params)

    def setup(self, warmup_steps, *args, **kwargs):
        self._warmup_steps = warmup_steps
        if self.model is not None:
            self._initialize_model_properties(args, kwargs)
        if self.initial_params:
            z = {k: v.detach() for k, v in self.initial_params.items()}
            z_grads, potential_energy = potential_grad(self.potential_fn, z)
        else:
            z_grads, potential_energy = {}, self.potential_fn(self.initial_params)
        self._cache(self.initial_params, potential_energy, z_grads)
        if self.initial_params:
            self._initialize_adapter()

    def cleanup(self):
        self._reset()

    def _cache(self, z, potential_energy, z_grads=None):
        self._z_last = z
        self._potential_energy_last = potential_energy
        self._z_grads_last = z_grads

    def clear_cache(self):
        self._z_last = None
        self._potential_energy_last = None
        self._z_grads_last = None

    def _fetch_from_cache(self):
        return self._z_last, self._potential_energy_last, self._z_grads_last

    def sample(self, params):
        z, potential_energy, z_grads = self._fetch_from_cache()
        if z is None:
            z = params
            z_grads, potential_energy = potential_grad(self.potential_fn, z)
            self._cache(z, potential_energy, z_grads)
        elif len(z) == 0:
            self._t += 1
            self._mean_accept_prob = 1.0
            if self._t > self._warmup_steps:
                self._accept_cnt += 1
            return params
        r, r_unscaled = self._sample_r(name='r_t={}'.format(self._t))
        energy_current = self._kinetic_energy(r_unscaled) + potential_energy
        with optional(pyro.validation_enabled(False), self._t < self._warmup_steps):
            z_new, r_new, z_grads_new, potential_energy_new = velocity_verlet(z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, self.step_size, self.num_steps, z_grads=z_grads)
            r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)
            energy_proposal = self._kinetic_energy(r_new_unscaled) + potential_energy_new
        delta_energy = energy_proposal - energy_current
        delta_energy = scalar_like(delta_energy, float('inf')) if torch_isnan(delta_energy) else delta_energy
        if delta_energy > self._max_sliced_energy and self._t >= self._warmup_steps:
            self._divergences.append(self._t - self._warmup_steps)
        accept_prob = (-delta_energy).exp().clamp(max=1.0)
        rand = pyro.sample('rand_t={}'.format(self._t), dist.Uniform(scalar_like(accept_prob, 0.0), scalar_like(accept_prob, 1.0)))
        accepted = False
        if rand < accept_prob:
            accepted = True
            z = z_new
            z_grads = z_grads_new
            self._cache(z, potential_energy_new, z_grads)
        self._t += 1
        if self._t > self._warmup_steps:
            n = self._t - self._warmup_steps
            if accepted:
                self._accept_cnt += 1
        else:
            n = self._t
            self._adapter.step(self._t, z, accept_prob, z_grads)
        self._mean_accept_prob += (accept_prob.item() - self._mean_accept_prob) / n
        return z.copy()

    def logging(self):
        return OrderedDict([('step size', '{:.2e}'.format(self.step_size)), ('acc. prob', '{:.3f}'.format(self._mean_accept_prob))])

    def diagnostics(self):
        return {'divergences': self._divergences, 'acceptance rate': self._accept_cnt / (self._t - self._warmup_steps)}


_TreeInfo = namedtuple('TreeInfo', ['z_left', 'r_left', 'r_left_unscaled', 'z_left_grads', 'z_right', 'r_right', 'r_right_unscaled', 'z_right_grads', 'z_proposal', 'z_proposal_pe', 'z_proposal_grads', 'r_sum', 'weight', 'turning', 'diverging', 'sum_accept_probs', 'num_proposals'])


def _logaddexp(x, y):
    minval, maxval = (x, y) if x < y else (y, x)
    return (minval - maxval).exp().log1p() + maxval


class NUTS(HMC):
    """
    No-U-Turn Sampler kernel, which provides an efficient and convenient way
    to run Hamiltonian Monte Carlo. The number of steps taken by the
    integrator is dynamically adjusted on each call to ``sample`` to ensure
    an optimal length for the Hamiltonian trajectory [1]. As such, the samples
    generated will typically have lower autocorrelation than those generated
    by the :class:`~pyro.infer.mcmc.HMC` kernel. Optionally, the NUTS kernel
    also provides the ability to adapt step size during the warmup phase.

    Refer to the `baseball example <https://github.com/pyro-ppl/pyro/blob/dev/examples/baseball.py>`_
    to see how to do Bayesian inference in Pyro using NUTS.

    **References**

    [1] `The No-U-turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo`,
        Matthew D. Hoffman, and Andrew Gelman.
    [2] `A Conceptual Introduction to Hamiltonian Monte Carlo`,
        Michael Betancourt
    [3] `Slice Sampling`,
        Radford M. Neal

    :param model: Python callable containing Pyro primitives.
    :param potential_fn: Python callable calculating potential energy with input
        is a dict of real support parameters.
    :param float step_size: Determines the size of a single step taken by the
        verlet integrator while computing the trajectory using Hamiltonian
        dynamics. If not specified, it will be set to 1.
    :param bool adapt_step_size: A flag to decide if we want to adapt step_size
        during warm-up phase using Dual Averaging scheme.
    :param bool adapt_mass_matrix: A flag to decide if we want to adapt mass
        matrix during warm-up phase using Welford scheme.
    :param bool full_mass: A flag to decide if mass matrix is dense or diagonal.
    :param bool use_multinomial_sampling: A flag to decide if we want to sample
        candidates along its trajectory using "multinomial sampling" or using
        "slice sampling". Slice sampling is used in the original NUTS paper [1],
        while multinomial sampling is suggested in [2]. By default, this flag is
        set to True. If it is set to `False`, NUTS uses slice sampling.
    :param dict transforms: Optional dictionary that specifies a transform
        for a sample site with constrained support to unconstrained space. The
        transform should be invertible, and implement `log_abs_det_jacobian`.
        If not specified and the model has sites with constrained support,
        automatic transformations will be applied, as specified in
        :mod:`torch.distributions.constraint_registry`.
    :param int max_plate_nesting: Optional bound on max number of nested
        :func:`pyro.plate` contexts. This is required if model contains
        discrete sample sites that can be enumerated over in parallel.
    :param bool jit_compile: Optional parameter denoting whether to use
        the PyTorch JIT to trace the log density computation, and use this
        optimized executable trace in the integrator.
    :param dict jit_options: A dictionary contains optional arguments for
        :func:`torch.jit.trace` function.
    :param bool ignore_jit_warnings: Flag to ignore warnings from the JIT
        tracer when ``jit_compile=True``. Default is False.
    :param float target_accept_prob: Target acceptance probability of step size
        adaptation scheme. Increasing this value will lead to a smaller step size,
        so the sampling will be slower but more robust. Default to 0.8.
    :param int max_tree_depth: Max depth of the binary tree created during the doubling
        scheme of NUTS sampler. Default to 10.
    :param callable init_strategy: A per-site initialization function.
        See :ref:`autoguide-initialization` section for available functions.

    Example:

        >>> true_coefs = torch.tensor([1., 2., 3.])
        >>> data = torch.randn(2000, 3)
        >>> dim = 3
        >>> labels = dist.Bernoulli(logits=(true_coefs * data).sum(-1)).sample()
        >>>
        >>> def model(data):
        ...     coefs_mean = torch.zeros(dim)
        ...     coefs = pyro.sample('beta', dist.Normal(coefs_mean, torch.ones(3)))
        ...     y = pyro.sample('y', dist.Bernoulli(logits=(coefs * data).sum(-1)), obs=labels)
        ...     return y
        >>>
        >>> nuts_kernel = NUTS(model, adapt_step_size=True)
        >>> mcmc = MCMC(nuts_kernel, num_samples=500, warmup_steps=300)
        >>> mcmc.run(data)
        >>> mcmc.get_samples()['beta'].mean(0)  # doctest: +SKIP
        tensor([ 0.9221,  1.9464,  2.9228])
    """

    def __init__(self, model=None, potential_fn=None, step_size=1, adapt_step_size=True, adapt_mass_matrix=True, full_mass=False, use_multinomial_sampling=True, transforms=None, max_plate_nesting=None, jit_compile=False, jit_options=None, ignore_jit_warnings=False, target_accept_prob=0.8, max_tree_depth=10, init_strategy=init_to_uniform):
        super().__init__(model, potential_fn, step_size, adapt_step_size=adapt_step_size, adapt_mass_matrix=adapt_mass_matrix, full_mass=full_mass, transforms=transforms, max_plate_nesting=max_plate_nesting, jit_compile=jit_compile, jit_options=jit_options, ignore_jit_warnings=ignore_jit_warnings, target_accept_prob=target_accept_prob, init_strategy=init_strategy)
        self.use_multinomial_sampling = use_multinomial_sampling
        self._max_tree_depth = max_tree_depth
        self._max_sliced_energy = 1000

    def _is_turning(self, r_left_unscaled, r_right_unscaled, r_sum):
        left_angle = 0.0
        right_angle = 0.0
        for site_names, value in r_sum.items():
            rho = value - (r_left_unscaled[site_names] + r_right_unscaled[site_names]) / 2
            left_angle += r_left_unscaled[site_names].dot(rho)
            right_angle += r_right_unscaled[site_names].dot(rho)
        return left_angle <= 0 or right_angle <= 0

    def _build_basetree(self, z, r, z_grads, log_slice, direction, energy_current):
        step_size = self.step_size if direction == 1 else -self.step_size
        z_new, r_new, z_grads, potential_energy = velocity_verlet(z, r, self.potential_fn, self.mass_matrix_adapter.kinetic_grad, step_size, z_grads=z_grads)
        r_new_unscaled = self.mass_matrix_adapter.unscale(r_new)
        energy_new = potential_energy + self._kinetic_energy(r_new_unscaled)
        energy_new = scalar_like(energy_new, float('inf')) if torch_isnan(energy_new) else energy_new
        sliced_energy = energy_new + log_slice
        diverging = sliced_energy > self._max_sliced_energy
        delta_energy = energy_new - energy_current
        accept_prob = (-delta_energy).exp().clamp(max=1.0)
        if self.use_multinomial_sampling:
            tree_weight = -sliced_energy
        else:
            tree_weight = scalar_like(sliced_energy, 1.0 if sliced_energy <= 0 else 0.0)
        r_sum = r_new_unscaled
        return _TreeInfo(z_new, r_new, r_new_unscaled, z_grads, z_new, r_new, r_new_unscaled, z_grads, z_new, potential_energy, z_grads, r_sum, tree_weight, False, diverging, accept_prob, 1)

    def _build_tree(self, z, r, z_grads, log_slice, direction, tree_depth, energy_current):
        if tree_depth == 0:
            return self._build_basetree(z, r, z_grads, log_slice, direction, energy_current)
        half_tree = self._build_tree(z, r, z_grads, log_slice, direction, tree_depth - 1, energy_current)
        z_proposal = half_tree.z_proposal
        z_proposal_pe = half_tree.z_proposal_pe
        z_proposal_grads = half_tree.z_proposal_grads
        if half_tree.turning or half_tree.diverging:
            return half_tree
        if direction == 1:
            z = half_tree.z_right
            r = half_tree.r_right
            z_grads = half_tree.z_right_grads
        else:
            z = half_tree.z_left
            r = half_tree.r_left
            z_grads = half_tree.z_left_grads
        other_half_tree = self._build_tree(z, r, z_grads, log_slice, direction, tree_depth - 1, energy_current)
        if self.use_multinomial_sampling:
            tree_weight = _logaddexp(half_tree.weight, other_half_tree.weight)
        else:
            tree_weight = half_tree.weight + other_half_tree.weight
        sum_accept_probs = half_tree.sum_accept_probs + other_half_tree.sum_accept_probs
        num_proposals = half_tree.num_proposals + other_half_tree.num_proposals
        r_sum = {site_names: (half_tree.r_sum[site_names] + other_half_tree.r_sum[site_names]) for site_names in self.inverse_mass_matrix}
        if self.use_multinomial_sampling:
            other_half_tree_prob = (other_half_tree.weight - tree_weight).exp()
        else:
            other_half_tree_prob = other_half_tree.weight / tree_weight if tree_weight > 0 else scalar_like(tree_weight, 0.0)
        is_other_half_tree = pyro.sample('is_other_half_tree', dist.Bernoulli(probs=other_half_tree_prob))
        if is_other_half_tree == 1:
            z_proposal = other_half_tree.z_proposal
            z_proposal_pe = other_half_tree.z_proposal_pe
            z_proposal_grads = other_half_tree.z_proposal_grads
        if direction == 1:
            z_left = half_tree.z_left
            r_left = half_tree.r_left
            r_left_unscaled = half_tree.r_left_unscaled
            z_left_grads = half_tree.z_left_grads
            z_right = other_half_tree.z_right
            r_right = other_half_tree.r_right
            r_right_unscaled = other_half_tree.r_right_unscaled
            z_right_grads = other_half_tree.z_right_grads
        else:
            z_left = other_half_tree.z_left
            r_left = other_half_tree.r_left
            r_left_unscaled = other_half_tree.r_left_unscaled
            z_left_grads = other_half_tree.z_left_grads
            z_right = half_tree.z_right
            r_right = half_tree.r_right
            r_right_unscaled = half_tree.r_right_unscaled
            z_right_grads = half_tree.z_right_grads
        turning = other_half_tree.turning or self._is_turning(r_left_unscaled, r_right_unscaled, r_sum)
        diverging = other_half_tree.diverging
        return _TreeInfo(z_left, r_left, r_left_unscaled, z_left_grads, z_right, r_right, r_right_unscaled, z_right_grads, z_proposal, z_proposal_pe, z_proposal_grads, r_sum, tree_weight, turning, diverging, sum_accept_probs, num_proposals)

    def sample(self, params):
        z, potential_energy, z_grads = self._fetch_from_cache()
        if z is None:
            z = params
            z_grads, potential_energy = potential_grad(self.potential_fn, z)
            self._cache(z, potential_energy, z_grads)
        elif len(z) == 0:
            self._t += 1
            self._mean_accept_prob = 1.0
            if self._t > self._warmup_steps:
                self._accept_cnt += 1
            return z
        r, r_unscaled = self._sample_r(name='r_t={}'.format(self._t))
        energy_current = self._kinetic_energy(r_unscaled) + potential_energy
        if self.use_multinomial_sampling:
            log_slice = -energy_current
        else:
            slice_exp_term = pyro.sample('slicevar_exp_t={}'.format(self._t), dist.Exponential(scalar_like(energy_current, 1.0)))
            log_slice = -energy_current - slice_exp_term
        z_left = z_right = z
        r_left = r_right = r
        r_left_unscaled = r_right_unscaled = r_unscaled
        z_left_grads = z_right_grads = z_grads
        accepted = False
        r_sum = r_unscaled
        sum_accept_probs = 0.0
        num_proposals = 0
        tree_weight = scalar_like(energy_current, 0.0 if self.use_multinomial_sampling else 1.0)
        with optional(pyro.validation_enabled(False), self._t < self._warmup_steps):
            tree_depth = 0
            while tree_depth < self._max_tree_depth:
                direction = pyro.sample('direction_t={}_treedepth={}'.format(self._t, tree_depth), dist.Bernoulli(probs=scalar_like(tree_weight, 0.5)))
                direction = int(direction.item())
                if direction == 1:
                    new_tree = self._build_tree(z_right, r_right, z_right_grads, log_slice, direction, tree_depth, energy_current)
                    z_right = new_tree.z_right
                    r_right = new_tree.r_right
                    r_right_unscaled = new_tree.r_right_unscaled
                    z_right_grads = new_tree.z_right_grads
                else:
                    new_tree = self._build_tree(z_left, r_left, z_left_grads, log_slice, direction, tree_depth, energy_current)
                    z_left = new_tree.z_left
                    r_left = new_tree.r_left
                    r_left_unscaled = new_tree.r_left_unscaled
                    z_left_grads = new_tree.z_left_grads
                sum_accept_probs = sum_accept_probs + new_tree.sum_accept_probs
                num_proposals = num_proposals + new_tree.num_proposals
                if new_tree.diverging:
                    if self._t >= self._warmup_steps:
                        self._divergences.append(self._t - self._warmup_steps)
                    break
                if new_tree.turning:
                    break
                tree_depth += 1
                if self.use_multinomial_sampling:
                    new_tree_prob = (new_tree.weight - tree_weight).exp()
                else:
                    new_tree_prob = new_tree.weight / tree_weight
                rand = pyro.sample('rand_t={}_treedepth={}'.format(self._t, tree_depth), dist.Uniform(scalar_like(new_tree_prob, 0.0), scalar_like(new_tree_prob, 1.0)))
                if rand < new_tree_prob:
                    accepted = True
                    z = new_tree.z_proposal
                    z_grads = new_tree.z_proposal_grads
                    self._cache(z, new_tree.z_proposal_pe, z_grads)
                r_sum = {site_names: (r_sum[site_names] + new_tree.r_sum[site_names]) for site_names in r_unscaled}
                if self._is_turning(r_left_unscaled, r_right_unscaled, r_sum):
                    break
                elif self.use_multinomial_sampling:
                    tree_weight = _logaddexp(tree_weight, new_tree.weight)
                else:
                    tree_weight = tree_weight + new_tree.weight
        accept_prob = sum_accept_probs / num_proposals
        self._t += 1
        if self._t > self._warmup_steps:
            n = self._t - self._warmup_steps
            if accepted:
                self._accept_cnt += 1
        else:
            n = self._t
            self._adapter.step(self._t, z, accept_prob, z_grads)
        self._mean_accept_prob += (accept_prob.item() - self._mean_accept_prob) / n
        return z.copy()


class AbstractMCMC(ABC):
    """
    Base class for MCMC methods.
    """

    def __init__(self, kernel, num_chains, transforms):
        self.kernel = kernel
        self.num_chains = num_chains
        self.transforms = transforms

    @abstractmethod
    def run(self, *args, **kwargs):
        raise NotImplementedError

    @abstractmethod
    def diagnostics(self):
        raise NotImplementedError

    def _set_transforms(self, *args, **kwargs):
        if getattr(self.kernel, 'transforms', None) is not None:
            self.transforms = self.kernel.transforms
        elif self.kernel.model:
            warmup_steps = 0
            self.kernel.setup(warmup_steps, *args, **kwargs)
            self.transforms = self.kernel.transforms
        else:
            self.transforms = {}

    def _validate_kernel(self, initial_params):
        if isinstance(self.kernel, (HMC, NUTS)) and self.kernel.potential_fn is not None:
            if initial_params is None:
                raise ValueError('Must provide valid initial parameters to begin sampling when using `potential_fn` in HMC/NUTS kernel.')

    def _validate_initial_params(self, initial_params):
        for v in initial_params.values():
            if v.shape[0] != self.num_chains:
                raise ValueError('The leading dimension of tensors in `initial_params` must match the number of chains.')


MAX_SEED = 2 ** 32 - 1


DIAGNOSTIC_MSG = 'DIAGNOSTICS'


def _add_logging_hook(logger, progress_bar=None, hook=None):

    def _add_logging(kernel, params, stage, i):
        diagnostics = json.dumps(kernel.logging())
        logger.info(diagnostics, extra={'msg_type': DIAGNOSTIC_MSG})
        if progress_bar:
            progress_bar.set_description(stage, refresh=False)
        if hook:
            hook(kernel, params, stage, i)
    return _add_logging


def _gen_samples(kernel, warmup_steps, num_samples, hook, chain_id, *args, **kwargs):
    kernel.setup(warmup_steps, *args, **kwargs)
    params = kernel.initial_params
    save_params = getattr(kernel, 'save_params', sorted(params))
    yield {name: params[name].shape for name in save_params}
    for i in range(warmup_steps):
        params = kernel.sample(params)
        hook(kernel, params, 'Warmup [{}]'.format(chain_id) if chain_id is not None else 'Warmup', i)
    for i in range(num_samples):
        params = kernel.sample(params)
        hook(kernel, params, 'Sample [{}]'.format(chain_id) if chain_id is not None else 'Sample', i)
        flat = [params[name].reshape(-1) for name in save_params]
        yield (torch.cat if flat else torch.tensor)(flat)
    yield kernel.diagnostics()
    kernel.cleanup()


class MCMCLoggingHandler(logging.Handler):
    """
    Main logging handler used by :class:`~pyro.infer.mcmc`,
    to handle both progress bar updates and regular `logging`
    messages.

    :param log_handler: default log handler for logging
        output.
    :param progress_bar: If provided, diagnostic information
        is updated using the bar.
    """

    def __init__(self, log_handler, progress_bar=None):
        logging.Handler.__init__(self)
        self.log_handler = log_handler
        self.progress_bar = progress_bar

    def emit(self, record):
        try:
            if self.progress_bar and record.msg_type == DIAGNOSTIC_MSG:
                diagnostics = json.loads(record.getMessage(), object_pairs_hook=OrderedDict)
                self.progress_bar.set_postfix(diagnostics, refresh=False)
                self.progress_bar.update()
            else:
                self.log_handler.handle(record)
        except (KeyboardInterrupt, SystemExit):
            raise
        except Exception:
            self.handleError(record)


LOG_MSG = 'LOG'


class MetadataFilter(logging.Filter):
    """
    Adds auxiliary information to log records, like `logger_id` and
    `msg_type`.
    """

    def __init__(self, logger_id):
        self.logger_id = logger_id
        super().__init__()

    def filter(self, record):
        record.logger_id = self.logger_id
        if not getattr(record, 'msg_type', None):
            record.msg_type = LOG_MSG
        return True


class QueueHandler(logging.Handler):
    """
    This handler sends events to a queue. Typically, it would be used together
    with a multiprocessing Queue to centralise logging to file in one process
    (in a multi-process application), so as to avoid file write contention
    between processes.

    This code is new in Python 3.2, but this class can be copy pasted into
    user code for use with earlier Python versions.
    """

    def __init__(self, queue):
        """
        Initialise an instance, using the passed queue.
        """
        logging.Handler.__init__(self)
        self.queue = queue

    def enqueue(self, record):
        """
        Enqueue a record.

        The base implementation uses put_nowait. You may want to override
        this method if you want to use blocking, timeouts or custom queue
        implementations.
        """
        self.queue.put_nowait(record)

    def prepare(self, record):
        """
        Prepares a record for queuing. The object returned by this method is
        enqueued.

        The base implementation formats the record to merge the message
        and arguments, and removes unpickleable items from the record
        in-place.

        You might want to override this method if you want to convert
        the record to a dict or JSON string, or send a modified copy
        of the record while leaving the original intact.
        """
        record.msg = self.format(record)
        record.args = None
        record.exc_info = None
        return record

    def emit(self, record):
        """
        Emit a record.

        Writes the LogRecord to the queue, preparing it for pickling first.
        """
        try:
            self.enqueue(self.prepare(record))
        except Exception:
            self.handleError(record)


class TqdmHandler(logging.StreamHandler):
    """
    Handler that synchronizes the log output with the
    :class:`~tqdm.tqdm` progress bar.
    """

    def emit(self, record):
        try:
            msg = self.format(record)
            self.flush()
            tqdm.write(msg, file=sys.stderr)
        except (KeyboardInterrupt, SystemExit):
            raise
        except Exception:
            self.handleError(record)


def initialize_logger(logger, logger_id, progress_bar=None, log_queue=None):
    """
    Initialize logger for the :class:`pyro.infer.mcmc` module.

    :param logger: logger instance.
    :param str logger_id: identifier for the log record,
        e.g. chain id in case of multiple samplers.
    :param progress_bar: a :class:`tqdm.tqdm` instance.
    """
    logger.handlers = []
    logger.propagate = False
    if log_queue:
        handler = QueueHandler(log_queue)
        format = '[%(levelname)s %(msg_type)s %(logger_id)s]%(message)s'
        progress_bar = None
    elif progress_bar:
        format = '%(levelname).1s \t %(message)s'
        handler = TqdmHandler()
    else:
        raise ValueError('Logger cannot be initialized without a valid handler.')
    handler.setFormatter(logging.Formatter(format))
    logging_handler = MCMCLoggingHandler(handler, progress_bar)
    logging_handler.addFilter(MetadataFilter(logger_id))
    logger.addHandler(logging_handler)
    return logger


class _Worker:

    def __init__(self, chain_id, result_queue, log_queue, event, kernel, num_samples, warmup_steps, initial_params=None, hook=None):
        self.chain_id = chain_id
        self.kernel = kernel
        if initial_params is not None:
            self.kernel.initial_params = initial_params
        self.num_samples = num_samples
        self.warmup_steps = warmup_steps
        self.rng_seed = (torch.initial_seed() + chain_id) % MAX_SEED
        self.log_queue = log_queue
        self.result_queue = result_queue
        self.default_tensor_type = torch.Tensor().type()
        self.hook = hook
        self.event = event

    def run(self, *args, **kwargs):
        pyro.set_rng_seed(self.rng_seed)
        torch.set_default_tensor_type(self.default_tensor_type)
        kwargs = kwargs
        logger = logging.getLogger('pyro.infer.mcmc')
        logger_id = 'CHAIN:{}'.format(self.chain_id)
        log_queue = self.log_queue
        logger = initialize_logger(logger, logger_id, None, log_queue)
        logging_hook = _add_logging_hook(logger, None, self.hook)
        try:
            for sample in _gen_samples(self.kernel, self.warmup_steps, self.num_samples, logging_hook, None, *args, **kwargs):
                self.result_queue.put_nowait((self.chain_id, sample))
                self.event.wait()
                self.event.clear()
            self.result_queue.put_nowait((self.chain_id, None))
        except Exception as e:
            logger.exception(e)
            self.result_queue.put_nowait((self.chain_id, e))


class ProgressBar:
    """
    Initialize progress bars using :class:`~tqdm.tqdm`.

    :param int warmup_steps: Number of warmup steps.
    :param int num_samples: Number of MCMC samples.
    :param int min_width: Minimum column width of the bar.
    :param int max_width: Maximum column width of the bar.
    :param bool disable: Disable progress bar.
    :param int num_bars: Number of progress bars to initialize.
        If multiple bars are initialized, they need to be separately
        updated via the ``pos`` kwarg.
    """

    def __init__(self, warmup_steps, num_samples, min_width=80, max_width=120, disable=False, num_bars=1):
        total_steps = warmup_steps + num_samples
        disable = disable or 'CI' in os.environ or 'PYTEST_XDIST_WORKER' in os.environ
        bar_format = '{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}, {rate_fmt}{postfix}]'
        pbar_cls = tqdm_nb if num_bars > 1 and ipython_env else tqdm
        self.progress_bars = []
        for i in range(num_bars):
            description = 'Warmup' if num_bars == 1 else 'Warmup [{}]'.format(i + 1)
            pbar = pbar_cls(total=total_steps, desc=description, bar_format=bar_format, position=i, file=sys.stderr, disable=disable)
            if getattr(pbar, 'ncols', None) is not None:
                pbar.ncols = max(min_width, pbar.ncols)
                pbar.ncols = min(max_width, pbar.ncols)
            self.progress_bars.append(pbar)
        self.disable = disable
        self.ipython_env = ipython_env

    def __enter__(self):
        return self

    def __exit__(self, *exc):
        self.close()
        return False

    def set_description(self, *args, **kwargs):
        pos = kwargs.pop('pos', 0)
        if not self.disable:
            self.progress_bars[pos].set_description(*args, **kwargs)

    def set_postfix(self, *args, **kwargs):
        pos = kwargs.pop('pos', 0)
        if not self.disable:
            self.progress_bars[pos].set_postfix(*args, **kwargs)

    def update(self, *args, **kwargs):
        pos = kwargs.pop('pos', 0)
        if not self.disable:
            self.progress_bars[pos].update(*args, **kwargs)

    def close(self):
        for pbar in self.progress_bars:
            pbar.close()
        if not self.ipython_env and not self.disable:
            sys.stderr.write('\n' * len(self.progress_bars))


def logger_thread(log_queue, warmup_steps, num_samples, num_chains, disable_progbar=False):
    """
    Logging thread that asynchronously consumes logging events from `log_queue`,
    and handles them appropriately.
    """
    progress_bars = ProgressBar(warmup_steps, num_samples, disable=disable_progbar, num_bars=num_chains)
    logger = logging.getLogger(__name__)
    logger.propagate = False
    logger.addHandler(TqdmHandler())
    num_samples = [0] * num_chains
    try:
        while True:
            try:
                record = log_queue.get(timeout=1)
            except queue.Empty:
                continue
            if record is None:
                break
            metadata, msg = record.getMessage().split(']', 1)
            _, msg_type, logger_id = metadata[1:].split()
            if msg_type == DIAGNOSTIC_MSG:
                pbar_pos = int(logger_id.split(':')[-1])
                num_samples[pbar_pos] += 1
                if num_samples[pbar_pos] == warmup_steps:
                    progress_bars.set_description('Sample [{}]'.format(pbar_pos + 1), pos=pbar_pos)
                diagnostics = json.loads(msg, object_pairs_hook=OrderedDict)
                progress_bars.set_postfix(diagnostics, pos=pbar_pos, refresh=False)
                progress_bars.update(pos=pbar_pos)
            else:
                logger.handle(record)
    finally:
        progress_bars.close()


class _MultiSampler:
    """
    Parallel runner class for running MCMC chains in parallel. This uses the
    `torch.multiprocessing` module (itself a light wrapper over the python
    `multiprocessing` module) to spin up parallel workers.
    """

    def __init__(self, kernel, num_samples, warmup_steps, num_chains, mp_context, disable_progbar, initial_params=None, hook=None):
        self.kernel = kernel
        self.warmup_steps = warmup_steps
        self.num_chains = num_chains
        self.hook = hook
        self.workers = []
        self.ctx = mp
        if mp_context:
            self.ctx = mp.get_context(mp_context)
        self.result_queue = self.ctx.Queue()
        self.log_queue = self.ctx.Queue()
        self.logger = initialize_logger(logging.getLogger('pyro.infer.mcmc'), 'MAIN', log_queue=self.log_queue)
        self.num_samples = num_samples
        self.initial_params = initial_params
        self.log_thread = threading.Thread(target=logger_thread, args=(self.log_queue, self.warmup_steps, self.num_samples, self.num_chains, disable_progbar))
        self.log_thread.daemon = True
        self.log_thread.start()
        self.events = [self.ctx.Event() for _ in range(num_chains)]

    def init_workers(self, *args, **kwargs):
        self.workers = []
        for i in range(self.num_chains):
            init_params = {k: v[i] for k, v in self.initial_params.items()} if self.initial_params is not None else None
            worker = _Worker(i, self.result_queue, self.log_queue, self.events[i], self.kernel, self.num_samples, self.warmup_steps, initial_params=init_params, hook=self.hook)
            worker.daemon = True
            self.workers.append(self.ctx.Process(name=str(i), target=worker.run, args=args, kwargs=kwargs))

    def terminate(self, terminate_workers=False):
        if self.log_thread.is_alive():
            self.log_queue.put_nowait(None)
            self.log_thread.join(timeout=1)
        if terminate_workers:
            for w in self.workers:
                if w.is_alive():
                    w.terminate()

    def run(self, *args, **kwargs):
        sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)
        self.init_workers(*args, **kwargs)
        signal.signal(signal.SIGINT, sigint_handler)
        active_workers = self.num_chains
        exc_raised = True
        try:
            for w in self.workers:
                w.start()
            while active_workers:
                try:
                    chain_id, val = self.result_queue.get(timeout=5)
                except queue.Empty:
                    continue
                if isinstance(val, Exception):
                    raise val
                if val is not None:
                    yield val, chain_id
                    self.events[chain_id].set()
                else:
                    active_workers -= 1
            exc_raised = False
        finally:
            self.terminate(terminate_workers=exc_raised)


class _UnarySampler:
    """
    Single process runner class optimized for the case chains are drawn sequentially.
    """

    def __init__(self, kernel, num_samples, warmup_steps, num_chains, disable_progbar, initial_params=None, hook=None):
        self.kernel = kernel
        self.initial_params = initial_params
        self.warmup_steps = warmup_steps
        self.num_samples = num_samples
        self.num_chains = num_chains
        self.logger = None
        self.disable_progbar = disable_progbar
        self.hook = hook
        super().__init__()

    def terminate(self, *args, **kwargs):
        pass

    def run(self, *args, **kwargs):
        logger = logging.getLogger('pyro.infer.mcmc')
        for i in range(self.num_chains):
            if self.initial_params is not None:
                initial_params = {k: v[i] for k, v in self.initial_params.items()}
                self.kernel.initial_params = initial_params
            progress_bar = ProgressBar(self.warmup_steps, self.num_samples, disable=self.disable_progbar)
            logger = initialize_logger(logger, '', progress_bar)
            hook_w_logging = _add_logging_hook(logger, progress_bar, self.hook)
            for sample in _gen_samples(self.kernel, self.warmup_steps, self.num_samples, hook_w_logging, i if self.num_chains > 1 else None, *args, **kwargs):
                yield sample, i
            self.kernel.cleanup()
            progress_bar.close()


def _safe(fn):
    """
    Safe version of utilities in the :mod:`pyro.ops.stats` module. Wrapped
    functions return `NaN` tensors instead of throwing exceptions.

    :param fn: stats function from :mod:`pyro.ops.stats` module.
    """

    @functools.wraps(fn)
    def wrapped(sample, *args, **kwargs):
        try:
            val = fn(sample, *args, **kwargs)
        except Exception:
            warnings.warn(tb.format_exc())
            val = torch.full(sample.shape[2:], float('nan'), dtype=sample.dtype, device=sample.device)
        return val
    return wrapped


def summary(samples, prob=0.9, group_by_chain=True):
    """
    Returns a summary table displaying diagnostics of ``samples`` from the
    posterior. The diagnostics displayed are mean, standard deviation, median,
    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,
    :func:`~pyro.ops.stats.split_gelman_rubin`.

    :param dict samples: dictionary of samples keyed by site name.
    :param float prob: the probability mass of samples within the credibility interval.
    :param bool group_by_chain: If True, each variable in `samples`
        will be treated as having shape `num_chains x num_samples x sample_shape`.
        Otherwise, the corresponding shape will be `num_samples x sample_shape`
        (i.e. without chain dimension).
    """
    if not group_by_chain:
        samples = {k: v.unsqueeze(0) for k, v in samples.items()}
    summary_dict = {}
    for name, value in samples.items():
        value_flat = torch.reshape(value, (-1,) + value.shape[2:])
        mean = value_flat.mean(dim=0)
        std = value_flat.std(dim=0)
        median = value_flat.median(dim=0)[0]
        hpdi = stats.hpdi(value_flat, prob=prob)
        n_eff = _safe(stats.effective_sample_size)(value)
        r_hat = stats.split_gelman_rubin(value)
        hpd_lower = '{:.1f}%'.format(50 * (1 - prob))
        hpd_upper = '{:.1f}%'.format(50 * (1 + prob))
        summary_dict[name] = OrderedDict([('mean', mean), ('std', std), ('median', median), (hpd_lower, hpdi[0]), (hpd_upper, hpdi[1]), ('n_eff', n_eff), ('r_hat', r_hat)])
    return summary_dict


def print_summary(samples, prob=0.9, group_by_chain=True):
    """
    Prints a summary table displaying diagnostics of ``samples`` from the
    posterior. The diagnostics displayed are mean, standard deviation, median,
    the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,
    :func:`~pyro.ops.stats.split_gelman_rubin`.

    :param dict samples: dictionary of samples keyed by site name.
    :param float prob: the probability mass of samples within the credibility interval.
    :param bool group_by_chain: If True, each variable in `samples`
        will be treated as having shape `num_chains x num_samples x sample_shape`.
        Otherwise, the corresponding shape will be `num_samples x sample_shape`
        (i.e. without chain dimension).
    """
    if len(samples) == 0:
        return
    summary_dict = summary(samples, prob, group_by_chain)
    row_names = {k: (k + '[' + ','.join(map(lambda x: str(x - 1), v.shape[2:])) + ']') for k, v in samples.items()}
    max_len = max(max(map(lambda x: len(x), row_names.values())), 10)
    name_format = '{:>' + str(max_len) + '}'
    header_format = name_format + ' {:>9}' * 7
    columns = [''] + list(list(summary_dict.values())[0].keys())
    None
    None
    row_format = name_format + ' {:>9.2f}' * 7
    for name, stats_dict in summary_dict.items():
        shape = stats_dict['mean'].shape
        if len(shape) == 0:
            None
        else:
            for idx in product(*map(range, shape)):
                idx_str = '[{}]'.format(','.join(map(str, idx)))
                None
    None


def select_samples(samples, num_samples=None, group_by_chain=False):
    """
    Performs selection from given MCMC samples.

    :param dictionary samples: Samples object to sample from.
    :param int num_samples: Number of samples to return. If `None`, all the samples
        from an MCMC chain are returned in their original ordering.
    :param bool group_by_chain: Whether to preserve the chain dimension. If True,
        all samples will have num_chains as the size of their leading dimension.
    :return: dictionary of samples keyed by site name.
    """
    if num_samples is None:
        if not group_by_chain:
            samples = {k: v.reshape((-1,) + v.shape[2:]) for k, v in samples.items()}
    else:
        if not samples:
            raise ValueError('No samples found from MCMC run.')
        if group_by_chain:
            batch_dim = 1
        else:
            samples = {k: v.reshape((-1,) + v.shape[2:]) for k, v in samples.items()}
            batch_dim = 0
        sample_tensor = list(samples.values())[0]
        batch_size, device = sample_tensor.shape[batch_dim], sample_tensor.device
        idxs = torch.randint(0, batch_size, size=(num_samples,), device=device)
        samples = {k: v.index_select(batch_dim, idxs) for k, v in samples.items()}
    return samples


class MCMC(AbstractMCMC):
    """
    Wrapper class for Markov Chain Monte Carlo algorithms. Specific MCMC algorithms
    are TraceKernel instances and need to be supplied as a ``kernel`` argument
    to the constructor.

    .. note:: The case of `num_chains > 1` uses python multiprocessing to
        run parallel chains in multiple processes. This goes with the usual
        caveats around multiprocessing in python, e.g. the model used to
        initialize the ``kernel`` must be serializable via `pickle`, and the
        performance / constraints will be platform dependent (e.g. only
        the "spawn" context is available in Windows). This has also not
        been extensively tested on the Windows platform.

    :param kernel: An instance of the ``TraceKernel`` class, which when
        given an execution trace returns another sample trace from the target
        (posterior) distribution.
    :param int num_samples: The number of samples that need to be generated,
        excluding the samples discarded during the warmup phase.
    :param int warmup_steps: Number of warmup iterations. The samples generated
        during the warmup phase are discarded. If not provided, default is
        is the same as `num_samples`.
    :param int num_chains: Number of MCMC chains to run in parallel. Depending on
        whether `num_chains` is 1 or more than 1, this class internally dispatches
        to either `_UnarySampler` or `_MultiSampler`.
    :param dict initial_params: dict containing initial tensors in unconstrained
        space to initiate the markov chain. The leading dimension's size must match
        that of `num_chains`. If not specified, parameter values will be sampled from
        the prior.
    :param hook_fn: Python callable that takes in `(kernel, samples, stage, i)`
        as arguments. stage is either `sample` or `warmup` and i refers to the
        i'th sample for the given stage. This can be used to implement additional
        logging, or more generally, run arbitrary code per generated sample.
    :param str mp_context: Multiprocessing context to use when `num_chains > 1`.
        Only applicable for Python 3.5 and above. Use `mp_context="spawn"` for
        CUDA.
    :param bool disable_progbar: Disable progress bar and diagnostics update.
    :param bool disable_validation: Disables distribution validation check.
        Defaults to ``True``, disabling validation, since divergent transitions
        will lead to exceptions. Switch to ``False`` to enable validation, or
        to ``None`` to preserve existing global values.
    :param dict transforms: dictionary that specifies a transform for a sample site
        with constrained support to unconstrained space.
    :param List[str] save_params: Optional list of a subset of parameter names to
        save during sampling and diagnostics. This is useful in models with
        large nuisance variables. Defaults to None, saving all params.
    """

    def __init__(self, kernel, num_samples, warmup_steps=None, initial_params=None, num_chains=1, hook_fn=None, mp_context=None, disable_progbar=False, disable_validation=True, transforms=None, save_params=None):
        super().__init__(kernel, num_chains, transforms)
        self.warmup_steps = num_samples if warmup_steps is None else warmup_steps
        self.num_samples = num_samples
        self.disable_validation = disable_validation
        self._samples = None
        self._args = None
        self._kwargs = None
        if save_params is not None:
            kernel.save_params = save_params
        self._validate_kernel(initial_params)
        parallel = False
        if num_chains > 1:
            if initial_params:
                self._validate_initial_params(initial_params)
                if mp_context is None:
                    if list(initial_params.values())[0].is_cuda:
                        mp_context = 'spawn'
            available_cpu = max(mp.cpu_count() - 1, 1)
            if num_chains <= available_cpu:
                parallel = True
            else:
                warnings.warn('num_chains={} is more than available_cpu={}. Chains will be drawn sequentially.'.format(num_chains, available_cpu))
        elif initial_params:
            initial_params = {k: v.unsqueeze(0) for k, v in initial_params.items()}
        self._diagnostics = [None] * num_chains
        if parallel:
            self.sampler = _MultiSampler(kernel, num_samples, self.warmup_steps, num_chains, mp_context, disable_progbar, initial_params=initial_params, hook=hook_fn)
        else:
            self.sampler = _UnarySampler(kernel, num_samples, self.warmup_steps, num_chains, disable_progbar, initial_params=initial_params, hook=hook_fn)

    @poutine.block
    def run(self, *args, **kwargs):
        """
        Run MCMC to generate samples and populate `self._samples`.

        Example usage:

        .. code-block:: python

            def model(data):
                ...

            nuts_kernel = NUTS(model)
            mcmc = MCMC(nuts_kernel, num_samples=500)
            mcmc.run(data)
            samples = mcmc.get_samples()

        :param args: optional arguments taken by
            :meth:`MCMCKernel.setup <pyro.infer.mcmc.mcmc_kernel.MCMCKernel.setup>`.
        :param kwargs: optional keywords arguments taken by
            :meth:`MCMCKernel.setup <pyro.infer.mcmc.mcmc_kernel.MCMCKernel.setup>`.
        """
        self._args, self._kwargs = args, kwargs
        num_samples = [0] * self.num_chains
        z_flat_acc = [[] for _ in range(self.num_chains)]
        with optional(pyro.validation_enabled(not self.disable_validation), self.disable_validation is not None):
            args = [(arg.detach() if torch.is_tensor(arg) else arg) for arg in args]
            for x, chain_id in self.sampler.run(*args, **kwargs):
                if num_samples[chain_id] == 0:
                    num_samples[chain_id] += 1
                    z_structure = x
                elif num_samples[chain_id] == self.num_samples + 1:
                    self._diagnostics[chain_id] = x
                else:
                    num_samples[chain_id] += 1
                    if self.num_chains > 1:
                        x_cloned = x.clone()
                        del x
                    else:
                        x_cloned = x
                    z_flat_acc[chain_id].append(x_cloned)
        z_flat_acc = torch.stack([torch.stack(l) for l in z_flat_acc])
        pos = 0
        z_acc = z_structure.copy()
        for k in sorted(z_structure):
            shape = z_structure[k]
            next_pos = pos + shape.numel()
            z_acc[k] = z_flat_acc[:, :, pos:next_pos].reshape((self.num_chains, self.num_samples) + shape)
            pos = next_pos
        assert pos == z_flat_acc.shape[-1]
        if self.transforms is None:
            self._set_transforms(*args, **kwargs)
        for name, z in z_acc.items():
            if name in self.transforms:
                z_acc[name] = self.transforms[name].inv(z)
        self._samples = z_acc
        self.sampler.terminate(True)

    def get_samples(self, num_samples=None, group_by_chain=False):
        """
        Get samples from the MCMC run, potentially resampling with replacement.

        For parameter details see: :meth:`select_samples <pyro.infer.mcmc.util.select_samples>`.
        """
        samples = self._samples
        return select_samples(samples, num_samples, group_by_chain)

    def diagnostics(self):
        """
        Gets some diagnostics statistics such as effective sample size, split
        Gelman-Rubin, or divergent transitions from the sampler.
        """
        diag = diagnostics(self._samples)
        for diag_name in self._diagnostics[0]:
            diag[diag_name] = {'chain {}'.format(i): self._diagnostics[i][diag_name] for i in range(self.num_chains)}
        return diag

    def summary(self, prob=0.9):
        """
        Prints a summary table displaying diagnostics of samples obtained from
        posterior. The diagnostics displayed are mean, standard deviation, median,
        the 90% Credibility Interval, :func:`~pyro.ops.stats.effective_sample_size`,
        :func:`~pyro.ops.stats.split_gelman_rubin`.

        :param float prob: the probability mass of samples within the credibility interval.
        """
        print_summary(self._samples, prob=prob)
        if 'divergences' in self._diagnostics[0]:
            None


class HMCForecaster(nn.Module):
    """
    Forecaster for a :class:`ForecastingModel` using Hamiltonian Monte Carlo.

    On initialization, this will run :class:`~pyro.infer.mcmc.nuts.NUTS`
    sampler to get posterior samples of the model.

    After construction, this can be called to generate sample forecasts.

    :param ForecastingModel model: A forecasting model subclass instance.
    :param data: A tensor dataset with time dimension -2.
    :type data: ~torch.Tensor
    :param covariates: A tensor of covariates with time dimension -2.
        For models not using covariates, pass a shaped empty tensor
        ``torch.empty(duration, 0)``.
    :type covariates: ~torch.Tensor
    :param int num_warmup: number of MCMC warmup steps.
    :param int num_samples: number of MCMC samples.
    :param int num_chains: number of parallel MCMC chains.
    :param bool dense_mass: a flag to control whether the mass matrix is dense
        or diagonal. Defaults to False.
    :param str time_reparam: If not None (default), reparameterize all
        time-dependent variables via the Haar wavelet transform (if "haar") or
        the discrete cosine transform (if "dct").
    :param bool jit_compile: whether to use the PyTorch JIT to trace the log
        density computation, and use this optimized executable trace in the
        integrator. Defaults to False.
    :param int max_tree_depth: Max depth of the binary tree created during the
        doubling scheme of the :class:`~pyro.infer.mcmc.nuts.NUTS` sampler.
        Defaults to 10.
    """

    def __init__(self, model, data, covariates=None, *, num_warmup=1000, num_samples=1000, num_chains=1, time_reparam=None, dense_mass=False, jit_compile=False, max_tree_depth=10):
        assert data.size(-2) == covariates.size(-2)
        super().__init__()
        if time_reparam == 'haar':
            model = poutine.reparam(model, time_reparam_haar)
        elif time_reparam == 'dct':
            model = poutine.reparam(model, time_reparam_dct)
        elif time_reparam is not None:
            raise ValueError('unknown time_reparam: {}'.format(time_reparam))
        self.model = model
        max_plate_nesting = _guess_max_plate_nesting(model, (data, covariates), {})
        self.max_plate_nesting = max(max_plate_nesting, 1)
        kernel = NUTS(model, full_mass=dense_mass, jit_compile=jit_compile, ignore_jit_warnings=True, max_tree_depth=max_tree_depth, max_plate_nesting=max_plate_nesting)
        mcmc = MCMC(kernel, warmup_steps=num_warmup, num_samples=num_samples, num_chains=num_chains)
        mcmc.run(data, covariates)
        if num_chains == 1 and num_samples >= 4 or num_chains > 1 and num_samples >= 2:
            mcmc.summary()
        with poutine.trace() as tr:
            with pyro.plate('particles', 1, dim=-self.max_plate_nesting - 1):
                model(data, covariates)
        self._trace = tr.trace
        self._samples = mcmc.get_samples()
        self._num_samples = num_samples * num_chains
        for name, node in list(self._trace.nodes.items()):
            if name not in self._samples:
                del self._trace.nodes[name]

    def __call__(self, data, covariates, num_samples, batch_size=None):
        """
        Samples forecasted values of data for time steps in ``[t1,t2)``, where
        ``t1 = data.size(-2)`` is the duration of observed data and ``t2 =
        covariates.size(-2)`` is the extended duration of covariates. For
        example to forecast 7 days forward conditioned on 30 days of
        observations, set ``t1=30`` and ``t2=37``.

        :param data: A tensor dataset with time dimension -2.
        :type data: ~torch.Tensor
        :param covariates: A tensor of covariates with time dimension -2.
            For models not using covariates, pass a shaped empty tensor
            ``torch.empty(duration, 0)``.
        :type covariates: ~torch.Tensor
        :param int num_samples: The number of samples to generate.
        :param int batch_size: Optional batch size for sampling. This is useful
            for generating many samples from models with large memory
            footprint. Defaults to ``num_samples``.
        :returns: A batch of joint posterior samples of shape
            ``(num_samples,1,...,1) + data.shape[:-2] + (t2-t1,data.size(-1))``,
            where the ``1``'s are inserted to avoid conflict with model plates.
        :rtype: ~torch.Tensor
        """
        return super().__call__(data, covariates, num_samples, batch_size)

    def forward(self, data, covariates, num_samples, batch_size=None):
        assert data.size(-2) <= covariates.size(-2)
        assert isinstance(num_samples, int) and num_samples > 0
        if batch_size is not None:
            batches = []
            while num_samples > 0:
                batch = self.forward(data, covariates, min(num_samples, batch_size))
                batches.append(batch)
                num_samples -= batch_size
            return torch.cat(batches)
        assert self.max_plate_nesting >= 1
        dim = -1 - self.max_plate_nesting
        with torch.no_grad():
            weights = torch.ones(self._num_samples, device=data.device)
            indices = torch.multinomial(weights, num_samples, replacement=num_samples > self._num_samples)
            for name, node in list(self._trace.nodes.items()):
                sample = self._samples[name].index_select(0, indices)
                node['value'] = sample.reshape((num_samples,) + (1,) * (node['value'].dim() - sample.dim()) + sample.shape[1:])
            with ExitStack() as stack:
                if data.size(-2) < covariates.size(-2):
                    stack.enter_context(PrefixReplayMessenger(self._trace))
                    stack.enter_context(PrefixConditionMessenger(self.model._prefix_condition_data))
                with pyro.plate('particles', num_samples, dim=dim):
                    return self.model(data, covariates)


class _SafeLog(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)
        return x.log()

    @staticmethod
    def backward(ctx, grad):
        x, = ctx.saved_tensors
        return grad / x.clamp(min=torch.finfo(x.dtype).eps)


def safe_log(x):
    """
    Like :func:`torch.log` but avoids infinite gradients at log(0)
    by clamping them to at most ``1 / finfo.eps``.
    """
    return _SafeLog.apply(x)


def _logmatmulexp(x, y):
    """
    Numerically stable version of ``(x.exp() @ y.exp()).log()``.
    """
    finfo = torch.finfo(x.dtype)
    x_shift = x.detach().max(-1, keepdim=True).values.clamp_(min=finfo.min)
    y_shift = y.detach().max(-2, keepdim=True).values.clamp_(min=finfo.min)
    xy = safe_log(torch.matmul((x - x_shift).exp(), (y - y_shift).exp()))
    return xy + x_shift + y_shift


def _sequential_logmatmulexp(logits):
    """
    For a tensor ``x`` whose time dimension is -3, computes::

        x[..., 0, :, :] @ x[..., 1, :, :] @ ... @ x[..., T-1, :, :]

    but does so numerically stably in log space.
    """
    batch_shape = logits.shape[:-3]
    state_dim = logits.size(-1)
    while logits.size(-3) > 1:
        time = logits.size(-3)
        even_time = time // 2 * 2
        even_part = logits[..., :even_time, :, :]
        x_y = even_part.reshape(batch_shape + (even_time // 2, 2, state_dim, state_dim))
        x, y = x_y.unbind(-3)
        contracted = _logmatmulexp(x, y)
        if time > even_time:
            contracted = torch.cat((contracted, logits[..., -1:, :, :]), dim=-3)
        logits = contracted
    return logits.squeeze(-3)


def mg2k(m, g, M):
    """Convert from (m, g) indexing to k indexing."""
    return m + M * g


class Profile(nn.Module):
    """
    Profile HMM state arrangement. Parameterizes an HMM according to
    Equation S40 in [1] (with r_{M+1,j} = 1 and u_{M+1,j} = 0
    for j in {0, 1, 2}). For further background on profile HMMs see [2].

    **References**

    [1] E. N. Weinstein, D. S. Marks (2021)
    "Generative probabilistic biological sequence models that account for
    mutational variability"
    https://www.biorxiv.org/content/10.1101/2020.07.31.231381v2.full.pdf

    [2] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison (1998)
    "Biological sequence analysis: probabilistic models of proteins and nucleic
    acids"
    Cambridge university press

    :param M: Length of regressor sequence.
    :type M: int
    :param epsilon: A small value for numerical stability.
    :type epsilon: float
    """

    def __init__(self, M, epsilon=1e-32):
        super().__init__()
        self.M = M
        self.K = 2 * M + 1
        self.epsilon = epsilon
        self._make_transfer()

    def _make_transfer(self):
        """Set up linear transformations (transfer matrices) for converting
        from profile HMM parameters to standard HMM parameters."""
        M, K = self.M, self.K
        self.register_buffer('r_transf_0', torch.zeros((M, 3, 2, K)))
        self.register_buffer('u_transf_0', torch.zeros((M, 3, 2, K)))
        self.register_buffer('null_transf_0', torch.zeros((K,)))
        m, g = -1, 0
        for gp in range(2):
            for mp in range(M + gp):
                kp = mg2k(mp, gp, M)
                if m + 1 - g == mp and gp == 0:
                    self.r_transf_0[m + 1 - g, g, 0, kp] = 1
                    self.u_transf_0[m + 1 - g, g, 0, kp] = 1
                elif m + 1 - g < mp and gp == 0:
                    self.r_transf_0[m + 1 - g, g, 0, kp] = 1
                    self.u_transf_0[m + 1 - g, g, 1, kp] = 1
                    for mpp in range(m + 2 - g, mp):
                        self.r_transf_0[mpp, 2, 0, kp] = 1
                        self.u_transf_0[mpp, 2, 1, kp] = 1
                    self.r_transf_0[mp, 2, 0, kp] = 1
                    self.u_transf_0[mp, 2, 0, kp] = 1
                elif m + 1 - g == mp and gp == 1:
                    if mp < M:
                        self.r_transf_0[m + 1 - g, g, 1, kp] = 1
                elif m + 1 - g < mp and gp == 1:
                    self.r_transf_0[m + 1 - g, g, 0, kp] = 1
                    self.u_transf_0[m + 1 - g, g, 1, kp] = 1
                    for mpp in range(m + 2 - g, mp):
                        self.r_transf_0[mpp, 2, 0, kp] = 1
                        self.u_transf_0[mpp, 2, 1, kp] = 1
                    if mp < M:
                        self.r_transf_0[mp, 2, 1, kp] = 1
                else:
                    self.null_transf_0[kp] = 1
        self.register_buffer('r_transf', torch.zeros((M, 3, 2, K, K)))
        self.register_buffer('u_transf', torch.zeros((M, 3, 2, K, K)))
        self.register_buffer('null_transf', torch.zeros((K, K)))
        for g in range(2):
            for m in range(M + g):
                for gp in range(2):
                    for mp in range(M + gp):
                        k, kp = mg2k(m, g, M), mg2k(mp, gp, M)
                        if m + 1 - g == mp and gp == 0:
                            self.r_transf[m + 1 - g, g, 0, k, kp] = 1
                            self.u_transf[m + 1 - g, g, 0, k, kp] = 1
                        elif m + 1 - g < mp and gp == 0:
                            self.r_transf[m + 1 - g, g, 0, k, kp] = 1
                            self.u_transf[m + 1 - g, g, 1, k, kp] = 1
                            self.r_transf[m + 2 - g:mp, 2, 0, k, kp] = 1
                            self.u_transf[m + 2 - g:mp, 2, 1, k, kp] = 1
                            self.r_transf[mp, 2, 0, k, kp] = 1
                            self.u_transf[mp, 2, 0, k, kp] = 1
                        elif m + 1 - g == mp and gp == 1:
                            if mp < M:
                                self.r_transf[m + 1 - g, g, 1, k, kp] = 1
                        elif m + 1 - g < mp and gp == 1:
                            self.r_transf[m + 1 - g, g, 0, k, kp] = 1
                            self.u_transf[m + 1 - g, g, 1, k, kp] = 1
                            self.r_transf[m + 2 - g:mp, 2, 0, k, kp] = 1
                            self.u_transf[m + 2 - g:mp, 2, 1, k, kp] = 1
                            if mp < M:
                                self.r_transf[mp, 2, 1, k, kp] = 1
                        else:
                            self.null_transf[k, kp] = 1
        self.register_buffer('vx_transf', torch.zeros((M, K)))
        self.register_buffer('vc_transf', torch.zeros((M + 1, K)))
        for g in range(2):
            for m in range(M + g):
                k = mg2k(m, g, M)
                if g == 0:
                    self.vx_transf[m, k] = 1
                elif g == 1:
                    self.vc_transf[m, k] = 1

    def forward(self, precursor_seq_logits, insert_seq_logits, insert_logits, delete_logits, substitute_logits=None):
        """
        Assemble HMM parameters given profile parameters.

        :param ~torch.Tensor precursor_seq_logits: Regressor sequence
            *log(x)*. Should have rightmost dimension ``(M, D)`` and be
            broadcastable to ``(batch_size, M, D)``, where
            D is the latent alphabet size. Should be normalized to one along the
            final axis, i.e. ``precursor_seq_logits.logsumexp(-1) = zeros``.
        :param ~torch.Tensor insert_seq_logits: Insertion sequence *log(c)*.
            Should have rightmost dimension ``(M+1, D)`` and be broadcastable
            to ``(batch_size, M+1, D)``. Should be normalized
            along the final axis.
        :param ~torch.Tensor insert_logits: Insertion probabilities *log(r)*.
            Should have rightmost dimension ``(M, 3, 2)`` and be broadcastable
            to ``(batch_size, M, 3, 2)``. Should be normalized along the
            final axis.
        :param ~torch.Tensor delete_logits: Deletion probabilities *log(u)*.
            Should have rightmost dimension ``(M, 3, 2)`` and be broadcastable
            to ``(batch_size, M, 3, 2)``. Should be normalized along the
            final axis.
        :param ~torch.Tensor substitute_logits: Substitution probabilities
            *log(l)*. Should have rightmost dimension ``(D, B)``, where
            B is the alphabet size of the data, and broadcastable to
            ``(batch_size, D, B)``. Must be normalized along the
            final axis.
        :return: *initial_logits*, *transition_logits*, and
            *observation_logits*. These parameters can be used to directly
            initialize the MissingDataDiscreteHMM distribution.
        :rtype: ~torch.Tensor, ~torch.Tensor, ~torch.Tensor
        """
        initial_logits = torch.einsum('...ijk,ijkl->...l', delete_logits, self.u_transf_0) + torch.einsum('...ijk,ijkl->...l', insert_logits, self.r_transf_0) + -1 / self.epsilon * self.null_transf_0
        transition_logits = torch.einsum('...ijk,ijklf->...lf', delete_logits, self.u_transf) + torch.einsum('...ijk,ijklf->...lf', insert_logits, self.r_transf) + -1 / self.epsilon * self.null_transf
        if len(precursor_seq_logits.size()) > len(insert_seq_logits.size()):
            insert_seq_logits = insert_seq_logits.unsqueeze(0).expand([precursor_seq_logits.size()[0], -1, -1])
        elif len(insert_seq_logits.size()) > len(precursor_seq_logits.size()):
            precursor_seq_logits = precursor_seq_logits.unsqueeze(0).expand([insert_seq_logits.size()[0], -1, -1])
        seq_logits = torch.cat([precursor_seq_logits, insert_seq_logits], dim=-2)
        if substitute_logits is not None:
            observation_logits = torch.logsumexp(seq_logits.unsqueeze(-1) + substitute_logits.unsqueeze(-3), dim=-2)
        else:
            observation_logits = seq_logits
        return initial_logits, transition_logits, observation_logits


class ProfileHMM(nn.Module):
    """
    Profile HMM.

    This model consists of a constant distribution (a delta function) over the
    regressor sequence, plus a MuE observation distribution. The priors
    are all Normal distributions, and are pushed through a softmax function
    onto the simplex.

    :param int latent_seq_length: Length of the latent regressor sequence M.
        Must be greater than or equal to 1.
    :param int alphabet_length: Length of the sequence alphabet (e.g. 20 for
        amino acids).
    :param float prior_scale: Standard deviation of the prior distribution.
    :param float indel_prior_bias: Mean of the prior distribution over the
        log probability of an indel not occurring. Higher values lead to lower
        probability of indels.
    :param bool cuda: Transfer data onto the GPU during training.
    :param bool pin_memory: Pin memory for faster GPU transfer.
    """

    def __init__(self, latent_seq_length, alphabet_length, prior_scale=1.0, indel_prior_bias=10.0, cuda=False, pin_memory=False):
        super().__init__()
        assert isinstance(cuda, bool)
        self.is_cuda = cuda
        assert isinstance(pin_memory, bool)
        self.pin_memory = pin_memory
        assert isinstance(latent_seq_length, int) and latent_seq_length > 0
        self.latent_seq_length = latent_seq_length
        assert isinstance(alphabet_length, int) and alphabet_length > 0
        self.alphabet_length = alphabet_length
        self.precursor_seq_shape = latent_seq_length, alphabet_length
        self.insert_seq_shape = latent_seq_length + 1, alphabet_length
        self.indel_shape = latent_seq_length, 3, 2
        assert isinstance(prior_scale, float)
        self.prior_scale = prior_scale
        assert isinstance(indel_prior_bias, float)
        self.indel_prior = torch.tensor([indel_prior_bias, 0.0])
        self.statearrange = Profile(latent_seq_length)

    def model(self, seq_data, local_scale):
        precursor_seq = pyro.sample('precursor_seq', dist.Normal(torch.zeros(self.precursor_seq_shape), self.prior_scale * torch.ones(self.precursor_seq_shape)).to_event(2))
        precursor_seq_logits = precursor_seq - precursor_seq.logsumexp(-1, True)
        insert_seq = pyro.sample('insert_seq', dist.Normal(torch.zeros(self.insert_seq_shape), self.prior_scale * torch.ones(self.insert_seq_shape)).to_event(2))
        insert_seq_logits = insert_seq - insert_seq.logsumexp(-1, True)
        insert = pyro.sample('insert', dist.Normal(self.indel_prior * torch.ones(self.indel_shape), self.prior_scale * torch.ones(self.indel_shape)).to_event(3))
        insert_logits = insert - insert.logsumexp(-1, True)
        delete = pyro.sample('delete', dist.Normal(self.indel_prior * torch.ones(self.indel_shape), self.prior_scale * torch.ones(self.indel_shape)).to_event(3))
        delete_logits = delete - delete.logsumexp(-1, True)
        initial_logits, transition_logits, observation_logits = self.statearrange(precursor_seq_logits, insert_seq_logits, insert_logits, delete_logits)
        with pyro.plate('batch', seq_data.shape[0]):
            with poutine.scale(scale=local_scale):
                pyro.sample('obs_seq', MissingDataDiscreteHMM(initial_logits, transition_logits, observation_logits), obs=seq_data)

    def guide(self, seq_data, local_scale):
        precursor_seq_q_mn = pyro.param('precursor_seq_q_mn', torch.zeros(self.precursor_seq_shape))
        precursor_seq_q_sd = pyro.param('precursor_seq_q_sd', torch.zeros(self.precursor_seq_shape))
        pyro.sample('precursor_seq', dist.Normal(precursor_seq_q_mn, softplus(precursor_seq_q_sd)).to_event(2))
        insert_seq_q_mn = pyro.param('insert_seq_q_mn', torch.zeros(self.insert_seq_shape))
        insert_seq_q_sd = pyro.param('insert_seq_q_sd', torch.zeros(self.insert_seq_shape))
        pyro.sample('insert_seq', dist.Normal(insert_seq_q_mn, softplus(insert_seq_q_sd)).to_event(2))
        insert_q_mn = pyro.param('insert_q_mn', torch.ones(self.indel_shape) * self.indel_prior)
        insert_q_sd = pyro.param('insert_q_sd', torch.zeros(self.indel_shape))
        pyro.sample('insert', dist.Normal(insert_q_mn, softplus(insert_q_sd)).to_event(3))
        delete_q_mn = pyro.param('delete_q_mn', torch.ones(self.indel_shape) * self.indel_prior)
        delete_q_sd = pyro.param('delete_q_sd', torch.zeros(self.indel_shape))
        pyro.sample('delete', dist.Normal(delete_q_mn, softplus(delete_q_sd)).to_event(3))

    def fit_svi(self, dataset, epochs=2, batch_size=1, scheduler=None, jit=False):
        """
        Infer approximate posterior with stochastic variational inference.

        This runs :class:`~pyro.infer.svi.SVI`. It is an approximate inference
        method useful for quickly iterating on probabilistic models.

        :param ~torch.utils.data.Dataset dataset: The training dataset.
        :param int epochs: Number of epochs of training.
        :param int batch_size: Minibatch size (number of sequences).
        :param pyro.optim.MultiStepLR scheduler: Optimization scheduler.
            (Default: Adam optimizer, 0.01 constant learning rate.)
        :param bool jit: Whether to use a jit compiled ELBO.
        """
        if batch_size is not None:
            self.batch_size = batch_size
        if scheduler is None:
            scheduler = MultiStepLR({'optimizer': Adam, 'optim_args': {'lr': 0.01}, 'milestones': [], 'gamma': 0.5})
        if self.is_cuda:
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        self.guide(None, None)
        dataload = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=self.pin_memory, generator=torch.Generator(device=device))
        if jit:
            elbo = JitTrace_ELBO(ignore_jit_warnings=True)
        else:
            elbo = Trace_ELBO()
        svi = SVI(self.model, self.guide, scheduler, loss=elbo)
        losses = []
        t0 = datetime.datetime.now()
        for epoch in range(epochs):
            for seq_data, L_data in dataload:
                if self.is_cuda:
                    seq_data = seq_data
                loss = svi.step(seq_data, torch.tensor(len(dataset) / seq_data.shape[0]))
                losses.append(loss)
                scheduler.step()
            None
        return losses

    def evaluate(self, dataset_train, dataset_test=None, jit=False):
        """
        Evaluate performance (log probability and per residue perplexity) on
        train and test datasets.

        :param ~torch.utils.data.Dataset dataset: The training dataset.
        :param ~torch.utils.data.Dataset dataset: The testing dataset.
        :param bool jit: Whether to use a jit compiled ELBO.
        """
        dataload_train = DataLoader(dataset_train, batch_size=1, shuffle=False)
        if dataset_test is not None:
            dataload_test = DataLoader(dataset_test, batch_size=1, shuffle=False)
        self.guide(None, None)
        if jit:
            elbo = JitTrace_ELBO(ignore_jit_warnings=True)
        else:
            elbo = Trace_ELBO()
        scheduler = MultiStepLR({'optimizer': Adam, 'optim_args': {'lr': 0.01}})
        svi = SVI(self.model, self.guide, scheduler, loss=elbo)
        train_lp, train_perplex = self._evaluate_local_elbo(svi, dataload_train, len(dataset_train))
        if dataset_test is not None:
            test_lp, test_perplex = self._evaluate_local_elbo(svi, dataload_test, len(dataset_test))
            return train_lp, test_lp, train_perplex, test_perplex
        else:
            return train_lp, None, train_perplex, None

    def _local_variables(self, name, site):
        """Return per datapoint random variables in model."""
        return name in ['obs_L', 'obs_seq']

    def _evaluate_local_elbo(self, svi, dataload, data_size):
        """Evaluate elbo and average per residue perplexity."""
        lp, perplex = 0.0, 0.0
        with torch.no_grad():
            for seq_data, L_data in dataload:
                if self.is_cuda:
                    seq_data, L_data = seq_data, L_data
                conditioned_model = poutine.condition(self.model, data={'obs_seq': seq_data})
                args = seq_data, torch.tensor(1.0)
                guide_tr = poutine.trace(self.guide).get_trace(*args)
                model_tr = poutine.trace(poutine.replay(conditioned_model, trace=guide_tr)).get_trace(*args)
                local_elbo = (model_tr.log_prob_sum(self._local_variables) - guide_tr.log_prob_sum(self._local_variables)).cpu().numpy()
                lp += local_elbo
                perplex += -local_elbo / L_data[0].cpu().numpy()
        perplex = np.exp(perplex / data_size)
        return lp, perplex


class FactorMuE(nn.Module):
    """
    FactorMuE

    This model consists of probabilistic PCA plus a MuE output distribution.

    The priors are all Normal distributions, and where relevant pushed through
    a softmax onto the simplex.

    :param int data_length: Length of the input sequence matrix, including
        zero padding at the end.
    :param int alphabet_length: Length of the sequence alphabet (e.g. 20 for
        amino acids).
    :param int z_dim: Number of dimensions of the z space.
    :param int batch_size: Minibatch size.
    :param int latent_seq_length: Length of the latent regressor sequence (M).
        Must be greater than or equal to 1. (Default: 1.1 x data_length.)
    :param bool indel_factor_dependence: Indel probabilities depend on the
        latent variable z.
    :param float indel_prior_scale: Standard deviation of the prior
        distribution on indel parameters.
    :param float indel_prior_bias: Mean of the prior distribution over the
        log probability of an indel not occurring. Higher values lead to lower
        probability of indels.
    :param float inverse_temp_prior: Mean of the prior distribution over the
        inverse temperature parameter.
    :param float weights_prior_scale: Standard deviation of the prior
        distribution over the factors.
    :param float offset_prior_scale: Standard deviation of the prior
        distribution over the offset (constant) in the pPCA model.
    :param str z_prior_distribution: Prior distribution over the latent
        variable z. Either 'Normal' (pPCA model) or 'Laplace' (an ICA model).
    :param bool ARD_prior: Use automatic relevance determination prior on
        factors.
    :param bool substitution_matrix: Use a learnable substitution matrix
        rather than the identity matrix.
    :param float substitution_prior_scale: Standard deviation of the prior
        distribution over substitution matrix parameters (when
        substitution_matrix is True).
    :param int latent_alphabet_length: Length of the alphabet in the latent
        regressor sequence.
    :param bool cuda: Transfer data onto the GPU during training.
    :param bool pin_memory: Pin memory for faster GPU transfer.
    :param float epsilon: A small value for numerical stability.
    """

    def __init__(self, data_length, alphabet_length, z_dim, batch_size=10, latent_seq_length=None, indel_factor_dependence=False, indel_prior_scale=1.0, indel_prior_bias=10.0, inverse_temp_prior=100.0, weights_prior_scale=1.0, offset_prior_scale=1.0, z_prior_distribution='Normal', ARD_prior=False, substitution_matrix=True, substitution_prior_scale=10.0, latent_alphabet_length=None, cuda=False, pin_memory=False, epsilon=1e-32):
        super().__init__()
        assert isinstance(cuda, bool)
        self.is_cuda = cuda
        assert isinstance(pin_memory, bool)
        self.pin_memory = pin_memory
        assert isinstance(data_length, int) and data_length > 0
        self.data_length = data_length
        if latent_seq_length is None:
            latent_seq_length = int(data_length * 1.1)
        else:
            assert isinstance(latent_seq_length, int) and latent_seq_length > 0
        self.latent_seq_length = latent_seq_length
        assert isinstance(alphabet_length, int) and alphabet_length > 0
        self.alphabet_length = alphabet_length
        assert isinstance(z_dim, int) and z_dim > 0
        self.z_dim = z_dim
        if not substitution_matrix or latent_alphabet_length is None:
            latent_alphabet_length = alphabet_length
        self.latent_alphabet_length = latent_alphabet_length
        self.indel_shape = latent_seq_length, 3, 2
        self.total_factor_size = (2 * latent_seq_length + 1) * latent_alphabet_length + 2 * indel_factor_dependence * latent_seq_length * 3 * 2
        self.indel_factor_dependence = indel_factor_dependence
        self.ARD_prior = ARD_prior
        self.substitution_matrix = substitution_matrix
        assert isinstance(indel_prior_scale, float)
        self.indel_prior_scale = torch.tensor(indel_prior_scale)
        assert isinstance(indel_prior_bias, float)
        self.indel_prior = torch.tensor([indel_prior_bias, 0.0])
        assert isinstance(inverse_temp_prior, float)
        self.inverse_temp_prior = torch.tensor(inverse_temp_prior)
        assert isinstance(weights_prior_scale, float)
        self.weights_prior_scale = torch.tensor(weights_prior_scale)
        assert isinstance(offset_prior_scale, float)
        self.offset_prior_scale = torch.tensor(offset_prior_scale)
        assert isinstance(epsilon, float)
        self.epsilon = torch.tensor(epsilon)
        assert isinstance(substitution_prior_scale, float)
        self.substitution_prior_scale = torch.tensor(substitution_prior_scale)
        self.z_prior_distribution = z_prior_distribution
        assert isinstance(batch_size, int)
        self.batch_size = batch_size
        self.encoder = Encoder(data_length, alphabet_length, z_dim)
        self.statearrange = Profile(latent_seq_length)

    def decoder(self, z, W, B, inverse_temp):
        v = torch.mm(z, W) + B
        out = dict()
        if self.indel_factor_dependence:
            ind0 = (2 * self.latent_seq_length + 1) * self.latent_alphabet_length
            ind1 = ind0 + self.latent_seq_length * 3 * 2
            ind2 = ind1 + self.latent_seq_length * 3 * 2
            insert_v, delete_v = v[:, ind0:ind1], v[:, ind1:ind2]
            insert_v = insert_v.reshape([-1, self.latent_seq_length, 3, 2]) + self.indel_prior
            out['insert_logits'] = insert_v - insert_v.logsumexp(-1, True)
            delete_v = delete_v.reshape([-1, self.latent_seq_length, 3, 2]) + self.indel_prior
            out['delete_logits'] = delete_v - delete_v.logsumexp(-1, True)
        ind0 = self.latent_seq_length * self.latent_alphabet_length
        ind1 = ind0 + (self.latent_seq_length + 1) * self.latent_alphabet_length
        precursor_seq_v, insert_seq_v = v[:, :ind0], v[:, ind0:ind1]
        precursor_seq_v = (precursor_seq_v * softplus(inverse_temp)).reshape([-1, self.latent_seq_length, self.latent_alphabet_length])
        out['precursor_seq_logits'] = precursor_seq_v - precursor_seq_v.logsumexp(-1, True)
        insert_seq_v = (insert_seq_v * softplus(inverse_temp)).reshape([-1, self.latent_seq_length + 1, self.latent_alphabet_length])
        out['insert_seq_logits'] = insert_seq_v - insert_seq_v.logsumexp(-1, True)
        return out

    def model(self, seq_data, local_scale, local_prior_scale):
        if self.ARD_prior:
            alpha = pyro.sample('alpha', dist.Gamma(torch.ones(self.z_dim), torch.ones(self.z_dim)).to_event(1))
        else:
            alpha = torch.ones(self.z_dim)
        W = pyro.sample('W', dist.Normal(torch.zeros([self.z_dim, self.total_factor_size]), torch.ones([self.z_dim, self.total_factor_size]) * self.weights_prior_scale / (alpha[:, None] + self.epsilon)).to_event(2))
        B = pyro.sample('B', dist.Normal(torch.zeros(self.total_factor_size), torch.ones(self.total_factor_size) * self.offset_prior_scale).to_event(1))
        if not self.indel_factor_dependence:
            insert = pyro.sample('insert', dist.Normal(self.indel_prior * torch.ones(self.indel_shape), self.indel_prior_scale * torch.ones(self.indel_shape)).to_event(3))
            insert_logits = insert - insert.logsumexp(-1, True)
            delete = pyro.sample('delete', dist.Normal(self.indel_prior * torch.ones(self.indel_shape), self.indel_prior_scale * torch.ones(self.indel_shape)).to_event(3))
            delete_logits = delete - delete.logsumexp(-1, True)
        inverse_temp = pyro.sample('inverse_temp', dist.Normal(self.inverse_temp_prior, torch.tensor(1.0)))
        if self.substitution_matrix:
            substitute = pyro.sample('substitute', dist.Normal(torch.zeros([self.latent_alphabet_length, self.alphabet_length]), self.substitution_prior_scale * torch.ones([self.latent_alphabet_length, self.alphabet_length])).to_event(2))
        with pyro.plate('batch', seq_data.shape[0]):
            with poutine.scale(scale=local_scale):
                with poutine.scale(scale=local_prior_scale):
                    if self.z_prior_distribution == 'Normal':
                        z = pyro.sample('latent', dist.Normal(torch.zeros(self.z_dim), torch.ones(self.z_dim)).to_event(1))
                    elif self.z_prior_distribution == 'Laplace':
                        z = pyro.sample('latent', dist.Laplace(torch.zeros(self.z_dim), torch.ones(self.z_dim)).to_event(1))
                decoded = self.decoder(z, W, B, inverse_temp)
                if self.indel_factor_dependence:
                    insert_logits = decoded['insert_logits']
                    delete_logits = decoded['delete_logits']
                if self.substitution_matrix:
                    initial_logits, transition_logits, observation_logits = self.statearrange(decoded['precursor_seq_logits'], decoded['insert_seq_logits'], insert_logits, delete_logits, substitute)
                else:
                    initial_logits, transition_logits, observation_logits = self.statearrange(decoded['precursor_seq_logits'], decoded['insert_seq_logits'], insert_logits, delete_logits)
                pyro.sample('obs_seq', MissingDataDiscreteHMM(initial_logits, transition_logits, observation_logits), obs=seq_data)

    def guide(self, seq_data, local_scale, local_prior_scale):
        pyro.module('encoder', self.encoder)
        if self.ARD_prior:
            alpha_conc = pyro.param('alpha_conc', torch.randn(self.z_dim))
            alpha_rate = pyro.param('alpha_rate', torch.randn(self.z_dim))
            pyro.sample('alpha', dist.Gamma(softplus(alpha_conc), softplus(alpha_rate)).to_event(1))
        W_q_mn = pyro.param('W_q_mn', torch.randn([self.z_dim, self.total_factor_size]))
        W_q_sd = pyro.param('W_q_sd', torch.ones([self.z_dim, self.total_factor_size]))
        pyro.sample('W', dist.Normal(W_q_mn, softplus(W_q_sd)).to_event(2))
        B_q_mn = pyro.param('B_q_mn', torch.randn(self.total_factor_size))
        B_q_sd = pyro.param('B_q_sd', torch.ones(self.total_factor_size))
        pyro.sample('B', dist.Normal(B_q_mn, softplus(B_q_sd)).to_event(1))
        if not self.indel_factor_dependence:
            insert_q_mn = pyro.param('insert_q_mn', torch.ones(self.indel_shape) * self.indel_prior)
            insert_q_sd = pyro.param('insert_q_sd', torch.zeros(self.indel_shape))
            pyro.sample('insert', dist.Normal(insert_q_mn, softplus(insert_q_sd)).to_event(3))
            delete_q_mn = pyro.param('delete_q_mn', torch.ones(self.indel_shape) * self.indel_prior)
            delete_q_sd = pyro.param('delete_q_sd', torch.zeros(self.indel_shape))
            pyro.sample('delete', dist.Normal(delete_q_mn, softplus(delete_q_sd)).to_event(3))
        inverse_temp_q_mn = pyro.param('inverse_temp_q_mn', torch.tensor(0.0))
        inverse_temp_q_sd = pyro.param('inverse_temp_q_sd', torch.tensor(0.0))
        pyro.sample('inverse_temp', dist.Normal(inverse_temp_q_mn, softplus(inverse_temp_q_sd)))
        if self.substitution_matrix:
            substitute_q_mn = pyro.param('substitute_q_mn', torch.zeros([self.latent_alphabet_length, self.alphabet_length]))
            substitute_q_sd = pyro.param('substitute_q_sd', torch.zeros([self.latent_alphabet_length, self.alphabet_length]))
            pyro.sample('substitute', dist.Normal(substitute_q_mn, softplus(substitute_q_sd)).to_event(2))
        with pyro.plate('batch', seq_data.shape[0]):
            z_loc, z_scale = self.encoder(seq_data)
            with poutine.scale(scale=local_scale * local_prior_scale):
                if self.z_prior_distribution == 'Normal':
                    pyro.sample('latent', dist.Normal(z_loc, z_scale).to_event(1))
                elif self.z_prior_distribution == 'Laplace':
                    pyro.sample('latent', dist.Laplace(z_loc, z_scale).to_event(1))

    def fit_svi(self, dataset, epochs=2, anneal_length=1.0, batch_size=None, scheduler=None, jit=False):
        """
        Infer approximate posterior with stochastic variational inference.

        This runs :class:`~pyro.infer.svi.SVI`. It is an approximate inference
        method useful for quickly iterating on probabilistic models.

        :param ~torch.utils.data.Dataset dataset: The training dataset.
        :param int epochs: Number of epochs of training.
        :param float anneal_length: Number of epochs over which to linearly
            anneal the prior KL divergence weight from 0 to 1, for improved
            training.
        :param int batch_size: Minibatch size (number of sequences).
        :param pyro.optim.MultiStepLR scheduler: Optimization scheduler.
            (Default: Adam optimizer, 0.01 constant learning rate.)
        :param bool jit: Whether to use a jit compiled ELBO.
        """
        if batch_size is not None:
            self.batch_size = batch_size
        if scheduler is None:
            scheduler = MultiStepLR({'optimizer': Adam, 'optim_args': {'lr': 0.01}, 'milestones': [], 'gamma': 0.5})
        if self.is_cuda:
            device = torch.device('cuda')
        else:
            device = torch.device('cpu')
        dataload = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=self.pin_memory, generator=torch.Generator(device=device))
        for seq_data, L_data in dataload:
            if self.is_cuda:
                seq_data = seq_data
            self.guide(seq_data, torch.tensor(1.0), torch.tensor(1.0))
            break
        if jit:
            elbo = JitTrace_ELBO(ignore_jit_warnings=True)
        else:
            elbo = Trace_ELBO()
        svi = SVI(self.model, self.guide, scheduler, loss=elbo)
        losses = []
        step_i = 1
        t0 = datetime.datetime.now()
        for epoch in range(epochs):
            for seq_data, L_data in dataload:
                if self.is_cuda:
                    seq_data = seq_data
                loss = svi.step(seq_data, torch.tensor(len(dataset) / seq_data.shape[0]), self._beta_anneal(step_i, batch_size, len(dataset), anneal_length))
                losses.append(loss)
                scheduler.step()
                step_i += 1
            None
        return losses

    def _beta_anneal(self, step, batch_size, data_size, anneal_length):
        """Annealing schedule for prior KL term (beta annealing)."""
        if np.allclose(anneal_length, 0.0):
            return torch.tensor(1.0)
        anneal_frac = step * batch_size / (anneal_length * data_size)
        return torch.tensor(min([anneal_frac, 1.0]))

    def evaluate(self, dataset_train, dataset_test=None, jit=False):
        """
        Evaluate performance (log probability and per residue perplexity) on
        train and test datasets.

        :param ~torch.utils.data.Dataset dataset: The training dataset.
        :param ~torch.utils.data.Dataset dataset: The testing dataset
            (optional).
        :param bool jit: Whether to use a jit compiled ELBO.
        """
        dataload_train = DataLoader(dataset_train, batch_size=1, shuffle=False)
        if dataset_test is not None:
            dataload_test = DataLoader(dataset_test, batch_size=1, shuffle=False)
        for seq_data, L_data in dataload_train:
            if self.is_cuda:
                seq_data = seq_data
            self.guide(seq_data, torch.tensor(1.0), torch.tensor(1.0))
            break
        if jit:
            elbo = JitTrace_ELBO(ignore_jit_warnings=True)
        else:
            elbo = Trace_ELBO()
        scheduler = MultiStepLR({'optimizer': Adam, 'optim_args': {'lr': 0.01}})
        svi = SVI(self.model, self.guide, scheduler, loss=elbo)
        train_lp, train_perplex = self._evaluate_local_elbo(svi, dataload_train, len(dataset_train))
        if dataset_test is not None:
            test_lp, test_perplex = self._evaluate_local_elbo(svi, dataload_test, len(dataset_test))
            return train_lp, test_lp, train_perplex, test_perplex
        else:
            return train_lp, None, train_perplex, None

    def _local_variables(self, name, site):
        """Return per datapoint random variables in model."""
        return name in ['latent', 'obs_L', 'obs_seq']

    def _evaluate_local_elbo(self, svi, dataload, data_size):
        """Evaluate elbo and average per residue perplexity."""
        lp, perplex = 0.0, 0.0
        with torch.no_grad():
            for seq_data, L_data in dataload:
                if self.is_cuda:
                    seq_data, L_data = seq_data, L_data
                conditioned_model = poutine.condition(self.model, data={'obs_seq': seq_data})
                args = seq_data, torch.tensor(1.0), torch.tensor(1.0)
                guide_tr = poutine.trace(self.guide).get_trace(*args)
                model_tr = poutine.trace(poutine.replay(conditioned_model, trace=guide_tr)).get_trace(*args)
                local_elbo = (model_tr.log_prob_sum(self._local_variables) - guide_tr.log_prob_sum(self._local_variables)).cpu().numpy()
                lp += local_elbo
                perplex += -local_elbo / L_data[0].cpu().numpy()
        perplex = np.exp(perplex / data_size)
        return lp, perplex

    def embed(self, dataset, batch_size=None):
        """
        Get the latent space embedding (mean posterior value of z).

        :param ~torch.utils.data.Dataset dataset: The dataset to embed.
        :param int batch_size: Minibatch size (number of sequences). (Defaults
            to batch_size of the model object.)
        """
        if batch_size is None:
            batch_size = self.batch_size
        dataload = DataLoader(dataset, batch_size=batch_size, shuffle=False)
        with torch.no_grad():
            z_locs, z_scales = [], []
            for seq_data, L_data in dataload:
                if self.is_cuda:
                    seq_data = seq_data
                z_loc, z_scale = self.encoder(seq_data)
                z_locs.append(z_loc.cpu())
                z_scales.append(z_scale.cpu())
        return torch.cat(z_locs), torch.cat(z_scales)

    def _reconstruct_regressor_seq(self, data, ind, param):
        """Reconstruct the latent regressor sequence given data."""
        with torch.no_grad():
            z_loc = self.encoder(data[ind][0])[0]
            decoded = self.decoder(z_loc, param('W_q_mn'), param('B_q_mn'), param('inverse_temp_q_mn'))
            return torch.exp(decoded['precursor_seq_logits'])


def lexpand(A, *dimensions):
    """Expand tensor, adding new dimensions on left."""
    return A.expand(tuple(dimensions) + A.shape)


def rmv(A, b):
    """Tensorized matrix vector multiplication of rightmost dimensions."""
    return torch.matmul(A, b.unsqueeze(-1)).squeeze(-1)


def rtril(M, diagonal=0, upper=False):
    """Takes the lower-triangular of the rightmost 2 dimensions."""
    if upper:
        return rtril(M, diagonal=diagonal, upper=False).transpose(-1, -2)
    return M * torch.tril(torch.ones(M.shape[-2], M.shape[-1]), diagonal=diagonal)


class LinearModelPosteriorGuide(nn.Module):

    def __init__(self, d, w_sizes, y_sizes, regressor_init=0.0, scale_tril_init=3.0, use_softplus=True, **kwargs):
        """
        Guide for linear models. No amortisation happens over designs.
        Amortisation over data is taken care of by analytic formulae for
        linear models (heavy use of truth).

        :param tuple d: the shape by which to expand the guide parameters, e.g. `(num_batches, num_designs)`.
        :param dict w_sizes: map from variable string names to int, indicating the dimension of each
                             weight vector in the linear model.
        :param float regressor_init: initial value for the regressor matrix used to learn the posterior mean.
        :param float scale_tril_init: initial value for posterior `scale_tril` parameter.
        :param bool use_softplus: whether to transform the regressor by a softplus transform: useful if the
                                  regressor should be nonnegative but close to zero.
        """
        super().__init__()
        if not isinstance(d, (tuple, list, torch.Tensor)):
            d = d,
        self.regressor = nn.ParameterDict({l: nn.Parameter(regressor_init * torch.ones(*(d + (p, sum(y_sizes.values()))))) for l, p in w_sizes.items()})
        self.scale_tril = nn.ParameterDict({l: nn.Parameter(scale_tril_init * lexpand(torch.eye(p), *d)) for l, p in w_sizes.items()})
        self.w_sizes = w_sizes
        self.use_softplus = use_softplus
        self.softplus = nn.Softplus()

    def get_params(self, y_dict, design, target_labels):
        y = torch.cat(list(y_dict.values()), dim=-1)
        return self.linear_model_formula(y, design, target_labels)

    def linear_model_formula(self, y, design, target_labels):
        if self.use_softplus:
            mu = {l: rmv(self.softplus(self.regressor[l]), y) for l in target_labels}
        else:
            mu = {l: rmv(self.regressor[l], y) for l in target_labels}
        scale_tril = {l: rtril(self.scale_tril[l]) for l in target_labels}
        return mu, scale_tril

    def forward(self, y_dict, design, observation_labels, target_labels):
        pyro.module('posterior_guide', self)
        mu, scale_tril = self.get_params(y_dict, design, target_labels)
        for l in target_labels:
            w_dist = dist.MultivariateNormal(mu[l], scale_tril=scale_tril[l])
            pyro.sample(l, w_dist)


def iter_plates_to_shape(shape):
    for i, s in enumerate(shape[::-1]):
        yield pyro.plate('plate_' + str(i), s)


def determinant_3d(H):
    """
    Returns the determinants of a batched 3-D matrix
    """
    detH = H[..., 0, 0] * (H[..., 1, 1] * H[..., 2, 2] - H[..., 2, 1] * H[..., 1, 2]) + H[..., 0, 1] * (H[..., 1, 2] * H[..., 2, 0] - H[..., 1, 0] * H[..., 2, 2]) + H[..., 0, 2] * (H[..., 1, 0] * H[..., 2, 1] - H[..., 2, 0] * H[..., 1, 1])
    return detH


def inv3d(H, sym=False):
    """
    Calculates the inverse of a batched 3-D matrix
    """
    detH = determinant_3d(H)
    Hinv = torch.empty_like(H)
    Hinv[..., 0, 0] = H[..., 1, 1] * H[..., 2, 2] - H[..., 1, 2] * H[..., 2, 1]
    Hinv[..., 1, 1] = H[..., 0, 0] * H[..., 2, 2] - H[..., 0, 2] * H[..., 2, 0]
    Hinv[..., 2, 2] = H[..., 0, 0] * H[..., 1, 1] - H[..., 0, 1] * H[..., 1, 0]
    Hinv[..., 0, 1] = H[..., 0, 2] * H[..., 2, 1] - H[..., 0, 1] * H[..., 2, 2]
    Hinv[..., 0, 2] = H[..., 0, 1] * H[..., 1, 2] - H[..., 0, 2] * H[..., 1, 1]
    Hinv[..., 1, 2] = H[..., 0, 2] * H[..., 1, 0] - H[..., 0, 0] * H[..., 1, 2]
    if sym:
        Hinv[..., 1, 0] = Hinv[..., 0, 1]
        Hinv[..., 2, 0] = Hinv[..., 0, 2]
        Hinv[..., 2, 1] = Hinv[..., 1, 2]
    else:
        Hinv[..., 1, 0] = H[..., 2, 0] * H[..., 1, 2] - H[..., 1, 0] * H[..., 2, 2]
        Hinv[..., 2, 0] = H[..., 1, 0] * H[..., 2, 1] - H[..., 2, 0] * H[..., 1, 1]
        Hinv[..., 2, 1] = H[..., 2, 0] * H[..., 0, 1] - H[..., 0, 0] * H[..., 2, 1]
    Hinv = Hinv / detH.unsqueeze(-1).unsqueeze(-1)
    return Hinv


def rinverse(M, sym=False):
    """Matrix inversion of rightmost dimensions (batched).

    For 1, 2, and 3 dimensions this uses the formulae.
    For larger matrices, it uses blockwise inversion to reduce to
    smaller matrices.
    """
    assert M.shape[-1] == M.shape[-2]
    if M.shape[-1] == 1:
        return 1.0 / M
    elif M.shape[-1] == 2:
        det = M[..., 0, 0] * M[..., 1, 1] - M[..., 1, 0] * M[..., 0, 1]
        inv = torch.empty_like(M)
        inv[..., 0, 0] = M[..., 1, 1]
        inv[..., 1, 1] = M[..., 0, 0]
        inv[..., 0, 1] = -M[..., 0, 1]
        inv[..., 1, 0] = -M[..., 1, 0]
        return inv / det.unsqueeze(-1).unsqueeze(-1)
    elif M.shape[-1] == 3:
        return inv3d(M, sym=sym)
    else:
        return torch.inverse(M)


def tensor_to_dict(sizes, tensor, subset=None):
    if subset is None:
        subset = sizes.keys()
    start = 0
    out = {}
    for label, size in sizes.items():
        end = start + size
        if label in subset:
            out[label] = tensor[..., start:end]
        start = end
    return out


class LinearModelLaplaceGuide(nn.Module):
    """
    Laplace approximation for a (G)LM.

    :param tuple d: the shape by which to expand the guide parameters, e.g. `(num_batches, num_designs)`.
    :param dict w_sizes: map from variable string names to int, indicating the dimension of each
                         weight vector in the linear model.
    :param str tau_label: the label used for inverse variance parameter sample site, or `None` to indicate a
                          fixed variance.
    :param float init_value: initial value for the posterior mean parameters.
    """

    def __init__(self, d, w_sizes, tau_label=None, init_value=0.1, **kwargs):
        super().__init__()
        self.train()
        if not isinstance(d, (tuple, list, torch.Tensor)):
            d = d,
        self.means = nn.ParameterDict()
        if tau_label is not None:
            w_sizes[tau_label] = 1
        for l, mu_l in tensor_to_dict(w_sizes, init_value * torch.ones(*(d + (sum(w_sizes.values()),)))).items():
            self.means[l] = nn.Parameter(mu_l)
        self.scale_trils = {}
        self.w_sizes = w_sizes

    @staticmethod
    def _hessian_diag(y, x, event_shape):
        batch_shape = x.shape[:-len(event_shape)]
        assert tuple(x.shape) == tuple(batch_shape) + tuple(event_shape)
        dy = torch.autograd.grad(y, [x], create_graph=True)[0]
        H = []
        batch_size = 1
        for batch_shape_dim in batch_shape:
            batch_size *= batch_shape_dim
        event_size = 1
        for event_shape_dim in event_shape:
            event_size *= event_shape_dim
        flat_dy = dy.reshape(batch_size, event_size)
        for i in range(flat_dy.shape[-1]):
            dyi = flat_dy.index_select(-1, torch.tensor([i]))
            Hi = torch.autograd.grad([dyi], [x], grad_outputs=[torch.ones_like(dyi)], retain_graph=True)[0]
            H.append(Hi)
        H = torch.stack(H, -1).reshape(*(x.shape + event_shape))
        return H

    def finalize(self, loss, target_labels):
        """
        Compute the Hessian of the parameters wrt ``loss``

        :param torch.Tensor loss: the output of evaluating a loss function such as
                                  `pyro.infer.Trace_ELBO().differentiable_loss` on the model, guide and design.
        :param list target_labels: list indicating the sample sites that are targets, i.e. for which information gain
                                   should be measured.
        """
        self.eval()
        for l, mu_l in self.means.items():
            if l not in target_labels:
                continue
            hess_l = self._hessian_diag(loss, mu_l, event_shape=(self.w_sizes[l],))
            cov_l = rinverse(hess_l)
            self.scale_trils[l] = torch.linalg.cholesky(cov_l.transpose(-2, -1)).transpose(-2, -1)

    def forward(self, design, target_labels=None):
        """
        Sample the posterior.

        :param torch.Tensor design: tensor of possible designs.
        :param list target_labels: list indicating the sample sites that are targets, i.e. for which information gain
                                   should be measured.
        """
        if target_labels is None:
            target_labels = list(self.means.keys())
        pyro.module('laplace_guide', self)
        with ExitStack() as stack:
            for plate in iter_plates_to_shape(design.shape[:-2]):
                stack.enter_context(plate)
            if self.training:
                for l in target_labels:
                    w_dist = dist.Delta(self.means[l]).to_event(1)
                    pyro.sample(l, w_dist)
            else:
                for l in target_labels:
                    w_dist = dist.MultivariateNormal(self.means[l], scale_tril=self.scale_trils[l])
                    pyro.sample(l, w_dist)


class SigmoidGuide(LinearModelPosteriorGuide):

    def __init__(self, d, n, w_sizes, **kwargs):
        super().__init__(d, w_sizes, **kwargs)
        self.inverse_sigmoid_scale = nn.Parameter(torch.ones(n))
        self.h1_weight = nn.Parameter(torch.ones(n))
        self.h1_bias = nn.Parameter(torch.zeros(n))

    def get_params(self, y_dict, design, target_labels):
        y = torch.cat(list(y_dict.values()), dim=-1)
        y, y1m = y.clamp(1e-35, 1), (1.0 - y).clamp(1e-35, 1)
        logited = y.log() - y1m.log()
        y_trans = logited / 0.1
        y_trans = y_trans * self.inverse_sigmoid_scale
        hidden = self.softplus(y_trans)
        y_trans = y_trans + hidden * self.h1_weight + self.h1_bias
        return self.linear_model_formula(y_trans, design, target_labels)


def rvv(a, b):
    """Tensorized vector vector multiplication of rightmost dimensions."""
    return torch.matmul(a.unsqueeze(-2), b.unsqueeze(-1)).squeeze(-2).squeeze(-1)


class NormalInverseGammaGuide(LinearModelPosteriorGuide):

    def __init__(self, d, w_sizes, mf=False, tau_label='tau', alpha_init=100.0, b0_init=100.0, **kwargs):
        super().__init__(d, w_sizes, **kwargs)
        self.alpha = nn.Parameter(alpha_init * torch.ones(d))
        self.b0 = nn.Parameter(b0_init * torch.ones(d))
        self.mf = mf
        self.tau_label = tau_label

    def get_params(self, y_dict, design, target_labels):
        y = torch.cat(list(y_dict.values()), dim=-1)
        coefficient_labels = [label for label in target_labels if label != self.tau_label]
        mu, scale_tril = self.linear_model_formula(y, design, coefficient_labels)
        mu_vec = torch.cat(list(mu.values()), dim=-1)
        yty = rvv(y, y)
        ytxmu = rvv(y, rmv(design, mu_vec))
        beta = self.b0 + 0.5 * (yty - ytxmu)
        return mu, scale_tril, self.alpha, beta

    def forward(self, y_dict, design, observation_labels, target_labels):
        pyro.module('ba_guide', self)
        mu, scale_tril, alpha, beta = self.get_params(y_dict, design, target_labels)
        if self.tau_label in target_labels:
            tau_dist = dist.Gamma(alpha, beta)
            tau = pyro.sample(self.tau_label, tau_dist)
            obs_sd = 1.0 / tau.sqrt().unsqueeze(-1).unsqueeze(-1)
        for label in target_labels:
            if label != self.tau_label:
                if self.mf:
                    w_dist = dist.MultivariateNormal(mu[label], scale_tril=scale_tril[label])
                else:
                    w_dist = dist.MultivariateNormal(mu[label], scale_tril=scale_tril[label] * obs_sd)
                pyro.sample(label, w_dist)


class GuideDV(nn.Module):
    """A Donsker-Varadhan `T` family based on a guide family via
    the relation `T = log p(theta) - log q(theta | y, d)`
    """

    def __init__(self, guide):
        super().__init__()
        self.guide = guide

    def forward(self, design, trace, observation_labels, target_labels):
        trace.compute_log_prob()
        prior_lp = sum(trace.nodes[l]['log_prob'] for l in target_labels)
        y_dict = {l: trace.nodes[l]['value'] for l in observation_labels}
        theta_dict = {l: trace.nodes[l]['value'] for l in target_labels}
        conditional_guide = pyro.condition(self.guide, data=theta_dict)
        cond_trace = poutine.trace(conditional_guide).get_trace(y_dict, design, observation_labels, target_labels)
        cond_trace.compute_log_prob()
        posterior_lp = sum(cond_trace.nodes[l]['log_prob'] for l in target_labels)
        return posterior_lp - prior_lp


class DynamicModel(nn.Module, metaclass=ABCMeta):
    """
    Dynamic model interface.

    :param dimension: native state dimension.
    :param dimension_pv: PV state dimension.
    :param num_process_noise_parameters: process noise parameter space dimension.
          This for UKF applications. Can be left as ``None`` for EKF and most
          other filters.
    """

    def __init__(self, dimension, dimension_pv, num_process_noise_parameters=None):
        self._dimension = dimension
        self._dimension_pv = dimension_pv
        self._num_process_noise_parameters = num_process_noise_parameters
        super().__init__()

    @property
    def dimension(self):
        """
        Native state dimension access.
        """
        return self._dimension

    @property
    def dimension_pv(self):
        """
        PV state dimension access.
        """
        return self._dimension_pv

    @property
    def num_process_noise_parameters(self):
        """
        Process noise parameters space dimension access.
        """
        return self._num_process_noise_parameters

    @abstractmethod
    def forward(self, x, dt, do_normalization=True):
        """
        Integrate native state ``x`` over time interval ``dt``.

        :param x: current native state. If the DynamicModel is non-differentiable,
              be sure to handle the case of ``x`` being augmented with process
              noise parameters.
        :param dt: time interval to integrate over.
        :param do_normalization: whether to perform normalization on output, e.g.,
              mod'ing angles into an interval.
        :return: Native state x integrated dt into the future.
        """
        raise NotImplementedError

    def geodesic_difference(self, x1, x0):
        """
        Compute and return the geodesic difference between 2 native states.
        This is a generalization of the Euclidean operation ``x1 - x0``.

        :param x1: native state.
        :param x0: native state.
        :return: Geodesic difference between native states ``x1`` and ``x2``.
        """
        return x1 - x0

    @abstractmethod
    def mean2pv(self, x):
        """
        Compute and return PV state from native state. Useful for combining
        state estimates of different types in IMM (Interacting Multiple Model)
        filtering.

        :param x: native state estimate mean.
        :return: PV state estimate mean.
        """
        raise NotImplementedError

    @abstractmethod
    def cov2pv(self, P):
        """
        Compute and return PV covariance from native covariance. Useful for
        combining state estimates of different types in IMM (Interacting
        Multiple Model) filtering.

        :param P: native state estimate covariance.
        :return: PV state estimate covariance.
        """
        raise NotImplementedError

    @abstractmethod
    def process_noise_cov(self, dt=0.0):
        """
        Compute and return process noise covariance (Q).

        :param dt: time interval to integrate over.
        :return: Read-only covariance (Q). For a DifferentiableDynamicModel, this is
            the covariance of the native state ``x`` resulting from stochastic
            integration (for use with EKF). Otherwise, it is the covariance
            directly of the process noise parameters (for use with UKF).
        """
        raise NotImplementedError

    def process_noise_dist(self, dt=0.0):
        """
        Return a distribution object of state displacement from the process noise
        distribution over a time interval.

        :param dt: time interval that process noise accumulates over.
        :return: :class:`~pyro.distributions.torch.MultivariateNormal`.
        """
        Q = self.process_noise_cov(dt)
        return dist.MultivariateNormal(torch.zeros(Q.shape[-1], dtype=Q.dtype, device=Q.device), Q)


class DifferentiableDynamicModel(DynamicModel):
    """
    DynamicModel for which state transition Jacobians can be efficiently
    calculated, usu. analytically or by automatic differentiation.
    """

    @abstractmethod
    def jacobian(self, dt):
        """
        Compute and return native state transition Jacobian (F) over time
        interval ``dt``.

        :param  dt: time interval to integrate over.
        :return: Read-only Jacobian (F) of integration map (f).
        """
        raise NotImplementedError


def eye_like(value, m, n=None):
    if n is None:
        n = m
    eye = torch.zeros(m, n, dtype=value.dtype, device=value.device)
    eye.view(-1)[:min(m, n) * n:n + 1] = 1
    return eye


class Ncp(DifferentiableDynamicModel):
    """
    NCP (Nearly-Constant Position) dynamic model. May be subclassed, e.g., with
    CWNV (Continuous White Noise Velocity) or DWNV (Discrete White Noise
    Velocity).

    :param dimension: native state dimension.
    :param sv2: variance of velocity. Usually chosen so that the standard
          deviation is roughly half of the max velocity one would ever expect
          to observe.
    """

    def __init__(self, dimension, sv2):
        dimension_pv = 2 * dimension
        super().__init__(dimension, dimension_pv, num_process_noise_parameters=1)
        if not isinstance(sv2, torch.Tensor):
            sv2 = torch.tensor(sv2)
        self.sv2 = Parameter(sv2)
        self._F_cache = eye_like(sv2, dimension)
        self._Q_cache = {}

    def forward(self, x, dt, do_normalization=True):
        """
        Integrate native state ``x`` over time interval ``dt``.

        :param x: current native state. If the DynamicModel is non-differentiable,
              be sure to handle the case of ``x`` being augmented with process
              noise parameters.
        :param dt: time interval to integrate over.
            do_normalization: whether to perform normalization on output, e.g.,
            mod'ing angles into an interval. Has no effect for this subclass.
        :return: Native state x integrated dt into the future.
        """
        return x

    def mean2pv(self, x):
        """
        Compute and return PV state from native state. Useful for combining
        state estimates of different types in IMM (Interacting Multiple Model)
        filtering.

        :param x: native state estimate mean.
        :return: PV state estimate mean.
        """
        with torch.no_grad():
            x_pv = torch.zeros(2 * self._dimension, dtype=x.dtype, device=x.device)
            x_pv[:self._dimension] = x
        return x_pv

    def cov2pv(self, P):
        """
        Compute and return PV covariance from native covariance. Useful for
        combining state estimates of different types in IMM (Interacting
        Multiple Model) filtering.

        :param P: native state estimate covariance.
        :return: PV state estimate covariance.
        """
        d = 2 * self._dimension
        with torch.no_grad():
            P_pv = torch.zeros(d, d, dtype=P.dtype, device=P.device)
            P_pv[:self._dimension, :self._dimension] = P
        return P_pv

    def jacobian(self, dt):
        """
        Compute and return cached native state transition Jacobian (F) over
        time interval ``dt``.

        :param dt: time interval to integrate over.
        :return: Read-only Jacobian (F) of integration map (f).
        """
        return self._F_cache

    @abstractmethod
    def process_noise_cov(self, dt=0.0):
        """
        Compute and return cached process noise covariance (Q).

        :param dt: time interval to integrate over.
        :return: Read-only covariance (Q) of the native state ``x`` resulting from
            stochastic integration (for use with EKF).
        """
        raise NotImplementedError


class Ncv(DifferentiableDynamicModel):
    """
    NCV (Nearly-Constant Velocity) dynamic model. May be subclassed, e.g., with
    CWNA (Continuous White Noise Acceleration) or DWNA (Discrete White Noise
    Acceleration).

    :param dimension: native state dimension.
    :param sa2: variance of acceleration. Usually chosen so that the standard
          deviation is roughly half of the max acceleration one would ever
          expect to observe.
    """

    def __init__(self, dimension, sa2):
        dimension_pv = dimension
        super().__init__(dimension, dimension_pv, num_process_noise_parameters=1)
        if not isinstance(sa2, torch.Tensor):
            sa2 = torch.tensor(sa2)
        self.sa2 = Parameter(sa2)
        self._F_cache = {}
        self._Q_cache = {}

    def forward(self, x, dt, do_normalization=True):
        """
        Integrate native state ``x`` over time interval ``dt``.

        :param x: current native state. If the DynamicModel is non-differentiable,
              be sure to handle the case of ``x`` being augmented with process
              noise parameters.
        :param dt: time interval to integrate over.
        :param do_normalization: whether to perform normalization on output, e.g.,
              mod'ing angles into an interval. Has no effect for this subclass.

        :return: Native state x integrated dt into the future.
        """
        F = self.jacobian(dt)
        return F.mm(x.unsqueeze(1)).squeeze(1)

    def mean2pv(self, x):
        """
        Compute and return PV state from native state. Useful for combining
        state estimates of different types in IMM (Interacting Multiple Model)
        filtering.

        :param x: native state estimate mean.
        :return: PV state estimate mean.
        """
        return x

    def cov2pv(self, P):
        """
        Compute and return PV covariance from native covariance. Useful for
        combining state estimates of different types in IMM (Interacting
        Multiple Model) filtering.

        :param P: native state estimate covariance.
        :return: PV state estimate covariance.
        """
        return P

    def jacobian(self, dt):
        """
        Compute and return cached native state transition Jacobian (F) over
        time interval ``dt``.

        :param dt: time interval to integrate over.
        :return: Read-only Jacobian (F) of integration map (f).
        """
        if dt not in self._F_cache:
            d = self._dimension
            with torch.no_grad():
                F = eye_like(self.sa2, d)
                F[:d // 2, d // 2:] = dt * eye_like(self.sa2, d // 2)
            self._F_cache[dt] = F
        return self._F_cache[dt]

    @abstractmethod
    def process_noise_cov(self, dt=0.0):
        """
        Compute and return cached process noise covariance (Q).

        :param dt: time interval to integrate over.
        :return: Read-only covariance (Q) of the native state ``x`` resulting from
            stochastic integration (for use with EKF).
        """
        raise NotImplementedError


class NcpContinuous(Ncp):
    """
    NCP (Nearly-Constant Position) dynamic model with CWNV (Continuous White
    Noise Velocity).

    References:
        "Estimation with Applications to Tracking and Navigation" by Y. Bar-
        Shalom et al, 2001, p.269.

    :param dimension: native state dimension.
    :param sv2: variance of velocity. Usually chosen so that the standard
          deviation is roughly half of the max velocity one would ever expect
          to observe.
    """

    def process_noise_cov(self, dt=0.0):
        """
        Compute and return cached process noise covariance (Q).

        :param dt: time interval to integrate over.
        :return: Read-only covariance (Q) of the native state ``x`` resulting from
            stochastic integration (for use with EKF).
        """
        if dt not in self._Q_cache:
            q = self.sv2 * dt
            Q = q * dt * eye_like(self.sv2, self._dimension)
            self._Q_cache[dt] = Q
        return self._Q_cache[dt]


class NcvContinuous(Ncv):
    """
    NCV (Nearly-Constant Velocity) dynamic model with CWNA (Continuous White
    Noise Acceleration).

    References:
        "Estimation with Applications to Tracking and Navigation" by Y. Bar-
        Shalom et al, 2001, p.269.

    :param dimension: native state dimension.
    :param sa2: variance of acceleration. Usually chosen so that the standard
          deviation is roughly half of the max acceleration one would ever
          expect to observe.
    """

    def process_noise_cov(self, dt=0.0):
        """
        Compute and return cached process noise covariance (Q).

        :param dt: time interval to integrate over.

        :return: Read-only covariance (Q) of the native state ``x`` resulting from
            stochastic integration (for use with EKF).
        """
        if dt not in self._Q_cache:
            with torch.no_grad():
                d = self._dimension
                dt2 = dt * dt
                dt3 = dt2 * dt
                Q = torch.zeros(d, d, dtype=self.sa2.dtype, device=self.sa2.device)
                eye = eye_like(self.sa2, d // 2)
                Q[:d // 2, :d // 2] = dt3 * eye / 3.0
                Q[:d // 2, d // 2:] = dt2 * eye / 2.0
                Q[d // 2:, :d // 2] = dt2 * eye / 2.0
                Q[d // 2:, d // 2:] = dt * eye
            Q = Q * (self.sa2 * dt)
            self._Q_cache[dt] = Q
        return self._Q_cache[dt]


class NcpDiscrete(Ncp):
    """
    NCP (Nearly-Constant Position) dynamic model with DWNV (Discrete White
    Noise Velocity).

    :param dimension: native state dimension.
    :param sv2: variance of velocity. Usually chosen so that the standard
          deviation is roughly half of the max velocity one would ever expect
          to observe.

    References:
        "Estimation with Applications to Tracking and Navigation" by Y. Bar-
        Shalom et al, 2001, p.273.
    """

    def process_noise_cov(self, dt=0.0):
        """
        Compute and return cached process noise covariance (Q).

        :param dt: time interval to integrate over.
        :return: Read-only covariance (Q) of the native state `x` resulting from
            stochastic integration (for use with EKF).
        """
        if dt not in self._Q_cache:
            Q = self.sv2 * dt * dt * eye_like(self.sv2, self._dimension)
            self._Q_cache[dt] = Q
        return self._Q_cache[dt]


class NcvDiscrete(Ncv):
    """
    NCV (Nearly-Constant Velocity) dynamic model with DWNA (Discrete White
    Noise Acceleration).

    :param dimension: native state dimension.
    :param sa2: variance of acceleration. Usually chosen so that the standard
          deviation is roughly half of the max acceleration one would ever
          expect to observe.

    References:
        "Estimation with Applications to Tracking and Navigation" by Y. Bar-
        Shalom et al, 2001, p.273.
    """

    def process_noise_cov(self, dt=0.0):
        """
        Compute and return cached process noise covariance (Q).

        :param dt: time interval to integrate over.
        :return: Read-only covariance (Q) of the native state `x` resulting from
            stochastic integration (for use with EKF). (Note that this Q, modulo
            numerical error, has rank `dimension/2`. So, it is only positive
            semi-definite.)
        """
        if dt not in self._Q_cache:
            with torch.no_grad():
                d = self._dimension
                dt2 = dt * dt
                dt3 = dt2 * dt
                dt4 = dt2 * dt2
                Q = torch.zeros(d, d, dtype=self.sa2.dtype, device=self.sa2.device)
                Q[:d // 2, :d // 2] = 0.25 * dt4 * eye_like(self.sa2, d // 2)
                Q[:d // 2, d // 2:] = 0.5 * dt3 * eye_like(self.sa2, d // 2)
                Q[d // 2:, :d // 2] = 0.5 * dt3 * eye_like(self.sa2, d // 2)
                Q[d // 2:, d // 2:] = dt2 * eye_like(self.sa2, d // 2)
            Q = Q * self.sa2
            self._Q_cache[dt] = Q
        return self._Q_cache[dt]


class ConditionalTransform(ABC):

    @abstractmethod
    def condition(self, context):
        """:rtype: torch.distributions.Transform"""
        raise NotImplementedError


class ConditionalTransformModule(ConditionalTransform, torch.nn.Module):
    """
    Conditional transforms with learnable parameters such as normalizing flows should inherit from this class rather
    than :class:`~pyro.distributions.conditional.ConditionalTransform` so they are also a subclass of
    :class:`~torch.nn.Module` and inherit all the useful methods of that class.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def __hash__(self):
        return super().__hash__()


class ComposeTransformModule(torch.distributions.ComposeTransform, torch.nn.ModuleList):
    """
    This allows us to use a list of `TransformModule` in the same way as
    :class:`~torch.distributions.transform.ComposeTransform`. This is needed
    so that transform parameters are automatically registered by Pyro's param
    store when used in :class:`~pyro.nn.module.PyroModule` instances.
    """

    def __init__(self, parts):
        super().__init__(parts)
        for part in parts:
            self.append(part)

    def __hash__(self):
        return super(torch.nn.Module, self).__hash__()


@copy_docs_from(ConditionalTransformModule)
class ConditionalAffineAutoregressive(ConditionalTransformModule):
    """
    An implementation of the bijective transform of Inverse Autoregressive Flow
    (IAF) that conditions on an additional context variable and uses, by default,
    Eq (10) from Kingma Et Al., 2016,

        :math:`\\mathbf{y} = \\mu_t + \\sigma_t\\odot\\mathbf{x}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    :math:`\\mu_t,\\sigma_t` are calculated from an autoregressive network on
    :math:`\\mathbf{x}` and context :math:`\\mathbf{z}\\in\\mathbb{R}^M`, and
    :math:`\\sigma_t>0`.

    If the stable keyword argument is set to True then the transformation used is,

        :math:`\\mathbf{y} = \\sigma_t\\odot\\mathbf{x} + (1-\\sigma_t)\\odot\\mu_t`

    where :math:`\\sigma_t` is restricted to :math:`(0,1)`. This variant of IAF is
    claimed by the authors to be more numerically stable than one using Eq (10),
    although in practice it leads to a restriction on the distributions that can be
    represented, presumably since the input is restricted to rescaling by a number
    on :math:`(0,1)`.

    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`
    this provides a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn import ConditionalAutoRegressiveNN
    >>> input_dim = 10
    >>> context_dim = 4
    >>> batch_size = 3
    >>> hidden_dims = [10*input_dim, 10*input_dim]
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> hypernet = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims)
    >>> transform = ConditionalAffineAutoregressive(hypernet)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP

    The inverse of the Bijector is required when, e.g., scoring the log density of a
    sample with :class:`~pyro.distributions.TransformedDistribution`. This
    implementation caches the inverse of the Bijector when its forward operation is
    called, e.g., when sampling from
    :class:`~pyro.distributions.TransformedDistribution`. However, if the cached
    value isn't available, either because it was overwritten during sampling a new
    value or an arbitrary value is being scored, it will calculate it manually. Note
    that this is an operation that scales as O(D) where D is the input dimension,
    and so should be avoided for large dimensional uses. So in general, it is cheap
    to sample from IAF and score a value that was sampled by IAF, but expensive to
    score an arbitrary value.

    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns a real-valued mean and logit-scale as a tuple
    :type autoregressive_nn: nn.Module
    :param log_scale_min_clip: The minimum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_min_clip: float
    :param log_scale_max_clip: The maximum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_max_clip: float
    :param sigmoid_bias: A term to add the logit of the input when using the stable
        tranform.
    :type sigmoid_bias: float
    :param stable: When true, uses the alternative "stable" version of the transform
        (see above).
    :type stable: bool

    References:

    [1] Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever,
    Max Welling. Improving Variational Inference with Inverse Autoregressive Flow.
    [arXiv:1606.04934]

    [2] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with
    Normalizing Flows. [arXiv:1505.05770]

    [3] Mathieu Germain, Karol Gregor, Iain Murray, Hugo Larochelle. MADE: Masked
    Autoencoder for Distribution Estimation. [arXiv:1502.03509]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, autoregressive_nn, **kwargs):
        super().__init__()
        self.nn = autoregressive_nn
        self.kwargs = kwargs

    def condition(self, context):
        """
        Conditions on a context variable, returning a non-conditional transform of
        of type :class:`~pyro.distributions.transforms.AffineAutoregressive`.
        """
        cond_nn = partial(self.nn, context=context)
        cond_nn.permutation = cond_nn.func.permutation
        cond_nn.get_permutation = cond_nn.func.get_permutation
        return AffineAutoregressive(cond_nn, **self.kwargs)


@copy_docs_from(TransformModule)
class AffineCoupling(TransformModule):
    """
    An implementation of the affine coupling layer of RealNVP (Dinh et al., 2017)
    that uses the bijective transform,

        :math:`\\mathbf{y}_{1:d} = \\mathbf{x}_{1:d}`
        :math:`\\mathbf{y}_{(d+1):D} = \\mu + \\sigma\\odot\\mathbf{x}_{(d+1):D}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    e.g. :math:`\\mathbf{x}_{1:d}` represents the first :math:`d` elements of the
    inputs, and :math:`\\mu,\\sigma` are shift and translation parameters calculated
    as the output of a function inputting only :math:`\\mathbf{x}_{1:d}`.

    That is, the first :math:`d` components remain unchanged, and the subsequent
    :math:`D-d` are shifted and translated by a function of the previous components.

    Together with :class:`~pyro.distributions.TransformedDistribution` this provides
    a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn import DenseNN
    >>> input_dim = 10
    >>> split_dim = 6
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [input_dim-split_dim, input_dim-split_dim]
    >>> hypernet = DenseNN(split_dim, [10*input_dim], param_dims)
    >>> transform = AffineCoupling(split_dim, hypernet)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    The inverse of the Bijector is required when, e.g., scoring the log density of a
    sample with :class:`~pyro.distributions.TransformedDistribution`. This
    implementation caches the inverse of the Bijector when its forward operation is
    called, e.g., when sampling from
    :class:`~pyro.distributions.TransformedDistribution`. However, if the cached
    value isn't available, either because it was overwritten during sampling a new
    value or an arbitary value is being scored, it will calculate it manually.

    This is an operation that scales as O(1), i.e. constant in the input dimension.
    So in general, it is cheap to sample *and* score (an arbitrary value) from
    :class:`~pyro.distributions.transforms.AffineCoupling`.

    :param split_dim: Zero-indexed dimension :math:`d` upon which to perform input/
        output split for transformation.
    :type split_dim: int
    :param hypernet: a neural network whose forward call returns a real-valued mean
        and logit-scale as a tuple. The input should have final dimension split_dim
        and the output final dimension input_dim-split_dim for each member of the
        tuple.
    :type hypernet: callable
    :param dim: the tensor dimension on which to split. This value must be negative
        and defines the event dim as `abs(dim)`.
    :type dim: int
    :param log_scale_min_clip: The minimum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_min_clip: float
    :param log_scale_max_clip: The maximum value for clipping the log(scale) from
        the autoregressive NN
    :type log_scale_max_clip: float

    References:

    [1] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation
    using Real NVP. ICLR 2017.

    """
    bijective = True

    def __init__(self, split_dim, hypernet, *, dim=-1, log_scale_min_clip=-5.0, log_scale_max_clip=3.0):
        super().__init__(cache_size=1)
        if dim >= 0:
            raise ValueError("'dim' keyword argument must be negative")
        self.split_dim = split_dim
        self.nn = hypernet
        self.dim = dim
        self._cached_log_scale = None
        self.log_scale_min_clip = log_scale_min_clip
        self.log_scale_max_clip = log_scale_max_clip

    @constraints.dependent_property(is_discrete=False)
    def domain(self):
        return constraints.independent(constraints.real, -self.dim)

    @constraints.dependent_property(is_discrete=False)
    def codomain(self):
        return constraints.independent(constraints.real, -self.dim)

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        x1, x2 = x.split([self.split_dim, x.size(self.dim) - self.split_dim], dim=self.dim)
        mean, log_scale = self.nn(x1.reshape(x1.shape[:self.dim] + (-1,)))
        mean = mean.reshape(mean.shape[:-1] + x2.shape[self.dim:])
        log_scale = log_scale.reshape(log_scale.shape[:-1] + x2.shape[self.dim:])
        log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)
        self._cached_log_scale = log_scale
        y1 = x1
        y2 = torch.exp(log_scale) * x2 + mean
        return torch.cat([y1, y2], dim=self.dim)

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. Uses a previously cached inverse if available, otherwise
        performs the inversion afresh.
        """
        y1, y2 = y.split([self.split_dim, y.size(self.dim) - self.split_dim], dim=self.dim)
        x1 = y1
        mean, log_scale = self.nn(x1.reshape(x1.shape[:self.dim] + (-1,)))
        mean = mean.reshape(mean.shape[:-1] + y2.shape[self.dim:])
        log_scale = log_scale.reshape(log_scale.shape[:-1] + y2.shape[self.dim:])
        log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)
        self._cached_log_scale = log_scale
        x2 = (y2 - mean) * torch.exp(-log_scale)
        return torch.cat([x1, x2], dim=self.dim)

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log jacobian
        """
        x_old, y_old = self._cached_x_y
        if self._cached_log_scale is not None and x is x_old and y is y_old:
            log_scale = self._cached_log_scale
        else:
            x1, x2 = x.split([self.split_dim, x.size(self.dim) - self.split_dim], dim=self.dim)
            _, log_scale = self.nn(x1.reshape(x1.shape[:self.dim] + (-1,)))
            log_scale = log_scale.reshape(log_scale.shape[:-1] + x2.shape[self.dim:])
            log_scale = clamp_preserve_gradients(log_scale, self.log_scale_min_clip, self.log_scale_max_clip)
        return _sum_rightmost(log_scale, self.event_dim)


@copy_docs_from(ConditionalTransformModule)
class ConditionalAffineCoupling(ConditionalTransformModule):
    """
    An implementation of the affine coupling layer of RealNVP (Dinh et al., 2017)
    that conditions on an additional context variable and uses the bijective
    transform,

        :math:`\\mathbf{y}_{1:d} = \\mathbf{x}_{1:d}`
        :math:`\\mathbf{y}_{(d+1):D} = \\mu + \\sigma\\odot\\mathbf{x}_{(d+1):D}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    e.g. :math:`\\mathbf{x}_{1:d}` represents the first :math:`d` elements of the
    inputs, and :math:`\\mu,\\sigma` are shift and translation parameters calculated
    as the output of a function input :math:`\\mathbf{x}_{1:d}` and a context
    variable :math:`\\mathbf{z}\\in\\mathbb{R}^M`.

    That is, the first :math:`d` components remain unchanged, and the subsequent
    :math:`D-d` are shifted and translated by a function of the previous components.

    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`
    this provides a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn import ConditionalDenseNN
    >>> input_dim = 10
    >>> split_dim = 6
    >>> context_dim = 4
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [input_dim-split_dim, input_dim-split_dim]
    >>> hypernet = ConditionalDenseNN(split_dim, context_dim, [10*input_dim],
    ... param_dims)
    >>> transform = ConditionalAffineCoupling(split_dim, hypernet)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP

    The inverse of the Bijector is required when, e.g., scoring the log density of a
    sample with :class:`~pyro.distributions.ConditionalTransformedDistribution`.
    This implementation caches the inverse of the Bijector when its forward
    operation is called, e.g., when sampling from
    :class:`~pyro.distributions.ConditionalTransformedDistribution`. However, if the
    cached value isn't available, either because it was overwritten during sampling
    a new value or an arbitary value is being scored, it will calculate it manually.

    This is an operation that scales as O(1), i.e. constant in the input dimension.
    So in general, it is cheap to sample *and* score (an arbitrary value) from
    :class:`~pyro.distributions.transforms.ConditionalAffineCoupling`.

    :param split_dim: Zero-indexed dimension :math:`d` upon which to perform input/
        output split for transformation.
    :type split_dim: int
    :param hypernet: A neural network whose forward call returns a real-valued mean
        and logit-scale as a tuple. The input should have final dimension split_dim
        and the output final dimension input_dim-split_dim for each member of the
        tuple. The network also inputs a context variable as a keyword argument in
        order to condition the output upon it.
    :type hypernet: callable
    :param log_scale_min_clip: The minimum value for clipping the log(scale) from
        the NN
    :type log_scale_min_clip: float
    :param log_scale_max_clip: The maximum value for clipping the log(scale) from
        the NN
    :type log_scale_max_clip: float

    References:

    Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using
    Real NVP. ICLR 2017.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, split_dim, hypernet, **kwargs):
        super().__init__()
        self.split_dim = split_dim
        self.nn = hypernet
        self.kwargs = kwargs

    def condition(self, context):
        cond_nn = partial(self.nn, context=context)
        return AffineCoupling(self.split_dim, cond_nn, **self.kwargs)


@copy_docs_from(TransformModule)
class BatchNorm(TransformModule):
    """
    A type of batch normalization that can be used to stabilize training in
    normalizing flows. The inverse operation is defined as

        :math:`x = (y - \\hat{\\mu}) \\oslash \\sqrt{\\hat{\\sigma^2}} \\otimes \\gamma + \\beta`

    that is, the standard batch norm equation, where :math:`x` is the input,
    :math:`y` is the output, :math:`\\gamma,\\beta` are learnable parameters, and
    :math:`\\hat{\\mu}`/:math:`\\hat{\\sigma^2}` are smoothed running averages of
    the sample mean and variance, respectively. The constraint :math:`\\gamma>0` is
    enforced to ease calculation of the log-det-Jacobian term.

    This is an element-wise transform, and when applied to a vector, learns two
    parameters (:math:`\\gamma,\\beta`) for each dimension of the input.

    When the module is set to training mode, the moving averages of the sample mean
    and variance are updated every time the inverse operator is called, e.g., when a
    normalizing flow scores a minibatch with the `log_prob` method.

    Also, when the module is set to training mode, the sample mean and variance on
    the current minibatch are used in place of the smoothed averages,
    :math:`\\hat{\\mu}` and :math:`\\hat{\\sigma^2}`, for the inverse operator. For
    this reason it is not the case that :math:`x=g(g^{-1}(x))` during training,
    i.e., that the inverse operation is the inverse of the forward one.

    Example usage:

    >>> from pyro.nn import AutoRegressiveNN
    >>> from pyro.distributions.transforms import AffineAutoregressive
    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> iafs = [AffineAutoregressive(AutoRegressiveNN(10, [40])) for _ in range(2)]
    >>> bn = BatchNorm(10)
    >>> flow_dist = dist.TransformedDistribution(base_dist, [iafs[0], bn, iafs[1]])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param input_dim: the dimension of the input
    :type input_dim: int
    :param momentum: momentum parameter for updating moving averages
    :type momentum: float
    :param epsilon: small number to add to variances to ensure numerical stability
    :type epsilon: float

    References:

    [1] Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift. In International
    Conference on Machine Learning, 2015. https://arxiv.org/abs/1502.03167

    [2] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density Estimation
    using Real NVP. In International Conference on Learning Representations, 2017.
    https://arxiv.org/abs/1605.08803

    [3] George Papamakarios, Theo Pavlakou, and Iain Murray. Masked Autoregressive
    Flow for Density Estimation. In Neural Information Processing Systems, 2017.
    https://arxiv.org/abs/1705.07057

    """
    domain = constraints.real
    codomain = constraints.real
    bijective = True

    def __init__(self, input_dim, momentum=0.1, epsilon=1e-05):
        super().__init__()
        self.input_dim = input_dim
        self.gamma = nn.Parameter(torch.ones(input_dim))
        self.beta = nn.Parameter(torch.zeros(input_dim))
        self.momentum = momentum
        self.epsilon = epsilon
        self.register_buffer('moving_mean', torch.zeros(input_dim))
        self.register_buffer('moving_variance', torch.ones(input_dim))

    @property
    def constrained_gamma(self):
        return F.relu(self.gamma) + 1e-06

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        return (x - self.beta) / self.constrained_gamma * torch.sqrt(self.moving_variance + self.epsilon) + self.moving_mean

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x.
        """
        if self.training:
            mean, var = y.mean(0), y.var(0)
            with torch.no_grad():
                self.moving_mean.mul_(1 - self.momentum).add_(mean * self.momentum)
                self.moving_variance.mul_(1 - self.momentum).add_(var * self.momentum)
        else:
            mean, var = self.moving_mean, self.moving_variance
        return (y - mean) * self.constrained_gamma / torch.sqrt(var + self.epsilon) + self.beta

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian, dx/dy
        """
        if self.training:
            var = torch.var(y, dim=0, keepdim=True)
        else:
            var = self.moving_variance
        return -self.constrained_gamma.log() + 0.5 * torch.log(var + self.epsilon)


class ELUTransform(Transform):
    """
    Bijective transform via the mapping :math:`y = \\text{ELU}(x)`.
    """
    domain = constraints.real
    codomain = constraints.positive
    bijective = True
    sign = +1

    def __eq__(self, other):
        return isinstance(other, ELUTransform)

    def _call(self, x):
        return F.elu(x)

    def _inverse(self, y, eps=1e-08):
        return torch.max(y, torch.zeros_like(y)) + torch.min(torch.log1p(y + eps), torch.zeros_like(y))

    def log_abs_det_jacobian(self, x, y):
        return -F.relu(-x)


class LeakyReLUTransform(Transform):
    """
    Bijective transform via the mapping :math:`y = \\text{LeakyReLU}(x)`.
    """
    domain = constraints.real
    codomain = constraints.positive
    bijective = True
    sign = +1

    def __eq__(self, other):
        return isinstance(other, LeakyReLUTransform)

    def _call(self, x):
        return F.leaky_relu(x)

    def _inverse(self, y):
        return F.leaky_relu(y, negative_slope=100.0)

    def log_abs_det_jacobian(self, x, y):
        return torch.where(x >= 0.0, torch.zeros_like(x), torch.ones_like(x) * math.log(0.01))


eps = 1e-08


class MaskedBlockLinear(torch.nn.Module):
    """
    Module that implements a linear layer with block matrices with positive diagonal
    blocks. Moreover, it uses Weight Normalization
    (https://arxiv.org/abs/1602.07868) for stability.
    """

    def __init__(self, in_features, out_features, dim, bias=True):
        super().__init__()
        self.in_features, self.out_features, self.dim = in_features, out_features, dim
        weight = torch.zeros(out_features, in_features)
        for i in range(dim):
            weight[i * out_features // dim:(i + 1) * out_features // dim, 0:(i + 1) * in_features // dim] = torch.nn.init.xavier_uniform_(torch.Tensor(out_features // dim, (i + 1) * in_features // dim))
        self._weight = torch.nn.Parameter(weight)
        self._diag_weight = torch.nn.Parameter(torch.nn.init.uniform_(torch.Tensor(out_features, 1)).log())
        self.bias = torch.nn.Parameter(torch.nn.init.uniform_(torch.Tensor(out_features), -1 / math.sqrt(out_features), 1 / math.sqrt(out_features))) if bias else 0
        mask_d = torch.eye(dim).unsqueeze(-1).repeat(1, out_features // dim, in_features // dim).view(out_features, in_features)
        self.register_buffer('mask_d', mask_d)
        mask_o = torch.tril(torch.ones(dim, dim), diagonal=-1).unsqueeze(-1)
        mask_o = mask_o.repeat(1, out_features // dim, in_features // dim).view(out_features, in_features)
        self.register_buffer('mask_o', mask_o)

    def get_weights(self):
        """
        Computes the weight matrix using masks and weight normalization.
        It also compute the log diagonal blocks of it.
        """
        w = torch.exp(self._weight) * self.mask_d + self._weight * self.mask_o
        w_squared_norm = (w ** 2).sum(-1, keepdim=True)
        w = self._diag_weight.exp() * w / (w_squared_norm.sqrt() + eps)
        wpl = self._diag_weight + self._weight - 0.5 * torch.log(w_squared_norm + eps)
        return w, wpl[self.mask_d.bool()].view(self.dim, self.out_features // self.dim, self.in_features // self.dim)

    def forward(self, x):
        w, wpl = self.get_weights()
        return (torch.matmul(w, x) + self.bias.unsqueeze(-1)).squeeze(-1), wpl


def log_matrix_product(A, B):
    """
    Computes the matrix products of two matrices in log-space, returning the result
    in log-space. This is useful for calculating the vector chain rule for Jacobian
    terms.
    """
    return torch.logsumexp(A.unsqueeze(-1) + B.unsqueeze(-3), dim=-2)


@copy_docs_from(TransformModule)
class BlockAutoregressive(TransformModule):
    """
    An implementation of Block Neural Autoregressive Flow (block-NAF)
    (De Cao et al., 2019) bijective transform. Block-NAF uses a similar
    transformation to deep dense NAF, building the autoregressive NN into the
    structure of the transform, in a sense.

    Together with :class:`~pyro.distributions.TransformedDistribution` this provides
    a way to create richer variational approximations.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> naf = BlockAutoregressive(input_dim=10)
    >>> pyro.module("my_naf", naf)  # doctest: +SKIP
    >>> naf_dist = dist.TransformedDistribution(base_dist, [naf])
    >>> naf_dist.sample()  # doctest: +SKIP

    The inverse operation is not implemented. This would require numerical
    inversion, e.g., using a root finding method - a possibility for a future
    implementation.

    :param input_dim: The dimensionality of the input and output variables.
    :type input_dim: int
    :param hidden_factors: Hidden layer i has hidden_factors[i] hidden units per
        input dimension. This corresponds to both :math:`a` and :math:`b` in De Cao
        et al. (2019). The elements of hidden_factors must be integers.
    :type hidden_factors: list
    :param activation: Activation function to use. One of 'ELU', 'LeakyReLU',
        'sigmoid', or 'tanh'.
    :type activation: string
    :param residual: Type of residual connections to use. Choices are "None",
        "normal" for :math:`\\mathbf{y}+f(\\mathbf{y})`, and "gated" for
        :math:`\\alpha\\mathbf{y} + (1 - \\alpha\\mathbf{y})` for learnable
        parameter :math:`\\alpha`.
    :type residual: string

    References:

    [1] Nicola De Cao, Ivan Titov, Wilker Aziz. Block Neural Autoregressive Flow.
    [arXiv:1904.04676]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    autoregressive = True

    def __init__(self, input_dim, hidden_factors=[8, 8], activation='tanh', residual=None):
        super().__init__(cache_size=1)
        if any([(h < 1) for h in hidden_factors]):
            raise ValueError('Hidden factors, {}, must all be >= 1'.format(hidden_factors))
        if residual not in [None, 'normal', 'gated']:
            raise ValueError('Invalid value {} for keyword argument "residual"'.format(residual))
        name_to_mixin = {'ELU': ELUTransform, 'LeakyReLU': LeakyReLUTransform, 'sigmoid': torch.distributions.transforms.SigmoidTransform, 'tanh': TanhTransform}
        if activation not in name_to_mixin:
            raise ValueError('Invalid activation function "{}"'.format(activation))
        self.T = name_to_mixin[activation]()
        self.residual = residual
        self.input_dim = input_dim
        self.layers = nn.ModuleList([MaskedBlockLinear(input_dim, input_dim * hidden_factors[0], input_dim)])
        for idx in range(1, len(hidden_factors)):
            self.layers.append(MaskedBlockLinear(input_dim * hidden_factors[idx - 1], input_dim * hidden_factors[idx], input_dim))
        self.layers.append(MaskedBlockLinear(input_dim * hidden_factors[-1], input_dim, input_dim))
        self._cached_logDetJ = None
        if residual == 'gated':
            self.gate = torch.nn.Parameter(torch.nn.init.normal_(torch.Tensor(1)))

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        y = x
        for idx in range(len(self.layers)):
            pre_activation, dy_dx = self.layers[idx](y.unsqueeze(-1))
            if idx == 0:
                y = self.T(pre_activation)
                J_act = self.T.log_abs_det_jacobian(pre_activation.view(*(list(x.size()) + [-1, 1])), y.view(*(list(x.size()) + [-1, 1])))
                logDetJ = dy_dx + J_act
            elif idx < len(self.layers) - 1:
                y = self.T(pre_activation)
                J_act = self.T.log_abs_det_jacobian(pre_activation.view(*(list(x.size()) + [-1, 1])), y.view(*(list(x.size()) + [-1, 1])))
                logDetJ = log_matrix_product(dy_dx, logDetJ) + J_act
            else:
                y = pre_activation
                logDetJ = log_matrix_product(dy_dx, logDetJ)
        self._cached_logDetJ = logDetJ.squeeze(-1).squeeze(-1)
        if self.residual == 'normal':
            y = y + x
            self._cached_logDetJ = F.softplus(self._cached_logDetJ)
        elif self.residual == 'gated':
            y = self.gate.sigmoid() * x + (1.0 - self.gate.sigmoid()) * y
            term1 = torch.log(self.gate.sigmoid() + eps)
            log1p_gate = torch.log1p(eps - self.gate.sigmoid())
            log_gate = torch.log(self.gate.sigmoid() + eps)
            term2 = F.softplus(log1p_gate - log_gate + self._cached_logDetJ)
            self._cached_logDetJ = term1 + term2
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. As noted above, this implementation is incapable of
        inverting arbitrary values `y`; rather it assumes `y` is the result of a
        previously computed application of the bijector to some `x` (which was
        cached on the forward call)
        """
        raise KeyError("BlockAutoregressive object expected to find key in intermediates cache but didn't")

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cached_logDetJ.sum(-1)


@copy_docs_from(Transform)
class ConditionedGeneralizedChannelPermute(Transform):
    domain = constraints.independent(constraints.real, 3)
    codomain = constraints.independent(constraints.real, 3)
    bijective = True

    def __init__(self, permutation=None, LU=None):
        super(ConditionedGeneralizedChannelPermute, self).__init__(cache_size=1)
        self.permutation = permutation
        self.LU = LU

    @property
    def U_diag(self):
        return self.LU.diag()

    @property
    def L(self):
        return self.LU.tril(diagonal=-1) + torch.eye(self.LU.size(-1), dtype=self.LU.dtype, device=self.LU.device)

    @property
    def U(self):
        return self.LU.triu()

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        """
        NOTE: As is the case for other conditional transforms, the batch dim of the
        context variable (reflected in the initial dimensions of filters in this
        case), if this is a conditional transform, must broadcast over the batch dim
        of the input variable.

        Also, the reason the following line uses matrix multiplication rather than
        F.conv2d is so we can perform multiple convolutions when the filters
        "kernel" has batch dimensions
        """
        filters = (self.permutation @ self.L @ self.U)[..., None, None]
        y = (filters * x.unsqueeze(-4)).sum(-3)
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x.
        """
        """
        NOTE: This method is equivalent to the following two lines. Using
        Tensor.inverse() would be numerically unstable, however.

        filters = (self.permutation @ self.L @ self.U).inverse()[..., None, None]
        x = F.conv2d(y.view(-1, *y.shape[-3:]), filters)
        return x.view_as(y)

        """
        y_flat = y.flatten(start_dim=-2)
        LUx = (y_flat.unsqueeze(-3) * self.permutation.T.unsqueeze(-1)).sum(-2)
        Ux = torch.linalg.solve_triangular(self.L, LUx, upper=False)
        x = torch.linalg.solve_triangular(self.U, Ux, upper=True)
        return x.reshape(x.shape[:-1] + y.shape[-2:])

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian, i.e.
        log(abs(det(dy/dx))).
        """
        h, w = x.shape[-2:]
        log_det = h * w * self.U_diag.abs().log().sum()
        return log_det * torch.ones(x.size()[:-3], dtype=x.dtype, layout=x.layout, device=x.device)


@copy_docs_from(ConditionedGeneralizedChannelPermute)
class GeneralizedChannelPermute(ConditionedGeneralizedChannelPermute, TransformModule):
    """
    A bijection that generalizes a permutation on the channels of a batch of 2D
    image in :math:`[\\ldots,C,H,W]` format. Specifically this transform performs
    the operation,

        :math:`\\mathbf{y} = \\text{torch.nn.functional.conv2d}(\\mathbf{x}, W)`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    and :math:`W\\sim C\\times C\\times 1\\times 1` is the filter matrix for a 1x1
    convolution with :math:`C` input and output channels.

    Ignoring the final two dimensions, :math:`W` is restricted to be the matrix
    product,

        :math:`W = PLU`

    where :math:`P\\sim C\\times C` is a permutation matrix on the channel
    dimensions, :math:`L\\sim C\\times C` is a lower triangular matrix with ones on
    the diagonal, and :math:`U\\sim C\\times C` is an upper triangular matrix.
    :math:`W` is initialized to a random orthogonal matrix. Then, :math:`P` is fixed
    and the learnable parameters set to :math:`L,U`.

    The input :math:`\\mathbf{x}` and output :math:`\\mathbf{y}` both have shape
    `[...,C,H,W]`, where `C` is the number of channels set at initialization.

    This operation was introduced in [1] for Glow normalizing flow, and is also
    known as 1x1 invertible convolution. It appears in other notable work such as
    [2,3], and corresponds to the class `tfp.bijectors.MatvecLU` of TensorFlow
    Probability.

    Example usage:

    >>> channels = 3
    >>> base_dist = dist.Normal(torch.zeros(channels, 32, 32),
    ... torch.ones(channels, 32, 32))
    >>> inv_conv = GeneralizedChannelPermute(channels=channels)
    >>> flow_dist = dist.TransformedDistribution(base_dist, [inv_conv])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param channels: Number of channel dimensions in the input.
    :type channels: int

    [1] Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible
    1x1 Convolutions. [arXiv:1807.03039]

    [2] Ryan Prenger, Rafael Valle, Bryan Catanzaro. WaveGlow: A Flow-based
    Generative Network for Speech Synthesis. [arXiv:1811.00002]

    [3] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural Spline
    Flows. [arXiv:1906.04032]

    """
    domain = constraints.independent(constraints.real, 3)
    codomain = constraints.independent(constraints.real, 3)
    bijective = True

    def __init__(self, channels=3, permutation=None):
        super(GeneralizedChannelPermute, self).__init__()
        self.__delattr__('permutation')
        W, _ = torch.linalg.qr(torch.randn(channels, channels))
        LU, pivots = torch.linalg.lu_factor(W)
        if permutation is None:
            P, _, _ = torch.lu_unpack(LU, pivots)
        else:
            if len(permutation) != channels:
                raise ValueError('Keyword argument "permutation" expected to have {} elements but {} found.'.format(channels, len(permutation)))
            P = torch.eye(channels, channels)[permutation.type(dtype=torch.int64)]
        self.register_buffer('permutation', P)
        self.LU = torch.nn.Parameter(LU)


@copy_docs_from(ConditionalTransformModule)
class ConditionalGeneralizedChannelPermute(ConditionalTransformModule):
    """
    A bijection that generalizes a permutation on the channels of a batch of 2D
    image in :math:`[\\ldots,C,H,W]` format conditioning on an additional context
    variable. Specifically this transform performs the operation,

        :math:`\\mathbf{y} = \\text{torch.nn.functional.conv2d}(\\mathbf{x}, W)`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    and :math:`W\\sim C\\times C\\times 1\\times 1` is the filter matrix for a 1x1
    convolution with :math:`C` input and output channels.

    Ignoring the final two dimensions, :math:`W` is restricted to be the matrix
    product,

        :math:`W = PLU`

    where :math:`P\\sim C\\times C` is a permutation matrix on the channel
    dimensions, and  :math:`LU\\sim C\\times C` is an invertible product of a lower
    triangular and an upper triangular matrix that is the output of an NN with
    input :math:`z\\in\\mathbb{R}^{M}` representing the context variable to
    condition on.

    The input :math:`\\mathbf{x}` and output :math:`\\mathbf{y}` both have shape
    `[...,C,H,W]`, where `C` is the number of channels set at initialization.

    This operation was introduced in [1] for Glow normalizing flow, and is also
    known as 1x1 invertible convolution. It appears in other notable work such as
    [2,3], and corresponds to the class `tfp.bijectors.MatvecLU` of TensorFlow
    Probability.

    Example usage:

    >>> from pyro.nn.dense_nn import DenseNN
    >>> context_dim = 5
    >>> batch_size = 3
    >>> channels = 3
    >>> base_dist = dist.Normal(torch.zeros(channels, 32, 32),
    ... torch.ones(channels, 32, 32))
    >>> hidden_dims = [context_dim*10, context_dim*10]
    >>> nn = DenseNN(context_dim, hidden_dims, param_dims=[channels*channels])
    >>> transform = ConditionalGeneralizedChannelPermute(nn, channels=channels)
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP

    :param nn: a function inputting the context variable and outputting
        real-valued parameters of dimension :math:`C^2`.
    :param channels: Number of channel dimensions in the input.
    :type channels: int

    [1] Diederik P. Kingma, Prafulla Dhariwal. Glow: Generative Flow with Invertible
    1x1 Convolutions. [arXiv:1807.03039]

    [2] Ryan Prenger, Rafael Valle, Bryan Catanzaro. WaveGlow: A Flow-based
    Generative Network for Speech Synthesis. [arXiv:1811.00002]

    [3] Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural Spline
    Flows. [arXiv:1906.04032]

    """
    domain = constraints.independent(constraints.real, 3)
    codomain = constraints.independent(constraints.real, 3)
    bijective = True

    def __init__(self, nn, channels=3, permutation=None):
        super().__init__()
        self.nn = nn
        self.channels = channels
        if permutation is None:
            permutation = torch.randperm(channels, device='cpu')
        P = torch.eye(len(permutation), len(permutation))[permutation.type(dtype=torch.int64)]
        self.register_buffer('permutation', P)

    def condition(self, context):
        LU = self.nn(context)
        LU = LU.view(LU.shape[:-1] + (self.channels, self.channels))
        return ConditionedGeneralizedChannelPermute(self.permutation, LU)


class ConditionedHouseholder(Transform):
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    volume_preserving = True

    def __init__(self, u_unnormed=None):
        super().__init__(cache_size=1)
        self.u_unnormed = u_unnormed

    def u(self):
        u_unnormed = self.u_unnormed() if callable(self.u_unnormed) else self.u_unnormed
        norm = torch.norm(u_unnormed, p=2, dim=-1, keepdim=True)
        return torch.div(u_unnormed, norm)

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        y = x
        u = self.u()
        for idx in range(u.size(-2)):
            projection = (u[..., idx, :] * y).sum(dim=-1, keepdim=True) * u[..., idx, :]
            y = y - 2.0 * projection
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. The Householder transformation, H, is "involutory," i.e.
        H^2 = I. If you reflect a point around a plane, then the same operation will
        reflect it back
        """
        x = y
        u = self.u()
        for jdx in reversed(range(u.size(-2))):
            projection = (u[..., jdx, :] * x).sum(dim=-1, keepdim=True) * u[..., jdx, :]
            x = x - 2.0 * projection
        return x

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log jacobian. Householder flow
        is measure preserving, so :math:`\\log(|detJ|) = 0`
        """
        return torch.zeros(x.size()[:-1], dtype=x.dtype, layout=x.layout, device=x.device)


@copy_docs_from(TransformModule)
class Householder(ConditionedHouseholder, TransformModule):
    """
    Represents multiple applications of the Householder bijective transformation. A
    single Householder transformation takes the form,

        :math:`\\mathbf{y} = (I - 2*\\frac{\\mathbf{u}\\mathbf{u}^T}{||\\mathbf{u}||^2})\\mathbf{x}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    and the learnable parameters are :math:`\\mathbf{u}\\in\\mathbb{R}^D` for input
    dimension :math:`D`.

    The transformation represents the reflection of :math:`\\mathbf{x}` through the
    plane passing through the origin with normal :math:`\\mathbf{u}`.

    :math:`D` applications of this transformation are able to transform standard
    i.i.d. standard Gaussian noise into a Gaussian variable with an arbitrary
    covariance matrix. With :math:`K<D` transformations, one is able to approximate
    a full-rank Gaussian distribution using a linear transformation of rank
    :math:`K`.

    Together with :class:`~pyro.distributions.TransformedDistribution` this provides
    a way to create richer variational approximations.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = Householder(10, count_transforms=5)
    >>> pyro.module("my_transform", p) # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int
    :param count_transforms: number of applications of Householder transformation to
        apply.
    :type count_transforms: int

    References:

    [1] Jakub M. Tomczak, Max Welling. Improving Variational Auto-Encoders using
    Householder Flow. [arXiv:1611.09630]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    volume_preserving = True

    def __init__(self, input_dim, count_transforms=1):
        super().__init__()
        self.input_dim = input_dim
        if count_transforms < 1:
            raise ValueError('Number of Householder transforms, {}, is less than 1!'.format(count_transforms))
        elif count_transforms > input_dim:
            warnings.warn('Number of Householder transforms, {}, is greater than input dimension {}, which is an over-parametrization!'.format(count_transforms, input_dim))
        self.u_unnormed = nn.Parameter(torch.Tensor(count_transforms, input_dim))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.u_unnormed.size(-1))
        self.u_unnormed.data.uniform_(-stdv, stdv)


@copy_docs_from(ConditionalTransformModule)
class ConditionalHouseholder(ConditionalTransformModule):
    """
    Represents multiple applications of the Householder bijective transformation
    conditioning on an additional context. A single Householder transformation takes
    the form,

        :math:`\\mathbf{y} = (I - 2*\\frac{\\mathbf{u}\\mathbf{u}^T}{||\\mathbf{u}||^2})\\mathbf{x}`

    where :math:`\\mathbf{x}` are the inputs with dimension :math:`D`,
    :math:`\\mathbf{y}` are the outputs, and :math:`\\mathbf{u}\\in\\mathbb{R}^D`
    is the output of a function, e.g. a NN, with input :math:`z\\in\\mathbb{R}^{M}`
    representing the context variable to condition on.

    The transformation represents the reflection of :math:`\\mathbf{x}` through the
    plane passing through the origin with normal :math:`\\mathbf{u}`.

    :math:`D` applications of this transformation are able to transform standard
    i.i.d. standard Gaussian noise into a Gaussian variable with an arbitrary
    covariance matrix. With :math:`K<D` transformations, one is able to approximate
    a full-rank Gaussian distribution using a linear transformation of rank
    :math:`K`.

    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`
    this provides a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn.dense_nn import DenseNN
    >>> input_dim = 10
    >>> context_dim = 5
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [input_dim]
    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)
    >>> transform = ConditionalHouseholder(input_dim, hypernet)
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int
    :param nn: a function inputting the context variable and outputting a triplet of
        real-valued parameters of dimensions :math:`(1, D, D)`.
    :type nn: callable
    :param count_transforms: number of applications of Householder transformation to
        apply.
    :type count_transforms: int

    References:

    [1] Jakub M. Tomczak, Max Welling. Improving Variational Auto-Encoders using
    Householder Flow. [arXiv:1611.09630]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim, nn, count_transforms=1):
        super().__init__()
        self.nn = nn
        self.input_dim = input_dim
        if count_transforms < 1:
            raise ValueError('Number of Householder transforms, {}, is less than 1!'.format(count_transforms))
        elif count_transforms > input_dim:
            warnings.warn('Number of Householder transforms, {}, is greater than input dimension {}, which is an over-parametrization!'.format(count_transforms, input_dim))
        self.count_transforms = count_transforms

    def _u_unnormed(self, context):
        u_unnormed = self.nn(context)
        if self.count_transforms == 1:
            u_unnormed = u_unnormed.unsqueeze(-2)
        else:
            u_unnormed = torch.stack(u_unnormed, dim=-2)
        return u_unnormed

    def condition(self, context):
        u_unnormed = partial(self._u_unnormed, context)
        return ConditionedHouseholder(u_unnormed)


@copy_docs_from(Transform)
class ConditionedMatrixExponential(Transform):
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, weights=None, iterations=8, normalization='none', bound=None):
        super().__init__(cache_size=1)
        assert iterations > 0
        self.weights = weights
        self.iterations = iterations
        self.normalization = normalization
        self.bound = bound
        if normalization == 'weight' or normalization == 'spectral':
            raise NotImplementedError('Normalization is currently not implemented.')
        elif normalization != 'none':
            raise ValueError('Unknown normalization method: {}'.format(normalization))

    def _exp(self, x, M):
        """
        Performs power series approximation to the vector product of x with the
        matrix exponential of M.
        """
        power_term = x.unsqueeze(-1)
        y = x.unsqueeze(-1)
        for idx in range(self.iterations):
            power_term = torch.matmul(M, power_term) / (idx + 1)
            y = y + power_term
        return y.squeeze(-1)

    def _trace(self, M):
        """
        Calculates the trace of a matrix and is able to do broadcasting over batch
        dimensions, unlike `torch.trace`.

        Broadcasting is necessary for the conditional version of the transform,
        where `self.weights` may have batch dimensions corresponding the batch
        dimensions of the context variable that was conditioned upon.
        """
        return M.diagonal(dim1=-2, dim2=-1).sum(-1)

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor
        Invokes the bijection x => y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        M = self.weights() if callable(self.weights) else self.weights
        return self._exp(x, M)

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor
        Inverts y => x.
        """
        M = self.weights() if callable(self.weights) else self.weights
        return self._exp(y, -M)

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the element-wise determinant of the log Jacobian
        """
        M = self.weights() if callable(self.weights) else self.weights
        return self._trace(M)


@copy_docs_from(ConditionedMatrixExponential)
class MatrixExponential(ConditionedMatrixExponential, TransformModule):
    """
    A dense matrix exponential bijective transform (Hoogeboom et al., 2020) with
    equation,

        :math:`\\mathbf{y} = \\exp(M)\\mathbf{x}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    :math:`\\exp(\\cdot)` represents the matrix exponential, and the learnable
    parameters are :math:`M\\in\\mathbb{R}^D\\times\\mathbb{R}^D` for input dimension
    :math:`D`. In general, :math:`M` is not required to be invertible.

    Due to the favourable mathematical properties of the matrix exponential, the
    transform has an exact inverse and a log-determinate-Jacobian that scales in
    time-complexity as :math:`O(D)`. Both the forward and reverse operations are
    approximated with a truncated power series. For numerical stability, the
    norm of :math:`M` can be restricted with the `normalization` keyword argument.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = MatrixExponential(10)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int
    :param iterations: the number of terms to use in the truncated power series that
        approximates matrix exponentiation.
    :type iterations: int
    :param normalization: One of `['none', 'weight', 'spectral']` normalization that
        selects what type of normalization to apply to the weight matrix. `weight`
        corresponds to weight normalization (Salimans and Kingma, 2016) and
        `spectral` to spectral normalization (Miyato et al, 2018).
    :type normalization: string
    :param bound: a bound on either the weight or spectral norm, when either of
        those two types of regularization are chosen by the `normalization`
        argument. A lower value for this results in fewer required terms of the
        truncated power series to closely approximate the exact value of the matrix
        exponential.
    :type bound: float

    References:

    [1] Emiel Hoogeboom, Victor Garcia Satorras, Jakub M. Tomczak, Max Welling. The
        Convolution Exponential and Generalized Sylvester Flows. [arXiv:2006.01910]
    [2] Tim Salimans, Diederik P. Kingma. Weight Normalization: A Simple
        Reparameterization to Accelerate Training of Deep Neural Networks.
        [arXiv:1602.07868]
    [3] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida. Spectral
        Normalization for Generative Adversarial Networks. ICLR 2018.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim, iterations=8, normalization='none', bound=None):
        super().__init__(iterations=iterations, normalization=normalization, bound=bound)
        self.weights = nn.Parameter(torch.Tensor(input_dim, input_dim))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.weights.size(0))
        self.weights.data.uniform_(-stdv, stdv)


@copy_docs_from(ConditionalTransformModule)
class ConditionalMatrixExponential(ConditionalTransformModule):
    """
    A dense matrix exponential bijective transform (Hoogeboom et al., 2020) that
    conditions on an additional context variable with equation,

        :math:`\\mathbf{y} = \\exp(M)\\mathbf{x}`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    :math:`\\exp(\\cdot)` represents the matrix exponential, and
    :math:`M\\in\\mathbb{R}^D\\times\\mathbb{R}^D` is the output of a neural network
    conditioning on a context variable :math:`\\mathbf{z}` for input dimension
    :math:`D`. In general, :math:`M` is not required to be invertible.

    Due to the favourable mathematical properties of the matrix exponential, the
    transform has an exact inverse and a log-determinate-Jacobian that scales in
    time-complexity as :math:`O(D)`. Both the forward and reverse operations are
    approximated with a truncated power series. For numerical stability, the
    norm of :math:`M` can be restricted with the `normalization` keyword argument.

    Example usage:

    >>> from pyro.nn.dense_nn import DenseNN
    >>> input_dim = 10
    >>> context_dim = 5
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [input_dim*input_dim]
    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)
    >>> transform = ConditionalMatrixExponential(input_dim, hypernet)
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int
    :param iterations: the number of terms to use in the truncated power series that
        approximates matrix exponentiation.
    :type iterations: int
    :param normalization: One of `['none', 'weight', 'spectral']` normalization that
        selects what type of normalization to apply to the weight matrix. `weight`
        corresponds to weight normalization (Salimans and Kingma, 2016) and
        `spectral` to spectral normalization (Miyato et al, 2018).
    :type normalization: string
    :param bound: a bound on either the weight or spectral norm, when either of
        those two types of regularization are chosen by the `normalization`
        argument. A lower value for this results in fewer required terms of the
        truncated power series to closely approximate the exact value of the matrix
        exponential.
    :type bound: float

    References:

    [1] Emiel Hoogeboom, Victor Garcia Satorras, Jakub M. Tomczak, Max Welling. The
        Convolution Exponential and Generalized Sylvester Flows. [arXiv:2006.01910]
    [2] Tim Salimans, Diederik P. Kingma. Weight Normalization: A Simple
        Reparameterization to Accelerate Training of Deep Neural Networks.
        [arXiv:1602.07868]
    [3] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, Yuichi Yoshida. Spectral
        Normalization for Generative Adversarial Networks. ICLR 2018.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim, nn, iterations=8, normalization='none', bound=None):
        super().__init__()
        self.input_dim = input_dim
        self.nn = nn
        self.iterations = iterations
        self.normalization = normalization
        self.bound = bound

    def _params(self, context):
        return self.nn(context)

    def condition(self, context):
        cond_nn = partial(self.nn, context)

        def weights():
            w = cond_nn()
            return w.view(w.shape[:-1] + (self.input_dim, self.input_dim))
        return ConditionedMatrixExponential(weights, iterations=self.iterations, normalization=self.normalization, bound=self.bound)


@copy_docs_from(TransformModule)
class NeuralAutoregressive(TransformModule):
    """
    An implementation of the deep Neural Autoregressive Flow (NAF) bijective
    transform of the "IAF flavour" that can be used for sampling and scoring samples
    drawn from it (but not arbitrary ones).

    Example usage:

    >>> from pyro.nn import AutoRegressiveNN
    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> arn = AutoRegressiveNN(10, [40], param_dims=[16]*3)
    >>> transform = NeuralAutoregressive(arn, hidden_units=16)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    The inverse operation is not implemented. This would require numerical
    inversion, e.g., using a root finding method - a possibility for a future
    implementation.

    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns a tuple of three real-valued tensors, whose last dimension is the
        input dimension, and whose penultimate dimension is equal to hidden_units.
    :type autoregressive_nn: nn.Module
    :param hidden_units: the number of hidden units to use in the NAF transformation
        (see Eq (8) in reference)
    :type hidden_units: int
    :param activation: Activation function to use. One of 'ELU', 'LeakyReLU',
        'sigmoid', or 'tanh'.
    :type activation: string

    Reference:

    [1] Chin-Wei Huang, David Krueger, Alexandre Lacoste, Aaron Courville. Neural
    Autoregressive Flows. [arXiv:1804.00779]


    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    eps = 1e-08
    autoregressive = True

    def __init__(self, autoregressive_nn, hidden_units=16, activation='sigmoid'):
        super().__init__(cache_size=1)
        name_to_mixin = {'ELU': ELUTransform, 'LeakyReLU': LeakyReLUTransform, 'sigmoid': SigmoidTransform, 'tanh': TanhTransform}
        if activation not in name_to_mixin:
            raise ValueError('Invalid activation function "{}"'.format(activation))
        self.T = name_to_mixin[activation]()
        self.arn = autoregressive_nn
        self.hidden_units = hidden_units
        self.logsoftmax = nn.LogSoftmax(dim=-2)
        self._cached_log_df_inv_dx = None
        self._cached_A = None
        self._cached_W_pre = None
        self._cached_C = None
        self._cached_T_C = None

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        A, W_pre, b = self.arn(x)
        T = self.T
        A = F.softplus(A)
        C = A * x.unsqueeze(-2) + b
        W = F.softmax(W_pre, dim=-2)
        T_C = T(C)
        D = (W * T_C).sum(dim=-2)
        y = T.inv(D)
        self._cached_log_df_inv_dx = T.inv.log_abs_det_jacobian(D, y)
        self._cached_A = A
        self._cached_W_pre = W_pre
        self._cached_C = C
        self._cached_T_C = T_C
        return y

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        A = self._cached_A
        W_pre = self._cached_W_pre
        C = self._cached_C
        T_C = self._cached_T_C
        T = self.T
        log_dydD = self._cached_log_df_inv_dx
        log_dDdx = torch.logsumexp(torch.log(A + self.eps) + self.logsoftmax(W_pre) + T.log_abs_det_jacobian(C, T_C), dim=-2)
        log_det = log_dydD + log_dDdx
        return log_det.sum(-1)


@copy_docs_from(ConditionalTransformModule)
class ConditionalNeuralAutoregressive(ConditionalTransformModule):
    """
    An implementation of the deep Neural Autoregressive Flow (NAF) bijective
    transform of the "IAF flavour" conditioning on an additiona context variable
    that can be used for sampling and scoring samples drawn from it (but not
    arbitrary ones).

    Example usage:

    >>> from pyro.nn import ConditionalAutoRegressiveNN
    >>> input_dim = 10
    >>> context_dim = 5
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> arn = ConditionalAutoRegressiveNN(input_dim, context_dim, [40],
    ... param_dims=[16]*3)
    >>> transform = ConditionalNeuralAutoregressive(arn, hidden_units=16)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP

    The inverse operation is not implemented. This would require numerical
    inversion, e.g., using a root finding method - a possibility for a future
    implementation.

    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns a tuple of three real-valued tensors, whose last dimension is the
        input dimension, and whose penultimate dimension is equal to hidden_units.
    :type autoregressive_nn: nn.Module
    :param hidden_units: the number of hidden units to use in the NAF transformation
        (see Eq (8) in reference)
    :type hidden_units: int
    :param activation: Activation function to use. One of 'ELU', 'LeakyReLU',
        'sigmoid', or 'tanh'.
    :type activation: string

    Reference:

    [1] Chin-Wei Huang, David Krueger, Alexandre Lacoste, Aaron Courville. Neural
    Autoregressive Flows. [arXiv:1804.00779]


    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, autoregressive_nn, **kwargs):
        super().__init__()
        self.nn = autoregressive_nn
        self.kwargs = kwargs

    def condition(self, context):
        """
        Conditions on a context variable, returning a non-conditional transform of
        of type :class:`~pyro.distributions.transforms.NeuralAutoregressive`.
        """
        cond_nn = partial(self.nn, context=context)
        cond_nn.permutation = cond_nn.func.permutation
        cond_nn.get_permutation = cond_nn.func.get_permutation
        return NeuralAutoregressive(cond_nn, **self.kwargs)


@copy_docs_from(Transform)
class ConditionedPlanar(Transform):
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, params):
        super().__init__(cache_size=1)
        self._params = params
        self._cached_logDetJ = None

    def u_hat(self, u, w):
        alpha = torch.matmul(u.unsqueeze(-2), w.unsqueeze(-1)).squeeze(-1)
        a_prime = -1 + F.softplus(alpha)
        return u + (a_prime - alpha) * w.div(w.pow(2).sum(dim=-1, keepdim=True))

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor
        Invokes the bijection x => y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        bias, u, w = self._params() if callable(self._params) else self._params
        act = torch.tanh(torch.matmul(w.unsqueeze(-2), x.unsqueeze(-1)).squeeze(-1) + bias)
        u_hat = self.u_hat(u, w)
        y = x + u_hat * act
        psi_z = (1.0 - act.pow(2)) * w
        self._cached_logDetJ = torch.log(torch.abs(1 + torch.matmul(psi_z.unsqueeze(-2), u_hat.unsqueeze(-1)).squeeze(-1).squeeze(-1)))
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor
        Inverts y => x. As noted above, this implementation is incapable of
        inverting arbitrary values `y`; rather it assumes `y` is the result of a
        previously computed application of the bijector to some `x` (which was
        cached on the forward call)
        """
        raise KeyError("ConditionedPlanar object expected to find key in intermediates cache but didn't")

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cached_logDetJ


@copy_docs_from(ConditionedPlanar)
class Planar(ConditionedPlanar, TransformModule):
    """
    A 'planar' bijective transform with equation,

        :math:`\\mathbf{y} = \\mathbf{x} + \\mathbf{u}\\tanh(\\mathbf{w}^T\\mathbf{z}+b)`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    and the learnable parameters are :math:`b\\in\\mathbb{R}`,
    :math:`\\mathbf{u}\\in\\mathbb{R}^D`, :math:`\\mathbf{w}\\in\\mathbb{R}^D` for
    input dimension :math:`D`. For this to be an invertible transformation, the
    condition :math:`\\mathbf{w}^T\\mathbf{u}>-1` is enforced.

    Together with :class:`~pyro.distributions.TransformedDistribution` this provides
    a way to create richer variational approximations.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = Planar(10)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    The inverse of this transform does not possess an analytical solution and is
    left unimplemented. However, the inverse is cached when the forward operation is
    called during sampling, and so samples drawn using the planar transform can be
    scored.

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int

    References:

    [1] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with
    Normalizing Flows. [arXiv:1505.05770]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim):
        super().__init__(self._params)
        self.bias = nn.Parameter(torch.Tensor(1))
        self.u = nn.Parameter(torch.Tensor(input_dim))
        self.w = nn.Parameter(torch.Tensor(input_dim))
        self.input_dim = input_dim
        self.reset_parameters()

    def _params(self):
        return self.bias, self.u, self.w

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.u.size(0))
        self.w.data.uniform_(-stdv, stdv)
        self.u.data.uniform_(-stdv, stdv)
        self.bias.data.zero_()


@copy_docs_from(ConditionalTransformModule)
class ConditionalPlanar(ConditionalTransformModule):
    """
    A conditional 'planar' bijective transform using the equation,

        :math:`\\mathbf{y} = \\mathbf{x} + \\mathbf{u}\\tanh(\\mathbf{w}^T\\mathbf{z}+b)`

    where :math:`\\mathbf{x}` are the inputs with dimension :math:`D`,
    :math:`\\mathbf{y}` are the outputs, and the pseudo-parameters
    :math:`b\\in\\mathbb{R}`, :math:`\\mathbf{u}\\in\\mathbb{R}^D`, and
    :math:`\\mathbf{w}\\in\\mathbb{R}^D` are the output of a function, e.g. a NN,
    with input :math:`z\\in\\mathbb{R}^{M}` representing the context variable to
    condition on. For this to be an invertible transformation, the condition
    :math:`\\mathbf{w}^T\\mathbf{u}>-1` is enforced.

    Together with :class:`~pyro.distributions.ConditionalTransformedDistribution`
    this provides a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn.dense_nn import DenseNN
    >>> input_dim = 10
    >>> context_dim = 5
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [1, input_dim, input_dim]
    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)
    >>> transform = ConditionalPlanar(hypernet)
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP

    The inverse of this transform does not possess an analytical solution and is
    left unimplemented. However, the inverse is cached when the forward operation is
    called during sampling, and so samples drawn using the planar transform can be
    scored.

    :param nn: a function inputting the context variable and outputting a triplet of
        real-valued parameters of dimensions :math:`(1, D, D)`.
    :type nn: callable

    References:
    [1] Variational Inference with Normalizing Flows [arXiv:1505.05770]
    Danilo Jimenez Rezende, Shakir Mohamed

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, nn):
        super().__init__()
        self.nn = nn

    def _params(self, context):
        return self.nn(context)

    def condition(self, context):
        params = partial(self._params, context)
        return ConditionedPlanar(params)


@copy_docs_from(TransformModule)
class Polynomial(TransformModule):
    """
    An autoregressive bijective transform as described in Jaini et al. (2019)
    applying following equation element-wise,

        :math:`y_n = c_n + \\int^{x_n}_0\\sum^K_{k=1}\\left(\\sum^R_{r=0}a^{(n)}_{r,k}u^r\\right)du`

    where :math:`x_n` is the :math:`n`th input, :math:`y_n` is the :math:`n`th
    output, and :math:`c_n\\in\\mathbb{R}`,
    :math:`\\left\\{a^{(n)}_{r,k}\\in\\mathbb{R}\\right\\}` are learnable parameters
    that are the output of an autoregressive NN inputting
    :math:`x_{\\prec n}={x_1,x_2,\\ldots,x_{n-1}}`.

    Together with :class:`~pyro.distributions.TransformedDistribution` this provides
    a way to create richer variational approximations.

    Example usage:

    >>> from pyro.nn import AutoRegressiveNN
    >>> input_dim = 10
    >>> count_degree = 4
    >>> count_sum = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [(count_degree + 1)*count_sum]
    >>> arn = AutoRegressiveNN(input_dim, [input_dim*10], param_dims)
    >>> transform = Polynomial(arn, input_dim=input_dim, count_degree=count_degree,
    ... count_sum=count_sum)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    The inverse of this transform does not possess an analytical solution and is
    left unimplemented. However, the inverse is cached when the forward operation is
    called during sampling, and so samples drawn using a polynomial transform can be
    scored.

    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns a tensor of real-valued
        numbers of size (batch_size, (count_degree+1)*count_sum, input_dim)
    :type autoregressive_nn: nn.Module
    :param count_degree: The degree of the polynomial to use for each element-wise
        transformation.
    :type count_degree: int
    :param count_sum: The number of polynomials to sum in each element-wise
        transformation.
    :type count_sum: int

    References:

    [1] Priyank Jaini, Kira A. Shelby, Yaoliang Yu. Sum-of-squares polynomial flow.
    [arXiv:1905.02325]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    autoregressive = True

    def __init__(self, autoregressive_nn, input_dim, count_degree, count_sum):
        super().__init__(cache_size=1)
        self.arn = autoregressive_nn
        self.input_dim = input_dim
        self.count_degree = count_degree
        self.count_sum = count_sum
        self._cached_logDetJ = None
        self.c = nn.Parameter(torch.Tensor(input_dim))
        self.reset_parameters()
        powers = torch.arange(1, count_degree + 2, dtype=torch.get_default_dtype())
        self.register_buffer('powers', powers)
        mask = self.powers + torch.arange(count_degree + 1).unsqueeze(-1).type_as(powers)
        power_mask = mask
        mask = mask.reciprocal()
        self.register_buffer('power_mask', power_mask)
        self.register_buffer('mask', mask)

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.c.size(0))
        self.c.data.uniform_(-stdv, stdv)

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        A = self.arn(x).view(-1, self.count_sum, self.count_degree + 1, self.input_dim)
        coefs = A.unsqueeze(-2) * A.unsqueeze(-3)
        x_view = x.view(-1, 1, 1, self.input_dim)
        x_pow_matrix = x_view.pow(self.power_mask.unsqueeze(-1)).unsqueeze(-4)
        y = self.c + (coefs * x_pow_matrix * self.mask.unsqueeze(-1)).sum((1, 2, 3)).view_as(x)
        x_pow_matrix = x_view.pow(self.power_mask.unsqueeze(-1) - 1).unsqueeze(-4)
        self._cached_logDetJ = torch.log((coefs * x_pow_matrix).sum((1, 2, 3)).view_as(x) + 1e-08).sum(-1)
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. As noted above, this implementation is incapable of
        inverting arbitrary values `y`; rather it assumes `y` is the result of a
        previously computed application of the bijector to some `x` (which was
        cached on the forward call)
        """
        raise KeyError("Polynomial object expected to find key in intermediates cache but didn't")

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cached_logDetJ


@copy_docs_from(Transform)
class ConditionedRadial(Transform):
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, params):
        super().__init__(cache_size=1)
        self._params = params
        self._cached_logDetJ = None

    def u_hat(self, u, w):
        alpha = torch.matmul(u.unsqueeze(-2), w.unsqueeze(-1)).squeeze(-1)
        a_prime = -1 + F.softplus(alpha)
        return u + (a_prime - alpha) * w.div(w.pow(2).sum(dim=-1, keepdim=True))

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from the base distribution (or the output
        of a previous transform)
        """
        x0, alpha_prime, beta_prime = self._params() if callable(self._params) else self._params
        alpha = F.softplus(alpha_prime)
        beta = -alpha + F.softplus(beta_prime)
        diff = x - x0
        r = diff.norm(dim=-1, keepdim=True)
        h = (alpha + r).reciprocal()
        h_prime = -h ** 2
        beta_h = beta * h
        self._cached_logDetJ = ((x0.size(-1) - 1) * torch.log1p(beta_h) + torch.log1p(beta_h + beta * h_prime * r)).sum(-1)
        return x + beta_h * diff

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor
        Inverts y => x. As noted above, this implementation is incapable of
        inverting arbitrary values `y`; rather it assumes `y` is the result of a
        previously computed application of the bijector to some `x` (which was
        cached on the forward call)
        """
        raise KeyError("ConditionedRadial object expected to find key in intermediates cache but didn't")

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cached_logDetJ


@copy_docs_from(ConditionedRadial)
class Radial(ConditionedRadial, TransformModule):
    """
    A 'radial' bijective transform using the equation,

        :math:`\\mathbf{y} = \\mathbf{x} + \\beta h(\\alpha,r)(\\mathbf{x} - \\mathbf{x}_0)`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    and the learnable parameters are :math:`\\alpha\\in\\mathbb{R}^+`,
    :math:`\\beta\\in\\mathbb{R}`, :math:`\\mathbf{x}_0\\in\\mathbb{R}^D`, for input
    dimension :math:`D`, :math:`r=||\\mathbf{x}-\\mathbf{x}_0||_2`,
    :math:`h(\\alpha,r)=1/(\\alpha+r)`. For this to be an invertible transformation,
    the condition :math:`\\beta>-\\alpha` is enforced.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = Radial(10)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    The inverse of this transform does not possess an analytical solution and is
    left unimplemented. However, the inverse is cached when the forward operation is
    called during sampling, and so samples drawn using the radial transform can be
    scored.

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int

    References:

    [1] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with
    Normalizing Flows. [arXiv:1505.05770]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim):
        super().__init__(self._params)
        self.x0 = nn.Parameter(torch.Tensor(input_dim))
        self.alpha_prime = nn.Parameter(torch.Tensor(1))
        self.beta_prime = nn.Parameter(torch.Tensor(1))
        self.input_dim = input_dim
        self.reset_parameters()

    def _params(self):
        return self.x0, self.alpha_prime, self.beta_prime

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.x0.size(0))
        self.alpha_prime.data.uniform_(-stdv, stdv)
        self.beta_prime.data.uniform_(-stdv, stdv)
        self.x0.data.uniform_(-stdv, stdv)


@copy_docs_from(ConditionalTransformModule)
class ConditionalRadial(ConditionalTransformModule):
    """
    A conditional 'radial' bijective transform context using the equation,

        :math:`\\mathbf{y} = \\mathbf{x} + \\beta h(\\alpha,r)(\\mathbf{x} - \\mathbf{x}_0)`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    and :math:`\\alpha\\in\\mathbb{R}^+`, :math:`\\beta\\in\\mathbb{R}`,
    and :math:`\\mathbf{x}_0\\in\\mathbb{R}^D`, are the output of a function, e.g. a NN,
    with input :math:`z\\in\\mathbb{R}^{M}` representing the context variable to
    condition on. The input dimension is :math:`D`,
    :math:`r=||\\mathbf{x}-\\mathbf{x}_0||_2`, and :math:`h(\\alpha,r)=1/(\\alpha+r)`.
    For this to be an invertible transformation, the condition :math:`\\beta>-\\alpha`
    is enforced.

    Example usage:

    >>> from pyro.nn.dense_nn import DenseNN
    >>> input_dim = 10
    >>> context_dim = 5
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [input_dim, 1, 1]
    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)
    >>> transform = ConditionalRadial(hypernet)
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP

    The inverse of this transform does not possess an analytical solution and is
    left unimplemented. However, the inverse is cached when the forward operation is
    called during sampling, and so samples drawn using the radial transform can be
    scored.

    :param input_dim: the dimension of the input (and output) variable.
    :type input_dim: int

    References:

    [1] Danilo Jimenez Rezende, Shakir Mohamed. Variational Inference with
    Normalizing Flows. [arXiv:1505.05770]

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, nn):
        super().__init__()
        self.nn = nn

    def _params(self, context):
        return self.nn(context)

    def condition(self, context):
        params = partial(self._params, context)
        return ConditionedRadial(params)


def _calculate_knots(lengths, lower, upper):
    """
    Given a tensor of unscaled bin lengths that sum to 1, plus the lower and upper
    limits, returns the shifted and scaled lengths plus knot positions
    """
    knots = torch.cumsum(lengths, dim=-1)
    knots = F.pad(knots, pad=(1, 0), mode='constant', value=0.0)
    knots = (upper - lower) * knots + lower
    knots[..., 0] = lower
    knots[..., -1] = upper
    lengths = knots[..., 1:] - knots[..., :-1]
    return lengths, knots


def _searchsorted(sorted_sequence, values):
    """
    Searches for which bin an input belongs to (in a way that is parallelizable and
    amenable to autodiff)

    TODO: Replace with torch.searchsorted once it is released
    """
    return torch.sum(values[..., None] >= sorted_sequence, dim=-1) - 1


def _select_bins(x, idx):
    """
    Performs gather to select the bin in the correct way on batched inputs
    """
    idx = idx.clamp(min=0, max=x.size(-1) - 1)
    """
    Broadcast dimensions of idx over x

    idx ~ (batch_dims, input_dim, 1)
    x ~ (context_batch_dims, input_dim, count_bins)

    Note that by convention, the context variable batch dimensions must broadcast
    over the input batch dimensions.
    """
    if len(idx.shape) >= len(x.shape):
        x = x.reshape((1,) * (len(idx.shape) - len(x.shape)) + x.shape)
        x = x.expand(idx.shape[:-2] + (-1,) * 2)
    return x.gather(-1, idx).squeeze(-1)


def _monotonic_rational_spline(inputs, widths, heights, derivatives, lambdas=None, inverse=False, bound=3.0, min_bin_width=0.001, min_bin_height=0.001, min_derivative=0.001, min_lambda=0.025, eps=1e-06):
    """
    Calculating a monotonic rational spline (linear or quadratic) or its inverse,
    plus the log(abs(detJ)) required for normalizing flows.
    NOTE: I omit the docstring with parameter descriptions for this method since it
    is not considered "public" yet!
    """
    assert bound > 0.0
    num_bins = widths.shape[-1]
    if min_bin_width * num_bins > 1.0:
        raise ValueError('Minimal bin width too large for the number of bins')
    if min_bin_height * num_bins > 1.0:
        raise ValueError('Minimal bin height too large for the number of bins')
    left, right = -bound, bound
    bottom, top = -bound, bound
    inside_interval_mask = (inputs >= left) & (inputs <= right)
    outside_interval_mask = ~inside_interval_mask
    outputs = torch.zeros_like(inputs)
    logabsdet = torch.zeros_like(inputs)
    widths = min_bin_width + (1.0 - min_bin_width * num_bins) * widths
    heights = min_bin_height + (1.0 - min_bin_height * num_bins) * heights
    derivatives = min_derivative + derivatives
    widths, cumwidths = _calculate_knots(widths, left, right)
    heights, cumheights = _calculate_knots(heights, bottom, top)
    derivatives = F.pad(derivatives, pad=(1, 1), mode='constant', value=1.0 - min_derivative)
    bin_idx = _searchsorted(cumheights + eps if inverse else cumwidths + eps, inputs).unsqueeze(-1)
    input_widths = _select_bins(widths, bin_idx)
    input_cumwidths = _select_bins(cumwidths, bin_idx)
    input_cumheights = _select_bins(cumheights, bin_idx)
    input_delta = _select_bins(heights / widths, bin_idx)
    input_derivatives = _select_bins(derivatives, bin_idx)
    input_derivatives_plus_one = _select_bins(derivatives[..., 1:], bin_idx)
    input_heights = _select_bins(heights, bin_idx)
    if lambdas is not None:
        lambdas = (1 - 2 * min_lambda) * lambdas + min_lambda
        input_lambdas = _select_bins(lambdas, bin_idx)
        wa = 1.0
        wb = torch.sqrt(input_derivatives / input_derivatives_plus_one) * wa
        wc = (input_lambdas * wa * input_derivatives + (1 - input_lambdas) * wb * input_derivatives_plus_one) / input_delta
        ya = input_cumheights
        yb = input_heights + input_cumheights
        yc = ((1.0 - input_lambdas) * wa * ya + input_lambdas * wb * yb) / ((1.0 - input_lambdas) * wa + input_lambdas * wb)
        if inverse:
            numerator = input_lambdas * wa * (ya - inputs) * (inputs <= yc).float() + ((wc - input_lambdas * wb) * inputs + input_lambdas * wb * yb - wc * yc) * (inputs > yc).float()
            denominator = ((wc - wa) * inputs + wa * ya - wc * yc) * (inputs <= yc).float() + ((wc - wb) * inputs + wb * yb - wc * yc) * (inputs > yc).float()
            theta = numerator / denominator
            outputs = theta * input_widths + input_cumwidths
            derivative_numerator = (wa * wc * input_lambdas * (yc - ya) * (inputs <= yc).float() + wb * wc * (1 - input_lambdas) * (yb - yc) * (inputs > yc).float()) * input_widths
            logabsdet = torch.log(derivative_numerator) - 2 * torch.log(torch.abs(denominator))
        else:
            theta = (inputs - input_cumwidths) / input_widths
            numerator = (wa * ya * (input_lambdas - theta) + wc * yc * theta) * (theta <= input_lambdas).float() + (wc * yc * (1 - theta) + wb * yb * (theta - input_lambdas)) * (theta > input_lambdas).float()
            denominator = (wa * (input_lambdas - theta) + wc * theta) * (theta <= input_lambdas).float() + (wc * (1 - theta) + wb * (theta - input_lambdas)) * (theta > input_lambdas).float()
            outputs = numerator / denominator
            derivative_numerator = (wa * wc * input_lambdas * (yc - ya) * (theta <= input_lambdas).float() + wb * wc * (1 - input_lambdas) * (yb - yc) * (theta > input_lambdas).float()) / input_widths
            logabsdet = torch.log(derivative_numerator) - 2 * torch.log(torch.abs(denominator))
    elif inverse:
        a = (inputs - input_cumheights) * (input_derivatives + input_derivatives_plus_one - 2 * input_delta) + input_heights * (input_delta - input_derivatives)
        b = input_heights * input_derivatives - (inputs - input_cumheights) * (input_derivatives + input_derivatives_plus_one - 2 * input_delta)
        c = -input_delta * (inputs - input_cumheights)
        discriminant = b.pow(2) - 4 * a * c
        discriminant = discriminant.masked_fill(outside_interval_mask, 0)
        assert (discriminant >= 0).all()
        root = 2 * c / (-b - torch.sqrt(discriminant))
        outputs = root * input_widths + input_cumwidths
        theta_one_minus_theta = root * (1 - root)
        denominator = input_delta + (input_derivatives + input_derivatives_plus_one - 2 * input_delta) * theta_one_minus_theta
        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * root.pow(2) + 2 * input_delta * theta_one_minus_theta + input_derivatives * (1 - root).pow(2))
        logabsdet = -(torch.log(derivative_numerator) - 2 * torch.log(denominator))
    else:
        theta = (inputs - input_cumwidths) / input_widths
        theta_one_minus_theta = theta * (1 - theta)
        numerator = input_heights * (input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta)
        denominator = input_delta + (input_derivatives + input_derivatives_plus_one - 2 * input_delta) * theta_one_minus_theta
        outputs = input_cumheights + numerator / denominator
        derivative_numerator = input_delta.pow(2) * (input_derivatives_plus_one * theta.pow(2) + 2 * input_delta * theta_one_minus_theta + input_derivatives * (1 - theta).pow(2))
        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)
    outputs[outside_interval_mask] = inputs[outside_interval_mask]
    logabsdet[outside_interval_mask] = 0.0
    return outputs, logabsdet


@copy_docs_from(Transform)
class ConditionedSpline(Transform):
    """
    Helper class to manage learnable splines. One could imagine this as a standard
    layer in PyTorch...
    """
    domain = constraints.real
    codomain = constraints.real
    bijective = True

    def __init__(self, params, bound=3.0, order='linear'):
        super().__init__(cache_size=1)
        self._params = params
        self.order = order
        self.bound = bound
        self._cache_log_detJ = None

    def _call(self, x):
        y, log_detJ = self.spline_op(x)
        self._cache_log_detJ = log_detJ
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. Uses a previously cached inverse if available,
        otherwise performs the inversion afresh.
        """
        x, log_detJ = self.spline_op(y, inverse=True)
        self._cache_log_detJ = -log_detJ
        return x

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cache_log_detJ

    def spline_op(self, x, **kwargs):
        w, h, d, l = self._params() if callable(self._params) else self._params
        y, log_detJ = _monotonic_rational_spline(x, w, h, d, l, bound=self.bound, **kwargs)
        return y, log_detJ


@copy_docs_from(ConditionedSpline)
class Spline(ConditionedSpline, TransformModule):
    """
    An implementation of the element-wise rational spline bijections of linear and
    quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020). Rational splines
    are functions that are comprised of segments that are the ratio of two
    polynomials. For instance, for the :math:`d`-th dimension and the :math:`k`-th
    segment on the spline, the function will take the form,

        :math:`y_d = \\frac{\\alpha^{(k)}(x_d)}{\\beta^{(k)}(x_d)},`

    where :math:`\\alpha^{(k)}` and :math:`\\beta^{(k)}` are two polynomials of
    order :math:`d`. For :math:`d=1`, we say that the spline is linear, and for
    :math:`d=2`, quadratic. The spline is constructed on the specified bounding box,
    :math:`[-K,K]\\times[-K,K]`, with the identity function used elsewhere.

    Rational splines offer an excellent combination of functional flexibility whilst
    maintaining a numerically stable inverse that is of the same computational and
    space complexities as the forward operation. This element-wise transform permits
    the accurate represention of complex univariate distributions.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = Spline(10, count_bins=4, bound=3.)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param input_dim: Dimension of the input vector. This is required so we know how
        many parameters to store.
    :type input_dim: int
    :param count_bins: The number of segments comprising the spline.
    :type count_bins: int
    :param bound: The quantity :math:`K` determining the bounding box,
        :math:`[-K,K]\\times[-K,K]`, of the spline.
    :type bound: float
    :param order: One of ['linear', 'quadratic'] specifying the order of the spline.
    :type order: string

    References:

    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural
    Spline Flows. NeurIPS 2019.

    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative
    Modeling using Linear Rational Splines. AISTATS 2020.

    """
    domain = constraints.real
    codomain = constraints.real
    bijective = True

    def __init__(self, input_dim, count_bins=8, bound=3.0, order='linear'):
        super(Spline, self).__init__(self._params)
        self.input_dim = input_dim
        self.count_bins = count_bins
        self.bound = bound
        self.order = order
        self.unnormalized_widths = nn.Parameter(torch.randn(self.input_dim, self.count_bins))
        self.unnormalized_heights = nn.Parameter(torch.randn(self.input_dim, self.count_bins))
        self.unnormalized_derivatives = nn.Parameter(torch.randn(self.input_dim, self.count_bins - 1))
        if self.order == 'linear':
            self.unnormalized_lambdas = nn.Parameter(torch.rand(self.input_dim, self.count_bins))
        elif self.order != 'quadratic':
            raise ValueError("Keyword argument 'order' must be one of ['linear', 'quadratic'], but '{}' was found!".format(self.order))

    def _params(self):
        w = F.softmax(self.unnormalized_widths, dim=-1)
        h = F.softmax(self.unnormalized_heights, dim=-1)
        d = F.softplus(self.unnormalized_derivatives)
        if self.order == 'linear':
            l = torch.sigmoid(self.unnormalized_lambdas)
        else:
            l = None
        return w, h, d, l


@copy_docs_from(ConditionalTransformModule)
class ConditionalSpline(ConditionalTransformModule):
    """
    An implementation of the element-wise rational spline bijections of linear and
    quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020) conditioning on
    an additional context variable.

    Rational splines are functions that are comprised of segments that are the ratio
    of two polynomials. For instance, for the :math:`d`-th dimension and the
    :math:`k`-th segment on the spline, the function will take the form,

        :math:`y_d = \\frac{\\alpha^{(k)}(x_d)}{\\beta^{(k)}(x_d)},`

    where :math:`\\alpha^{(k)}` and :math:`\\beta^{(k)}` are two polynomials of
    order :math:`d` whose parameters are the output of a function, e.g. a NN, with
    input :math:`z\\\\in\\\\mathbb{R}^{M}` representing the context variable to
    condition on.. For :math:`d=1`, we say that the spline is linear, and for
    :math:`d=2`, quadratic. The spline is constructed on the specified bounding box,
    :math:`[-K,K]\\times[-K,K]`, with the identity function used elsewhere.

    Rational splines offer an excellent combination of functional flexibility whilst
    maintaining a numerically stable inverse that is of the same computational and
    space complexities as the forward operation. This element-wise transform permits
    the accurate represention of complex univariate distributions.

    Example usage:

    >>> from pyro.nn.dense_nn import DenseNN
    >>> input_dim = 10
    >>> context_dim = 5
    >>> batch_size = 3
    >>> count_bins = 8
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [input_dim * count_bins, input_dim * count_bins,
    ... input_dim * (count_bins - 1), input_dim * count_bins]
    >>> hypernet = DenseNN(context_dim, [50, 50], param_dims)
    >>> transform = ConditionalSpline(hypernet, input_dim, count_bins)
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size])) # doctest: +SKIP

    :param input_dim: Dimension of the input vector. This is required so we know how
        many parameters to store.
    :type input_dim: int
    :param count_bins: The number of segments comprising the spline.
    :type count_bins: int
    :param bound: The quantity :math:`K` determining the bounding box,
        :math:`[-K,K]\\times[-K,K]`, of the spline.
    :type bound: float
    :param order: One of ['linear', 'quadratic'] specifying the order of the spline.
    :type order: string

    References:

    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural
    Spline Flows. NeurIPS 2019.

    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative
    Modeling using Linear Rational Splines. AISTATS 2020.

    """
    domain = constraints.real
    codomain = constraints.real
    bijective = True

    def __init__(self, nn, input_dim, count_bins, bound=3.0, order='linear'):
        super().__init__()
        self.nn = nn
        self.input_dim = input_dim
        self.count_bins = count_bins
        self.bound = bound
        self.order = order

    def _params(self, context):
        if self.order == 'linear':
            w, h, d, l = self.nn(context)
            if w.shape[-1] == self.input_dim:
                l = l.transpose(-1, -2)
            else:
                l = l.reshape(l.shape[:-1] + (self.input_dim, self.count_bins))
            l = torch.sigmoid(l)
        elif self.order == 'quadratic':
            w, h, d = self.nn(context)
            l = None
        else:
            raise ValueError("Keyword argument 'order' must be one of ['linear', 'quadratic'], but '{}' was found!".format(self.order))
        if w.shape[-1] == self.input_dim:
            w = w.transpose(-1, -2)
            h = h.transpose(-1, -2)
            d = d.transpose(-1, -2)
        else:
            w = w.reshape(w.shape[:-1] + (self.input_dim, self.count_bins))
            h = h.reshape(h.shape[:-1] + (self.input_dim, self.count_bins))
            d = d.reshape(d.shape[:-1] + (self.input_dim, self.count_bins - 1))
        w = F.softmax(w, dim=-1)
        h = F.softmax(h, dim=-1)
        d = F.softplus(d)
        return w, h, d, l

    def condition(self, context):
        params = partial(self._params, context)
        return ConditionedSpline(params, bound=self.bound, order=self.order)


@copy_docs_from(TransformModule)
class SplineAutoregressive(TransformModule):
    """
    An implementation of the autoregressive layer with rational spline bijections of
    linear and quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020).
    Rational splines are functions that are comprised of segments that are the ratio
    of two polynomials (see :class:`~pyro.distributions.transforms.Spline`).

    The autoregressive layer uses the transformation,

        :math:`y_d = g_{\\theta_d}(x_d)\\ \\ \\ d=1,2,\\ldots,D`

    where :math:`\\mathbf{x}=(x_1,x_2,\\ldots,x_D)` are the inputs,
    :math:`\\mathbf{y}=(y_1,y_2,\\ldots,y_D)` are the outputs, :math:`g_{\\theta_d}` is
    an elementwise rational monotonic spline with parameters :math:`\\theta_d`, and
    :math:`\\theta=(\\theta_1,\\theta_2,\\ldots,\\theta_D)` is the output of an
    autoregressive NN inputting :math:`\\mathbf{x}`.

    Example usage:

    >>> from pyro.nn import AutoRegressiveNN
    >>> input_dim = 10
    >>> count_bins = 8
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> hidden_dims = [input_dim * 10, input_dim * 10]
    >>> param_dims = [count_bins, count_bins, count_bins - 1, count_bins]
    >>> hypernet = AutoRegressiveNN(input_dim, hidden_dims, param_dims=param_dims)
    >>> transform = SplineAutoregressive(input_dim, hypernet, count_bins=count_bins)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param input_dim: Dimension of the input vector. Despite operating element-wise,
        this is required so we know how many parameters to store.
    :type input_dim: int
    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns tuple of the spline parameters
    :type autoregressive_nn: callable
    :param count_bins: The number of segments comprising the spline.
    :type count_bins: int
    :param bound: The quantity :math:`K` determining the bounding box,
        :math:`[-K,K]\\times[-K,K]`, of the spline.
    :type bound: float
    :param order: One of ['linear', 'quadratic'] specifying the order of the spline.
    :type order: string

    References:

    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural
    Spline Flows. NeurIPS 2019.

    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative
    Modeling using Linear Rational Splines. AISTATS 2020.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True
    autoregressive = True

    def __init__(self, input_dim, autoregressive_nn, count_bins=8, bound=3.0, order='linear'):
        super(SplineAutoregressive, self).__init__(cache_size=1)
        self.arn = autoregressive_nn
        self.spline = ConditionalSpline(autoregressive_nn, input_dim, count_bins, bound, order)

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        spline = self.spline.condition(x)
        y = spline(x)
        self._cache_log_detJ = spline._cache_log_detJ
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. Uses a previously cached inverse if available, otherwise
        performs the inversion afresh.
        """
        input_dim = y.size(-1)
        x = torch.zeros_like(y)
        for _ in range(input_dim):
            spline = self.spline.condition(x)
            x = spline._inverse(y)
        self._cache_log_detJ = spline._cache_log_detJ
        return x

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cache_log_detJ.sum(-1)


@copy_docs_from(ConditionalTransformModule)
class ConditionalSplineAutoregressive(ConditionalTransformModule):
    """
    An implementation of the autoregressive layer with rational spline bijections of
    linear and quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020) that
    conditions on an additional context variable. Rational splines are functions
    that are comprised of segments that are the ratio of two polynomials (see
    :class:`~pyro.distributions.transforms.Spline`).

    The autoregressive layer uses the transformation,

        :math:`y_d = g_{\\theta_d}(x_d)\\ \\ \\ d=1,2,\\ldots,D`

    where :math:`\\mathbf{x}=(x_1,x_2,\\ldots,x_D)` are the inputs,
    :math:`\\mathbf{y}=(y_1,y_2,\\ldots,y_D)` are the outputs, :math:`g_{\\theta_d}` is
    an elementwise rational monotonic spline with parameters :math:`\\theta_d`, and
    :math:`\\theta=(\\theta_1,\\theta_2,\\ldots,\\theta_D)` is the output of a
    conditional autoregressive NN inputting :math:`\\mathbf{x}` and conditioning on
    the context variable :math:`\\mathbf{z}`.

    Example usage:

    >>> from pyro.nn import ConditionalAutoRegressiveNN
    >>> input_dim = 10
    >>> count_bins = 8
    >>> context_dim = 5
    >>> batch_size = 3
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> hidden_dims = [input_dim * 10, input_dim * 10]
    >>> param_dims = [count_bins, count_bins, count_bins - 1, count_bins]
    >>> hypernet = ConditionalAutoRegressiveNN(input_dim, context_dim, hidden_dims,
    ... param_dims=param_dims)
    >>> transform = ConditionalSplineAutoregressive(input_dim, hypernet,
    ... count_bins=count_bins)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> z = torch.rand(batch_size, context_dim)
    >>> flow_dist = dist.ConditionalTransformedDistribution(base_dist,
    ... [transform]).condition(z)
    >>> flow_dist.sample(sample_shape=torch.Size([batch_size]))  # doctest: +SKIP

    :param input_dim: Dimension of the input vector. Despite operating element-wise,
        this is required so we know how many parameters to store.
    :type input_dim: int
    :param autoregressive_nn: an autoregressive neural network whose forward call
        returns tuple of the spline parameters
    :type autoregressive_nn: callable
    :param count_bins: The number of segments comprising the spline.
    :type count_bins: int
    :param bound: The quantity :math:`K` determining the bounding box,
        :math:`[-K,K]\\times[-K,K]`, of the spline.
    :type bound: float
    :param order: One of ['linear', 'quadratic'] specifying the order of the spline.
    :type order: string

    References:

    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural
    Spline Flows. NeurIPS 2019.

    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative
    Modeling using Linear Rational Splines. AISTATS 2020.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim, autoregressive_nn, **kwargs):
        super().__init__()
        self.input_dim = input_dim
        self.nn = autoregressive_nn
        self.kwargs = kwargs

    def condition(self, context):
        """
        Conditions on a context variable, returning a non-conditional transform of
        of type :class:`~pyro.distributions.transforms.SplineAutoregressive`.
        """
        cond_nn = partial(self.nn, context=context)
        cond_nn.permutation = cond_nn.func.permutation
        cond_nn.get_permutation = cond_nn.func.get_permutation
        return SplineAutoregressive(self.input_dim, cond_nn, **self.kwargs)


@copy_docs_from(TransformModule)
class SplineCoupling(TransformModule):
    """
    An implementation of the coupling layer with rational spline bijections of
    linear and quadratic order (Durkan et al., 2019; Dolatabadi et al., 2020).
    Rational splines are functions that are comprised of segments that are the ratio
    of two polynomials (see :class:`~pyro.distributions.transforms.Spline`).

    The spline coupling layer uses the transformation,

        :math:`\\mathbf{y}_{1:d} = g_\\theta(\\mathbf{x}_{1:d})`
        :math:`\\mathbf{y}_{(d+1):D} = h_\\phi(\\mathbf{x}_{(d+1):D};\\mathbf{x}_{1:d})`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    e.g. :math:`\\mathbf{x}_{1:d}` represents the first :math:`d` elements of the
    inputs, :math:`g_\\theta` is either the identity function or an elementwise
    rational monotonic spline with parameters :math:`\\theta`, and :math:`h_\\phi` is
    a conditional elementwise spline spline, conditioning on the first :math:`d`
    elements.

    Example usage:

    >>> from pyro.nn import DenseNN
    >>> input_dim = 10
    >>> split_dim = 6
    >>> count_bins = 8
    >>> base_dist = dist.Normal(torch.zeros(input_dim), torch.ones(input_dim))
    >>> param_dims = [(input_dim - split_dim) * count_bins,
    ... (input_dim - split_dim) * count_bins,
    ... (input_dim - split_dim) * (count_bins - 1),
    ... (input_dim - split_dim) * count_bins]
    >>> hypernet = DenseNN(split_dim, [10*input_dim], param_dims)
    >>> transform = SplineCoupling(input_dim, split_dim, hypernet)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP

    :param input_dim: Dimension of the input vector. Despite operating element-wise,
        this is required so we know how many parameters to store.
    :type input_dim: int
    :param split_dim: Zero-indexed dimension :math:`d` upon which to perform input/
        output split for transformation.
    :param hypernet: a neural network whose forward call returns a tuple of spline
        parameters (see :class:`~pyro.distributions.transforms.ConditionalSpline`).
    :type hypernet: callable
    :param count_bins: The number of segments comprising the spline.
    :type count_bins: int
    :param bound: The quantity :math:`K` determining the bounding box,
        :math:`[-K,K]\\times[-K,K]`, of the spline.
    :type bound: float
    :param order: One of ['linear', 'quadratic'] specifying the order of the spline.
    :type order: string

    References:

    Conor Durkan, Artur Bekasov, Iain Murray, George Papamakarios. Neural
    Spline Flows. NeurIPS 2019.

    Hadi M. Dolatabadi, Sarah Erfani, Christopher Leckie. Invertible Generative
    Modeling using Linear Rational Splines. AISTATS 2020.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim, split_dim, hypernet, count_bins=8, bound=3.0, order='linear', identity=False):
        super(SplineCoupling, self).__init__(cache_size=1)
        self.lower_spline = Spline(split_dim, count_bins, bound, order)
        self.upper_spline = ConditionalSpline(hypernet, input_dim - split_dim, count_bins, bound, order)
        self.split_dim = split_dim
        self.identity = identity

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        x1, x2 = x[..., :self.split_dim], x[..., self.split_dim:]
        if not self.identity:
            y1 = self.lower_spline(x1)
            log_detK = self.lower_spline._cache_log_detJ
        else:
            y1 = x1
        upper_spline = self.upper_spline.condition(x1)
        y2 = upper_spline(x2)
        log_detJ = upper_spline._cache_log_detJ
        if not self.identity:
            log_detJ = torch.cat([log_detJ, log_detK], dim=-1)
        self._cache_log_detJ = log_detJ
        return torch.cat([y1, y2], dim=-1)

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor

        Inverts y => x. Uses a previously cached inverse if available,
        otherwise performs the inversion afresh.
        """
        y1, y2 = y[..., :self.split_dim], y[..., self.split_dim:]
        if not self.identity:
            x1 = self.lower_spline._inv_call(y1)
            log_detK = self.lower_spline._cache_log_detJ
        else:
            x1 = y1
        upper_spline = self.upper_spline.condition(x1)
        x2 = upper_spline._inv_call(y2)
        log_detJ = upper_spline._cache_log_detJ
        if not self.identity:
            log_detJ = torch.cat([log_detJ, log_detK], dim=-1)
        self._cache_log_detJ = log_detJ
        return torch.cat([x1, x2], dim=-1)

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cache_log_detJ.sum(-1)


@copy_docs_from(TransformModule)
class Sylvester(Householder):
    """
    An implementation of the Sylvester bijective transform of the Householder
    variety (Van den Berg Et Al., 2018),

        :math:`\\mathbf{y} = \\mathbf{x} + QR\\tanh(SQ^T\\mathbf{x}+\\mathbf{b})`

    where :math:`\\mathbf{x}` are the inputs, :math:`\\mathbf{y}` are the outputs,
    :math:`R,S\\sim D\\times D` are upper triangular matrices for input dimension
    :math:`D`, :math:`Q\\sim D\\times D` is an orthogonal matrix, and
    :math:`\\mathbf{b}\\sim D` is learnable bias term.

    The Sylvester transform is a generalization of
    :class:`~pyro.distributions.transforms.Planar`. In the Householder type of the
    Sylvester transform, the orthogonality of :math:`Q` is enforced by representing
    it as the product of Householder transformations.

    Together with :class:`~pyro.distributions.TransformedDistribution` it provides a
    way to create richer variational approximations.

    Example usage:

    >>> base_dist = dist.Normal(torch.zeros(10), torch.ones(10))
    >>> transform = Sylvester(10, count_transforms=4)
    >>> pyro.module("my_transform", transform)  # doctest: +SKIP
    >>> flow_dist = dist.TransformedDistribution(base_dist, [transform])
    >>> flow_dist.sample()  # doctest: +SKIP
        tensor([-0.4071, -0.5030,  0.7924, -0.2366, -0.2387, -0.1417,  0.0868,
                0.1389, -0.4629,  0.0986])

    The inverse of this transform does not possess an analytical solution and is
    left unimplemented. However, the inverse is cached when the forward operation is
    called during sampling, and so samples drawn using the Sylvester transform can
    be scored.

    References:

    [1] Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, Max Welling.
    Sylvester Normalizing Flows for Variational Inference. UAI 2018.

    """
    domain = constraints.real_vector
    codomain = constraints.real_vector
    bijective = True

    def __init__(self, input_dim, count_transforms=1):
        super().__init__(input_dim, count_transforms)
        self.R_dense = nn.Parameter(torch.Tensor(input_dim, input_dim))
        self.S_dense = nn.Parameter(torch.Tensor(input_dim, input_dim))
        self.R_diag = nn.Parameter(torch.Tensor(input_dim))
        self.S_diag = nn.Parameter(torch.Tensor(input_dim))
        self.b = nn.Parameter(torch.Tensor(input_dim))
        triangular_mask = torch.triu(torch.ones(input_dim, input_dim), diagonal=1)
        self.register_buffer('triangular_mask', triangular_mask)
        self._cached_logDetJ = None
        self.tanh = nn.Tanh()
        self.reset_parameters2()

    def dtanh_dx(self, x):
        return 1.0 - self.tanh(x).pow(2)

    def R(self):
        return self.R_dense * self.triangular_mask + torch.diag(self.tanh(self.R_diag))

    def S(self):
        return self.S_dense * self.triangular_mask + torch.diag(self.tanh(self.S_diag))

    def Q(self, x):
        u = self.u()
        partial_Q = torch.eye(self.input_dim, dtype=x.dtype, layout=x.layout, device=x.device) - 2.0 * torch.ger(u[0], u[0])
        for idx in range(1, self.u_unnormed.size(-2)):
            partial_Q = torch.matmul(partial_Q, torch.eye(self.input_dim) - 2.0 * torch.ger(u[idx], u[idx]))
        return partial_Q

    def reset_parameters2(self):
        for v in [self.b, self.R_diag, self.S_diag, self.R_dense, self.S_dense]:
            v.data.uniform_(-0.01, 0.01)

    def _call(self, x):
        """
        :param x: the input into the bijection
        :type x: torch.Tensor

        Invokes the bijection x=>y; in the prototypical context of a
        :class:`~pyro.distributions.TransformedDistribution` `x` is a sample from
        the base distribution (or the output of a previous transform)
        """
        Q = self.Q(x)
        R = self.R()
        S = self.S()
        A = torch.matmul(Q, R)
        B = torch.matmul(S, Q.t())
        preactivation = torch.matmul(x, B) + self.b
        y = x + torch.matmul(self.tanh(preactivation), A)
        self._cached_logDetJ = torch.log1p(self.dtanh_dx(preactivation) * R.diagonal() * S.diagonal() + 1e-08).sum(-1)
        return y

    def _inverse(self, y):
        """
        :param y: the output of the bijection
        :type y: torch.Tensor
        Inverts y => x. As noted above, this implementation is incapable of
        inverting arbitrary values `y`; rather it assumes `y` is the result of a
        previously computed application of the bijector to some `x` (which was
        cached on the forward call)
        """
        raise KeyError("Sylvester object expected to find key in intermediates cache but didn't")

    def log_abs_det_jacobian(self, x, y):
        """
        Calculates the elementwise determinant of the log Jacobian
        """
        x_old, y_old = self._cached_x_y
        if x is not x_old or y is not y_old:
            self(x)
        return self._cached_logDetJ


class AutoCallable(AutoGuide):
    """
    :class:`AutoGuide` wrapper for simple callable guides.

    This is used internally for composing autoguides with custom user-defined
    guides that are simple callables, e.g.::

        def my_local_guide(*args, **kwargs):
            ...

        guide = AutoGuideList(model)
        guide.add(AutoDelta(poutine.block(model, expose=['my_global_param']))
        guide.add(my_local_guide)  # automatically wrapped in an AutoCallable

    To specify a median callable, you can instead::

        def my_local_median(*args, **kwargs)
            ...

        guide.add(AutoCallable(model, my_local_guide, my_local_median))

    For more complex guides that need e.g. access to plates, users should
    instead subclass ``AutoGuide``.

    :param callable model: a Pyro model
    :param callable guide: a Pyro guide (typically over only part of the model)
    :param callable median: an optional callable returning a dict mapping
        sample site name to computed median tensor.
    """

    def __init__(self, model, guide, median=lambda *args, **kwargs: {}):
        super().__init__(model)
        self._guide = guide
        self.median = median

    def forward(self, *args, **kwargs):
        result = self._guide(*args, **kwargs)
        return {} if result is None else result


class AutoGuideList(AutoGuide, nn.ModuleList):
    """
    Container class to combine multiple automatic guides.

    Example usage::

        guide = AutoGuideList(my_model)
        guide.append(AutoDiagonalNormal(poutine.block(model, hide=["assignment"])))
        guide.append(AutoDiscreteParallel(poutine.block(model, expose=["assignment"])))
        svi = SVI(model, guide, optim, Trace_ELBO())

    :param callable model: a Pyro model
    """

    def _check_prototype(self, part_trace):
        for name, part_site in part_trace.iter_stochastic_nodes():
            self_site = self.prototype_trace.nodes[name]
            assert part_site['fn'].batch_shape == self_site['fn'].batch_shape
            assert part_site['fn'].event_shape == self_site['fn'].event_shape
            assert part_site['value'].shape == self_site['value'].shape

    def append(self, part):
        """
        Add an automatic guide for part of the model. The guide should
        have been created by blocking the model to restrict to a subset of
        sample sites. No two parts should operate on any one sample site.

        :param part: a partial guide to add
        :type part: AutoGuide or callable
        """
        if not isinstance(part, AutoGuide):
            part = AutoCallable(self.model, part)
        if part.master is not None:
            raise RuntimeError('The module `{}` is already added.'.format(self._pyro_name))
        setattr(self, str(len(self)), part)

    def add(self, part):
        """Deprecated alias for :meth:`append`."""
        warnings.warn('The method `.add` has been deprecated in favor of `.append`.', DeprecationWarning)
        self.append(part)

    def forward(self, *args, **kwargs):
        """
        A composite guide with the same ``*args, **kwargs`` as the base ``model``.

        .. note:: This method is used internally by :class:`~torch.nn.Module`.
            Users should instead use :meth:`~torch.nn.Module.__call__`.

        :return: A dict mapping sample site name to sampled value.
        :rtype: dict
        """
        if self.prototype_trace is None:
            self._setup_prototype(*args, **kwargs)
        self._create_plates(*args, **kwargs)
        result = {}
        for part in self:
            result.update(part(*args, **kwargs))
        return result

    def median(self, *args, **kwargs):
        """
        Returns the posterior median value of each latent variable.

        :return: A dict mapping sample site name to median tensor.
        :rtype: dict
        """
        result = {}
        for part in self:
            result.update(part.median(*args, **kwargs))
        return result

    def quantiles(self, quantiles, *args, **kwargs):
        """
        Returns the posterior quantile values of each latent variable.

        :param list quantiles: A list of requested quantiles between 0 and 1.
        :returns: A dict mapping sample site name to quantiles tensor.
        :rtype: dict
        """
        result = {}
        for part in self:
            result.update(part.quantiles(quantiles, *args, **kwargs))
        return result


def _predictive_sequential(model, posterior_samples, model_args, model_kwargs, num_samples, return_site_shapes, return_trace=False):
    collected = []
    samples = [{k: v[i] for k, v in posterior_samples.items()} for i in range(num_samples)]
    for i in range(num_samples):
        trace = poutine.trace(poutine.condition(model, samples[i])).get_trace(*model_args, **model_kwargs)
        if return_trace:
            collected.append(trace)
        else:
            collected.append({site: trace.nodes[site]['value'] for site in return_site_shapes})
    if return_trace:
        return collected
    else:
        return {site: torch.stack([s[site] for s in collected]).reshape(shape) for site, shape in return_site_shapes.items()}


def _predictive(model, posterior_samples, num_samples, return_sites=(), return_trace=False, parallel=False, model_args=(), model_kwargs={}):
    model = torch.no_grad()(poutine.mask(model, mask=False))
    max_plate_nesting = _guess_max_plate_nesting(model, model_args, model_kwargs)
    vectorize = pyro.plate('_num_predictive_samples', num_samples, dim=-max_plate_nesting - 1)
    model_trace = prune_subsample_sites(poutine.trace(model).get_trace(*model_args, **model_kwargs))
    reshaped_samples = {}
    for name, sample in posterior_samples.items():
        sample_shape = sample.shape[1:]
        sample = sample.reshape((num_samples,) + (1,) * (max_plate_nesting - len(sample_shape)) + sample_shape)
        reshaped_samples[name] = sample
    if return_trace:
        trace = poutine.trace(poutine.condition(vectorize(model), reshaped_samples)).get_trace(*model_args, **model_kwargs)
        return trace
    return_site_shapes = {}
    for site in (model_trace.stochastic_nodes + model_trace.observation_nodes):
        append_ndim = max_plate_nesting - len(model_trace.nodes[site]['fn'].batch_shape)
        site_shape = (num_samples,) + (1,) * append_ndim + model_trace.nodes[site]['value'].shape
        if return_sites:
            if site in return_sites:
                return_site_shapes[site] = site_shape
        elif return_sites is None:
            return_site_shapes[site] = site_shape
        elif site not in posterior_samples:
            return_site_shapes[site] = site_shape
    if return_sites is not None and '_RETURN' in return_sites:
        value = model_trace.nodes['_RETURN']['value']
        shape = (num_samples,) + value.shape if torch.is_tensor(value) else None
        return_site_shapes['_RETURN'] = shape
    if not parallel:
        return _predictive_sequential(model, posterior_samples, model_args, model_kwargs, num_samples, return_site_shapes, return_trace=False)
    trace = poutine.trace(poutine.condition(vectorize(model), reshaped_samples)).get_trace(*model_args, **model_kwargs)
    predictions = {}
    for site, shape in return_site_shapes.items():
        value = trace.nodes[site]['value']
        if site == '_RETURN' and shape is None:
            predictions[site] = value
            continue
        if value.numel() < reduce(lambda x, y: x * y, shape):
            predictions[site] = value.expand(shape)
        else:
            predictions[site] = value.reshape(shape)
    return predictions


class Predictive(torch.nn.Module):
    """
    EXPERIMENTAL class used to construct predictive distribution. The predictive
    distribution is obtained by running the `model` conditioned on latent samples
    from `posterior_samples`. If a `guide` is provided, then posterior samples
    from all the latent sites are also returned.

    .. warning::
        The interface for the :class:`Predictive` class is experimental, and
        might change in the future.

    :param model: Python callable containing Pyro primitives.
    :param dict posterior_samples: dictionary of samples from the posterior.
    :param callable guide: optional guide to get posterior samples of sites not present
        in `posterior_samples`.
    :param int num_samples: number of samples to draw from the predictive distribution.
        This argument has no effect if ``posterior_samples`` is non-empty, in which case,
        the leading dimension size of samples in ``posterior_samples`` is used.
    :param return_sites: sites to return; by default only sample sites not present
        in `posterior_samples` are returned.
    :type return_sites: list, tuple, or set
    :param bool parallel: predict in parallel by wrapping the existing model
        in an outermost `plate` messenger. Note that this requires that the model has
        all batch dims correctly annotated via :class:`~pyro.plate`. Default is `False`.
    """

    def __init__(self, model, posterior_samples=None, guide=None, num_samples=None, return_sites=(), parallel=False):
        super().__init__()
        if posterior_samples is None:
            if num_samples is None:
                raise ValueError('Either posterior_samples or num_samples must be specified.')
            posterior_samples = {}
        for name, sample in posterior_samples.items():
            batch_size = sample.shape[0]
            if num_samples is None:
                num_samples = batch_size
            elif num_samples != batch_size:
                warnings.warn("Sample's leading dimension size {} is different from the provided {} num_samples argument. Defaulting to {}.".format(batch_size, num_samples, batch_size), UserWarning)
                num_samples = batch_size
        if num_samples is None:
            raise ValueError('No sample sites in posterior samples to infer `num_samples`.')
        if guide is not None and posterior_samples:
            raise ValueError('`posterior_samples` cannot be provided with the `guide` argument.')
        if return_sites is not None:
            assert isinstance(return_sites, (list, tuple, set))
        self.model = model
        self.posterior_samples = {} if posterior_samples is None else posterior_samples
        self.num_samples = num_samples
        self.guide = guide
        self.return_sites = return_sites
        self.parallel = parallel

    def call(self, *args, **kwargs):
        """
        Method that calls :meth:`forward` and returns parameter values of the
        guide as a `tuple` instead of a `dict`, which is a requirement for
        JIT tracing. Unlike :meth:`forward`, this method can be traced by
        :func:`torch.jit.trace_module`.

        .. warning::
            This method may be removed once PyTorch JIT tracer starts accepting
            `dict` as valid return types. See
            `issue <https://github.com/pytorch/pytorch/issues/27743>`_.
        """
        result = self.forward(*args, **kwargs)
        return tuple(v for _, v in sorted(result.items()))

    def forward(self, *args, **kwargs):
        """
        Returns dict of samples from the predictive distribution. By default, only sample sites not
        contained in `posterior_samples` are returned. This can be modified by changing the
        `return_sites` keyword argument of this :class:`Predictive` instance.

        .. note:: This method is used internally by :class:`~torch.nn.Module`.
            Users should instead use :meth:`~torch.nn.Module.__call__` as in
            ``Predictive(model)(*args, **kwargs)``.

        :param args: model arguments.
        :param kwargs: model keyword arguments.
        """
        posterior_samples = self.posterior_samples
        return_sites = self.return_sites
        if self.guide is not None:
            return_sites = None if not return_sites else return_sites
            posterior_samples = _predictive(self.guide, posterior_samples, self.num_samples, return_sites=None, parallel=self.parallel, model_args=args, model_kwargs=kwargs)
        return _predictive(self.model, posterior_samples, self.num_samples, return_sites=return_sites, parallel=self.parallel, model_args=args, model_kwargs=kwargs)

    def get_samples(self, *args, **kwargs):
        warnings.warn('The method `.get_samples` has been deprecated in favor of `.forward`.', DeprecationWarning)
        return self.forward(*args, **kwargs)

    def get_vectorized_trace(self, *args, **kwargs):
        """
        Returns a single vectorized `trace` from the predictive distribution. Note that this
        requires that the model has all batch dims correctly annotated via :class:`~pyro.plate`.

        :param args: model arguments.
        :param kwargs: model keyword arguments.
        """
        posterior_samples = self.posterior_samples
        if self.guide is not None:
            posterior_samples = _predictive(self.guide, posterior_samples, self.num_samples, parallel=self.parallel, model_args=args, model_kwargs=kwargs)
        return _predictive(self.model, posterior_samples, self.num_samples, return_trace=True, model_args=args, model_kwargs=kwargs)


class ConditionalDenseNN(torch.nn.Module):
    """
    An implementation of a simple dense feedforward network taking a context variable, for use in, e.g.,
    some conditional flows such as :class:`pyro.distributions.transforms.ConditionalAffineCoupling`.

    Example usage:

    >>> input_dim = 10
    >>> context_dim = 5
    >>> x = torch.rand(100, input_dim)
    >>> z = torch.rand(100, context_dim)
    >>> nn = ConditionalDenseNN(input_dim, context_dim, [50], param_dims=[1, input_dim, input_dim])
    >>> a, b, c = nn(x, context=z)  # parameters of size (100, 1), (100, 10), (100, 10)

    :param input_dim: the dimensionality of the input
    :type input_dim: int
    :param context_dim: the dimensionality of the context variable
    :type context_dim: int
    :param hidden_dims: the dimensionality of the hidden units per layer
    :type hidden_dims: list[int]
    :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims
        when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().
    :type param_dims: list[int]
    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
        nonlinearity is applied to the final network output, so the output is an unbounded real number.
    :type nonlinearity: torch.nn.Module

    """

    def __init__(self, input_dim, context_dim, hidden_dims, param_dims=[1, 1], nonlinearity=torch.nn.ReLU()):
        super().__init__()
        self.input_dim = input_dim
        self.context_dim = context_dim
        self.hidden_dims = hidden_dims
        self.param_dims = param_dims
        self.count_params = len(param_dims)
        self.output_multiplier = sum(param_dims)
        ends = torch.cumsum(torch.tensor(param_dims), dim=0)
        starts = torch.cat((torch.zeros(1).type_as(ends), ends[:-1]))
        self.param_slices = [slice(s.item(), e.item()) for s, e in zip(starts, ends)]
        layers = [torch.nn.Linear(input_dim + context_dim, hidden_dims[0])]
        for i in range(1, len(hidden_dims)):
            layers.append(torch.nn.Linear(hidden_dims[i - 1], hidden_dims[i]))
        layers.append(torch.nn.Linear(hidden_dims[-1], self.output_multiplier))
        self.layers = torch.nn.ModuleList(layers)
        self.f = nonlinearity

    def forward(self, x, context):
        context = context.expand(x.size()[:-1] + (context.size(-1),))
        x = torch.cat([context, x], dim=-1)
        return self._forward(x)

    def _forward(self, x):
        """
        The forward method
        """
        h = x
        for layer in self.layers[:-1]:
            h = self.f(layer(h))
        h = self.layers[-1](h)
        if self.output_multiplier == 1:
            return h
        else:
            h = h.reshape(list(x.size()[:-1]) + [self.output_multiplier])
            if self.count_params == 1:
                return h
            else:
                return tuple([h[..., s] for s in self.param_slices])


class DenseNN(ConditionalDenseNN):
    """
    An implementation of a simple dense feedforward network, for use in, e.g., some conditional flows such as
    :class:`pyro.distributions.transforms.ConditionalPlanarFlow` and other unconditional flows such as
    :class:`pyro.distributions.transforms.AffineCoupling` that do not require an autoregressive network.

    Example usage:

    >>> input_dim = 10
    >>> context_dim = 5
    >>> z = torch.rand(100, context_dim)
    >>> nn = DenseNN(context_dim, [50], param_dims=[1, input_dim, input_dim])
    >>> a, b, c = nn(z)  # parameters of size (100, 1), (100, 10), (100, 10)

    :param input_dim: the dimensionality of the input
    :type input_dim: int
    :param hidden_dims: the dimensionality of the hidden units per layer
    :type hidden_dims: list[int]
    :param param_dims: shape the output into parameters of dimension (p_n,) for p_n in param_dims
        when p_n > 1 and dimension () when p_n == 1. The default is [1, 1], i.e. output two parameters of dimension ().
    :type param_dims: list[int]
    :param nonlinearity: The nonlinearity to use in the feedforward network such as torch.nn.ReLU(). Note that no
        nonlinearity is applied to the final network output, so the output is an unbounded real number.
    :type nonlinearity: torch.nn.module

    """

    def __init__(self, input_dim, hidden_dims, param_dims=[1, 1], nonlinearity=torch.nn.ReLU()):
        super(DenseNN, self).__init__(input_dim, 0, hidden_dims, param_dims=param_dims, nonlinearity=nonlinearity)

    def forward(self, x):
        return self._forward(x)


five_thirds = 5.0 / 3.0


def pyro_method(fn):
    """
    Decorator for top-level methods of a :class:`PyroModule` to enable pyro
    effects and cache ``pyro.sample`` statements.

    This should be applied to all public methods that read Pyro-managed
    attributes, but is not needed for ``.forward()``.
    """

    @functools.wraps(fn)
    def cached_fn(self, *args, **kwargs):
        with self._pyro_context:
            return fn(self, *args, **kwargs)
    return cached_fn


root_five = math.sqrt(5.0)


root_three = math.sqrt(3.0)


class MaternKernel(PyroModule):
    """
    Provides the building blocks for representing univariate Gaussian Processes (GPs)
    with Matern kernels as state space models.

    :param float nu: The order of the Matern kernel (one of 0.5, 1.5 or 2.5)
    :param int num_gps: the number of GPs
    :param torch.Tensor length_scale_init: optional `num_gps`-dimensional vector of initializers
        for the length scale
    :param torch.Tensor kernel_scale_init: optional `num_gps`-dimensional vector of initializers
        for the kernel scale

    **References**

    [1] `Kalman Filtering and Smoothing Solutions to Temporal Gaussian Process Regression Models`,
        Jouni Hartikainen and Simo Sarkka.
    [2] `Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression`,
        Arno Solin.
    """

    def __init__(self, nu=1.5, num_gps=1, length_scale_init=None, kernel_scale_init=None):
        if nu not in [0.5, 1.5, 2.5]:
            raise NotImplementedError('The only supported values of nu are 0.5, 1.5 and 2.5')
        self.nu = nu
        self.state_dim = {(0.5): 1, (1.5): 2, (2.5): 3}[nu]
        self.num_gps = num_gps
        if length_scale_init is None:
            length_scale_init = torch.ones(num_gps)
        assert length_scale_init.shape == (num_gps,)
        if kernel_scale_init is None:
            kernel_scale_init = torch.ones(num_gps)
        assert kernel_scale_init.shape == (num_gps,)
        super().__init__()
        self.length_scale = PyroParam(length_scale_init, constraint=constraints.positive)
        self.kernel_scale = PyroParam(kernel_scale_init, constraint=constraints.positive)
        if self.state_dim > 1:
            for x in range(self.state_dim):
                for y in range(self.state_dim):
                    mask = torch.zeros(self.state_dim, self.state_dim)
                    mask[x, y] = 1.0
                    self.register_buffer('mask{}{}'.format(x, y), mask)

    @pyro_method
    def transition_matrix(self, dt):
        """
        Compute the (exponentiated) transition matrix of the GP latent space.
        The resulting matrix has layout (num_gps, old_state, new_state), i.e. this
        matrix multiplies states from the right.

        See section 5 in reference [1] for details.

        :param float dt: the time interval over which the GP latent space evolves.
        :returns torch.Tensor: a 3-dimensional tensor of transition matrices of shape
            (num_gps, state_dim, state_dim).
        """
        if self.nu == 0.5:
            rho = self.length_scale.unsqueeze(-1).unsqueeze(-1)
            return torch.exp(-dt / rho)
        elif self.nu == 1.5:
            rho = self.length_scale.unsqueeze(-1).unsqueeze(-1)
            dt_rho = dt / rho
            trans = (1.0 + root_three * dt_rho) * self.mask00 + -3.0 * dt_rho / rho * self.mask01 + dt * self.mask10 + (1.0 - root_three * dt_rho) * self.mask11
            return torch.exp(-root_three * dt_rho) * trans
        elif self.nu == 2.5:
            rho = self.length_scale.unsqueeze(-1).unsqueeze(-1)
            dt_rho = root_five * dt / rho
            dt_rho_sq = dt_rho.pow(2.0)
            dt_rho_cu = dt_rho.pow(3.0)
            dt_rho_qu = dt_rho.pow(4.0)
            dt_sq = dt ** 2.0
            trans = (1.0 + dt_rho + 0.5 * dt_rho_sq) * self.mask00 + -0.5 * dt_rho_cu / dt * self.mask01 + (0.5 * dt_rho_qu - dt_rho_cu) / dt_sq * self.mask02 + (dt_rho + 1.0) * dt * self.mask10 + (1.0 + dt_rho - dt_rho_sq) * self.mask11 + (dt_rho_cu - 3.0 * dt_rho_sq) / dt * self.mask12 + 0.5 * dt_sq * self.mask20 + (1.0 - 0.5 * dt_rho) * dt * self.mask21 + (1.0 - 2.0 * dt_rho + 0.5 * dt_rho_sq) * self.mask22
            return torch.exp(-dt_rho) * trans

    @pyro_method
    def stationary_covariance(self):
        """
        Compute the stationary state covariance. See Eqn. 3.26 in reference [2].

        :returns torch.Tensor: a 3-dimensional tensor of covariance matrices of shape
            (num_gps, state_dim, state_dim).
        """
        if self.nu == 0.5:
            sigmasq = self.kernel_scale.pow(2).unsqueeze(-1).unsqueeze(-1)
            return sigmasq
        elif self.nu == 1.5:
            sigmasq = self.kernel_scale.pow(2).unsqueeze(-1).unsqueeze(-1)
            rhosq = self.length_scale.pow(2).unsqueeze(-1).unsqueeze(-1)
            p_infinity = self.mask00 + 3.0 / rhosq * self.mask11
            return sigmasq * p_infinity
        elif self.nu == 2.5:
            sigmasq = self.kernel_scale.pow(2).unsqueeze(-1).unsqueeze(-1)
            rhosq = self.length_scale.pow(2).unsqueeze(-1).unsqueeze(-1)
            p_infinity = 0.0
            p_infinity = self.mask00 + five_thirds / rhosq * (self.mask11 - self.mask02 - self.mask20) + 25.0 / rhosq.pow(2.0) * self.mask22
            return sigmasq * p_infinity

    @pyro_method
    def process_covariance(self, A):
        """
        Given a transition matrix `A` computed with `transition_matrix` compute the
        the process covariance as described in Eqn. 3.11 in reference [2].

        :returns torch.Tensor: a batched covariance matrix of shape (num_gps, state_dim, state_dim)
        """
        assert A.shape[-2:] == (self.state_dim, self.state_dim)
        p = self.stationary_covariance()
        q = p - torch.matmul(A.transpose(-1, -2), torch.matmul(p, A))
        return q

    @pyro_method
    def transition_matrix_and_covariance(self, dt):
        """
        Get the transition matrix and process covariance corresponding to a time interval `dt`.

        :param float dt: the time interval over which the GP latent space evolves.
        :returns tuple: (`transition_matrix`, `process_covariance`) both 3-dimensional tensors of
            shape (num_gps, state_dim, state_dim)
        """
        trans_matrix = self.transition_matrix(dt)
        process_covar = self.process_covariance(trans_matrix)
        return trans_matrix, process_covar


class AutoGaussianMeta(type(AutoGuide), ABCMeta):
    backends = {}
    default_backend = 'dense'

    def __init__(cls, *args, **kwargs):
        super().__init__(*args, **kwargs)
        assert cls.__name__.startswith('AutoGaussian')
        key = cls.__name__.replace('AutoGaussian', '').lower()
        cls.backends[key] = cls

    def __call__(cls, *args, **kwargs):
        if cls is AutoGaussian:
            backend = kwargs.pop('backend', cls.default_backend)
            cls = cls.backends[backend]
        return super(AutoGaussianMeta, cls).__call__(*args, **kwargs)


def _plates_to_shape(plates):
    shape = [1] * max([0] + [(-f.dim) for f in plates])
    for f in plates:
        shape[f.dim] = f.size
    return torch.Size(shape)


@singledispatch
def track_provenance(x, provenance: frozenset):
    """
    Adds provenance info to the :class:`torch.Tensor` leaves of a data structure.

    :param x: an object to add provenence info to.
    :param frozenset provenance: A provenence set.
    :returns: A provenence-tracking version of ``x``.
    """
    return x


class ProvenanceTensor(torch.Tensor):
    """
    Provenance tracking implementation in Pytorch.

    This class wraps a :class:`torch.Tensor` to track provenance through
    PyTorch ops, where provenance is a user-defined frozenset of objects. The
    provenance of the output tensors of any op is the union of provenances of
    input tensors.

    -   To start tracking provenance, wrap a :class:`torch.Tensor` in a
        :class:`ProvenanceTensor` with user-defined initial provenance.
    -   To read the provenance of a tensor use :meth:`get_provenance` .
    -   To detach provenance during a computation (similar to
        :meth:`~torch.Tensor.detach` to detach gradients during Pytorch
        computations), use the :meth:`detach_provenance` . This is useful to
        distinguish direct vs indirect provenance.

    Example::

        >>> a = ProvenanceTensor(torch.randn(3), frozenset({"a"}))
        >>> b = ProvenanceTensor(torch.randn(3), frozenset({"b"}))
        >>> c = torch.randn(3)
        >>> assert get_provenance(a + b + c) == frozenset({"a", "b"})
        >>> assert get_provenance(a + detach_provenance(b) + c) == frozenset({"a"})

    **References**

    [1] David Wingate, Noah Goodman, Andreas Stuhlmller, Jeffrey Siskind (2011)
        Nonstandard Interpretations of Probabilistic Programs for Efficient Inference
        http://papers.neurips.cc/paper/4309-nonstandard-interpretations-of-probabilistic-programs-for-efficient-inference.pdf

    :param torch.Tensor data: An initial tensor to start tracking.
    :param frozenset provenance: An initial provenance set.
    """

    def __new__(cls, data: torch.Tensor, provenance=frozenset(), **kwargs):
        assert not isinstance(data, ProvenanceTensor)
        if not provenance:
            return data
        return super().__new__(cls)

    def __init__(self, data, provenance=frozenset()):
        assert isinstance(provenance, frozenset)
        if isinstance(data, ProvenanceTensor):
            provenance |= data._provenance
            data = data._t
        self._t = data
        self._provenance = provenance

    def __repr__(self):
        return 'Provenance:\n{}\nTensor:\n{}'.format(self._provenance, self._t)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        provenance = frozenset()
        _args = []
        for arg in args:
            _arg, _provenance = extract_provenance(arg)
            _args.append(_arg)
            provenance |= _provenance
        _kwargs = {}
        for k, v in kwargs.items():
            _v, _provenance = extract_provenance(v)
            _kwargs[k] = _v
            provenance |= provenance
        ret = func(*_args, **_kwargs)
        _ret = track_provenance(ret, provenance)
        return _ret


def is_sample_site(msg):
    if msg['type'] != 'sample':
        return False
    if site_is_subsample(msg):
        return False
    if msg['is_observed'] and msg['mask'] is False:
        return False
    fn = msg['fn']
    while hasattr(fn, 'base_dist'):
        fn = fn.base_dist
    if type(fn).__name__ == 'Delta':
        return False
    return True


class TrackProvenance(Messenger):

    def _pyro_post_sample(self, msg):
        if is_sample_site(msg):
            provenance = frozenset({msg['name']})
            value = detach_provenance(msg['value'])
            msg['value'] = ProvenanceTensor(value, provenance)

    def _pyro_post_param(self, msg):
        if msg['type'] == 'param':
            provenance = frozenset({msg['name']})
            value = detach_provenance(msg['value'])
            msg['value'] = ProvenanceTensor(value, provenance)


def get_provenance(x) ->frozenset:
    """
    Reads the provenance of a recursive datastructure possibly containing
    :class:`torch.Tensor` s.

    :param torch.Tensor tensor: An input tensor.
    :returns: A provenance frozenset.
    :rtype: frozenset
    """
    _, provenance = extract_provenance(x)
    return provenance


@torch.enable_grad()
def get_dependencies(model: Callable, model_args: Optional[tuple]=None, model_kwargs: Optional[dict]=None) ->Dict[str, object]:
    """
    Infers dependency structure about a conditioned model.

    This returns a nested dictionary with structure like::

        {
            "prior_dependencies": {
                "variable1": {"variable1": set()},
                "variable2": {"variable1": set(), "variable2": set()},
                ...
            },
            "posterior_dependencies": {
                "variable1": {"variable1": {"plate1"}, "variable2": set()},
                ...
            },
        }

    where

    -   `prior_dependencies` is a dict mapping downstream latent and observed
        variables to dictionaries mapping upstream latent variables on which
        they depend to sets of plates inducing full dependencies.
        That is, included plates introduce quadratically many dependencies as
        in complete-bipartite graphs, whereas excluded plates introduce only
        linearly many dependencies as in independent sets of parallel edges.
        Prior dependencies follow the original model order.
    -   `posterior_dependencies` is a similar dict, but mapping latent
        variables to the latent or observed sits on which they depend in the
        posterior. Posterior dependencies are reversed from the model order.

    Dependencies elide ``pyro.deterministic`` sites and ``pyro.sample(...,
    Delta(...))`` sites.

    **Examples**

    Here is a simple example with no plates. We see every node depends on
    itself, and only the latent variables appear in the posterior::

        def model_1():
            a = pyro.sample("a", dist.Normal(0, 1))
            pyro.sample("b", dist.Normal(a, 1), obs=torch.tensor(0.0))

        assert get_dependencies(model_1) == {
            "prior_dependencies": {
                "a": {"a": set()},
                "b": {"a": set(), "b": set()},
            },
            "posterior_dependencies": {
                "a": {"a": set(), "b": set()},
            },
        }

    Here is an example where two variables ``a`` and ``b`` start out
    conditionally independent in the prior, but become conditionally dependent
    in the posterior do the so-called collider variable ``c`` on which they
    both depend. This is called "moralization" in the graphical model
    literature::

        def model_2():
            a = pyro.sample("a", dist.Normal(0, 1))
            b = pyro.sample("b", dist.LogNormal(0, 1))
            c = pyro.sample("c", dist.Normal(a, b))
            pyro.sample("d", dist.Normal(c, 1), obs=torch.tensor(0.))

        assert get_dependencies(model_2) == {
            "prior_dependencies": {
                "a": {"a": set()},
                "b": {"b": set()},
                "c": {"a": set(), "b": set(), "c": set()},
                "d": {"c": set(), "d": set()},
            },
            "posterior_dependencies": {
                "a": {"a": set(), "b": set(), "c": set()},
                "b": {"b": set(), "c": set()},
                "c": {"c": set(), "d": set()},
            },
        }

    Dependencies can be more complex in the presence of plates. So far all the
    dict values have been empty sets of plates, but in the following posterior
    we see that ``c`` depends on itself across the plate ``p``. This means
    that, among the elements of ``c``, e.g. ``c[0]`` depends on ``c[1]`` (this
    is why we explicitly allow variables to depend on themselves)::

        def model_3():
            with pyro.plate("p", 5):
                a = pyro.sample("a", dist.Normal(0, 1))
            pyro.sample("b", dist.Normal(a.sum(), 1), obs=torch.tensor(0.0))

        assert get_dependencies(model_3) == {
            "prior_dependencies": {
                "a": {"a": set()},
                "b": {"a": set(), "b": set()},
            },
            "posterior_dependencies": {
                "a": {"a": {"p"}, "b": set()},
            },
        }

    [1] S.Webb, A.Goliski, R.Zinkov, N.Siddharth, T.Rainforth, Y.W.Teh, F.Wood (2018)
        "Faithful inversion of generative models for effective amortized inference"
        https://dl.acm.org/doi/10.5555/3327144.3327229

    :param callable model: A model.
    :param tuple model_args: Optional tuple of model args.
    :param dict model_kwargs: Optional dict of model kwargs.
    :returns: A dictionary of metadata (see above).
    :rtype: dict
    """
    if model_args is None:
        model_args = ()
    if model_kwargs is None:
        model_kwargs = {}
    with torch.random.fork_rng(), torch.no_grad(), pyro.validation_enabled(False):
        with TrackProvenance():
            trace = poutine.trace(model).get_trace(*model_args, **model_kwargs)
    sample_sites = [msg for msg in trace.nodes.values() if is_sample_site(msg)]
    observed = {msg['name'] for msg in sample_sites if msg['is_observed']}
    plates = {msg['name']: {f.name for f in msg['cond_indep_stack'] if f.vectorized} for msg in sample_sites}
    prior_dependencies = {n: {n: set()} for n in plates}
    for i, downstream in enumerate(sample_sites):
        upstreams = [u for u in sample_sites[:i] if not u['is_observed'] if u['value'].numel()]
        if not upstreams:
            continue
        log_prob = downstream['fn'].log_prob(downstream['value'])
        provenance = get_provenance(log_prob)
        for upstream in upstreams:
            u = upstream['name']
            if u in provenance:
                d = downstream['name']
                prior_dependencies[d][u] = set()
    posterior_dependencies = {n: {} for n in plates if n not in observed}
    for d, upstreams in prior_dependencies.items():
        for u, p in upstreams.items():
            if u not in observed:
                posterior_dependencies[u][d] = p.copy()
    order = {msg['name']: i for i, msg in enumerate(reversed(sample_sites))}
    for d, upstreams in prior_dependencies.items():
        upstreams = {u: p for u, p in upstreams.items() if u not in observed}
        for u1, p1 in upstreams.items():
            for u2, p2 in upstreams.items():
                if order[u1] <= order[u2]:
                    p12 = posterior_dependencies[u2].setdefault(u1, set())
                    p12 |= plates[u1] & plates[u2] - plates[d]
                    p12 |= plates[u2] & p1
                    p12 |= plates[u1] & p2
    return {'prior_dependencies': prior_dependencies, 'posterior_dependencies': posterior_dependencies}


class PoissonGuide(AutoGuideList):

    def __init__(self, model, backend):
        super().__init__(model)
        self.append(AutoGaussian(poutine.block(model, hide_fn=self.hide_fn_1), backend=backend))
        self.append(AutoGaussian(poutine.block(model, hide_fn=self.hide_fn_2), backend=backend))

    @staticmethod
    def hide_fn_1(msg):
        return msg['type'] == 'sample' and 'pois' in msg['name']

    @staticmethod
    def hide_fn_2(msg):
        return msg['type'] == 'sample' and 'pois' not in msg['name']


class AttributeModel(PyroModule):

    def __init__(self, size):
        super().__init__()
        self.x = PyroParam(torch.zeros(size))
        self.y = PyroParam(lambda : torch.randn(size))
        self.z = PyroParam(torch.ones(size), constraint=constraints.positive, event_dim=1)
        self.s = PyroSample(dist.Normal(0, 1))
        self.t = PyroSample(lambda self: dist.Normal(self.s, self.z))

    def forward(self):
        return self.x + self.y + self.t


class inner(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.l0 = nn.Linear(2, 2)
        self.l1 = nn.ReLU()

    def forward(self, s):
        pass


class outer(torch.nn.Module):

    def __init__(self):
        super().__init__()
        self.l0 = inner()
        self.l1 = nn.Linear(2, 2)

    def forward(self, s):
        pass


class outest(nn.Module):

    def __init__(self):
        super().__init__()
        self.l0 = outer()
        self.l1 = nn.Linear(2, 2)
        self.l2 = inner()

    def forward(self, s):
        pass


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AutoRegressiveNN,
     lambda: ([], {'input_dim': 4, 'hidden_dims': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BaselineNet,
     lambda: ([], {'hidden_1': 4, 'hidden_2': 4}),
     lambda: ([torch.rand([4, 784])], {}),
     True),
    (BatchNorm,
     lambda: ([], {'input_dim': 4}),
     lambda: ([], {'x': 4}),
     False),
    (BernoulliNet,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Combiner,
     lambda: ([], {'z_dim': 4, 'rnn_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (DiagNormalNet,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Emitter,
     lambda: ([], {'input_dim': 4, 'z_dim': 4, 'emission_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Encoder,
     lambda: ([], {'data_length': 4, 'alphabet_length': 4, 'z_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Exp,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ExponentialNet,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FullyConnected,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GatedTransition,
     lambda: ([], {'z_dim': 4, 'transition_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Householder,
     lambda: ([], {'input_dim': 4}),
     lambda: ([], {'x': 4}),
     False),
    (Identity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LaplaceNet,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MaskedBCELoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (MaskedBlockLinear,
     lambda: ([], {'in_features': 4, 'out_features': 4, 'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (NormalNet,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Radial,
     lambda: ([], {'input_dim': 4}),
     lambda: ([], {'x': 4}),
     False),
    (Spline,
     lambda: ([], {'input_dim': 4}),
     lambda: ([], {'x': torch.rand([4, 4])}),
     False),
    (StudentTNet,
     lambda: ([], {'sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Sylvester,
     lambda: ([], {'input_dim': 4}),
     lambda: ([], {'x': torch.rand([4, 4])}),
     False),
    (TonesGenerator,
     lambda: ([], {'args': _mock_config(hidden_dim=4, nn_dim=4, nn_channels=4), 'data_dim': 4}),
     lambda: ([torch.ones([4], dtype=torch.int64), torch.rand([4, 4, 4, 4])], {}),
     False),
    (inner,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (outer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (outest,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_pyro_ppl_pyro(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

