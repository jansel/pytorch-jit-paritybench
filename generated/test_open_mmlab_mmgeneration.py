import sys
_module = sys.modules[__name__]
del sys
update_model_index = _module
conditional_interpolate = _module
interpolate_sample = _module
modified_sefa = _module
styleclip = _module
stylegan_projector = _module
Inception_Score = _module
cifar10 = _module
cifar10_inception_stat = _module
cifar10_noaug = _module
cifar10_nopad = _module
cifar10_random_noise = _module
cifar10_rgb = _module
ffhq_flip = _module
grow_scale_imgs_128x128 = _module
grow_scale_imgs_ffhq_styleganv1 = _module
imagenet_128 = _module
imagenet_128x128_inception_stat = _module
imagenet_256 = _module
imagenet_64x64_inception_stat = _module
imagenet_noaug_128 = _module
imagenet_noaug_256 = _module
imagenet_noaug_64 = _module
imagenet_rgb = _module
lsun_stylegan = _module
paired_imgs_256x256 = _module
paired_imgs_256x256_crop = _module
singan = _module
unconditional_imgs_128x128 = _module
unconditional_imgs_64x64 = _module
unconditional_imgs_flip_256x256 = _module
unconditional_imgs_flip_512x512 = _module
unconditional_imgs_flip_lanczos_resize_256x256 = _module
unpaired_imgs_256x256 = _module
default_metrics = _module
default_runtime = _module
biggan_128x128 = _module
biggan_32x32 = _module
cyclegan_lsgan_resnet = _module
dcgan_128x128 = _module
dcgan_64x64 = _module
ddpm_32x32 = _module
ddpm_64x64 = _module
lsgan_128x128 = _module
pggan_1024 = _module
pggan_128x128 = _module
pix2pix_vanilla_unet_bn = _module
sagan_128x128 = _module
sagan_32x32 = _module
sngan_proj_128x128 = _module
sngan_proj_32x32 = _module
stylegan2_base = _module
stylegan3_base = _module
styleganv1_base = _module
wgangp_base = _module
biggan_cifar10_32x32_b25x2_500k = _module
cyclegan_lsgan_id0_resnet_in_facades_b1x1_80k = _module
cyclegan_lsgan_id0_resnet_in_horse2zebra_b1x1_270k = _module
cyclegan_lsgan_id0_resnet_in_summer2winter_b1x1_250k = _module
cyclegan_lsgan_resnet_in_facades_b1x1_80k = _module
cyclegan_lsgan_resnet_in_horse2zebra_b1x1_270k = _module
cyclegan_lsgan_resnet_in_summer2winter_b1x1_250k = _module
pix2pix_vanilla_unet_bn_aerial2maps_b1x1_220k = _module
pix2pix_vanilla_unet_bn_facades_b1x1_80k = _module
pix2pix_vanilla_unet_bn_maps2aerial_b1x1_220k = _module
pix2pix_vanilla_unet_bn_wo_jitter_flip_edges2shoes_b1x4_190k = _module
singan_csg_bohemian = _module
singan_csg_fish = _module
stylegan2_c2_ffhq_256_b3x8_1100k = _module
stylegan2_c2_ffhq_512_b3x8_1100k = _module
sagan_128_cvt_studioGAN = _module
sagan_32_cvt_studioGAN = _module
singan_balloons = _module
singan_bohemian = _module
singan_fish = _module
sngan_proj_128_cvt_studioGAN = _module
sngan_proj_32_cvt_studioGAN = _module
styleganv1_ffhq_1024_g8_25Mimg = _module
styleganv1_ffhq_256_g8_25Mimg = _module
stylegan2_c2_apex_fp16_quicktest_ffhq_256_b4x8_800k = _module
stylegan2_c2_ffhq_1024_b4x8 = _module
stylegan2_c2_ffhq_256_b4x8_800k = _module
stylegan2_c2_fp16_quicktest_ffhq_256_b4x8_800k = _module
stylegan3_r_afhqv2_512_b4x8_cvt_official_rgb = _module
stylegan3_r_ffhq_1024_b4x8_cvt_official_rgb = _module
stylegan3_r_ffhqu_256_b4x8_cvt_official_rgb = _module
stylegan3_t_afhqv2_512_b4x8_cvt_official_rgb = _module
stylegan3_t_ffhq_1024_b4x8_cvt_official_rgb = _module
stylegan3_t_ffhqu_256_b4x8_cvt_official_rgb = _module
conditional_demo = _module
ddpm_demo = _module
translation_demo = _module
unconditional_demo = _module
conf = _module
stat = _module
mmgen = _module
apis = _module
inference = _module
train = _module
core = _module
ddp_wrapper = _module
evaluation = _module
eval_hooks = _module
evaluation = _module
metric_utils = _module
metrics = _module
hooks = _module
ceph_hooks = _module
ema_hook = _module
pggan_fetch_data_hook = _module
pickle_data_hook = _module
visualization = _module
visualize_training_samples = _module
optimizer = _module
builder = _module
registry = _module
runners = _module
apex_amp_utils = _module
checkpoint = _module
dynamic_iterbased_runner = _module
fp16_utils = _module
scheduler = _module
lr_updater = _module
datasets = _module
builder = _module
dataset_wrappers = _module
grow_scale_image_dataset = _module
paired_image_dataset = _module
pipelines = _module
augmentation = _module
compose = _module
crop = _module
formatting = _module
loading = _module
normalize = _module
quick_test_dataset = _module
samplers = _module
distributed_sampler = _module
singan_dataset = _module
unconditional_image_dataset = _module
unpaired_image_dataset = _module
models = _module
architectures = _module
arcface = _module
helpers = _module
id_loss = _module
model_irse = _module
biggan = _module
biggan_snmodule = _module
generator_discriminator = _module
generator_discriminator_deep = _module
modules = _module
common = _module
cyclegan = _module
generator_discriminator = _module
modules = _module
dcgan = _module
generator_discriminator = _module
ddpm = _module
denoising = _module
modules = _module
fid_inception = _module
lpips = _module
networks_basic = _module
perceptual_loss = _module
pretrained_networks = _module
lsgan = _module
generator_discriminator = _module
pggan = _module
generator_discriminator = _module
modules = _module
pix2pix = _module
generator_discriminator = _module
modules = _module
positional_encoding = _module
generator_discriminator = _module
modules = _module
positional_encoding = _module
sngan_proj = _module
generator_discriminator = _module
modules = _module
stylegan = _module
augment = _module
grid_sample_gradfix = _module
misc = _module
upfirdn2d = _module
generator_discriminator_v1 = _module
generator_discriminator_v2 = _module
generator_discriminator_v3 = _module
styleganv1_modules = _module
styleganv2_modules = _module
styleganv3_modules = _module
mspie = _module
utils = _module
wgan_gp = _module
generator_discriminator = _module
modules = _module
builder = _module
dist_utils = _module
model_utils = _module
diffusions = _module
base_diffusion = _module
sampler = _module
utils = _module
gans = _module
base_gan = _module
basic_conditional_gan = _module
mspie_stylegan2 = _module
progressive_growing_unconditional_gan = _module
singan = _module
static_unconditional_gan = _module
losses = _module
ddpm_loss = _module
disc_auxiliary_loss = _module
gan_loss = _module
gen_auxiliary_loss = _module
pixelwise_loss = _module
utils = _module
misc = _module
translation_models = _module
base_translation_model = _module
cyclegan = _module
pix2pix = _module
static_translation_gan = _module
ops = _module
conv2d_gradfix = _module
stylegan3 = _module
custom_ops = _module
bias_act = _module
filtered_lrelu = _module
upfirdn2d = _module
collect_env = _module
dist_util = _module
io_utils = _module
logger = _module
version = _module
setup = _module
test_inference = _module
test_ema_hooks = _module
test_fp16_utils = _module
test_metrics = _module
test_optimizers = _module
test_scheduler = _module
test_tensor2img = _module
test_visualization_hook = _module
test_dataset_wrappers = _module
test_grow_scale_img_dataset = _module
test_paired_image_dataset = _module
test_persistent_worker = _module
test_augmentation = _module
test_compose = _module
test_crop = _module
test_formatting = _module
test_loading = _module
test_normalize = _module
test_quicktest_dataset = _module
test_singan_dataset = _module
test_unconditional_image_dataset = _module
test_unpaired_image_dataset = _module
test_ddpm_loss = _module
test_disc_auxilary_loss = _module
test_gan_loss = _module
test_gen_auxiliary_loss = _module
test_pixelwise_loss = _module
test_base_ddpm = _module
test_base_gan = _module
test_basic_conditional_gan = _module
test_cyclegan = _module
test_ddpm = _module
test_mspie_styelgan2 = _module
test_pggan = _module
test_pix2pix = _module
test_sagan = _module
test_singan = _module
test_sngan_proj = _module
test_static_unconditional_gan = _module
test_stylegan1 = _module
test_wgan_gp = _module
test_arcface = _module
test_biggan_archs = _module
test_biggan_deep_archs = _module
test_cyclegan_archs = _module
test_dcgan_archs = _module
test_ddpm_denoising_archs = _module
test_fid_inception = _module
test_lpips = _module
test_lsgan_archs = _module
test_mspie_archs = _module
test_pe = _module
test_pggan_archs = _module
test_pix2pix_archs = _module
test_sagan_archs = _module
test_singan_archs = _module
test_sngan_proj_archs = _module
test_stylev1_archs = _module
test_stylev2_archs = _module
test_stylev3_archs = _module
test_wgangp_archs = _module
test_conv_gradfix = _module
test_stylegan3_ops = _module
test_io_utils = _module
mmgen2torchserver = _module
mmgen_unconditional_handler = _module
test_torchserver = _module
evaluation = _module
print_config = _module
publish_model = _module
train = _module
inception_stat = _module
singan_inference = _module
translation_eval = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import torch


import torch.nn as nn


from torchvision.utils import save_image


import numpy as np


from torchvision import utils


import math


import torchvision


from torch import optim


from collections import OrderedDict


import torch.nn.functional as F


from torchvision import transforms


from copy import deepcopy


from torch.cuda._utils import _get_device_index


import warnings


import torch.distributed as dist


from abc import ABC


from abc import abstractmethod


from functools import partial


from scipy import linalg


from scipy import signal


from scipy.stats import entropy


from torchvision import models


from torchvision.models.inception import inception_v3


import logging


import time


from torch.optim import Optimizer


from torch.utils.data import DataLoader


import functools


from collections import abc


from inspect import getfullargspec


import random


from torch.utils.data import Dataset


import copy


from collections.abc import Sequence


from torch.utils.data import DistributedSampler as _DistributedSampler


from collections import namedtuple


from torch.nn import AdaptiveAvgPool2d


from torch.nn import BatchNorm2d


from torch.nn import Conv2d


from torch.nn import MaxPool2d


from torch.nn import Module


from torch.nn import PReLU


from torch.nn import ReLU


from torch.nn import Sequential


from torch.nn import Sigmoid


from torch import nn


from torch.nn import BatchNorm1d


from torch.nn import Dropout


from torch.nn import Linear


from torch.nn.utils import spectral_norm


from torch.nn import Parameter


from torch.nn.modules.batchnorm import SyncBatchNorm


from torch.utils.model_zoo import load_url


from torchvision import models as tv


from torch.nn.init import _calculate_correct_fan


from torch.nn import init


from torch.nn.init import xavier_uniform_


import scipy.signal


import scipy


import torch.autograd as autograd


from abc import ABCMeta


from collections import defaultdict


from torch.nn.parallel.distributed import _find_tensors


from torch.nn.parallel import DataParallel


from torch.nn.parallel import DistributedDataParallel


import torchvision.models.vgg as vgg


from torchvision.utils import make_grid


import re


import uuid


import torch.utils.cpp_extension


from typing import Any


from torch.utils.cpp_extension import BuildExtension


from torch.utils.cpp_extension import CppExtension


from torch.utils.cpp_extension import CUDAExtension


import numpy.testing as npt


from torch.distributions.normal import Normal


from torch.autograd import gradgradcheck


class DistributedDataParallelWrapper(nn.Module):
    """A DistributedDataParallel wrapper for models in MMGeneration.

    In MMedting, there is a need to wrap different modules in the models
    with separate DistributedDataParallel. Otherwise, it will cause
    errors for GAN training.
    More specific, the GAN model, usually has two sub-modules:
    generator and discriminator. If we wrap both of them in one
    standard DistributedDataParallel, it will cause errors during training,
    because when we update the parameters of the generator (or discriminator),
    the parameters of the discriminator (or generator) is not updated, which is
    not allowed for DistributedDataParallel.
    So we design this wrapper to separately wrap DistributedDataParallel
    for generator and discriminator.
    In this wrapper, we perform two operations:
    1. Wrap the modules in the models with separate MMDistributedDataParallel.
        Note that only modules with parameters will be wrapped.
    2. Do scatter operation for 'forward', 'train_step' and 'val_step'.
    Note that the arguments of this wrapper is the same as those in
    `torch.nn.parallel.distributed.DistributedDataParallel`.
    Args:
        module (nn.Module): Module that needs to be wrapped.
        device_ids (list[int | `torch.device`]): Same as that in
            `torch.nn.parallel.distributed.DistributedDataParallel`.
        dim (int, optional): Same as that in the official scatter function in
            pytorch. Defaults to 0.
        broadcast_buffers (bool): Same as that in
            `torch.nn.parallel.distributed.DistributedDataParallel`.
            Defaults to False.
        find_unused_parameters (bool, optional): Same as that in
            `torch.nn.parallel.distributed.DistributedDataParallel`.
            Traverse the autograd graph of all tensors contained in returned
            value of the wrapped module’s forward function. Defaults to False.
        kwargs (dict): Other arguments used in
            `torch.nn.parallel.distributed.DistributedDataParallel`.
    """

    def __init__(self, module, device_ids, dim=0, broadcast_buffers=False, find_unused_parameters=False, **kwargs):
        super().__init__()
        assert len(device_ids) == 1, f'Currently, DistributedDataParallelWrapper only supports onesingle CUDA device for each process.The length of device_ids must be 1, but got {len(device_ids)}.'
        self.module = module
        self.dim = dim
        self.to_ddp(device_ids=device_ids, dim=dim, broadcast_buffers=broadcast_buffers, find_unused_parameters=find_unused_parameters, **kwargs)
        self.output_device = _get_device_index(device_ids[0], True)

    def to_ddp(self, device_ids, dim, broadcast_buffers, find_unused_parameters, **kwargs):
        """Wrap models with separate MMDistributedDataParallel.

        It only wraps the modules with parameters.
        """
        for name, module in self.module._modules.items():
            if next(module.parameters(), None) is None:
                module = module
            elif all(not p.requires_grad for p in module.parameters()):
                module = module
            else:
                module = MMDistributedDataParallel(module, device_ids=device_ids, dim=dim, broadcast_buffers=broadcast_buffers, find_unused_parameters=find_unused_parameters, **kwargs)
            self.module._modules[name] = module

    def scatter(self, inputs, kwargs, device_ids):
        """Scatter function.

        Args:
            inputs (Tensor): Input Tensor.
            kwargs (dict): Args for
                ``mmcv.parallel.scatter_gather.scatter_kwargs``.
            device_ids (int): Device id.
        """
        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)

    def forward(self, *inputs, **kwargs):
        """Forward function.

        Args:
            inputs (tuple): Input data.
            kwargs (dict): Args for
                ``mmcv.parallel.scatter_gather.scatter_kwargs``.
        """
        inputs, kwargs = self.scatter(inputs, kwargs, [torch.cuda.current_device()])
        return self.module(*inputs[0], **kwargs[0])

    def train_step(self, *inputs, **kwargs):
        """Train step function.

        Args:
            inputs (Tensor): Input Tensor.
            kwargs (dict): Args for
                ``mmcv.parallel.scatter_gather.scatter_kwargs``.
        """
        inputs, kwargs = self.scatter(inputs, kwargs, [torch.cuda.current_device()])
        output = self.module.train_step(*inputs[0], **kwargs[0])
        return output

    def val_step(self, *inputs, **kwargs):
        """Validation step function.

        Args:
            inputs (tuple): Input data.
            kwargs (dict): Args for ``scatter_kwargs``.
        """
        inputs, kwargs = self.scatter(inputs, kwargs, [torch.cuda.current_device()])
        output = self.module.val_step(*inputs[0], **kwargs[0])
        return output


class Flatten(Module):
    """Flatten Module."""

    def forward(self, input):
        return input.view(input.size(0), -1)


class SEModule(Module):
    """Squeeze-and-Excitation Modules.

    Args:
        channels (int): Input channels.
        reduction (int): Intermediate channels reduction ratio.
    """

    def __init__(self, channels, reduction):
        super(SEModule, self).__init__()
        self.avg_pool = AdaptiveAvgPool2d(1)
        self.fc1 = Conv2d(channels, channels // reduction, kernel_size=1, padding=0, bias=False)
        self.relu = ReLU(inplace=True)
        self.fc2 = Conv2d(channels // reduction, channels, kernel_size=1, padding=0, bias=False)
        self.sigmoid = Sigmoid()

    def forward(self, x):
        """Forward Function."""
        module_input = x
        x = self.avg_pool(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return module_input * x


class bottleneck_IR(Module):
    """Intermediate Resblock of bottleneck.

    Args:
        in_channel (int): Input channels.
        depth (int): Output channels.
        stride (int): Conv2d stride.
    """

    def __init__(self, in_channel, depth, stride):
        """Intermediate Resblock of bottleneck.

        Args:
            in_channel (int): Input channels.
            depth (int): Output channels.
            stride (int): Conv2d stride.
        """
        super(bottleneck_IR, self).__init__()
        if in_channel == depth:
            self.shortcut_layer = MaxPool2d(1, stride)
        else:
            self.shortcut_layer = Sequential(Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))
        self.res_layer = Sequential(BatchNorm2d(in_channel), Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth), Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth))

    def forward(self, x):
        """Forward function."""
        shortcut = self.shortcut_layer(x)
        res = self.res_layer(x)
        return res + shortcut


class bottleneck_IR_SE(Module):
    """Intermediate Resblock of bottleneck with SEModule.

    Args:
        in_channel (int): Input channels.
        depth (int): Output channels.
        stride (int): Conv2d stride.
    """

    def __init__(self, in_channel, depth, stride):
        super(bottleneck_IR_SE, self).__init__()
        if in_channel == depth:
            self.shortcut_layer = MaxPool2d(1, stride)
        else:
            self.shortcut_layer = Sequential(Conv2d(in_channel, depth, (1, 1), stride, bias=False), BatchNorm2d(depth))
        self.res_layer = Sequential(BatchNorm2d(in_channel), Conv2d(in_channel, depth, (3, 3), (1, 1), 1, bias=False), PReLU(depth), Conv2d(depth, depth, (3, 3), stride, 1, bias=False), BatchNorm2d(depth), SEModule(depth, 16))

    def forward(self, x):
        """Forward function."""
        shortcut = self.shortcut_layer(x)
        res = self.res_layer(x)
        return res + shortcut


class Bottleneck(namedtuple('Block', ['in_channel', 'depth', 'stride'])):
    """A named tuple describing a ResNet block."""


def get_block(in_channel, depth, num_units, stride=2):
    """Get a single block config.

    Args:
        in_channel (int): Input channels.
        depth (int): Output channels.
        num_units (int): Number of unit modules.
        stride (int, optional): Conv2d stride. Defaults to 2.

    Returns:
        list: A list of unit modules' config.
    """
    return [Bottleneck(in_channel, depth, stride)] + [Bottleneck(depth, depth, 1) for i in range(num_units - 1)]


def get_blocks(num_layers):
    """Get block configs of backbone.

    Args:
        num_layers (int): Number of ConvBlock layers in backbone.

    Raises:
        ValueError: `num_layers` must be one of [50, 100, 152].

    Returns:
        list: A list of block configs.
    """
    if num_layers == 50:
        blocks = [get_block(in_channel=64, depth=64, num_units=3), get_block(in_channel=64, depth=128, num_units=4), get_block(in_channel=128, depth=256, num_units=14), get_block(in_channel=256, depth=512, num_units=3)]
    elif num_layers == 100:
        blocks = [get_block(in_channel=64, depth=64, num_units=3), get_block(in_channel=64, depth=128, num_units=13), get_block(in_channel=128, depth=256, num_units=30), get_block(in_channel=256, depth=512, num_units=3)]
    elif num_layers == 152:
        blocks = [get_block(in_channel=64, depth=64, num_units=3), get_block(in_channel=64, depth=128, num_units=8), get_block(in_channel=128, depth=256, num_units=36), get_block(in_channel=256, depth=512, num_units=3)]
    else:
        raise ValueError('Invalid number of layers: {}. Must be one of [50, 100, 152]'.format(num_layers))
    return blocks


def l2_norm(input, axis=1):
    """l2 normalization.

    Args:
        input (torch.Tensor): The input tensor.
        axis (int, optional): Specifies which axis of input to calculate the
            norm across. Defaults to 1.

    Returns:
        Tensor: Tensor after L2 normalization per-instance.
    """
    norm = torch.norm(input, 2, axis, True)
    output = torch.div(input, norm)
    return output


class Backbone(Module):
    """ Arcface backbone.
    There are many repos follow this codes for facial recognition, and we also
    follow this routine.
    Ref: https://github.com/orpatashnik/StyleCLIP/blob/main/models/facial_recognition/helpers.py # noqa

    Args:
        input_size (int): Input size of image.
        num_layers (int): Number of layer in backbone.
        mode (str, optional): Bottle neck mode. If set to 'ir_se', then
            SEModule will be applied. Defaults to 'ir'.
        drop_ratio (float, optional): Drop out ratio. Defaults to 0.4.
        affine (bool, optional): Whether use affine in BatchNorm1d.
            Defaults to True.
    """

    def __init__(self, input_size, num_layers, mode='ir', drop_ratio=0.4, affine=True):
        super(Backbone, self).__init__()
        assert input_size in [112, 224], 'input_size should be 112 or 224'
        assert num_layers in [50, 100, 152], 'num_layers should be 50, 100 or 152'
        assert mode in ['ir', 'ir_se'], 'mode should be ir or ir_se'
        blocks = get_blocks(num_layers)
        if mode == 'ir':
            unit_module = bottleneck_IR
        elif mode == 'ir_se':
            unit_module = bottleneck_IR_SE
        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False), BatchNorm2d(64), PReLU(64))
        if input_size == 112:
            self.output_layer = Sequential(BatchNorm2d(512), Dropout(drop_ratio), Flatten(), Linear(512 * 7 * 7, 512), BatchNorm1d(512, affine=affine))
        else:
            self.output_layer = Sequential(BatchNorm2d(512), Dropout(drop_ratio), Flatten(), Linear(512 * 14 * 14, 512), BatchNorm1d(512, affine=affine))
        modules = []
        for block in blocks:
            for bottleneck in block:
                modules.append(unit_module(bottleneck.in_channel, bottleneck.depth, bottleneck.stride))
        self.body = Sequential(*modules)

    def forward(self, x):
        """Forward function."""
        x = self.input_layer(x)
        x = self.body(x)
        x = self.output_layer(x)
        return l2_norm(x)


class IDLossModel(nn.Module):
    _ir_se50_url = 'https://gg0ltg.by.files.1drv.com/y4m3fNNszG03z9n8JQ7EhdtQKW8tQVQMFBisPVRgoXi_UfP8pKSSqv8RJNmHy2JampcPmEazo_Mx6NTFSqBpZmhPniROm9uNoghnzaavvYpxkCfiNmDH9YyIF3g-0nwt6bsjk2X80JDdL5z88OAblSDmB-kuQkWSWvA9BM3Xt8DHMCY8lO4HOQCZ5YWUtFyPAVwEyzTGDM-JRA5EJoN2bF1cg'

    def __init__(self, ir_se50_weights=None, device='cuda'):
        super(IDLossModel, self).__init__()
        mmcv.print_log('Loading ResNet ArcFace', 'mmgen')
        self.facenet = Backbone(input_size=112, num_layers=50, drop_ratio=0.6, mode='ir_se')
        if ir_se50_weights is None:
            ir_se50_weights = self._ir_se50_url
        self.facenet.load_state_dict(torch.hub.load_state_dict_from_url(ir_se50_weights))
        self.pool = torch.nn.AdaptiveAvgPool2d((256, 256))
        self.face_pool = torch.nn.AdaptiveAvgPool2d((112, 112))
        self.facenet = self.facenet.eval()

    def extract_feats(self, x):
        if x.shape[2] != 256:
            x = self.pool(x)
        x = x[:, :, 35:223, 32:220]
        x = self.face_pool(x)
        x_feats = self.facenet(x)
        return x_feats

    def forward(self, pred=None, gt=None):
        n_samples = gt.shape[0]
        y_feats = self.extract_feats(gt)
        y_hat_feats = self.extract_feats(pred)
        y_feats = y_feats.detach()
        loss = 0
        sim_improvement = 0
        count = 0
        for i in range(n_samples):
            diff_target = y_hat_feats[i].dot(y_feats[i])
            loss += 1 - diff_target
            count += 1
        return loss / count, sim_improvement / count


def proj(x, y):
    """Calculate Projection of x onto y.

    Args:
        x (torch.Tensor): Projection vector x.
        y (torch.Tensor): Direction vector y.

    Returns:
        torch.Tensor: Projection of x onto y.
    """
    return torch.mm(y, x.t()) * y / torch.mm(y, y.t())


def gram_schmidt(x, ys):
    """Orthogonalize x w.r.t list of vectors ys.

    Args:
        x (torch.Tensor): Vector to be added into the
            orthogonal vectors.
        ys (list[torch.Tensor]): A set of orthogonal vectors.

    Returns:
        torch.Tensor: Result of Gram–Schmidt orthogonalization.
    """
    for y in ys:
        x = x - proj(x, y)
    return x


@torch.no_grad()
def power_iteration(weight, u_list, update=True, eps=1e-12):
    """Power iteration method for calculating spectral norm.

    Args:
        weight (torch.Tensor): Module weight.
        u_list (list[torch.Tensor]): list of left singular vector.
            The length of list equals to the simulation times.
        update (bool, optional): Whether update left singular
            vector. Defaults to True.
        eps (float, optional): Vector Normalization epsilon.
            Defaults to 1e-12.

    Returns:
        tuple[list[tensor.Tensor]]: Tuple consist of three lists
            which contain singular values, left singular
            vector and right singular vector respectively.
    """
    us, vs, svs = [], [], []
    for i, u in enumerate(u_list):
        v = torch.matmul(u, weight)
        v = F.normalize(gram_schmidt(v, vs), eps=eps)
        vs += [v]
        u = torch.matmul(v, weight.t())
        u = F.normalize(gram_schmidt(u, us), eps=eps)
        us += [u]
        if update:
            u_list[i][:] = u
        svs += [torch.squeeze(torch.matmul(torch.matmul(v, weight.t()), u.t()))]
    return svs, us, vs


class SpectralNorm(object):
    """Spectral normalization base class.

    Args:
        num_svs (int): Number of singular values.
        num_iters (int): Number of power iterations per step.
        num_outputs (int): Number of output channels.
        transpose (bool, optional): If set to `True`, weight
            matrix will be transposed before power iteration.
            Defaults to False.
        eps (float, optional): Vector Normalization epsilon for
            avoiding divide by zero. Defaults to 1e-12.
    """

    def __init__(self, num_svs, num_iters, num_outputs, transpose=False, eps=1e-12):
        self.num_iters = num_iters
        self.num_svs = num_svs
        self.transpose = transpose
        self.eps = eps
        for i in range(self.num_svs):
            self.register_buffer('u%d' % i, torch.randn(1, num_outputs))
            self.register_buffer('sv%d' % i, torch.ones(1))

    @property
    def u(self):
        """Get left singular vectors."""
        return [getattr(self, 'u%d' % i) for i in range(self.num_svs)]

    @property
    def sv(self):
        """Get singular values."""
        return [getattr(self, 'sv%d' % i) for i in range(self.num_svs)]

    def sn_weight(self):
        """Compute the spectrally-normalized weight."""
        W_mat = self.weight.view(self.weight.size(0), -1)
        if self.transpose:
            W_mat = W_mat.t()
        for _ in range(self.num_iters):
            svs, us, vs = power_iteration(W_mat, self.u, update=self.training, eps=self.eps)
        if self.training:
            with torch.no_grad():
                for i, sv in enumerate(svs):
                    self.sv[i][:] = sv
        return self.weight / svs[-1]


class SNConv2d(nn.Conv2d, SpectralNorm):
    """2D Conv layer with spectral norm.

    Args:
        in_channels (int): Number of channels in the input feature map.
        out_channels (int): Number of channels produced by the convolution.
        kernel_size (int): Size of the convolving kernel.
        stride (int, optional): Stride of the convolution.. Defaults to 1.
        padding (int, optional): Zero-padding added to both sides of
            the input. Defaults to 0.
        dilation (int, optional): Spacing between kernel elements.
            Defaults to 1.
        groups (int, optional): Number of blocked connections from input
            channels to output channels. Defaults to 1.
        bias (bool, optional): Whether to use bias parameter.
            Defaults to True.
        num_svs (int): Number of singular values.
        num_iters (int): Number of power iterations per step.
        eps (float, optional): Vector Normalization epsilon for
            avoiding divide by zero. Defaults to 1e-12.
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, num_svs=1, num_iters=1, eps=1e-12):
        nn.Conv2d.__init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)
        SpectralNorm.__init__(self, num_svs, num_iters, out_channels, eps=eps)

    def forward(self, x):
        """Forward function."""
        return F.conv2d(x, self.sn_weight(), self.bias, self.stride, self.padding, self.dilation, self.groups)


class SNLinear(nn.Linear, SpectralNorm):
    """Linear layer with spectral norm.

    Args:
        in_features (int): Number of channels in the input feature.
        out_features (int): Number of channels in the out feature.
        bias (bool, optional):  Whether to use bias parameter.
            Defaults to True.
        num_svs (int): Number of singular values.
        num_iters (int): Number of power iterations per step.
        eps (float, optional): Vector Normalization epsilon for
            avoiding divide by zero. Defaults to 1e-12.
    """

    def __init__(self, in_features, out_features, bias=True, num_svs=1, num_iters=1, eps=1e-12):
        nn.Linear.__init__(self, in_features, out_features, bias)
        SpectralNorm.__init__(self, num_svs, num_iters, out_features, eps=eps)

    def forward(self, x):
        """Forward function."""
        return F.linear(x, self.sn_weight(), self.bias)


class SNEmbedding(nn.Embedding, SpectralNorm):
    """Embedding layer with spectral norm.

    Args:
        num_embeddings (int): Size of the dictionary of embeddings.
        embedding_dim (int): The size of each embedding vector.
        padding_idx (int, optional):  If specified, the entries at
            padding_idx do not contribute to the gradient; therefore,
            the embedding vector at padding_idx is not updated during
            training, i.e. it remains as a fixed “pad”. For a newly
            constructed Embedding, the embedding vector at padding_idx
            will default to all zeros, but can be updated to another value
            to be used as the padding vector. Defaults to None.
        max_norm (float, optional): If given, each embedding vector with
            norm larger than max_norm is renormalized to have norm
            max_norm. Defaults to None.
        norm_type (int, optional):  The p of the p-norm to compute for
            the max_norm option. Default 2.
        scale_grad_by_freq (bool, optional): If given, this will scale
            gradients by the inverse of frequency of the words in the
            mini-batch. Default False.
        sparse (bool, optional):  If True, gradient w.r.t. weight matrix
            will be a sparse tensor. See Notes for more details regarding
            sparse gradients. Defaults to False.
        _weight (torch.Tensor, optional): Initial Weight. Defaults to None.
        num_svs (int): Number of singular values.
        num_iters (int): Number of power iterations per step.
        eps (float, optional): Vector Normalization epsilon for
            avoiding divide by zero. Defaults to 1e-12.
    """

    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None, num_svs=1, num_iters=1, eps=1e-12):
        nn.Embedding.__init__(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight)
        SpectralNorm.__init__(self, num_svs, num_iters, num_embeddings, eps=eps)

    def forward(self, x):
        """Forward function."""
        return F.embedding(x, self.sn_weight())


class SelfAttentionBlock(nn.Module):
    """Self-Attention block used in BigGAN.

    Args:
        in_channels (int): The channel number of the input feature map.
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
    """

    def __init__(self, in_channels, with_spectral_norm=True, sn_eps=1e-06, sn_style='ajbrock'):
        super(SelfAttentionBlock, self).__init__()
        self.in_channels = in_channels
        self.theta = SNConvModule(self.in_channels, self.in_channels // 8, kernel_size=1, padding=0, bias=False, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.phi = SNConvModule(self.in_channels, self.in_channels // 8, kernel_size=1, padding=0, bias=False, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.g = SNConvModule(self.in_channels, self.in_channels // 2, kernel_size=1, padding=0, bias=False, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.o = SNConvModule(self.in_channels // 2, self.in_channels, kernel_size=1, padding=0, bias=False, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.gamma = Parameter(torch.tensor(0.0), requires_grad=True)

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        theta = self.theta(x)
        phi = F.max_pool2d(self.phi(x), [2, 2])
        g = F.max_pool2d(self.g(x), [2, 2])
        theta = theta.view(-1, self.in_channels // 8, x.shape[2] * x.shape[3])
        phi = phi.view(-1, self.in_channels // 8, x.shape[2] * x.shape[3] // 4)
        g = g.view(-1, self.in_channels // 2, x.shape[2] * x.shape[3] // 4)
        beta = F.softmax(torch.bmm(theta.transpose(1, 2), phi), -1)
        o = self.o(torch.bmm(g, beta.transpose(1, 2)).view(-1, self.in_channels // 2, x.shape[2], x.shape[3]))
        return self.gamma * o + x


def build(cfg, registry, default_args=None):
    """Build a module.

    Args:
        cfg (dict, list[dict]): The config of modules, is is either a dict
            or a list of configs.
        registry (:obj:`Registry`): A registry the module belongs to.
        default_args (dict, optional): Default arguments to build the module.
            Defaults to None.
    Returns:
        nn.Module: A built nn module.
    """
    if isinstance(cfg, list):
        modules = [build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg]
        return nn.ModuleList(modules)
    return build_from_cfg(cfg, registry, default_args)


def build_module(cfg, default_args=None):
    """Build a module or modules from a list."""
    return build(cfg, MODULES, default_args)


def get_module_device(module):
    """Get the device of a module.

    Args:
        module (nn.Module): A module contains the parameters.

    Returns:
        torch.device: The device of the module.
    """
    try:
        next(module.parameters())
    except StopIteration:
        raise ValueError('The input module should contain parameters.')
    if next(module.parameters()).is_cuda:
        return next(module.parameters()).get_device()
    return torch.device('cpu')


def get_root_logger(log_file=None, log_level=logging.INFO, file_mode='w'):
    """Initialize and get a logger with name of mmgen.

    If the logger has not been initialized, this method will initialize the
    logger by adding one or two handlers, otherwise the initialized logger will
    be directly returned. During initialization, a StreamHandler will always be
    added. If `log_file` is specified and the process rank is 0, a FileHandler
    will also be added.

    Args:
        log_file (str | None): The log filename. If specified, a FileHandler
            will be added to the logger. Defaults to ``None``.
        log_level (int): The logger level. Note that only the process of
            rank 0 is affected, and other processes will set the level to
            "Error" thus be silent most of the time.
            Defaults to ``logging.INFO``.
        file_mode (str): The file mode used in opening log file.
            Defaults to 'w'.

    Returns:
        logging.Logger: The expected logger.
    """
    return get_logger('mmgen', log_file, log_level, file_mode=file_mode)


class BigGANGenerator(nn.Module):
    """BigGAN Generator. The implementation refers to
    https://github.com/ajbrock/BigGAN-PyTorch/blob/master/BigGAN.py # noqa.

    In BigGAN, we use a SAGAN-based architecture composing of an self-attention
    block and number of convolutional residual blocks with spectral
    normalization.

    More details can be found in: Large Scale GAN Training for High Fidelity
    Natural Image Synthesis (ICLR2019).

    The design of the model structure is highly corresponding to the output
    resolution. For the original BigGAN's generator, you can set ``output_scale``
    as you need and use the default value of ``arch_cfg`` and ``blocks_cfg``.
    If you want to customize the model, you can set the arguments in this way:

    ``arch_cfg``: Config for the architecture of this generator. You can refer
    the ``_default_arch_cfgs`` in the ``_get_default_arch_cfg`` function to see
    the format of the ``arch_cfg``. Basically, you need to provide information
    of each block such as the numbers of input and output channels, whether to
    perform upsampling, etc.

    ``blocks_cfg``: Config for the convolution block. You can replace the block
    type to your registered customized block and adjust block params here.
    However, you should notice that some params are shared among these blocks
    like ``act_cfg``, ``with_spectral_norm``, ``sn_eps``, etc.

    Args:
        output_scale (int): Output scale for the generated image.
        noise_size (int, optional): Size of the input noise vector. Defaults
            to 120.
        num_classes (int, optional): The number of conditional classes. If set
            to 0, this model will be degraded to an unconditional model.
            Defaults to 0.
        out_channels (int, optional): Number of channels in output images.
            Defaults to 3.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Defaults to 96.
        input_scale (int, optional): The scale of the input 2D feature map.
            Defaults to 4.
        with_shared_embedding (bool, optional): Whether to use shared
            embedding. Defaults to True.
        shared_dim (int, optional): The output channels of shared embedding.
            Defaults to 128.
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        init_type (str, optional): The name of an initialization method:
            ortho | N02 | xavier. Defaults to 'ortho'.
        split_noise (bool, optional): Whether to split input noise vector.
            Defaults to True.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU').
        upsample_cfg (dict, optional): Config for the upsampling operation.
            Defaults to dict(type='nearest', scale_factor=2).
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        auto_sync_bn (bool, optional): Whether to use synchronized batch
            normalization. Defaults to True.
        blocks_cfg (dict, optional): Config for the convolution block. Defaults
            to dict(type='BigGANGenResBlock').
        arch_cfg (dict, optional): Config for the architecture of this
            generator. Defaults to None.
        out_norm_cfg (dict, optional): Config for the norm of output layer.
            Defaults to dict(type='BN').
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict. Defaults to None.
        rgb2bgr (bool, optional): Whether to reformat the output channels
                with order `bgr`. We provide several pre-trained BigGAN
                weights whose output channels order is `rgb`. You can set
                this argument to True to use the weights.
    """

    def __init__(self, output_scale, noise_size=120, num_classes=0, out_channels=3, base_channels=96, input_scale=4, with_shared_embedding=True, shared_dim=128, sn_eps=1e-06, sn_style='ajbrock', init_type='ortho', split_noise=True, act_cfg=dict(type='ReLU'), upsample_cfg=dict(type='nearest', scale_factor=2), with_spectral_norm=True, auto_sync_bn=True, blocks_cfg=dict(type='BigGANGenResBlock'), arch_cfg=None, out_norm_cfg=dict(type='BN'), pretrained=None, rgb2bgr=False):
        super().__init__()
        self.noise_size = noise_size
        self.num_classes = num_classes
        self.shared_dim = shared_dim
        self.with_shared_embedding = with_shared_embedding
        self.output_scale = output_scale
        self.arch = arch_cfg if arch_cfg else self._get_default_arch_cfg(self.output_scale, base_channels)
        self.input_scale = input_scale
        self.split_noise = split_noise
        self.blocks_cfg = deepcopy(blocks_cfg)
        self.upsample_cfg = deepcopy(upsample_cfg)
        self.rgb2bgr = rgb2bgr
        self.sn_style = sn_style
        if num_classes == 0:
            assert not self.with_shared_embedding
        elif not self.with_shared_embedding:
            assert not self.split_noise
        if self.split_noise:
            self.num_slots = len(self.arch['in_channels']) + 1
            self.noise_chunk_size = self.noise_size // self.num_slots
            self.noise_size = self.noise_chunk_size * self.num_slots
        else:
            self.num_slots = 1
            self.noise_chunk_size = 0
        self.noise2feat = nn.Linear(self.noise_size // self.num_slots, self.arch['in_channels'][0] * self.input_scale ** 2)
        if with_spectral_norm:
            if sn_style == 'torch':
                self.noise2feat = spectral_norm(self.noise2feat, eps=sn_eps)
            elif sn_style == 'ajbrock':
                self.noise2feat = SNLinear(self.noise_size // self.num_slots, self.arch['in_channels'][0] * self.input_scale ** 2, eps=sn_eps)
            else:
                raise NotImplementedError(f'Your {sn_style} is not supported')
        if with_shared_embedding:
            self.shared_embedding = nn.Embedding(num_classes, shared_dim)
        else:
            self.shared_embedding = nn.Identity()
        if num_classes > 0:
            self.dim_after_concat = self.shared_dim + self.noise_chunk_size if self.with_shared_embedding else self.num_classes
        else:
            self.dim_after_concat = self.noise_chunk_size
        self.blocks_cfg.update(dict(dim_after_concat=self.dim_after_concat, act_cfg=act_cfg, sn_eps=sn_eps, sn_style=sn_style, input_is_label=num_classes > 0 and not with_shared_embedding, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn))
        self.conv_blocks = nn.ModuleList()
        for index, out_ch in enumerate(self.arch['out_channels']):
            self.blocks_cfg.update(dict(in_channels=self.arch['in_channels'][index], out_channels=out_ch, upsample_cfg=self.upsample_cfg if self.arch['upsample'][index] else None))
            self.conv_blocks.append(build_module(self.blocks_cfg))
            if self.arch['attention'][index]:
                self.conv_blocks.append(SelfAttentionBlock(out_ch, with_spectral_norm=with_spectral_norm, sn_eps=sn_eps, sn_style=sn_style))
        self.output_layer = SNConvModule(self.arch['out_channels'][-1], out_channels, kernel_size=3, padding=1, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style), act_cfg=act_cfg, norm_cfg=out_norm_cfg, bias=True, order=('norm', 'act', 'conv'))
        self.init_weights(pretrained=pretrained, init_type=init_type)

    def _get_default_arch_cfg(self, output_scale, base_channels):
        assert output_scale in [32, 64, 128, 256, 512]
        _default_arch_cfgs = {'32': {'in_channels': [(base_channels * item) for item in [4, 4, 4]], 'out_channels': [(base_channels * item) for item in [4, 4, 4]], 'upsample': [True] * 3, 'resolution': [8, 16, 32], 'attention': [False, False, False]}, '64': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 4]], 'out_channels': [(base_channels * item) for item in [16, 8, 4, 2]], 'upsample': [True] * 4, 'resolution': [8, 16, 32, 64], 'attention': [False, False, False, True]}, '128': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 4, 2]], 'out_channels': [(base_channels * item) for item in [16, 8, 4, 2, 1]], 'upsample': [True] * 5, 'resolution': [8, 16, 32, 64, 128], 'attention': [False, False, False, True, False]}, '256': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 8, 4, 2]], 'out_channels': [(base_channels * item) for item in [16, 8, 8, 4, 2, 1]], 'upsample': [True] * 6, 'resolution': [8, 16, 32, 64, 128, 256], 'attention': [False, False, False, True, False, False]}, '512': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 8, 4, 2, 1]], 'out_channels': [(base_channels * item) for item in [16, 8, 8, 4, 2, 1, 1]], 'upsample': [True] * 7, 'resolution': [8, 16, 32, 64, 128, 256, 512], 'attention': [False, False, False, True, False, False, False]}}
        return _default_arch_cfgs[str(output_scale)]

    def forward(self, noise, label=None, num_batches=0, return_noise=False, truncation=-1.0, use_outside_embedding=False):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            label (torch.Tensor | callable | None): You can directly give a
                batch of label through a ``torch.Tensor`` or offer a callable
                function to sample a batch of label data. Otherwise, the
                ``None`` indicates to use the default label sampler.
                Defaults to None.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` and
                ``label`` will be returned in a dict with ``fake_img``.
                Defaults to False.
            truncation (float, optional): Truncation factor. Give value not
                less than 0., the truncation trick will be adopted.
                Otherwise, the truncation trick will not be adopted.
                Defaults to -1..
            use_outside_embedding (bool, optional): Whether to use outside
                embedding or use `shared_embedding`. Set to `True` if
                embedding has already be performed outside this function.
                Default to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output image
                will be returned. Otherwise, a dict contains ``fake_img``,
                ``noise_batch`` and ``label`` will be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            assert noise.ndim == 2, f'The noise should be in shape of (n, c), but got {noise.shape}'
            noise_batch = noise
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size))
        if truncation >= 0.0:
            noise_batch = torch.clamp(noise_batch, -1.0 * truncation, 1.0 * truncation)
        if self.num_classes == 0:
            label_batch = None
        elif isinstance(label, torch.Tensor):
            if not use_outside_embedding:
                assert label.ndim == 1, f'The label shoube be in shape of (n, )but got {label.shape}.'
            label_batch = label
        elif callable(label):
            label_generator = label
            assert num_batches > 0
            label_batch = label_generator((num_batches,))
        else:
            assert num_batches > 0
            label_batch = torch.randint(0, self.num_classes, (num_batches,))
        noise_batch = noise_batch
        if label_batch is not None:
            label_batch = label_batch
            if not use_outside_embedding:
                class_vector = self.shared_embedding(label_batch)
            else:
                class_vector = label_batch
        else:
            class_vector = None
        if self.split_noise:
            zs = torch.split(noise_batch, self.noise_chunk_size, dim=1)
            z = zs[0]
            if class_vector is not None:
                ys = [torch.cat([class_vector, item], 1) for item in zs[1:]]
            else:
                ys = zs[1:]
        else:
            ys = [class_vector] * len(self.conv_blocks)
            z = noise_batch
        x = self.noise2feat(z)
        x = x.view(x.size(0), -1, self.input_scale, self.input_scale)
        counter = 0
        for conv_block in self.conv_blocks:
            if isinstance(conv_block, SelfAttentionBlock):
                x = conv_block(x)
            else:
                x = conv_block(x, ys[counter])
                counter += 1
        out_img = torch.tanh(self.output_layer(x))
        if self.rgb2bgr:
            out_img = out_img[:, [2, 1, 0], ...]
        if return_noise:
            output = dict(fake_img=out_img, noise_batch=noise_batch, label=label_batch)
            return output
        return out_img

    def init_weights(self, pretrained=None, init_type='ortho'):
        """Init weights for models.

        Args:
            pretrained (str | dict, optional): Path for the pretrained model or
                dict containing information for pretained models whose
                necessary key is 'ckpt_path'. Besides, you can also provide
                'prefix' to load the generator part from the whole state dict.
                Defaults to None.
            init_type (str, optional): The name of an initialization method:
                ortho | N02 | xavier. Defaults to 'ortho'.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif isinstance(pretrained, dict):
            ckpt_path = pretrained.get('ckpt_path', None)
            assert ckpt_path is not None
            prefix = pretrained.get('prefix', '')
            map_location = pretrained.get('map_location', 'cpu')
            strict = pretrained.get('strict', True)
            state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
            self.load_state_dict(state_dict, strict=strict)
            mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                    if init_type == 'ortho':
                        nn.init.orthogonal_(m.weight)
                    elif init_type == 'N02':
                        normal_init(m, 0.0, 0.02)
                    elif init_type == 'xavier':
                        xavier_init(m)
                    else:
                        raise NotImplementedError(f'{init_type} initialization                             not supported now.')
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class BigGANDiscriminator(nn.Module):
    """BigGAN Discriminator. The implementation refers to
    https://github.com/ajbrock/BigGAN-PyTorch/blob/master/BigGAN.py # noqa.

    In BigGAN, we use a SAGAN-based architecture composing of an self-attention
    block and number of convolutional residual blocks with spectral
    normalization.

    More details can be found in: Large Scale GAN Training for High Fidelity
    Natural Image Synthesis (ICLR2019).

    The design of the model structure is highly corresponding to the output
    resolution. For the original BigGAN's generator, you can set ``output_scale``
    as you need and use the default value of ``arch_cfg`` and ``blocks_cfg``.
    If you want to customize the model, you can set the arguments in this way:

    ``arch_cfg``: Config for the architecture of this generator. You can refer
    the ``_default_arch_cfgs`` in the ``_get_default_arch_cfg`` function to see
    the format of the ``arch_cfg``. Basically, you need to provide information
    of each block such as the numbers of input and output channels, whether to
    perform upsampling, etc.

    ``blocks_cfg``: Config for the convolution block. You can replace the block
    type to your registered customized block and adjust block params here.
    However, you should notice that some params are shared among these blocks
    like ``act_cfg``, ``with_spectral_norm``, ``sn_eps``, etc.

    Args:
        input_scale (int): The scale of the input image.
        num_classes (int, optional): The number of conditional classes.
            Defaults to 0.
        in_channels (int, optional): The channel number of the input image.
            Defaults to 3.
        out_channels (int, optional): The channel number of the final output.
            Defaults to 1.
        base_channels (int, optional): The basic channel number of the
            discriminator. The other layers contains channels based on this
            number. Defaults to 96.
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        init_type (str, optional): The name of an initialization method:
            ortho | N02 | xavier. Defaults to 'ortho'.
        act_cfg (dict, optional): Config for the activation layer.
            Defaults to dict(type='ReLU').
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        blocks_cfg (dict, optional): Config for the convolution block.
            Defaults to dict(type='BigGANDiscResBlock').
        arch_cfg (dict, optional): Config for the architecture of this
            discriminator. Defaults to None.
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict. Defaults to None.
    """

    def __init__(self, input_scale, num_classes=0, in_channels=3, out_channels=1, base_channels=96, sn_eps=1e-06, sn_style='ajbrock', init_type='ortho', act_cfg=dict(type='ReLU'), with_spectral_norm=True, blocks_cfg=dict(type='BigGANDiscResBlock'), arch_cfg=None, pretrained=None):
        super().__init__()
        self.num_classes = num_classes
        self.out_channels = out_channels
        self.input_scale = input_scale
        self.in_channels = in_channels
        self.base_channels = base_channels
        self.arch = arch_cfg if arch_cfg else self._get_default_arch_cfg(self.input_scale, self.in_channels, self.base_channels)
        self.blocks_cfg = deepcopy(blocks_cfg)
        self.blocks_cfg.update(dict(act_cfg=act_cfg, sn_eps=sn_eps, sn_style=sn_style, with_spectral_norm=with_spectral_norm))
        self.sn_style = sn_style
        self.conv_blocks = nn.ModuleList()
        for index, out_ch in enumerate(self.arch['out_channels']):
            self.blocks_cfg.update(dict(in_channels=self.arch['in_channels'][index], out_channels=out_ch, with_downsample=self.arch['downsample'][index], is_head_block=index == 0))
            self.conv_blocks.append(build_module(self.blocks_cfg))
            if self.arch['attention'][index]:
                self.conv_blocks.append(SelfAttentionBlock(out_ch, with_spectral_norm=with_spectral_norm, sn_eps=sn_eps, sn_style=sn_style))
        self.activate = build_activation_layer(act_cfg)
        self.decision = nn.Linear(self.arch['out_channels'][-1], out_channels)
        if with_spectral_norm:
            if sn_style == 'torch':
                self.decision = spectral_norm(self.decision, eps=sn_eps)
            elif sn_style == 'ajbrock':
                self.decision = SNLinear(self.arch['out_channels'][-1], out_channels, eps=sn_eps)
            else:
                raise NotImplementedError('sn style')
        if self.num_classes > 0:
            self.proj_y = nn.Embedding(self.num_classes, self.arch['out_channels'][-1])
            if with_spectral_norm:
                if sn_style == 'torch':
                    self.proj_y = spectral_norm(self.proj_y, eps=sn_eps)
                elif sn_style == 'ajbrock':
                    self.proj_y = SNEmbedding(self.num_classes, self.arch['out_channels'][-1], eps=sn_eps)
                else:
                    raise NotImplementedError('sn style')
        self.init_weights(pretrained=pretrained, init_type=init_type)

    def _get_default_arch_cfg(self, input_scale, in_channels, base_channels):
        assert input_scale in [32, 64, 128, 256, 512]
        _default_arch_cfgs = {'32': {'in_channels': [in_channels] + [(base_channels * item) for item in [4, 4, 4]], 'out_channels': [(base_channels * item) for item in [4, 4, 4, 4]], 'downsample': [True, True, False, False], 'resolution': [16, 8, 8, 8], 'attention': [False, False, False, False]}, '64': {'in_channels': [in_channels] + [(base_channels * item) for item in [1, 2, 4, 8]], 'out_channels': [(base_channels * item) for item in [1, 2, 4, 8, 16]], 'downsample': [True] * 4 + [False], 'resolution': [32, 16, 8, 4, 4], 'attention': [False, False, False, False, False]}, '128': {'in_channels': [in_channels] + [(base_channels * item) for item in [1, 2, 4, 8, 16]], 'out_channels': [(base_channels * item) for item in [1, 2, 4, 8, 16, 16]], 'downsample': [True] * 5 + [False], 'resolution': [64, 32, 16, 8, 4, 4], 'attention': [True, False, False, False, False, False]}, '256': {'in_channels': [in_channels] + [(base_channels * item) for item in [1, 2, 4, 8, 8, 16]], 'out_channels': [(base_channels * item) for item in [1, 2, 4, 8, 8, 16, 16]], 'downsample': [True] * 6 + [False], 'resolution': [128, 64, 32, 16, 8, 4, 4], 'attention': [False, True, False, False, False, False]}, '512': {'in_channels': [in_channels] + [(base_channels * item) for item in [1, 1, 2, 4, 8, 8, 16]], 'out_channels': [(base_channels * item) for item in [1, 1, 2, 4, 8, 8, 16, 16]], 'downsample': [True] * 7 + [False], 'resolution': [256, 128, 64, 32, 16, 8, 4, 4], 'attention': [False, False, False, True, False, False, False]}}
        return _default_arch_cfgs[str(input_scale)]

    def forward(self, x, label=None):
        """Forward function.

        Args:
            x (torch.Tensor): Fake or real image tensor.
            label (torch.Tensor | None): Label Tensor. Defaults to None.

        Returns:
            torch.Tensor: Prediction for the reality of the input image with
                given label.
        """
        x0 = x
        for conv_block in self.conv_blocks:
            x0 = conv_block(x0)
        x0 = self.activate(x0)
        x0 = torch.sum(x0, dim=[2, 3])
        out = self.decision(x0)
        if self.num_classes > 0:
            w_y = self.proj_y(label)
            out = out + torch.sum(w_y * x0, dim=1, keepdim=True)
        return out

    def init_weights(self, pretrained=None, init_type='ortho'):
        """Init weights for models.

        Args:
            pretrained (str | dict, optional): Path for the pretrained model or
                dict containing information for pretained models whose
                necessary key is 'ckpt_path'. Besides, you can also provide
                'prefix' to load the generator part from the whole state dict.
                Defaults to None.
            init_type (str, optional): The name of an initialization method:
                ortho | N02 | xavier. Defaults to 'ortho'.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif isinstance(pretrained, dict):
            ckpt_path = pretrained.get('ckpt_path', None)
            assert ckpt_path is not None
            prefix = pretrained.get('prefix', '')
            map_location = pretrained.get('map_location', 'cpu')
            strict = pretrained.get('strict', True)
            state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
            self.load_state_dict(state_dict, strict=strict)
            mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                    if init_type == 'ortho':
                        nn.init.orthogonal_(m.weight)
                    elif init_type == 'N02':
                        normal_init(m, 0.0, 0.02)
                    elif init_type == 'xavier':
                        xavier_init(m)
                    else:
                        raise NotImplementedError(f'{init_type} initialization                             not supported now.')
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class BigGANDeepGenerator(nn.Module):
    """BigGAN-Deep Generator. The implementation refers to
    https://github.com/ajbrock/BigGAN-PyTorch/blob/master/BigGANdeep.py # noqa.

    In BigGAN, we use a SAGAN-based architecture composing of an
    self-attention block and number of convolutional residual blocks
    with spectral normalization. BigGAN-deep follow the same architecture.

    The main difference between BigGAN and BigGAN-deep is that
    BigGAN-deep uses deeper residual blocks to construct the whole
    model.

    More details can be found in: Large Scale GAN Training for High Fidelity
    Natural Image Synthesis (ICLR2019).

    The design of the model structure is highly corresponding to the output
    resolution. For the original BigGAN-Deep's generator, you can set ``output_scale``
    as you need and use the default value of ``arch_cfg`` and ``blocks_cfg``.
    If you want to customize the model, you can set the arguments in this way:

    ``arch_cfg``: Config for the architecture of this generator. You can refer
    the ``_default_arch_cfgs`` in the ``_get_default_arch_cfg`` function to see
    the format of the ``arch_cfg``. Basically, you need to provide information
    of each block such as the numbers of input and output channels, whether to
    perform upsampling, etc.

    ``blocks_cfg``: Config for the convolution block. You can adjust block params
    like ``channel_ratio`` here. You can also replace the block type
    to your registered customized block. However, you should notice that some
    params are shared among these blocks like ``act_cfg``, ``with_spectral_norm``,
    ``sn_eps``, etc.

    Args:
        output_scale (int): Output scale for the generated image.
        noise_size (int, optional): Size of the input noise vector. Defaults
            to 120.
        num_classes (int, optional): The number of conditional classes. If set
            to 0, this model will be degraded to an unconditional model.
            Defaults to 0.
        out_channels (int, optional): Number of channels in output images.
            Defaults to 3.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Defaults to 96.
        block_depth (int, optional): The repeat times of Residual Blocks in
            each level of architecture. Defaults to 2.
        input_scale (int, optional): The scale of the input 2D feature map.
            Defaults to 4.
        with_shared_embedding (bool, optional): Whether to use shared
            embedding. Defaults to True.
        shared_dim (int, optional): The output channels of shared embedding.
            Defaults to 128.
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        init_type (str, optional): The name of an initialization method:
            ortho | N02 | xavier. Defaults to 'ortho'.
        concat_noise (bool, optional): Whether to concat input noise vector
            with class vector. Defaults to True.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU').
        upsample_cfg (dict, optional): Config for the upsampling operation.
            Defaults to dict(type='nearest', scale_factor=2).
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        auto_sync_bn (bool, optional): Whether to use synchronized batch
            normalization. Defaults to True.
        blocks_cfg (dict, optional): Config for the convolution block. Defaults
            to dict(type='BigGANGenResBlock').
        arch_cfg (dict, optional): Config for the architecture of this
            generator. Defaults to None.
        out_norm_cfg (dict, optional): Config for the norm of output layer.
            Defaults to dict(type='BN').
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict. Defaults to None.
        rgb2bgr (bool, optional): Whether to reformat the output channels
                with order `bgr`. We provide several pre-trained BigGAN-Deep
                weights whose output channels order is `rgb`. You can set
                this argument to True to use the weights.
    """

    def __init__(self, output_scale, noise_size=120, num_classes=0, out_channels=3, base_channels=96, block_depth=2, input_scale=4, with_shared_embedding=True, shared_dim=128, sn_eps=1e-06, sn_style='ajbrock', init_type='ortho', concat_noise=True, act_cfg=dict(type='ReLU', inplace=False), upsample_cfg=dict(type='nearest', scale_factor=2), with_spectral_norm=True, auto_sync_bn=True, blocks_cfg=dict(type='BigGANDeepGenResBlock'), arch_cfg=None, out_norm_cfg=dict(type='BN'), pretrained=None, rgb2bgr=False):
        super().__init__()
        self.noise_size = noise_size
        self.num_classes = num_classes
        self.shared_dim = shared_dim
        self.with_shared_embedding = with_shared_embedding
        self.output_scale = output_scale
        self.arch = arch_cfg if arch_cfg else self._get_default_arch_cfg(self.output_scale, base_channels)
        self.input_scale = input_scale
        self.concat_noise = concat_noise
        self.blocks_cfg = deepcopy(blocks_cfg)
        self.upsample_cfg = deepcopy(upsample_cfg)
        self.block_depth = block_depth
        self.rgb2bgr = rgb2bgr
        self.sn_style = sn_style
        if num_classes == 0:
            assert not self.with_shared_embedding
            assert not self.concat_noise
        elif not self.with_shared_embedding:
            assert not self.concat_noise
        if self.concat_noise:
            self.noise2feat = nn.Linear(self.noise_size + self.shared_dim, self.arch['in_channels'][0] * self.input_scale ** 2)
        else:
            self.noise2feat = nn.Linear(self.noise_size, self.arch['in_channels'][0] * self.input_scale ** 2)
        if with_spectral_norm:
            if sn_style == 'torch':
                self.noise2feat = spectral_norm(self.noise2feat, eps=sn_eps)
            elif sn_style == 'ajbrock':
                self.noise2feat = SNLinear(self.noise_size + (self.shared_dim if self.concat_noise else 0), self.arch['in_channels'][0] * self.input_scale ** 2, eps=sn_eps)
            else:
                NotImplementedError(f'{sn_style} style SN is not supported')
        if with_shared_embedding:
            self.shared_embedding = nn.Embedding(num_classes, shared_dim)
        else:
            self.shared_embedding = nn.Identity()
        if num_classes > 0:
            if self.concat_noise:
                self.dim_after_concat = self.shared_dim + self.noise_size if self.with_shared_embedding else self.num_classes
            else:
                self.dim_after_concat = self.shared_dim if self.with_shared_embedding else self.num_classes
        else:
            self.dim_after_concat = 0
        self.blocks_cfg.update(dict(dim_after_concat=self.dim_after_concat, act_cfg=act_cfg, sn_eps=sn_eps, sn_style=sn_style, input_is_label=num_classes > 0 and not with_shared_embedding, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn))
        self.conv_blocks = nn.ModuleList()
        for index, out_ch in enumerate(self.arch['out_channels']):
            for depth in range(self.block_depth):
                block_cfg_ = deepcopy(self.blocks_cfg)
                block_cfg_.update(dict(in_channels=self.arch['in_channels'][index], out_channels=out_ch if depth == self.block_depth - 1 else self.arch['in_channels'][index], upsample_cfg=self.upsample_cfg if self.arch['upsample'][index] and depth == self.block_depth - 1 else None))
                self.conv_blocks.append(build_module(block_cfg_))
            if self.arch['attention'][index]:
                self.conv_blocks.append(SelfAttentionBlock(out_ch, with_spectral_norm=with_spectral_norm, sn_eps=sn_eps, sn_style=sn_style))
        self.output_layer = SNConvModule(self.arch['out_channels'][-1], out_channels, kernel_size=3, padding=1, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style), act_cfg=act_cfg, norm_cfg=out_norm_cfg, bias=True, order=('norm', 'act', 'conv'))
        self.init_weights(pretrained=pretrained, init_type=init_type)

    def _get_default_arch_cfg(self, output_scale, base_channels):
        assert output_scale in [32, 64, 128, 256, 512]
        _default_arch_cfgs = {'32': {'in_channels': [(base_channels * item) for item in [4, 4, 4]], 'out_channels': [(base_channels * item) for item in [4, 4, 4]], 'upsample': [True] * 3, 'resolution': [8, 16, 32], 'attention': [False, False, False]}, '64': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 4]], 'out_channels': [(base_channels * item) for item in [16, 8, 4, 2]], 'upsample': [True] * 4, 'resolution': [8, 16, 32, 64], 'attention': [False, False, False, True]}, '128': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 4, 2]], 'out_channels': [(base_channels * item) for item in [16, 8, 4, 2, 1]], 'upsample': [True] * 5, 'resolution': [8, 16, 32, 64, 128], 'attention': [False, False, False, True, False]}, '256': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 8, 4, 2]], 'out_channels': [(base_channels * item) for item in [16, 8, 8, 4, 2, 1]], 'upsample': [True] * 6, 'resolution': [8, 16, 32, 64, 128, 256], 'attention': [False, False, False, True, False, False]}, '512': {'in_channels': [(base_channels * item) for item in [16, 16, 8, 8, 4, 2, 1]], 'out_channels': [(base_channels * item) for item in [16, 8, 8, 4, 2, 1, 1]], 'upsample': [True] * 7, 'resolution': [8, 16, 32, 64, 128, 256, 512], 'attention': [False, False, False, True, False, False, False]}}
        return _default_arch_cfgs[str(output_scale)]

    def forward(self, noise, label=None, num_batches=0, return_noise=False, truncation=-1.0, use_outside_embedding=False):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            label (torch.Tensor | callable | None): You can directly give a
                batch of label through a ``torch.Tensor`` or offer a callable
                function to sample a batch of label data. Otherwise, the
                ``None`` indicates to use the default label sampler.
                Defaults to None.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` and
                ``label`` will be returned in a dict with ``fake_img``.
                Defaults to False.
            truncation (float, optional): Truncation factor. Give value not
                less than 0., the truncation trick will be adopted.
                Otherwise, the truncation trick will not be adopted.
                Defaults to -1..
            use_outside_embedding (bool, optional): Whether to use outside
                embedding or use `shared_embedding`. Set to `True` if
                embedding has already be performed outside this function.
                Default to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output image
                will be returned. Otherwise, a dict contains ``fake_img``,
                ``noise_batch`` and ``label`` will be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            assert noise.ndim == 2, f'The noise should be in shape of (n, c), but got {noise.shape}'
            noise_batch = noise
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size))
        if truncation >= 0.0:
            noise_batch = torch.clamp(noise_batch, -1.0 * truncation, 1.0 * truncation)
        if self.num_classes == 0:
            label_batch = None
        elif isinstance(label, torch.Tensor):
            if not use_outside_embedding:
                assert label.ndim == 1, f'The label shoube be in shape of (n, )but got {label.shape}.'
            label_batch = label
        elif callable(label):
            label_generator = label
            assert num_batches > 0
            label_batch = label_generator((num_batches,))
        else:
            assert num_batches > 0
            label_batch = torch.randint(0, self.num_classes, (num_batches,))
        noise_batch = noise_batch
        if label_batch is not None:
            label_batch = label_batch
            if not use_outside_embedding:
                class_vector = self.shared_embedding(label_batch)
            else:
                class_vector = label_batch
        else:
            class_vector = None
        if self.concat_noise:
            if class_vector is not None:
                z = torch.cat([noise_batch, class_vector], dim=1)
                y = z
        elif self.num_classes > 0:
            z = noise_batch
            y = class_vector
        else:
            z = noise_batch
            y = None
        x = self.noise2feat(z)
        x = x.view(x.size(0), self.input_scale, self.input_scale, -1)
        x = x.permute(0, 3, 1, 2).contiguous()
        for idx, conv_block in enumerate(self.conv_blocks):
            if isinstance(conv_block, SelfAttentionBlock):
                x = conv_block(x)
            else:
                x = conv_block(x, y)
        x = self.output_layer(x)
        out_img = torch.tanh(x)
        if self.rgb2bgr:
            out_img = out_img[:, [2, 1, 0], ...]
        if return_noise:
            output = dict(fake_img=out_img, noise_batch=noise_batch, label=label_batch)
            return output
        return out_img

    def init_weights(self, pretrained=None, init_type='ortho'):
        """Init weights for models.

        Args:
            pretrained (str | dict, optional): Path for the pretrained model or
                dict containing information for pretained models whose
                necessary key is 'ckpt_path'. Besides, you can also provide
                'prefix' to load the generator part from the whole state dict.
                Defaults to None.
            init_type (str, optional): The name of an initialization method:
                ortho | N02 | xavier. Defaults to 'ortho'.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif isinstance(pretrained, dict):
            ckpt_path = pretrained.get('ckpt_path', None)
            assert ckpt_path is not None
            prefix = pretrained.get('prefix', '')
            map_location = pretrained.get('map_location', 'cpu')
            strict = pretrained.get('strict', True)
            state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
            self.load_state_dict(state_dict, strict=strict)
            mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                    if init_type == 'ortho':
                        nn.init.orthogonal_(m.weight)
                    elif init_type == 'N02':
                        normal_init(m, 0.0, 0.02)
                    elif init_type == 'xavier':
                        xavier_init(m)
                    else:
                        raise NotImplementedError(f'{init_type} initialization                             not supported now.')
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class BigGANDeepDiscriminator(nn.Module):
    """BigGAN-Deep Discriminator. The implementation refers to
    https://github.com/ajbrock/BigGAN-PyTorch/blob/master/BigGANdeep.py # noqa.

    The overall structure of BigGAN's discriminator is the same with
    the projection discriminator.

    The main difference between BigGAN and BigGAN-deep is that
    BigGAN-deep use more deeper residual blocks to construct the whole
    model.

    More details can be found in: Large Scale GAN Training for High Fidelity
    Natural Image Synthesis (ICLR2019).

    The design of the model structure is highly corresponding to the output
    resolution. For origin BigGAN-Deep's generator, you can set ``output_scale``
    as you need and use the default value of ``arch_cfg`` and ``blocks_cfg``.
    If you want to customize the model, you can set the arguments in this way:

    ``arch_cfg``: Config for the architecture of this generator. You can refer
    the ``_default_arch_cfgs`` in the ``_get_default_arch_cfg`` function to see
    the format of the ``arch_cfg``. Basically, you need to provide information
    of each block such as the numbers of input and output channels, whether to
    perform upsampling etc.

    ``blocks_cfg``: Config for the convolution block. You can adjust block params
    like ``channel_ratio`` here. You can also replace the block type
    to your registered customized block. However, you should notice that some
    params are shared between these blocks like ``act_cfg``, ``with_spectral_norm``,
    ``sn_eps`` etc.

    Args:
        input_scale (int): The scale of the input image.
        num_classes (int, optional): The number of conditional classes.
            Defaults to 0.
        in_channels (int, optional): The channel number of the input image.
            Defaults to 3.
        out_channels (int, optional): The channel number of the final output.
            Defaults to 1.
        base_channels (int, optional): The basic channel number of the
            discriminator. The other layers contains channels based on this
            number. Defaults to 96.
        block_depth (int, optional): The repeat times of Residual Blocks in
            each level of architecture. Defaults to 2.
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        init_type (str, optional): The name of an initialization method:
            ortho | N02 | xavier. Defaults to 'ortho'.
        act_cfg (dict, optional): Config for the activation layer.
            Defaults to dict(type='ReLU').
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        blocks_cfg (dict, optional): Config for the convolution block.
            Defaults to dict(type='BigGANDiscResBlock').
        arch_cfg (dict, optional): Config for the architecture of this
            discriminator. Defaults to None.
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict. Defaults to None.
    """

    def __init__(self, input_scale, num_classes=0, in_channels=3, out_channels=1, base_channels=96, block_depth=2, sn_eps=1e-06, sn_style='ajbrock', init_type='ortho', act_cfg=dict(type='ReLU', inplace=False), with_spectral_norm=True, blocks_cfg=dict(type='BigGANDeepDiscResBlock'), arch_cfg=None, pretrained=None):
        super().__init__()
        self.num_classes = num_classes
        self.out_channels = out_channels
        self.input_scale = input_scale
        self.in_channels = in_channels
        self.base_channels = base_channels
        self.block_depth = block_depth
        self.arch = arch_cfg if arch_cfg else self._get_default_arch_cfg(self.input_scale, self.base_channels)
        self.blocks_cfg = deepcopy(blocks_cfg)
        self.blocks_cfg.update(dict(act_cfg=act_cfg, sn_eps=sn_eps, sn_style=sn_style, with_spectral_norm=with_spectral_norm))
        self.input_conv = SNConvModule(3, self.arch['in_channels'][0], kernel_size=3, padding=1, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style), act_cfg=None)
        self.conv_blocks = nn.ModuleList()
        for index, out_ch in enumerate(self.arch['out_channels']):
            for depth in range(self.block_depth):
                block_cfg_ = deepcopy(self.blocks_cfg)
                block_cfg_.update(dict(in_channels=self.arch['in_channels'][index] if depth == 0 else out_ch, out_channels=out_ch, with_downsample=self.arch['downsample'][index] and depth == 0))
                self.conv_blocks.append(build_module(block_cfg_))
            if self.arch['attention'][index]:
                self.conv_blocks.append(SelfAttentionBlock(out_ch, with_spectral_norm=with_spectral_norm, sn_eps=sn_eps, sn_style=sn_style))
        self.activate = build_activation_layer(act_cfg)
        self.decision = nn.Linear(self.arch['out_channels'][-1], out_channels)
        if with_spectral_norm:
            if sn_style == 'torch':
                self.decision = spectral_norm(self.decision, eps=sn_eps)
            elif sn_style == 'ajbrock':
                self.decision = SNLinear(self.arch['out_channels'][-1], out_channels, eps=sn_eps)
            else:
                raise NotImplementedError(f'{sn_style} style SN is not supported yet')
        if self.num_classes > 0:
            self.proj_y = nn.Embedding(self.num_classes, self.arch['out_channels'][-1])
            if with_spectral_norm:
                if sn_style == 'torch':
                    self.proj_y = spectral_norm(self.proj_y, eps=sn_eps)
                elif sn_style == 'ajbrock':
                    self.proj_y = SNEmbedding(self.num_classes, self.arch['out_channels'][-1], eps=sn_eps)
                else:
                    raise NotImplementedError(f'{sn_style} style SN is not supported yet')
        self.init_weights(pretrained=pretrained, init_type=init_type)

    def _get_default_arch_cfg(self, input_scale, base_channels):
        assert input_scale in [32, 64, 128, 256, 512]
        _default_arch_cfgs = {'32': {'in_channels': [(base_channels * item) for item in [4, 4, 4]], 'out_channels': [(base_channels * item) for item in [4, 4, 4]], 'downsample': [True, True, False, False], 'resolution': [16, 8, 8, 8], 'attention': [False, False, False, False]}, '64': {'in_channels': [(base_channels * item) for item in [1, 2, 4, 8]], 'out_channels': [(base_channels * item) for item in [2, 4, 8, 16]], 'downsample': [True] * 4 + [False], 'resolution': [32, 16, 8, 4, 4], 'attention': [False, False, False, False, False]}, '128': {'in_channels': [(base_channels * item) for item in [1, 2, 4, 8, 16]], 'out_channels': [(base_channels * item) for item in [2, 4, 8, 16, 16]], 'downsample': [True] * 5 + [False], 'resolution': [64, 32, 16, 8, 4, 4], 'attention': [True, False, False, False, False, False]}, '256': {'in_channels': [(base_channels * item) for item in [1, 2, 4, 8, 8, 16]], 'out_channels': [(base_channels * item) for item in [2, 4, 8, 8, 16, 16]], 'downsample': [True] * 6 + [False], 'resolution': [128, 64, 32, 16, 8, 4, 4], 'attention': [False, True, False, False, False, False]}, '512': {'in_channels': [(base_channels * item) for item in [1, 1, 2, 4, 8, 8, 16]], 'out_channels': [(base_channels * item) for item in [1, 2, 4, 8, 8, 16, 16]], 'downsample': [True] * 7 + [False], 'resolution': [256, 128, 64, 32, 16, 8, 4, 4], 'attention': [False, False, False, True, False, False, False]}}
        return _default_arch_cfgs[str(input_scale)]

    def forward(self, x, label=None):
        """Forward function.

        Args:
            x (torch.Tensor): Fake or real image tensor.
            label (torch.Tensor | None): Label Tensor. Defaults to None.

        Returns:
            torch.Tensor: Prediction for the reality of the input image with
                given label.
        """
        x0 = self.input_conv(x)
        for conv_block in self.conv_blocks:
            x0 = conv_block(x0)
        x0 = self.activate(x0)
        x0 = torch.sum(x0, dim=[2, 3])
        out = self.decision(x0)
        if self.num_classes > 0:
            w_y = self.proj_y(label)
            out = out + torch.sum(w_y * x0, dim=1, keepdim=True)
        return out

    def init_weights(self, pretrained=None, init_type='ortho'):
        """Init weights for models.

        Args:
            pretrained (str | dict, optional): Path for the pretrained model or
                dict containing information for pretained models whose
                necessary key is 'ckpt_path'. Besides, you can also provide
                'prefix' to load the generator part from the whole state dict.
                Defaults to None.
            init_type (str, optional): The name of an initialization method:
                ortho | N02 | xavier. Defaults to 'ortho'.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif isinstance(pretrained, dict):
            ckpt_path = pretrained.get('ckpt_path', None)
            assert ckpt_path is not None
            prefix = pretrained.get('prefix', '')
            map_location = pretrained.get('map_location', 'cpu')
            strict = pretrained.get('strict', True)
            state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
            self.load_state_dict(state_dict, strict=strict)
            mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                    if init_type == 'ortho':
                        nn.init.orthogonal_(m.weight)
                    elif init_type == 'N02':
                        normal_init(m, 0.0, 0.02)
                    elif init_type == 'xavier':
                        xavier_init(m)
                    else:
                        raise NotImplementedError(f'{init_type} initialization                             not supported now.')
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class BigGANConditionBN(nn.Module):
    """Conditional Batch Normalization used in BigGAN.

    Args:
        num_features (int): The channel number of the input feature map tensor.
        linear_input_channels (int): The channel number of the linear layers'
            input tensor.
        bn_eps (float, optional): Epsilon value for batch normalization.
            Defaults to 1e-5.
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        momentum (float, optional): The value used for the running_mean and
            running_var computation. Defaults to 0.1.
        input_is_label (bool, optional): Whether the input of BNs' linear layer
            is raw label instead of class vector. Defaults to False.
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        auto_sync_bn (bool, optional): Whether to use synchronized batch
            normalization. Defaults to True.
    """

    def __init__(self, num_features, linear_input_channels, bn_eps=1e-05, sn_eps=1e-06, sn_style='ajbrock', momentum=0.1, input_is_label=False, with_spectral_norm=True, auto_sync_bn=True):
        super().__init__()
        assert num_features > 0
        if linear_input_channels > 0:
            self.use_cbn = True
        else:
            self.use_cbn = False
        if self.use_cbn:
            if not input_is_label:
                self.gain = nn.Linear(linear_input_channels, num_features, bias=False)
                self.bias = nn.Linear(linear_input_channels, num_features, bias=False)
                if with_spectral_norm:
                    if sn_style == 'torch':
                        self.gain = spectral_norm(self.gain, eps=sn_eps)
                        self.bias = spectral_norm(self.bias, eps=sn_eps)
                    elif sn_style == 'ajbrock':
                        self.gain = SNLinear(linear_input_channels, num_features, bias=False, eps=sn_eps)
                        self.bias = SNLinear(linear_input_channels, num_features, bias=False, eps=sn_eps)
                    else:
                        raise NotImplementedError('sn style')
            else:
                self.gain = nn.Embedding(linear_input_channels, num_features)
                self.bias = nn.Embedding(linear_input_channels, num_features)
        self.bn = nn.BatchNorm2d(num_features, eps=bn_eps, momentum=momentum, affine=not self.use_cbn)
        if auto_sync_bn and dist.is_initialized():
            self.bn = SyncBatchNorm.convert_sync_batchnorm(self.bn)

    def forward(self, x, y):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.
            y (torch.Tensor): Label tensor or class embedding concatenated with
                noise tensor.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        if self.use_cbn:
            gain = (1.0 + self.gain(y)).view(y.size(0), -1, 1, 1)
            bias = self.bias(y).view(y.size(0), -1, 1, 1)
            out = self.bn(x)
            out = out * gain + bias
        else:
            out = self.bn(x)
        return out


class BigGANGenResBlock(nn.Module):
    """Residual block used in BigGAN's generator.

    Args:
        in_channels (int): The channel number of the input feature map.
        out_channels (int): The channel number of the output feature map.
        dim_after_concat (int): The channel number of the noise concatenated
            with the class vector.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU').
        upsample_cfg (dict, optional): Config for the upsampling operation.
            Defaults to dict(type='nearest', scale_factor=2).
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization in this block. Defaults to True.
        input_is_label (bool, optional): Whether the input of BNs' linear layer
            is raw label instead of class vector. Defaults to False.
        auto_sync_bn (bool, optional): Whether to use synchronized batch
            normalization. Defaults to True.
    """

    def __init__(self, in_channels, out_channels, dim_after_concat, act_cfg=dict(type='ReLU'), upsample_cfg=dict(type='nearest', scale_factor=2), sn_eps=1e-06, sn_style='ajbrock', with_spectral_norm=True, input_is_label=False, auto_sync_bn=True):
        super().__init__()
        self.activation = build_activation_layer(act_cfg)
        self.upsample_cfg = deepcopy(upsample_cfg)
        self.with_upsample = upsample_cfg is not None
        if self.with_upsample:
            self.upsample_layer = build_upsample_layer(self.upsample_cfg)
        self.learnable_sc = in_channels != out_channels or self.with_upsample
        if self.learnable_sc:
            self.shortcut = SNConvModule(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.bn1 = BigGANConditionBN(in_channels, dim_after_concat, sn_eps=sn_eps, sn_style=sn_style, input_is_label=input_is_label, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn)
        self.bn2 = BigGANConditionBN(out_channels, dim_after_concat, sn_eps=sn_eps, sn_style=sn_style, input_is_label=input_is_label, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn)
        self.conv1 = SNConvModule(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv2 = SNConvModule(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))

    def forward(self, x, y):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.
            y (torch.Tensor): Label tensor or class embedding concatenated with
                noise tensor.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        x0 = self.bn1(x, y)
        x0 = self.activation(x0)
        if self.with_upsample:
            x0 = self.upsample_layer(x0)
            x = self.upsample_layer(x)
        x0 = self.conv1(x0)
        x0 = self.bn2(x0, y)
        x0 = self.activation(x0)
        x0 = self.conv2(x0)
        if self.learnable_sc:
            x = self.shortcut(x)
        return x0 + x


class BigGANDiscResBlock(nn.Module):
    """Residual block used in BigGAN's discriminator.

    Args:
        in_channels (int): The channel number of the input tensor.
        out_channels (int): The channel number of the output tensor.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU', inplace=False).
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        with_downsample (bool, optional): Whether to use downsampling in this
            block. Defaults to True.
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
        is_head_block (bool, optional): Whether this block is the first block
            of BigGAN. Defaults to False.
    """

    def __init__(self, in_channels, out_channels, act_cfg=dict(type='ReLU', inplace=False), sn_eps=1e-06, sn_style='ajbrock', with_downsample=True, with_spectral_norm=True, is_head_block=False):
        super().__init__()
        self.activation = build_activation_layer(act_cfg)
        self.with_downsample = with_downsample
        self.is_head_block = is_head_block
        if self.with_downsample:
            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2)
        self.learnable_sc = in_channels != out_channels or self.with_downsample
        if self.learnable_sc:
            self.shortcut = SNConvModule(in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv1 = SNConvModule(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv2 = SNConvModule(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))

    def forward_sc(self, x):
        """Forward function of shortcut.

        Args:
            x (torch.Tensor): Input feature map tensor.

        Returns:
            torch.Tensor: Output tensor of shortcut.
        """
        if self.is_head_block:
            if self.with_downsample:
                x = self.downsample(x)
            if self.learnable_sc:
                x = self.shortcut(x)
        else:
            if self.learnable_sc:
                x = self.shortcut(x)
            if self.with_downsample:
                x = self.downsample(x)
        return x

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        if self.is_head_block:
            x0 = x
        else:
            x0 = self.activation(x)
        x0 = self.conv1(x0)
        x0 = self.activation(x0)
        x0 = self.conv2(x0)
        if self.with_downsample:
            x0 = self.downsample(x0)
        x1 = self.forward_sc(x)
        return x0 + x1


class BigGANDeepGenResBlock(nn.Module):
    """Residual block used in BigGAN-Deep's generator.

    Args:
        in_channels (int): The channel number of the input feature map.
        out_channels (int): The channel number of the output feature map.
        dim_after_concat (int): The channel number of the noise concatenated
            with the class vector.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU').
        upsample_cfg (dict, optional): Config for the upsampling operation.
            Defaults to dict(type='nearest', scale_factor=2).
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        bn_eps (float, optional): Epsilon value for batch normalization.
            Defaults to 1e-5.
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization in this block. Defaults to True.
        input_is_label (bool, optional): Whether the input of BNs' linear layer
            is raw label instead of class vector. Defaults to False.
        auto_sync_bn (bool, optional): Whether to use synchronized batch
            normalization. Defaults to True.
        channel_ratio (int, optional): The ratio of the input channels' number
            to the hidden channels' number. Defaults to 4.
    """

    def __init__(self, in_channels, out_channels, dim_after_concat, act_cfg=dict(type='ReLU'), upsample_cfg=dict(type='nearest', scale_factor=2), sn_eps=1e-06, sn_style='ajbrock', bn_eps=1e-05, with_spectral_norm=True, input_is_label=False, auto_sync_bn=True, channel_ratio=4):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.hidden_channels = self.in_channels // channel_ratio
        self.activation = build_activation_layer(act_cfg)
        self.upsample_cfg = deepcopy(upsample_cfg)
        self.with_upsample = upsample_cfg is not None
        if self.with_upsample:
            self.upsample_layer = build_upsample_layer(self.upsample_cfg)
        self.bn1 = BigGANConditionBN(in_channels, dim_after_concat, sn_eps=sn_eps, sn_style=sn_style, bn_eps=bn_eps, input_is_label=input_is_label, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn)
        self.bn2 = BigGANConditionBN(self.hidden_channels, dim_after_concat, sn_eps=sn_eps, sn_style=sn_style, bn_eps=bn_eps, input_is_label=input_is_label, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn)
        self.bn3 = BigGANConditionBN(self.hidden_channels, dim_after_concat, sn_eps=sn_eps, sn_style=sn_style, bn_eps=bn_eps, input_is_label=input_is_label, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn)
        self.bn4 = BigGANConditionBN(self.hidden_channels, dim_after_concat, sn_eps=sn_eps, sn_style=sn_style, bn_eps=bn_eps, input_is_label=input_is_label, with_spectral_norm=with_spectral_norm, auto_sync_bn=auto_sync_bn)
        self.conv1 = SNConvModule(in_channels=in_channels, out_channels=self.hidden_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv2 = SNConvModule(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=3, stride=1, padding=1, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv3 = SNConvModule(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=3, stride=1, padding=1, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv4 = SNConvModule(in_channels=self.hidden_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))

    def forward(self, x, y):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.
            y (torch.Tensor): Label tensor or class embedding concatenated with
                noise tensor.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        x0 = self.bn1(x, y)
        x0 = self.activation(x0)
        x0 = self.conv1(x0)
        x0 = self.bn2(x0, y)
        x0 = self.activation(x0)
        if self.in_channels != self.out_channels:
            x = x[:, :self.out_channels]
        if self.with_upsample:
            x0 = self.upsample_layer(x0)
            x = self.upsample_layer(x)
        x0 = self.conv2(x0)
        x0 = self.bn3(x0, y)
        x0 = self.activation(x0)
        x0 = self.conv3(x0)
        x0 = self.bn4(x0, y)
        x0 = self.activation(x0)
        x0 = self.conv4(x0)
        return x0 + x


class BigGANDeepDiscResBlock(nn.Module):
    """Residual block used in BigGAN-Deep's discriminator.

    Args:
        in_channels (int): The channel number of the input tensor.
        out_channels (int): The channel number of the output tensor.
        channel_ratio (int, optional): The ratio of the input channels' number
            to the hidden channels' number. Defaults to 4.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU', inplace=False).
        sn_eps (float, optional): Epsilon value for spectral normalization.
            Defaults to 1e-6.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `ajbrock`.
        with_downsample (bool, optional): Whether to use downsampling in this
            block. Defaults to True.
        with_spectral_norm (bool, optional): Whether to use spectral
            normalization. Defaults to True.
    """

    def __init__(self, in_channels, out_channels, channel_ratio=4, act_cfg=dict(type='ReLU', inplace=False), sn_eps=1e-06, sn_style='ajbrock', with_downsample=True, with_spectral_norm=True):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.hidden_channels = self.out_channels // channel_ratio
        self.activation = build_activation_layer(act_cfg)
        self.with_downsample = with_downsample
        if self.with_downsample:
            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2)
        self.learnable_sc = in_channels != out_channels
        if self.learnable_sc:
            self.shortcut = SNConvModule(in_channels=in_channels, out_channels=out_channels - in_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))
        self.conv1 = SNConvModule(in_channels=in_channels, out_channels=self.hidden_channels, kernel_size=1, stride=1, padding=0, act_cfg=act_cfg, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style), order=('act', 'conv', 'norm'))
        self.conv2 = SNConvModule(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=3, stride=1, padding=1, act_cfg=act_cfg, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style), order=('act', 'conv', 'norm'))
        self.conv3 = SNConvModule(in_channels=self.hidden_channels, out_channels=self.hidden_channels, kernel_size=3, stride=1, padding=1, act_cfg=act_cfg, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style), order=('act', 'conv', 'norm'))
        self.conv4 = SNConvModule(in_channels=self.hidden_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=dict(eps=sn_eps, sn_style=sn_style))

    def forward_sc(self, x):
        """Forward function of shortcut.

        Args:
            x (torch.Tensor): Input feature map tensor.

        Returns:
            torch.Tensor: Output tensor of shortcut.
        """
        if self.with_downsample:
            x = self.downsample(x)
        if self.learnable_sc:
            x0 = self.shortcut(x)
            x = torch.cat([x, x0], dim=1)
        return x

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        x0 = self.conv1(x)
        x0 = self.conv2(x0)
        x0 = self.conv3(x0)
        x0 = self.activation(x0)
        if self.with_downsample:
            x0 = self.downsample(x0)
        x0 = self.conv4(x0)
        x1 = self.forward_sc(x)
        return x0 + x1


class ResidualBlockWithDropout(nn.Module):
    """Define a Residual Block with dropout layers.

    Ref:
    Deep Residual Learning for Image Recognition

    A residual block is a conv block with skip connections. A dropout layer is
    added between two common conv modules.

    Args:
        channels (int): Number of channels in the conv layer.
        padding_mode (str): The name of padding layer:
            'reflect' | 'replicate' | 'zeros'.
        norm_cfg (dict): Config dict to build norm layer. Default:
            `dict(type='IN')`.
        use_dropout (bool): Whether to use dropout layers. Default: True.
    """

    def __init__(self, channels, padding_mode, norm_cfg=dict(type='BN'), use_dropout=True):
        super().__init__()
        assert isinstance(norm_cfg, dict), f"'norm_cfg' should be dict, butgot {type(norm_cfg)}"
        assert 'type' in norm_cfg, "'norm_cfg' must have key 'type'"
        use_bias = norm_cfg['type'] == 'IN'
        block = [ConvModule(in_channels=channels, out_channels=channels, kernel_size=3, padding=1, bias=use_bias, norm_cfg=norm_cfg, padding_mode=padding_mode)]
        if use_dropout:
            block += [nn.Dropout(0.5)]
        block += [ConvModule(in_channels=channels, out_channels=channels, kernel_size=3, padding=1, bias=use_bias, norm_cfg=norm_cfg, act_cfg=None, padding_mode=padding_mode)]
        self.block = nn.Sequential(*block)

    def forward(self, x):
        """Forward function. Add skip connections without final ReLU.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        out = x + self.block(x)
        return out


def generation_init_weights(module, init_type='normal', init_gain=0.02):
    """Default initialization of network weights for image generation.

    By default, we use normal init, but xavier and kaiming might work
    better for some applications.

    Args:
        module (nn.Module): Module to be initialized.
        init_type (str): The name of an initialization method:
            normal | xavier | kaiming | orthogonal.
        init_gain (float): Scaling factor for normal, xavier and
            orthogonal.
    """

    def init_func(m):
        """Initialization function.

        Args:
            m (nn.Module): Module to be initialized.
        """
        classname = m.__class__.__name__
        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):
            if init_type == 'normal':
                normal_init(m, 0.0, init_gain)
            elif init_type == 'xavier':
                xavier_init(m, gain=init_gain, distribution='normal')
            elif init_type == 'kaiming':
                kaiming_init(m, a=0, mode='fan_in', nonlinearity='leaky_relu', distribution='normal')
            elif init_type == 'orthogonal':
                init.orthogonal_(m.weight, gain=init_gain)
                init.constant_(m.bias.data, 0.0)
            else:
                raise NotImplementedError(f"Initialization method '{init_type}' is not implemented")
        elif classname.find('BatchNorm2d') != -1:
            normal_init(m, 1.0, init_gain)
    module.apply(init_func)


class ResnetGenerator(nn.Module):
    """Construct a Resnet-based generator that consists of residual blocks
    between a few downsampling/upsampling operations.

    Args:
        in_channels (int): Number of channels in input images.
        out_channels (int): Number of channels in output images.
        base_channels (int): Number of filters at the last conv layer.
            Default: 64.
        norm_cfg (dict): Config dict to build norm layer. Default:
            `dict(type='IN')`.
        use_dropout (bool): Whether to use dropout layers. Default: False.
        num_blocks (int): Number of residual blocks. Default: 9.
        padding_mode (str): The name of padding layer in conv layers:
            'reflect' | 'replicate' | 'zeros'. Default: 'reflect'.
        init_cfg (dict): Config dict for initialization.
            `type`: The name of our initialization method. Default: 'normal'.
            `gain`: Scaling factor for normal, xavier and orthogonal.
            Default: 0.02.
    """

    def __init__(self, in_channels, out_channels, base_channels=64, norm_cfg=dict(type='IN'), use_dropout=False, num_blocks=9, padding_mode='reflect', init_cfg=dict(type='normal', gain=0.02)):
        super().__init__()
        assert num_blocks >= 0, f'Number of residual blocks must be non-negative, but got {num_blocks}.'
        assert isinstance(norm_cfg, dict), f"'norm_cfg' should be dict, butgot {type(norm_cfg)}"
        assert 'type' in norm_cfg, "'norm_cfg' must have key 'type'"
        use_bias = norm_cfg['type'] == 'IN'
        model = []
        model += [ConvModule(in_channels=in_channels, out_channels=base_channels, kernel_size=7, padding=3, bias=use_bias, norm_cfg=norm_cfg, padding_mode=padding_mode)]
        num_down = 2
        for i in range(num_down):
            multiple = 2 ** i
            model += [ConvModule(in_channels=base_channels * multiple, out_channels=base_channels * multiple * 2, kernel_size=3, stride=2, padding=1, bias=use_bias, norm_cfg=norm_cfg)]
        multiple = 2 ** num_down
        for i in range(num_blocks):
            model += [ResidualBlockWithDropout(base_channels * multiple, padding_mode=padding_mode, norm_cfg=norm_cfg, use_dropout=use_dropout)]
        for i in range(num_down):
            multiple = 2 ** (num_down - i)
            model += [ConvModule(in_channels=base_channels * multiple, out_channels=base_channels * multiple // 2, kernel_size=3, stride=2, padding=1, bias=use_bias, conv_cfg=dict(type='deconv', output_padding=1), norm_cfg=norm_cfg)]
        model += [ConvModule(in_channels=base_channels, out_channels=out_channels, kernel_size=7, padding=3, bias=True, norm_cfg=None, act_cfg=dict(type='Tanh'), padding_mode=padding_mode)]
        self.model = nn.Sequential(*model)
        self.init_type = 'normal' if init_cfg is None else init_cfg.get('type', 'normal')
        self.init_gain = 0.02 if init_cfg is None else init_cfg.get('gain', 0.02)

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        return self.model(x)

    def init_weights(self, pretrained=None, strict=True):
        """Initialize weights for the model.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Default: None.
            strict (bool, optional): Whether to allow different params for the
                model and checkpoint. Default: True.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=strict, logger=logger)
        elif pretrained is None:
            generation_init_weights(self, init_type=self.init_type, init_gain=self.init_gain)
        else:
            raise TypeError(f"'pretrained' must be a str or None. But received {type(pretrained)}.")


class DCGANGenerator(nn.Module):
    """Generator for DCGAN.

    Implementation Details for DCGAN architecture:

    #. Adopt transposed convolution in the generator;
    #. Use batchnorm in the generator except for the final output layer;
    #. Use ReLU in the generator in addition to the final output layer.

    More details can be found in the original paper:
    Unsupervised Representation Learning with Deep Convolutional
    Generative Adversarial Networks
    http://arxiv.org/abs/1511.06434

    Args:
        output_scale (int | tuple[int]): Output scale for the generated
            image. If only a integer is provided, the output image will
            be a square shape. The tuple of two integers will set the
            height and width for the output image, respectively.
        out_channels (int, optional): The channel number of the output feature.
            Default to 3.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Default to 1024.
        input_scale (int | tuple[int], optional): Output scale for the
            generated image. If only a integer is provided, the input feature
            ahead of the convolutional generator will be a square shape. The
            tuple of two integers will set the height and width for the input
            convolutional feature, respectively. Defaults to 4.
        noise_size (int, optional): Size of the input noise
            vector. Defaults to 100.
        default_norm_cfg (dict, optional): Norm config for all of layers
            except for the final output layer. Defaults to ``dict(type='BN')``.
        default_act_cfg (dict, optional): Activation config for all of layers
            except for the final output layer. Defaults to
            ``dict(type='ReLU')``.
        out_act_cfg (dict, optional): Activation config for the final output
            layer. Defaults to ``dict(type='Tanh')``.
        pretrained (str, optional): Path for the pretrained model. Default to
            ``None``.
    """

    def __init__(self, output_scale, out_channels=3, base_channels=1024, input_scale=4, noise_size=100, default_norm_cfg=dict(type='BN'), default_act_cfg=dict(type='ReLU'), out_act_cfg=dict(type='Tanh'), pretrained=None):
        super().__init__()
        self.output_scale = output_scale
        self.base_channels = base_channels
        self.input_scale = input_scale
        self.noise_size = noise_size
        self.num_upsamples = int(np.log2(output_scale // input_scale))
        self.noise2feat = ConvModule(noise_size, base_channels, kernel_size=4, stride=1, padding=0, conv_cfg=dict(type='ConvTranspose2d'), norm_cfg=default_norm_cfg, act_cfg=default_act_cfg)
        upsampling = []
        curr_channel = base_channels
        for _ in range(self.num_upsamples - 1):
            upsampling.append(ConvModule(curr_channel, curr_channel // 2, kernel_size=4, stride=2, padding=1, conv_cfg=dict(type='ConvTranspose2d'), norm_cfg=default_norm_cfg, act_cfg=default_act_cfg))
            curr_channel //= 2
        self.upsampling = nn.Sequential(*upsampling)
        self.output_layer = ConvModule(curr_channel, out_channels, kernel_size=4, stride=2, padding=1, conv_cfg=dict(type='ConvTranspose2d'), norm_cfg=None, act_cfg=out_act_cfg)
        self.init_weights(pretrained=pretrained)

    def forward(self, noise, num_batches=0, return_noise=False):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output image
                will be returned. Otherwise, a dict contains ``fake_img`` and
                ``noise_batch`` will be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            if noise.ndim == 2:
                noise_batch = noise[:, :, None, None]
            elif noise.ndim == 4:
                noise_batch = noise
            else:
                raise ValueError(f'The noise should be in shape of (n, c) or (n, c, 1, 1), but got {noise.shape}')
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size, 1, 1))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size, 1, 1))
        noise_batch = noise_batch
        x = self.noise2feat(noise_batch)
        x = self.upsampling(x)
        x = self.output_layer(x)
        if return_noise:
            return dict(fake_img=x, noise_batch=noise_batch)
        return x

    def init_weights(self, pretrained=None):
        """Init weights for models.

        We just use the initialization method proposed in the original paper.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Defaults to None.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                    normal_init(m, 0, 0.02)
                elif isinstance(m, _BatchNorm):
                    nn.init.normal_(m.weight.data)
                    nn.init.constant_(m.bias.data, 0)
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class DCGANDiscriminator(nn.Module):
    """Discriminator for DCGAN.

    Implementation Details for DCGAN architecture:

    #. Adopt convolution in the discriminator;
    #. Use batchnorm in the discriminator except for the input and final        output layer;
    #. Use LeakyReLU in the discriminator in addition to the output layer.

    Args:
        input_scale (int): The scale of the input image.
        output_scale (int): The final scale of the convolutional feature.
        out_channels (int): The channel number of the final output layer.
        in_channels (int, optional): The channel number of the input image.
            Defaults to 3.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Defaults to 128.
        default_norm_cfg (dict, optional): Norm config for all of layers
            except for the final output layer. Defaults to ``dict(type='BN')``.
        default_act_cfg (dict, optional): Activation config for all of layers
            except for the final output layer. Defaults to
            ``dict(type='ReLU')``.
        out_act_cfg (dict, optional): Activation config for the final output
            layer. Defaults to ``dict(type='Tanh')``.
        pretrained (str, optional): Path for the pretrained model. Default to
            ``None``.
    """

    def __init__(self, input_scale, output_scale, out_channels, in_channels=3, base_channels=128, default_norm_cfg=dict(type='BN'), default_act_cfg=dict(type='LeakyReLU'), out_act_cfg=None, pretrained=None):
        super().__init__()
        self.input_scale = input_scale
        self.output_scale = output_scale
        self.out_channels = out_channels
        self.base_channels = base_channels
        self.num_downsamples = int(np.log2(input_scale // output_scale))
        downsamples = []
        for i in range(self.num_downsamples):
            norm_cfg_ = None if i == 0 else default_norm_cfg
            in_ch = in_channels if i == 0 else base_channels * 2 ** (i - 1)
            downsamples.append(ConvModule(in_ch, base_channels * 2 ** i, kernel_size=4, stride=2, padding=1, conv_cfg=dict(type='Conv2d'), norm_cfg=norm_cfg_, act_cfg=default_act_cfg))
            curr_channels = base_channels * 2 ** i
        self.downsamples = nn.Sequential(*downsamples)
        self.output_layer = ConvModule(curr_channels, out_channels, kernel_size=4, stride=1, padding=0, conv_cfg=dict(type='Conv2d'), norm_cfg=None, act_cfg=out_act_cfg)
        self.init_weights(pretrained=pretrained)

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Fake or real image tensor.

        Returns:
            torch.Tensor: Prediction for the reality of the input image.
        """
        n = x.shape[0]
        x = self.downsamples(x)
        x = self.output_layer(x)
        return x.view(n, -1)

    def init_weights(self, pretrained=None):
        """Init weights for models.

        We just use the initialization method proposed in the original paper.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Defaults to None.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):
                    normal_init(m, 0, 0.02)
                elif isinstance(m, _BatchNorm):
                    nn.init.normal_(m.weight.data)
                    nn.init.constant_(m.bias.data, 0)
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class DenoisingResBlock(nn.Module):
    """Resblock for the denoising network. If `in_channels` not equals to
    `out_channels`, a learnable shortcut with conv layers will be added.

    Args:
        in_channels (int): Number of channels of the input feature map.
        embedding_channels (int): Number of channels of the input embedding.
        use_scale_shift_norm (bool): Whether use scale-shift-norm in
            `NormWithEmbedding` layer.
        dropout (float): Probability of the dropout layers.
        out_channels (int, optional): Number of output channels of the
            ResBlock. If not defined, the output channels will equal to the
            `in_channels`. Defaults to `None`.
        norm_cfg (dict, optional): The config for the normalization layers.
            Defaults too ``dict(type='GN', num_groups=32)``.
        act_cfg (dict, optional): The config for the activation layers.
            Defaults to ``dict(type='SiLU', inplace=False)``.
        shortcut_kernel_size (int, optional): The kernel size for the shortcut
            conv. Defaults to ``1``.
    """

    def __init__(self, in_channels, embedding_channels, use_scale_shift_norm, dropout, out_channels=None, norm_cfg=dict(type='GN', num_groups=32), act_cfg=dict(type='SiLU', inplace=False), shortcut_kernel_size=1):
        super().__init__()
        out_channels = in_channels if out_channels is None else out_channels
        _norm_cfg = deepcopy(norm_cfg)
        _, norm_1 = build_norm_layer(_norm_cfg, in_channels)
        conv_1 = [norm_1, build_activation_layer(act_cfg), nn.Conv2d(in_channels, out_channels, 3, padding=1)]
        self.conv_1 = nn.Sequential(*conv_1)
        norm_with_embedding_cfg = dict(in_channels=out_channels, embedding_channels=embedding_channels, use_scale_shift=use_scale_shift_norm, norm_cfg=_norm_cfg)
        self.norm_with_embedding = build_module(dict(type='NormWithEmbedding'), default_args=norm_with_embedding_cfg)
        conv_2 = [build_activation_layer(act_cfg), nn.Dropout(dropout), nn.Conv2d(out_channels, out_channels, 3, padding=1)]
        self.conv_2 = nn.Sequential(*conv_2)
        assert shortcut_kernel_size in [1, 3], f'Only support `1` and `3` for `shortcut_kernel_size`, but receive {shortcut_kernel_size}.'
        self.learnable_shortcut = out_channels != in_channels
        if self.learnable_shortcut:
            shortcut_padding = 1 if shortcut_kernel_size == 3 else 0
            self.shortcut = nn.Conv2d(in_channels, out_channels, shortcut_kernel_size, padding=shortcut_padding)
        self.init_weights()

    def forward_shortcut(self, x):
        if self.learnable_shortcut:
            return self.shortcut(x)
        return x

    def forward(self, x, y):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.
            y (torch.Tensor): Shared time embedding or shared label embedding.

        Returns:
            torch.Tensor : Output feature map tensor.
        """
        shortcut = self.forward_shortcut(x)
        x = self.conv_1(x)
        x = self.norm_with_embedding(x, y)
        x = self.conv_2(x)
        return x + shortcut

    def init_weights(self):
        constant_init(self.conv_2[-1], 0)


class EmbedSequential(nn.Sequential):
    """A sequential module that passes timestep embeddings to the children that
    support it as an extra input.

    Modified from
    https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/unet.py#L35
    """

    def forward(self, x, y):
        for layer in self:
            if isinstance(layer, DenoisingResBlock):
                x = layer(x, y)
            else:
                x = layer(x)
        return x


class TimeEmbedding(nn.Module):
    """Time embedding layer, reference to Two level embedding. First embedding
    time by an embedding function, then feed to neural networks.

    Args:
        in_channels (int): The channel number of the input feature map.
        embedding_channels (int): The channel number of the output embedding.
        embedding_mode (str, optional): Embedding mode for the time embedding.
            Defaults to 'sin'.
        embedding_cfg (dict, optional): Config for time embedding.
            Defaults to None.
        act_cfg (dict, optional): Config for activation layer. Defaults to
            ``dict(type='SiLU', inplace=False)``.
    """

    def __init__(self, in_channels, embedding_channels, embedding_mode='sin', embedding_cfg=None, act_cfg=dict(type='SiLU', inplace=False)):
        super().__init__()
        self.blocks = nn.Sequential(nn.Linear(in_channels, embedding_channels), build_activation_layer(act_cfg), nn.Linear(embedding_channels, embedding_channels))
        embedding_cfg_ = dict(dim=in_channels)
        if embedding_cfg is not None:
            embedding_cfg_.update(embedding_cfg)
        if embedding_mode.upper() == 'SIN':
            self.embedding_fn = partial(self.sinusodial_embedding, **embedding_cfg_)
        else:
            raise ValueError(f'Only support `SIN` for time embedding, but receive {embedding_mode}.')

    @staticmethod
    def sinusodial_embedding(timesteps, dim, max_period=10000):
        """Create sinusoidal timestep embeddings.

        Args:
            timesteps (torch.Tensor): Timestep to embedding. 1-D tensor shape
                as ``[bz, ]``,  one per batch element.
            dim (int): The dimension of the embedding.
            max_period (int, optional): Controls the minimum frequency of the
                embeddings. Defaults to ``10000``.

        Returns:
            torch.Tensor: Embedding results shape as `[bz, dim]`.
        """
        half = dim // 2
        freqs = torch.exp(-np.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half)
        args = timesteps[:, None].float() * freqs[None]
        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
        if dim % 2:
            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
        return embedding

    def forward(self, t):
        """Forward function for time embedding layer.
        Args:
            t (torch.Tensor): Input timesteps.

        Returns:
            torch.Tensor: Timesteps embedding.

        """
        return self.blocks(self.embedding_fn(t))


class DenoisingUnet(nn.Module):
    """Denoising Unet. This network receives a diffused image ``x_t`` and
    current timestep ``t``, and returns a ``output_dict`` corresponding to the
    passed ``output_cfg``.

    ``output_cfg`` defines the number of channels and the meaning of the
    output. ``output_cfg`` mainly contains keys of ``mean`` and ``var``,
    denoting how the network outputs mean and variance required for the
    denoising process.
    For ``mean``:
    1. ``dict(mean='EPS')``: Model will predict noise added in the
        diffusion process, and the ``output_dict`` will contain a key named
        ``eps_t_pred``.
    2. ``dict(mean='START_X')``: Model will direct predict the mean of the
        original image `x_0`, and the ``output_dict`` will contain a key named
        ``x_0_pred``.
    3. ``dict(mean='X_TM1_PRED')``: Model will predict the mean of diffused
        image at `t-1` timestep, and the ``output_dict`` will contain a key
        named ``x_tm1_pred``.

    For ``var``:
    1. ``dict(var='FIXED_SMALL')`` or ``dict(var='FIXED_LARGE')``: Variance in
        the denoising process is regarded as a fixed value. Therefore only
        'mean' will be predicted, and the output channels will equal to the
        input image (e.g., three channels for RGB image.)
    2. ``dict(var='LEARNED')``: Model will predict `log_variance` in the
        denoising process, and the ``output_dict`` will contain a key named
        ``log_var``.
    3. ``dict(var='LEARNED_RANGE')``: Model will predict an interpolation
        factor and the `log_variance` will be calculated as
        `factor * upper_bound + (1-factor) * lower_bound`. The ``output_dict``
        will contain a key named ``factor``.

    If ``var`` is not ``FIXED_SMALL`` or ``FIXED_LARGE``, the number of output
    channels will be the double of input channels, where the first half part
    contains predicted mean values and the other part is the predicted
    variance values. Otherwise, the number of output channels equals to the
    input channels, only containing the predicted mean values.

    Args:
        image_size (int | list[int]): The size of image to denoise.
        in_channels (int, optional): The input channels of the input image.
            Defaults as ``3``.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contain channels based on this number.
            Defaults to ``128``.
        resblocks_per_downsample (int, optional): Number of ResBlock used
            between two downsample operations. The number of ResBlock between
            upsample operations will be the same value to keep symmetry.
            Defaults to 3.
        num_timesteps (int, optional): The total timestep of the denoising
            process and the diffusion process. Defaults to ``1000``.
        use_rescale_timesteps (bool, optional): Whether rescale the input
            timesteps in range of [0, 1000].  Defaults to ``True``.
        dropout (float, optional): The probability of dropout operation of
            each ResBlock. Pass ``0`` to do not use dropout. Defaults as 0.
        embedding_channels (int, optional): The output channels of time
            embedding layer and label embedding layer. If not passed (or
            passed ``-1``), output channels of the embedding layers will set
            as four times of ``base_channels``. Defaults to ``-1``.
        num_classes (int, optional): The number of conditional classes. If set
            to 0, this model will be degraded to an unconditional model.
            Defaults to 0.
        channels_cfg (list | dict[list], optional): Config for input channels
            of the intermedia blocks. If list is passed, each element of the
            list indicates the scale factor for the input channels of the
            current block with regard to the ``base_channels``. For block
            ``i``, the input and output channels should be
            ``channels_cfg[i] * base_channels`` and
            ``channels_cfg[i+1] * base_channels`` If dict is provided, the key
            of the dict should be the output scale and corresponding value
            should be a list to define channels. Default: Please refer to
            ``_defualt_channels_cfg``.
        output_cfg (dict, optional): Config for output variables. Defaults to
            ``dict(mean='eps', var='learned_range')``.
        norm_cfg (dict, optional): The config for normalization layers.
            Defaults to ``dict(type='GN', num_groups=32)``.
        act_cfg (dict, optional): The config for activation layers. Defaults
            to ``dict(type='SiLU', inplace=False)``.
        shortcut_kernel_size (int, optional): The kernel size for shortcut
            conv in ResBlocks. The value of this argument will overwrite the
            default value of `resblock_cfg`. Defaults to `3`.
        use_scale_shift_norm (bool, optional): Whether perform scale and shift
            after normalization operation. Defaults to True.
        num_heads (int, optional): The number of attention heads. Defaults to
            4.
        time_embedding_mode (str, optional): Embedding method of
            ``time_embedding``. Defaults to 'sin'.
        time_embedding_cfg (dict, optional): Config for ``time_embedding``.
            Defaults to None.
        resblock_cfg (dict, optional): Config for ResBlock. Defaults to
            ``dict(type='DenoisingResBlock')``.
        attention_cfg (dict, optional): Config for attention operation.
            Defaults to ``dict(type='MultiHeadAttention')``.
        upsample_conv (bool, optional): Whether use conv in upsample block.
            Defaults to ``True``.
        downsample_conv (bool, optional): Whether use conv operation in
            downsample block.  Defaults to ``True``.
        upsample_cfg (dict, optional): Config for upsample blocks.
            Defaults to ``dict(type='DenoisingUpsample')``.
        downsample_cfg (dict, optional): Config for downsample blocks.
            Defaults to ``dict(type='DenoisingDownsample')``.
        attention_res (int | list[int], optional): Resolution of feature maps
            to apply attention operation. Defaults to ``[16, 8]``.
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict.  Defaults to None.
    """
    _default_channels_cfg = {(256): [1, 1, 2, 2, 4, 4], (64): [1, 2, 3, 4], (32): [1, 2, 2, 2]}

    def __init__(self, image_size, in_channels=3, base_channels=128, resblocks_per_downsample=3, num_timesteps=1000, use_rescale_timesteps=True, dropout=0, embedding_channels=-1, num_classes=0, channels_cfg=None, output_cfg=dict(mean='eps', var='learned_range'), norm_cfg=dict(type='GN', num_groups=32), act_cfg=dict(type='SiLU', inplace=False), shortcut_kernel_size=1, use_scale_shift_norm=False, num_heads=4, time_embedding_mode='sin', time_embedding_cfg=None, resblock_cfg=dict(type='DenoisingResBlock'), attention_cfg=dict(type='MultiHeadAttention'), downsample_conv=True, upsample_conv=True, downsample_cfg=dict(type='DenoisingDownsample'), upsample_cfg=dict(type='DenoisingUpsample'), attention_res=[16, 8], pretrained=None):
        super().__init__()
        self.num_classes = num_classes
        self.num_timesteps = num_timesteps
        self.use_rescale_timesteps = use_rescale_timesteps
        self.output_cfg = deepcopy(output_cfg)
        self.mean_mode = self.output_cfg.get('mean', 'eps')
        self.var_mode = self.output_cfg.get('var', 'learned_range')
        out_channels = in_channels if 'FIXED' in self.var_mode.upper() else 2 * in_channels
        self.out_channels = out_channels
        if not isinstance(image_size, int) and not isinstance(image_size, list):
            raise TypeError('Only support `int` and `list[int]` for `image_size`.')
        if isinstance(image_size, list):
            assert len(image_size) == 2, 'The length of `image_size` should be 2.'
            assert image_size[0] == image_size[1], 'Width and height of the image should be same.'
            image_size = image_size[0]
        self.image_size = image_size
        channels_cfg = deepcopy(self._default_channels_cfg) if channels_cfg is None else deepcopy(channels_cfg)
        if isinstance(channels_cfg, dict):
            if image_size not in channels_cfg:
                raise KeyError(f'`image_size={image_size} is not found in `channels_cfg`, only support configs for {[chn for chn in channels_cfg.keys()]}')
            self.channel_factor_list = channels_cfg[image_size]
        elif isinstance(channels_cfg, list):
            self.channel_factor_list = channels_cfg
        else:
            raise ValueError(f'Only support list or dict for `channels_cfg`, receive {type(channels_cfg)}')
        embedding_channels = base_channels * 4 if embedding_channels == -1 else embedding_channels
        self.time_embedding = TimeEmbedding(base_channels, embedding_channels=embedding_channels, embedding_mode=time_embedding_mode, embedding_cfg=time_embedding_cfg, act_cfg=act_cfg)
        if self.num_classes != 0:
            self.label_embedding = nn.Embedding(self.num_classes, embedding_channels)
        self.resblock_cfg = deepcopy(resblock_cfg)
        self.resblock_cfg.setdefault('dropout', dropout)
        self.resblock_cfg.setdefault('norm_cfg', norm_cfg)
        self.resblock_cfg.setdefault('act_cfg', act_cfg)
        self.resblock_cfg.setdefault('embedding_channels', embedding_channels)
        self.resblock_cfg.setdefault('use_scale_shift_norm', use_scale_shift_norm)
        self.resblock_cfg.setdefault('shortcut_kernel_size', shortcut_kernel_size)
        attention_scale = [(image_size // int(res)) for res in attention_res]
        self.attention_cfg = deepcopy(attention_cfg)
        self.attention_cfg.setdefault('num_heads', num_heads)
        self.attention_cfg.setdefault('norm_cfg', norm_cfg)
        self.downsample_cfg = deepcopy(downsample_cfg)
        self.downsample_cfg.setdefault('with_conv', downsample_conv)
        self.upsample_cfg = deepcopy(upsample_cfg)
        self.upsample_cfg.setdefault('with_conv', upsample_conv)
        scale = 1
        self.in_blocks = nn.ModuleList([EmbedSequential(nn.Conv2d(in_channels, base_channels, 3, 1, padding=1))])
        self.in_channels_list = [base_channels]
        for level, factor in enumerate(self.channel_factor_list):
            in_channels_ = base_channels if level == 0 else base_channels * self.channel_factor_list[level - 1]
            out_channels_ = base_channels * factor
            for _ in range(resblocks_per_downsample):
                layers = [build_module(self.resblock_cfg, {'in_channels': in_channels_, 'out_channels': out_channels_})]
                in_channels_ = out_channels_
                if scale in attention_scale:
                    layers.append(build_module(self.attention_cfg, {'in_channels': in_channels_}))
                self.in_channels_list.append(in_channels_)
                self.in_blocks.append(EmbedSequential(*layers))
            if level != len(self.channel_factor_list) - 1:
                self.in_blocks.append(EmbedSequential(build_module(self.downsample_cfg, {'in_channels': in_channels_})))
                self.in_channels_list.append(in_channels_)
                scale *= 2
        self.mid_blocks = EmbedSequential(build_module(self.resblock_cfg, {'in_channels': in_channels_}), build_module(self.attention_cfg, {'in_channels': in_channels_}), build_module(self.resblock_cfg, {'in_channels': in_channels_}))
        in_channels_list = deepcopy(self.in_channels_list)
        self.out_blocks = nn.ModuleList()
        for level, factor in enumerate(self.channel_factor_list[::-1]):
            for idx in range(resblocks_per_downsample + 1):
                layers = [build_module(self.resblock_cfg, {'in_channels': in_channels_ + in_channels_list.pop(), 'out_channels': base_channels * factor})]
                in_channels_ = base_channels * factor
                if scale in attention_scale:
                    layers.append(build_module(self.attention_cfg, {'in_channels': in_channels_}))
                if level != len(self.channel_factor_list) - 1 and idx == resblocks_per_downsample:
                    layers.append(build_module(self.upsample_cfg, {'in_channels': in_channels_}))
                    scale //= 2
                self.out_blocks.append(EmbedSequential(*layers))
        self.out = ConvModule(in_channels=in_channels_, out_channels=out_channels, kernel_size=3, padding=1, act_cfg=act_cfg, norm_cfg=norm_cfg, bias=True, order=('norm', 'act', 'conv'))
        self.init_weights(pretrained)

    def forward(self, x_t, t, label=None, return_noise=False):
        """Forward function.
        Args:
            x_t (torch.Tensor): Diffused image at timestep `t` to denoise.
            t (torch.Tensor): Current timestep.
            label (torch.Tensor | callable | None): You can directly give a
                batch of label through a ``torch.Tensor`` or offer a callable
                function to sample a batch of label data. Otherwise, the
                ``None`` indicates to use the default label sampler.
            return_noise (bool, optional): If True, inputted ``x_t`` and ``t``
                will be returned in a dict with output desired by
                ``output_cfg``. Defaults to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``
        """
        if self.use_rescale_timesteps:
            t = t.float() * (1000.0 / self.num_timesteps)
        embedding = self.time_embedding(t)
        if label is not None:
            assert hasattr(self, 'label_embedding')
            embedding = self.label_embedding(label) + embedding
        h, hs = x_t, []
        for block in self.in_blocks:
            h = block(h, embedding)
            hs.append(h)
        h = self.mid_blocks(h, embedding)
        for block in self.out_blocks:
            h = block(torch.cat([h, hs.pop()], dim=1), embedding)
        outputs = self.out(h)
        output_dict = dict()
        if 'FIXED' not in self.var_mode.upper():
            mean, var = outputs.split(self.out_channels // 2, dim=1)
            if self.var_mode.upper() == 'LEARNED_RANGE':
                output_dict['factor'] = (var + 1) / 2
            elif self.var_mode.upper() == 'LEARNED':
                output_dict['logvar'] = var
            else:
                raise AttributeError(f"Only support 'FIXED', 'LEARNED_RANGE' and 'LEARNED' for variance output format. But receive '{self.var_mode}'.")
        else:
            mean = outputs
        if self.mean_mode.upper() == 'EPS':
            output_dict['eps_t_pred'] = mean
        elif self.mean_mode.upper() == 'START_X':
            output_dict['x_0_pred'] = mean
        elif self.mean_mode.upper() == 'PREVIOUS_X':
            output_dict['x_tm1_pred'] = mean
        else:
            raise AttributeError(f"Only support 'EPS', 'START_X' and 'PREVIOUS_X' for mean output format. But receive '{self.mean_mode}'.")
        if return_noise:
            output_dict['x_t'] = x_t
            output_dict['t_rescaled'] = t
            if self.num_classes > 0:
                output_dict['label'] = label
        return output_dict

    def init_weights(self, pretrained=None):
        """Init weights for models.

        We just use the initialization method proposed in the original paper.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Defaults to None.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            for n, m in self.named_modules():
                if isinstance(m, nn.Conv2d) and ('conv_2' in n or 'out' in n and 'out_blocks' not in n):
                    constant_init(m, 0)
                if isinstance(m, nn.Conv1d) and 'proj' in n:
                    constant_init(m, 0)
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class MultiHeadAttention(nn.Module):
    """An attention block allows spatial position to attend to each other.

    Originally ported from here, but adapted to the N-d case.
    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.  # noqa

    Args:
        in_channels (int): Channels of the input feature map.
        num_heads (int, optional): Number of heads in the attention.
        norm_cfg (dict, optional): Config for normalization layer. Default
            to ``dict(type='GN', num_groups=32)``
    """

    def __init__(self, in_channels, num_heads=1, norm_cfg=dict(type='GN', num_groups=32)):
        super().__init__()
        self.num_heads = num_heads
        _, self.norm = build_norm_layer(norm_cfg, in_channels)
        self.qkv = nn.Conv1d(in_channels, in_channels * 3, 1)
        self.proj = nn.Conv1d(in_channels, in_channels, 1)
        self.init_weights()

    @staticmethod
    def QKVAttention(qkv):
        channel = qkv.shape[1] // 3
        q, k, v = torch.chunk(qkv, 3, dim=1)
        scale = 1 / np.sqrt(np.sqrt(channel))
        weight = torch.einsum('bct,bcs->bts', q * scale, k * scale)
        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
        weight = torch.einsum('bts,bcs->bct', weight, v)
        return weight

    def forward(self, x):
        """Forward function for multi head attention.
        Args:
            x (torch.Tensor): Input feature map.

        Returns:
            torch.Tensor: Feature map after attention.
        """
        b, c, *spatial = x.shape
        x = x.reshape(b, c, -1)
        qkv = self.qkv(self.norm(x))
        qkv = qkv.reshape(b * self.num_heads, -1, qkv.shape[2])
        h = self.QKVAttention(qkv)
        h = h.reshape(b, -1, h.shape[-1])
        h = self.proj(h)
        return (h + x).reshape(b, c, *spatial)

    def init_weights(self):
        constant_init(self.proj, 0)


class NormWithEmbedding(nn.Module):
    """Nornalization with embedding layer. If `use_scale_shift == True`,
    embedding results will be chunked and used to re-shift and re-scale
    normalization results. Otherwise, embedding results will directly add to
    input of normalization layer.

    Args:
        in_channels (int): Number of channels of the input feature map.
        embedding_channels (int) Number of channels of the input embedding.
        norm_cfg (dict, optional): Config for the normalization operation.
            Defaults to `dict(type='GN', num_groups=32)`.
        act_cfg (dict, optional): Config for the activation layer. Defaults
            to `dict(type='SiLU', inplace=False)`.
        use_scale_shift (bool): If True, the output of Embedding layer will be
            split to 'scale' and 'shift' and map the output of normalization
            layer to ``out * (1 + scale) + shift``. Otherwise, the output of
            Embedding layer will be added with the input before normalization
            operation. Defaults to True.
    """

    def __init__(self, in_channels, embedding_channels, norm_cfg=dict(type='GN', num_groups=32), act_cfg=dict(type='SiLU', inplace=False), use_scale_shift=True):
        super().__init__()
        self.use_scale_shift = use_scale_shift
        _, self.norm = build_norm_layer(norm_cfg, in_channels)
        embedding_output = in_channels * 2 if use_scale_shift else in_channels
        self.embedding_layer = nn.Sequential(build_activation_layer(act_cfg), nn.Linear(embedding_channels, embedding_output))

    def forward(self, x, y):
        """Forward function.

        Args:
            x (torch.Tensor): Input feature map tensor.
            y (torch.Tensor): Shared time embedding or shared label embedding.

        Returns:
            torch.Tensor : Output feature map tensor.
        """
        embedding = self.embedding_layer(y)[:, :, None, None]
        if self.use_scale_shift:
            scale, shift = torch.chunk(embedding, 2, dim=1)
            x = self.norm(x)
            x = x * (1 + scale) + shift
        else:
            x = self.norm(x + embedding)
        return x


class DenoisingDownsample(nn.Module):
    """Downsampling operation used in the denoising network. Support average
    pooling and convolution for downsample operation.

    Args:
        in_channels (int): Number of channels of the input feature map to be
            downsampled.
        with_conv (bool, optional): Whether use convolution operation for
            downsampling.  Defaults to `True`.
    """

    def __init__(self, in_channels, with_conv=True):
        super().__init__()
        if with_conv:
            self.downsample = nn.Conv2d(in_channels, in_channels, 3, 2, 1)
        else:
            self.downsample = nn.AvgPool2d(stride=2)

    def forward(self, x):
        """Forward function for downsampling operation.
        Args:
            x (torch.Tensor): Feature map to downsample.

        Returns:
            torch.Tensor: Feature map after downsampling.
        """
        return self.downsample(x)


class DenoisingUpsample(nn.Module):
    """Upsampling operation used in the denoising network. Allows users to
    apply an additional convolution layer after the nearest interpolation
    operation.

    Args:
        in_channels (int): Number of channels of the input feature map to be
            downsampled.
        with_conv (bool, optional): Whether apply an additional convolution
            layer after upsampling.  Defaults to `True`.
    """

    def __init__(self, in_channels, with_conv=True):
        super().__init__()
        if with_conv:
            self.with_conv = True
            self.conv = nn.Conv2d(in_channels, in_channels, 3, 1, 1)

    def forward(self, x):
        """Forward function for upsampling operation.
        Args:
            x (torch.Tensor): Feature map to upsample.

        Returns:
            torch.Tensor: Feature map after upsampling.
        """
        x = F.interpolate(x, scale_factor=2, mode='nearest')
        if self.with_conv:
            x = self.conv(x)
        return x


class FIDInceptionA(models.inception.InceptionA):
    """InceptionA block patched for FID computation."""

    def __init__(self, in_channels, pool_features):
        super().__init__(in_channels, pool_features)

    def forward(self, x):
        """Get InceptionA feature maps.

        Args:
            x (torch.Tensor): Input tensor of shape BxCxHxW.

        Returns:
            torch.Tensor: Feature Maps of x outputted by this block.
        """
        branch1x1 = self.branch1x1(x)
        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1, count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionC(models.inception.InceptionC):
    """InceptionC block patched for FID computation."""

    def __init__(self, in_channels, channels_7x7):
        super().__init__(in_channels, channels_7x7)

    def forward(self, x):
        """Get InceptionC feature maps.

        Args:
            x (torch.Tensor): Input tensor of shape BxCxHxW.

        Returns:
            torch.Tensor: Feature Maps of x outputted by this block.
        """
        branch1x1 = self.branch1x1(x)
        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)
        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1, count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionE_1(models.inception.InceptionE):
    """First InceptionE block patched for FID computation."""

    def __init__(self, in_channels):
        super().__init__(in_channels)

    def forward(self, x):
        """Get first InceptionE feature maps.

        Args:
            x (torch.Tensor): Input tensor of shape BxCxHxW.

        Returns:
            torch.Tensor: Feature Maps of x outputted by this block.
        """
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)]
        branch3x3 = torch.cat(branch3x3, 1)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [self.branch3x3dbl_3a(branch3x3dbl), self.branch3x3dbl_3b(branch3x3dbl)]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1, count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionE_2(models.inception.InceptionE):
    """Second InceptionE block patched for FID computation."""

    def __init__(self, in_channels):
        super().__init__(in_channels)

    def forward(self, x):
        """Get second InceptionE feature maps.

        Args:
            x (torch.Tensor): Input tensor of shape BxCxHxW.

        Returns:
            torch.Tensor: Feature Maps of x outputted by this block.
        """
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)]
        branch3x3 = torch.cat(branch3x3, 1)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [self.branch3x3dbl_3a(branch3x3dbl), self.branch3x3dbl_3b(branch3x3dbl)]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)
        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'


def fid_inception_v3(load_ckpt=True):
    """Build pretrained Inception model for FID computation.

    The Inception model for FID computation uses a different set of weights
    and has a slightly different structure than torchvision's Inception.

    This method first constructs torchvision's Inception and then patches the
    necessary parts that are different in the FID Inception model.
    """
    inception = models.inception_v3(num_classes=1008, aux_logits=False, pretrained=False)
    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)
    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)
    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)
    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)
    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)
    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)
    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)
    inception.Mixed_7b = FIDInceptionE_1(1280)
    inception.Mixed_7c = FIDInceptionE_2(2048)
    if load_ckpt:
        state_dict = load_url(FID_WEIGHTS_URL, progress=True)
        inception.load_state_dict(state_dict)
    return inception


class InceptionV3(nn.Module):
    """Pretrained InceptionV3 network returning feature maps."""
    DEFAULT_BLOCK_INDEX = 3
    BLOCK_INDEX_BY_DIM = {(64): 0, (192): 1, (768): 2, (2048): 3}

    def __init__(self, output_blocks=[DEFAULT_BLOCK_INDEX], resize_input=True, normalize_input=True, requires_grad=False, use_fid_inception=True, load_fid_inception=True):
        """Build pretrained InceptionV3.

        Args:
            output_blocks (list[int]): Indices of blocks to return features of.
                Possible values are:
                    - 0: corresponds to output of first max pooling
                    - 1: corresponds to output of second max pooling
                    - 2: corresponds to output which is fed to aux classifier
                    - 3: corresponds to output of final average pooling
            resize_input (bool): If true, bilinearly resizes input to width and
                height 299 before feeding input to model. As the network
                without fully connected layers is fully convolutional, it
                should be able to handle inputs of arbitrary size, so resizing
                might not be strictly needed.
            normalize_input (bool): If true, scales the input from range (0, 1)
                to the range the pretrained Inception network expects, namely
                (-1, 1).
            requires_grad (bool): If true, parameters of the model require
                gradients. Possibly useful for finetuning the network.
            use_fid_inception (bool): If true, uses the pretrained Inception
                model used in Tensorflow's FID implementation. If false, uses
                the pretrained Inception model available in torchvision. The
                FID Inception model has different weights and a slightly
                different structure from torchvision's Inception model. If you
                want to compute FID scores, you are strongly advised to set
                this parameter to true to get comparable results.
        """
        super().__init__()
        self.resize_input = resize_input
        self.normalize_input = normalize_input
        self.output_blocks = sorted(output_blocks)
        self.last_needed_block = max(output_blocks)
        assert self.last_needed_block <= 3, 'Last possible output block index is 3'
        self.blocks = nn.ModuleList()
        if use_fid_inception:
            inception = fid_inception_v3(load_fid_inception)
        else:
            inception = models.inception_v3(pretrained=True)
        block0 = [inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3, inception.Conv2d_2b_3x3, nn.MaxPool2d(kernel_size=3, stride=2)]
        self.blocks.append(nn.Sequential(*block0))
        if self.last_needed_block >= 1:
            block1 = [inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3, nn.MaxPool2d(kernel_size=3, stride=2)]
            self.blocks.append(nn.Sequential(*block1))
        if self.last_needed_block >= 2:
            block2 = [inception.Mixed_5b, inception.Mixed_5c, inception.Mixed_5d, inception.Mixed_6a, inception.Mixed_6b, inception.Mixed_6c, inception.Mixed_6d, inception.Mixed_6e]
            self.blocks.append(nn.Sequential(*block2))
        if self.last_needed_block >= 3:
            block3 = [inception.Mixed_7a, inception.Mixed_7b, inception.Mixed_7c, nn.AdaptiveAvgPool2d(output_size=(1, 1))]
            self.blocks.append(nn.Sequential(*block3))
        for param in self.parameters():
            param.requires_grad = requires_grad

    def forward(self, inp):
        """Get Inception feature maps.

        Args:
            inp (torch.Tensor): Input tensor of shape Bx3xHxW.
                Values are expected to be in range (0, 1)

        Returns:
            list(torch.Tensor): Corresponding to the selected output                 block, sorted ascending by index.
        """
        outp = []
        x = inp
        if self.resize_input:
            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)
        if self.normalize_input:
            x = 2 * x - 1
        for idx, block in enumerate(self.blocks):
            x = block(x)
            if idx in self.output_blocks:
                outp.append(x)
            if idx == self.last_needed_block:
                break
        return outp


class NetLinLayer(nn.Module):
    """A single linear layer which does a 1x1 conv."""

    def __init__(self, chn_in, chn_out=1, use_dropout=False):
        super().__init__()
        layers = [nn.Dropout()] if use_dropout else []
        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False)]
        self.model = nn.Sequential(*layers)


class ScalingLayer(nn.Module):

    def __init__(self):
        super().__init__()
        self.register_buffer('shift', torch.Tensor([-0.03, -0.088, -0.188])[None, :, None, None])
        self.register_buffer('scale', torch.Tensor([0.458, 0.448, 0.45])[None, :, None, None])

    def forward(self, inp):
        return (inp - self.shift) / self.scale


def normalize_tensor(in_feat, eps=1e-10):
    """L2 normalization.

    Args:
        in_feat (Tensor): Tensor with shape [N, C, H, W].
        eps (float, optional): Epsilon value to avoid computation error.
            Defaults to 1e-10.

    Returns:
        Tensor: Tensor after L2 normalization per-instance.
    """
    norm_factor = torch.sqrt(torch.sum(in_feat ** 2, dim=1, keepdim=True))
    return in_feat / (norm_factor + eps)


def spatial_average(in_tens, keepdim=True):
    """Returns the mean value of each row of the input tensor in the spatial
    dimension.

    Args:
        in_tens (Tensor): Tensor with shape [N, C, H, W].
        keepdim (bool, optional): If keepdim is True, the output tensor is of
            the shape [N, C, 1, 1]. Otherwise, the output will have shape
            [N, C]. Defaults to True.

    Returns:
        Tensor: Tensor after average pooling to 1x1 with shape [N, C, 1, 1] or
            [N, C].
    """
    return in_tens.mean([2, 3], keepdim=keepdim)


def upsample(in_tens, out_H=64):
    """Upsamples the input to the given size.

    Args:
        in_tens (Tensor): Tensor with shape [N, C, H, W].
        out_H (int, optional): Output spatial size. Defaults to 64.

    Returns:
        Tensor: Output Tensor.
    """
    in_H = in_tens.shape[2]
    scale_factor = 1.0 * out_H / in_H
    return nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)(in_tens)


class vgg16(torch.nn.Module):
    """VGG16 feature extractor for LPIPS metric.

        Ref : https://github.com/richzhang/PerceptualSimilarity/blob/master/lpips/pretrained_networks.py # noqa
    """

    def __init__(self, requires_grad=False, pretrained=True):
        super().__init__()
        vgg_pretrained_features = tv.vgg16(pretrained=pretrained).features
        self.slice1 = torch.nn.Sequential()
        self.slice2 = torch.nn.Sequential()
        self.slice3 = torch.nn.Sequential()
        self.slice4 = torch.nn.Sequential()
        self.slice5 = torch.nn.Sequential()
        self.N_slices = 5
        for x in range(4):
            self.slice1.add_module(str(x), vgg_pretrained_features[x])
        for x in range(4, 9):
            self.slice2.add_module(str(x), vgg_pretrained_features[x])
        for x in range(9, 16):
            self.slice3.add_module(str(x), vgg_pretrained_features[x])
        for x in range(16, 23):
            self.slice4.add_module(str(x), vgg_pretrained_features[x])
        for x in range(23, 30):
            self.slice5.add_module(str(x), vgg_pretrained_features[x])
        if not requires_grad:
            for param in self.parameters():
                param.requires_grad = False

    def forward(self, X):
        h = self.slice1(X)
        h_relu1_2 = h
        h = self.slice2(h)
        h_relu2_2 = h
        h = self.slice3(h)
        h_relu3_3 = h
        h = self.slice4(h)
        h_relu4_3 = h
        h = self.slice5(h)
        h_relu5_3 = h
        vgg_outputs = namedtuple('VggOutputs', ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])
        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)
        return out


class PNetLin(nn.Module):
    """
        Ref: https://github.com/richzhang/PerceptualSimilarity/blob/master/lpips/lpips.py # noqa
    """

    def __init__(self, pnet_rand=False, pnet_tune=False, use_dropout=True, spatial=False, version='0.1', lpips=True):
        super().__init__()
        self.pnet_tune = pnet_tune
        self.pnet_rand = pnet_rand
        self.spatial = spatial
        self.lpips = lpips
        self.version = version
        self.scaling_layer = ScalingLayer()
        self.channels = [64, 128, 256, 512, 512]
        self.L = len(self.channels)
        self.net = vgg16(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)
        self.lin0 = NetLinLayer(self.channels[0], use_dropout=use_dropout)
        self.lin1 = NetLinLayer(self.channels[1], use_dropout=use_dropout)
        self.lin2 = NetLinLayer(self.channels[2], use_dropout=use_dropout)
        self.lin3 = NetLinLayer(self.channels[3], use_dropout=use_dropout)
        self.lin4 = NetLinLayer(self.channels[4], use_dropout=use_dropout)
        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]

    def forward(self, in0, in1, retPerLayer=False):
        in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version == '0.1' else (in0, in1)
        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)
        feats0, feats1, diffs = {}, {}, {}
        for kk in range(self.L):
            feats0[kk], feats1[kk] = normalize_tensor(outs0[kk]), normalize_tensor(outs1[kk])
            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2
        if self.lpips:
            if self.spatial:
                res = [upsample(self.lins[kk].model(diffs[kk]), out_H=in0.shape[2]) for kk in range(self.L)]
            else:
                res = [spatial_average(self.lins[kk].model(diffs[kk]), keepdim=True) for kk in range(self.L)]
        elif self.spatial:
            res = [upsample(diffs[kk].sum(dim=1, keepdim=True), out_H=in0.shape[2]) for kk in range(self.L)]
        else:
            res = [spatial_average(diffs[kk].sum(dim=1, keepdim=True), keepdim=True) for kk in range(self.L)]
        val = sum(res)
        if retPerLayer:
            return val, res
        return val


class Dist2LogitLayer(nn.Module):
    """takes 2 distances, puts through fc layers, spits out value between [0,
    1] (if use_sigmoid is True)"""

    def __init__(self, chn_mid=32, use_sigmoid=True):
        super().__init__()
        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True)]
        layers += [nn.LeakyReLU(0.2, True)]
        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1, padding=0, bias=True)]
        layers += [nn.LeakyReLU(0.2, True)]
        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True)]
        if use_sigmoid:
            layers += [nn.Sigmoid()]
        self.model = nn.Sequential(*layers)

    def forward(self, d0, d1, eps=0.1):
        return self.model.forward(torch.cat((d0, d1, d0 - d1, d0 / (d1 + eps), d1 / (d0 + eps)), dim=1))


class BCERankingLoss(nn.Module):

    def __init__(self, chn_mid=32):
        super().__init__()
        self.net = Dist2LogitLayer(chn_mid=chn_mid)
        self.loss = torch.nn.BCELoss()

    def forward(self, d0, d1, judge):
        per = (judge + 1.0) / 2.0
        self.logit = self.net.forward(d0, d1)
        return self.loss(self.logit, per)


class PerceptualVGG(nn.Module):
    """VGG network used in calculating perceptual loss.

    In this implementation, we allow users to choose whether use normalization
    in the input feature and the type of vgg network. Note that the pretrained
    path must fit the vgg type.

    Args:
        layer_name_list (list[str]): According to the name in this list,
            forward function will return the corresponding features. This
            list contains the name each layer in `vgg.feature`. An example
            of this list is ['4', '10'].
        vgg_type (str): Set the type of vgg network. Default: 'vgg19'.
        use_input_norm (bool): If True, normalize the input image.
            Importantly, the input feature must in the range [0, 1].
            Default: True.
        pretrained (str): Path for pretrained weights. Default:
            'torchvision://vgg19'
    """

    def __init__(self, layer_name_list, vgg_type='vgg19', use_input_norm=True, pretrained='torchvision://vgg19'):
        super().__init__()
        if pretrained.startswith('torchvision://'):
            assert vgg_type in pretrained
        self.layer_name_list = layer_name_list
        self.use_input_norm = use_input_norm
        _vgg = getattr(vgg, vgg_type)()
        self.init_weights(_vgg, pretrained)
        num_layers = max(map(int, layer_name_list)) + 1
        assert len(_vgg.features) >= num_layers
        self.vgg_layers = _vgg.features[:num_layers]
        if self.use_input_norm:
            self.register_buffer('mean', torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))
            self.register_buffer('std', torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))
        for v in self.vgg_layers.parameters():
            v.requires_grad = False

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        if self.use_input_norm:
            x = (x - self.mean) / self.std
        output = {}
        for name, module in self.vgg_layers.named_children():
            x = module(x)
            if name in self.layer_name_list:
                output[name] = x.clone()
        return output

    def init_weights(self, model, pretrained):
        """Init weights.

        Args:
            model (nn.Module): Models to be inited.
            pretrained (str): Path for pretrained weights.
        """
        logger = get_root_logger()
        load_checkpoint(model, pretrained, logger=logger)


def reduce_loss(loss, reduction):
    """Reduce loss as specified.

    Args:
        loss (Tensor): Elementwise loss tensor.
        reduction (str): Options are "none", "mean", "sum", "flatmean" and
            "batchmean". 'none': no reduction will be applied. 'mean': the
            output will be divided by the number of elements in the output.
            'sum': the output will be summed. 'batchmean': the sum of the
            output will be divided by batchsize. 'flatmean': each sample
            will be divided by the number of element respectively and
            output will shape as [bz, ].

    Return:
        Tensor: Reduced loss tensor.
    """
    if reduction == 'batchmean':
        return loss.sum() / loss.shape[0]
    if reduction == 'flatmean':
        return loss.mean(dim=list(range(1, loss.ndim)))
    reduction_enum = F._Reduction.get_enum(reduction)
    if reduction_enum == 0:
        return loss
    if reduction_enum == 1:
        return loss.mean()
    if reduction_enum == 2:
        return loss.sum()
    raise ValueError(f'reduction type {reduction} not supported')


def weight_reduce_loss(loss, weight=None, reduction='mean', avg_factor=None):
    """Apply element-wise weight and reduce loss.

    Args:
        loss (Tensor): Element-wise loss.
        weight (Tensor): Element-wise weights.
        reduction (str): Same as built-in losses of PyTorch.
        avg_factor (float): Average factor when computing the mean of losses.

    Returns:
        Tensor: Processed loss values.
    """
    if weight is not None:
        loss = loss * weight
    if avg_factor is None:
        loss = reduce_loss(loss, reduction)
    elif reduction == 'mean':
        loss = loss.sum() / avg_factor
    elif reduction != 'none':
        raise ValueError('avg_factor can not be used with reduction="sum"')
    return loss


def weighted_loss(loss_func):
    """Create a weighted version of a given loss function.

    To use this decorator, the loss function must have the signature like
    `loss_func(pred, target, **kwargs)`. The function only needs to compute
    element-wise loss without any reduction. This decorator will add weight
    and reduction arguments to the function. The decorated function will have
    the signature like `loss_func(pred, target, weight=None, reduction='mean',
    avg_factor=None, **kwargs)`.

    :Example:

    >>> import torch
    >>> @weighted_loss
    >>> def l1_loss(pred, target):
    >>>     return (pred - target).abs()

    >>> pred = torch.Tensor([0, 2, 3])
    >>> target = torch.Tensor([1, 1, 1])
    >>> weight = torch.Tensor([1, 0, 1])

    >>> l1_loss(pred, target)
    tensor(1.3333)
    >>> l1_loss(pred, target, weight)
    tensor(1.)
    >>> l1_loss(pred, target, reduction='none')
    tensor([1., 1., 2.])
    >>> l1_loss(pred, target, weight, avg_factor=2)
    tensor(1.5000)
    """

    @functools.wraps(loss_func)
    def wrapper(*args, weight=None, reduction='mean', avg_factor=None, **kwargs):
        loss = loss_func(*args, **kwargs)
        loss = weight_reduce_loss(loss, weight, reduction, avg_factor)
        return loss
    return wrapper


@weighted_loss
def l1_loss(pred, target):
    """L1 loss.

    Args:
        pred (Tensor): Prediction Tensor with shape (n, c, h, w).
        target (Tensor): Target Tensor with shape (n, c, h, w).

    Returns:
        Tensor: Calculated L1 loss.
    """
    return F.l1_loss(pred, target, reduction='none')


@weighted_loss
def mse_loss(pred, target):
    """MSE loss.

    Args:
        pred (Tensor): Prediction Tensor with shape (n, c, h, w).
        target (Tensor): Target Tensor with shape (n, c, h, w).

    Returns:
        Tensor: Calculated MSE loss.
    """
    return F.mse_loss(pred, target, reduction='none')


class PerceptualLoss(nn.Module):
    """Perceptual loss with commonly used style loss.

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            disc_pred_fake=disc_pred_fake,
            disc_pred_real=disc_pred_real,
            fake_imgs=fake_imgs,
            real_imgs=real_imgs,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we may need to provide ``pred`` and ``target`` as input.
    Thus, an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            pred='fake_imgs',
            target='real_imgs',
            layer_weights={
                     '4': 1.,
                     '9': 1.,
                     '18': 1.},
            )

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_mse'.
        layers_weights (dict): The weight for each layer of vgg feature for
            perceptual loss. Here is an example: {'4': 1., '9': 1., '18': 1.},
            which means the 5th, 10th and 18th feature layer will be
            extracted with weight 1.0 in calculating losses. Defaults to
            '{'4': 1., '9': 1., '18': 1.}'.
        layers_weights_style (dict): The weight for each layer of vgg feature
            for style loss. If set to 'None', the weights are set equal to
            the weights for perceptual loss. Default: None.
        vgg_type (str): The type of vgg network used as feature extractor.
            Default: 'vgg19'.
        use_input_norm (bool):  If True, normalize the input image in vgg.
            Default: True.
        perceptual_weight (float): If `perceptual_weight > 0`, the perceptual
            loss will be calculated and the loss will multiplied by the
            weight. Default: 1.0.
        style_weight (float): If `style_weight > 0`, the style loss will be
            calculated and the loss will multiplied by the weight.
            Default: 1.0.
        norm_img (bool): If True, the image will be normed to [0, 1]. Note that
            this is different from the `use_input_norm` which norm the input in
            in forward function of vgg according to the statistics of dataset.
            Importantly, the input image must be in range [-1, 1].
        pretrained (str): Path for pretrained weights. Default:
            'torchvision://vgg19'.
        criterion (str): Criterion type. Options are 'l1' and 'mse'.
            Default: 'l1'.
        split_style_loss (bool): Whether return a separate style loss item.
            Options are True and False. Default: False
    """

    def __init__(self, data_info=None, loss_name='loss_perceptual', layer_weights={'4': 1.0, '9': 1.0, '18': 1.0}, layer_weights_style=None, vgg_type='vgg19', use_input_norm=True, perceptual_weight=1.0, style_weight=1.0, norm_img=True, pretrained='torchvision://vgg19', criterion='l1', split_style_loss=False):
        super().__init__()
        self.data_info = data_info
        self._loss_name = loss_name
        self.norm_img = norm_img
        self.perceptual_weight = perceptual_weight
        self.style_weight = style_weight
        self.layer_weights = layer_weights
        self.layer_weights_style = layer_weights_style
        self.split_style_loss = split_style_loss
        self.vgg = PerceptualVGG(layer_name_list=list(self.layer_weights.keys()), vgg_type=vgg_type, use_input_norm=use_input_norm, pretrained=pretrained)
        if self.layer_weights_style is not None and self.layer_weights_style != self.layer_weights:
            self.vgg_style = PerceptualVGG(layer_name_list=list(self.layer_weights_style.keys()), vgg_type=vgg_type, use_input_norm=use_input_norm, pretrained=pretrained)
        else:
            self.layer_weights_style = self.layer_weights
            self.vgg_style = None
        criterion = criterion.lower()
        if criterion == 'l1':
            self.criterion = l1_loss
        elif criterion == 'mse':
            self.criterion = mse_loss
        else:
            raise NotImplementedError(f'{criterion} criterion has not been supported in this version.')

    def forward(self, *args, **kwargs):
        """Forward function. If ``self.data_info`` is not ``None``, a
        dictionary containing all of the data and necessary modules should be
        passed into this function. If this dictionary is given as a non-keyword
        argument, it should be offered as the first argument. If you are using
        keyword argument, please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function, ``mse_loss``.

        Args:
            pred (Tensor): Input tensor with shape (n, c, h, w).
            target (Tensor): Ground-truth tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            return self.perceptual_loss(**kwargs)
        else:
            return self.perceptual_loss(*args, **kwargs)

    def perceptual_loss(self, pred, target):
        if self.norm_img:
            pred = (pred + 1.0) * 0.5
            target = (target + 1.0) * 0.5
        pred_features = self.vgg(pred)
        target_features = self.vgg(target.detach())
        if self.perceptual_weight > 0:
            percep_loss = 0
            for k in pred_features.keys():
                percep_loss += self.criterion(pred_features[k], target_features[k], weight=self.layer_weights[k])
            percep_loss *= self.perceptual_weight
        else:
            percep_loss = 0.0
        if self.style_weight > 0:
            if self.vgg_style is not None:
                pred_features = self.vgg_style(pred)
                target_features = self.vgg_style(target.detach())
            style_loss = 0
            for k in pred_features.keys():
                style_loss += self.criterion(self._gram_mat(pred_features[k]), self._gram_mat(target_features[k])) * self.layer_weights_style[k]
            style_loss *= self.style_weight
        else:
            style_loss = 0.0
        if self.split_style_loss:
            return percep_loss, style_loss
        else:
            return percep_loss + style_loss

    def _gram_mat(self, x):
        """Calculate Gram matrix.

        Args:
            x (torch.Tensor): Tensor with shape of (n, c, h, w).

        Returns:
            torch.Tensor: Gram matrix.
        """
        n, c, h, w = x.size()
        features = x.view(n, c, w * h)
        features_t = features.transpose(1, 2)
        gram = features.bmm(features_t) / (c * h * w)
        return gram

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


class LSGANGenerator(nn.Module):
    """Generator for LSGAN.

    Implementation Details for LSGAN architecture:

    #. Adopt transposed convolution in the generator;
    #. Use batchnorm in the generator except for the final output layer;
    #. Use ReLU in the generator in addition to the final output layer;
    #. Keep channels of feature maps unchanged in the convolution backbone;
    #. Use one more 3x3 conv every upsampling in the convolution backbone.

    We follow the implementation details of the origin paper:
    Least Squares Generative Adversarial Networks
    https://arxiv.org/pdf/1611.04076.pdf

    Args:
        output_scale (int, optional): Output scale for the generated image.
            Defaults to 128.
        out_channels (int, optional): The channel number of the output feature.
            Defaults to 3.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Defaults to 256.
        input_scale (int, optional): The scale of the input 2D feature map.
            Defaults to 8.
        noise_size (int, optional): Size of the input noise
            vector. Defaults to 1024.
        conv_cfg (dict, optional): Config for the convolution module used in
            this generator. Defaults to dict(type='ConvTranspose2d').
        default_norm_cfg (dict, optional): Norm config for all of layers
            except for the final output layer. Defaults to dict(type='BN').
        default_act_cfg (dict, optional): Activation config for all of layers
            except for the final output layer. Defaults to dict(type='ReLU').
        out_act_cfg (dict, optional): Activation config for the final output
            layer. Defaults to dict(type='Tanh').
    """

    def __init__(self, output_scale=128, out_channels=3, base_channels=256, input_scale=8, noise_size=1024, conv_cfg=dict(type='ConvTranspose2d'), default_norm_cfg=dict(type='BN'), default_act_cfg=dict(type='ReLU'), out_act_cfg=dict(type='Tanh')):
        super().__init__()
        assert output_scale % input_scale == 0
        assert output_scale // input_scale >= 4
        self.output_scale = output_scale
        self.base_channels = base_channels
        self.input_scale = input_scale
        self.noise_size = noise_size
        self.noise2feat_head = nn.Sequential(nn.Linear(noise_size, input_scale * input_scale * base_channels))
        self.noise2feat_tail = nn.Sequential(nn.BatchNorm2d(base_channels))
        if default_act_cfg is not None:
            self.noise2feat_tail.add_module('act', build_activation_layer(default_act_cfg))
        self.num_upsamples = int(np.log2(output_scale // input_scale)) - 2
        self.conv_blocks = nn.ModuleList()
        for _ in range(self.num_upsamples):
            self.conv_blocks.append(ConvModule(base_channels, base_channels, kernel_size=3, stride=2, padding=1, conv_cfg=dict(conv_cfg, output_padding=1), norm_cfg=default_norm_cfg, act_cfg=default_act_cfg))
            self.conv_blocks.append(ConvModule(base_channels, base_channels, kernel_size=3, stride=1, padding=1, conv_cfg=conv_cfg, norm_cfg=default_norm_cfg, act_cfg=default_act_cfg))
        self.conv_blocks.append(ConvModule(base_channels, int(base_channels // 2), kernel_size=3, stride=2, padding=1, conv_cfg=dict(conv_cfg, output_padding=1), norm_cfg=default_norm_cfg, act_cfg=default_act_cfg))
        self.conv_blocks.append(ConvModule(int(base_channels // 2), int(base_channels // 4), kernel_size=3, stride=2, padding=1, conv_cfg=dict(conv_cfg, output_padding=1), norm_cfg=default_norm_cfg, act_cfg=default_act_cfg))
        self.conv_blocks.append(ConvModule(int(base_channels // 4), out_channels, kernel_size=3, stride=1, padding=1, conv_cfg=conv_cfg, norm_cfg=None, act_cfg=out_act_cfg))

    def forward(self, noise, num_batches=0, return_noise=False):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output image
                will be returned. Otherwise, a dict contains ``fake_img`` and
                ``noise_batch`` will be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            if noise.ndim == 2:
                noise_batch = noise
            else:
                raise ValueError(f'The noise should be in shape of (n, c)but got {noise.shape}')
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size))
        noise_batch = noise_batch
        x = self.noise2feat_head(noise_batch)
        x = x.reshape((-1, self.base_channels, self.input_scale, self.input_scale))
        x = self.noise2feat_tail(x)
        for conv in self.conv_blocks:
            x = conv(x)
        if return_noise:
            return dict(fake_img=x, noise_batch=noise_batch)
        return x


class LSGANDiscriminator(nn.Module):
    """Discriminator for LSGAN.

    Implementation Details for LSGAN architecture:

    #. Adopt convolution in the discriminator;
    #. Use batchnorm in the discriminator except for the input and final        output layer;
    #. Use LeakyReLU in the discriminator in addition to the output layer;
    #. Use fully connected layer in the output layer;
    #. Use 5x5 conv rather than 4x4 conv in DCGAN.

    Args:
        input_scale (int, optional): The scale of the input image. Defaults to
            128.
        output_scale (int, optional): The final scale of the convolutional
            feature. Defaults to 8.
        out_channels (int, optional): The channel number of the final output
            layer. Defaults to 1.
        in_channels (int, optional): The channel number of the input image.
            Defaults to 3.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Defaults to 128.
        conv_cfg (dict, optional): Config for the convolution module used in
            this discriminator. Defaults to dict(type='Conv2d').
        default_norm_cfg (dict, optional): Norm config for all of layers
            except for the final output layer. Defaults to ``dict(type='BN')``.
        default_act_cfg (dict, optional): Activation config for all of layers
            except for the final output layer. Defaults to
            ``dict(type='LeakyReLU', negative_slope=0.2)``.
        out_act_cfg (dict, optional): Activation config for the final output
            layer. Defaults to ``dict(type='Tanh')``.
    """

    def __init__(self, input_scale=128, output_scale=8, out_channels=1, in_channels=3, base_channels=64, conv_cfg=dict(type='Conv2d'), default_norm_cfg=dict(type='BN'), default_act_cfg=dict(type='LeakyReLU', negative_slope=0.2), out_act_cfg=None):
        super().__init__()
        assert input_scale % output_scale == 0
        assert input_scale // output_scale >= 2
        self.input_scale = input_scale
        self.output_scale = output_scale
        self.out_channels = out_channels
        self.base_channels = base_channels
        self.with_out_activation = out_act_cfg is not None
        self.conv_blocks = nn.ModuleList()
        self.conv_blocks.append(ConvModule(in_channels, base_channels, kernel_size=5, stride=2, padding=2, conv_cfg=conv_cfg, norm_cfg=None, act_cfg=default_act_cfg))
        self.num_downsamples = int(np.log2(input_scale // output_scale)) - 1
        curr_channels = base_channels
        for _ in range(self.num_downsamples):
            self.conv_blocks.append(ConvModule(curr_channels, curr_channels * 2, kernel_size=5, stride=2, padding=2, conv_cfg=conv_cfg, norm_cfg=default_norm_cfg, act_cfg=default_act_cfg))
            curr_channels = curr_channels * 2
        self.decision = nn.Sequential(nn.Linear(output_scale * output_scale * curr_channels, out_channels))
        if self.with_out_activation:
            self.out_activation = build_activation_layer(out_act_cfg)

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Fake or real image tensor.

        Returns:
            torch.Tensor: Prediction for the reality of the input image.
        """
        n = x.shape[0]
        for conv in self.conv_blocks:
            x = conv(x)
        x = x.reshape(n, -1)
        x = self.decision(x)
        if self.with_out_activation:
            x = self.out_activation(x)
        return x


class EqualizedLR:
    """Equalized Learning Rate.

    This trick is proposed in:
    Progressive Growing of GANs for Improved Quality, Stability, and Variation

    The general idea is to dynamically rescale the weight in training instead
    of in initializing so that the variance of the responses in each layer is
    guaranteed with some statistical properties.

    Note that this function is always combined with a convolution module which
    is initialized with :math:`\\mathcal{N}(0, 1)`.

    Args:
        name (str | optional): The name of weights. Defaults to 'weight'.
        mode (str, optional): The mode of computing ``fan`` which is the
            same as ``kaiming_init`` in pytorch. You can choose one from
            ['fan_in', 'fan_out']. Defaults to 'fan_in'.
    """

    def __init__(self, name='weight', gain=2 ** 0.5, mode='fan_in', lr_mul=1.0):
        self.name = name
        self.mode = mode
        self.gain = gain
        self.lr_mul = lr_mul

    def compute_weight(self, module):
        """Compute weight with equalized learning rate.

        Args:
            module (nn.Module): A module that is wrapped with equalized lr.

        Returns:
            torch.Tensor: Updated weight.
        """
        weight = getattr(module, self.name + '_orig')
        if weight.ndim == 5:
            fan = _calculate_correct_fan(weight[0], self.mode)
        else:
            assert weight.ndim <= 4
            fan = _calculate_correct_fan(weight, self.mode)
        weight = weight * torch.tensor(self.gain, device=weight.device) * torch.sqrt(torch.tensor(1.0 / fan, device=weight.device)) * self.lr_mul
        return weight

    def __call__(self, module, inputs):
        """Standard interface for forward pre hooks."""
        setattr(module, self.name, self.compute_weight(module))

    @staticmethod
    def apply(module, name, gain=2 ** 0.5, mode='fan_in', lr_mul=1.0):
        """Apply function.

        This function is to register an equalized learning rate hook in an
        ``nn.Module``.

        Args:
            module (nn.Module): Module to be wrapped.
            name (str | optional): The name of weights. Defaults to 'weight'.
            mode (str, optional): The mode of computing ``fan`` which is the
                same as ``kaiming_init`` in pytorch. You can choose one from
                ['fan_in', 'fan_out']. Defaults to 'fan_in'.

        Returns:
            nn.Module: Module that is registered with equalized lr hook.
        """
        for _, hook in module._forward_pre_hooks.items():
            if isinstance(hook, EqualizedLR):
                raise RuntimeError(f'Cannot register two equalized_lr hooks on the same parameter {name} in {module} module.')
        fn = EqualizedLR(name, gain=gain, mode=mode, lr_mul=lr_mul)
        weight = module._parameters[name]
        delattr(module, name)
        module.register_parameter(name + '_orig', weight)
        setattr(module, name, weight.data)
        module.register_forward_pre_hook(fn)
        return fn


def equalized_lr(module, name='weight', gain=2 ** 0.5, mode='fan_in', lr_mul=1.0):
    """Equalized Learning Rate.

    This trick is proposed in:
    Progressive Growing of GANs for Improved Quality, Stability, and Variation

    The general idea is to dynamically rescale the weight in training instead
    of in initializing so that the variance of the responses in each layer is
    guaranteed with some statistical properties.

    Note that this function is always combined with a convolution module which
    is initialized with :math:`\\mathcal{N}(0, 1)`.

    Args:
        module (nn.Module): Module to be wrapped.
        name (str | optional): The name of weights. Defaults to 'weight'.
        mode (str, optional): The mode of computing ``fan`` which is the
            same as ``kaiming_init`` in pytorch. You can choose one from
            ['fan_in', 'fan_out']. Defaults to 'fan_in'.

    Returns:
        nn.Module: Module that is registered with equalized lr hook.
    """
    EqualizedLR.apply(module, name, gain=gain, mode=mode, lr_mul=lr_mul)
    return module


class EqualizedLRLinearModule(nn.Linear):
    """Equalized LR LinearModule.

    In this module, we adopt equalized lr in ``nn.Linear``. The equalized
    learning rate is proposed in:
    Progressive Growing of GANs for Improved Quality, Stability, and Variation

    Note that, the initialization of ``self.weight`` will be overwritten as
    :math:`\\mathcal{N}(0, 1)`.

    Args:
        equalized_lr_cfg (dict | None, optional): Config for ``EqualizedLR``.
            If ``None``, equalized learning rate is ignored. Defaults to
            dict(mode='fan_in').
    """

    def __init__(self, *args, equalized_lr_cfg=dict(mode='fan_in'), **kwargs):
        super().__init__(*args, **kwargs)
        self.with_equalized_lr = equalized_lr_cfg is not None
        if self.with_equalized_lr:
            self.lr_mul = equalized_lr_cfg.get('lr_mul', 1.0)
        else:
            self.lr_mul = 1.0
        if self.with_equalized_lr:
            equalized_lr(self, **equalized_lr_cfg)
            self._init_linear_weights()

    def _init_linear_weights(self):
        """Initialize linear weights as described in PGGAN."""
        nn.init.normal_(self.weight, 0, 1.0 / self.lr_mul)
        if self.bias is not None:
            nn.init.constant_(self.bias, 0.0)


def pixel_norm(x, eps=1e-06):
    """Pixel Normalization.

    This normalization is proposed in:
    Progressive Growing of GANs for Improved Quality, Stability, and Variation

    Args:
        x (torch.Tensor): Tensor to be normalized.
        eps (float, optional): Epsilon to avoid dividing zero.
            Defaults to 1e-6.

    Returns:
        torch.Tensor: Normalized tensor.
    """
    if torch.__version__ >= '1.7.0':
        norm = torch.linalg.norm(x, ord=2, dim=1, keepdim=True)
    else:
        norm = torch.norm(x, p=2, dim=1, keepdim=True)
    norm = norm / torch.sqrt(torch.tensor(x.shape[1]))
    return x / (norm + eps)


class PGGANNoiseTo2DFeat(nn.Module):

    def __init__(self, noise_size, out_channels, act_cfg=dict(type='LeakyReLU', negative_slope=0.2), norm_cfg=dict(type='PixelNorm'), normalize_latent=True, order=('linear', 'act', 'norm')):
        super().__init__()
        self.noise_size = noise_size
        self.out_channels = out_channels
        self.normalize_latent = normalize_latent
        self.with_activation = act_cfg is not None
        self.with_norm = norm_cfg is not None
        self.order = order
        assert len(order) == 3 and set(order) == set(['linear', 'act', 'norm'])
        self.linear = EqualizedLRLinearModule(noise_size, out_channels * 16, equalized_lr_cfg=dict(gain=np.sqrt(2) / 4), bias=False)
        if self.with_activation:
            self.activation = build_activation_layer(act_cfg)
        self.register_parameter('bias', nn.Parameter(torch.zeros(1, out_channels, 1, 1)))
        if self.with_norm:
            _, self.norm = build_norm_layer(norm_cfg, out_channels)

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input noise tensor with shape (n, c).

        Returns:
            Tensor: Forward results with shape (n, c, 4, 4).
        """
        assert x.ndim == 2
        if self.normalize_latent:
            x = pixel_norm(x)
        for order in self.order:
            if order == 'linear':
                x = self.linear(x)
                x = torch.reshape(x, (-1, self.out_channels, 4, 4))
                x = x + self.bias
            elif order == 'act' and self.with_activation:
                x = self.activation(x)
            elif order == 'norm' and self.with_norm:
                x = self.norm(x)
        return x


class PGGANGenerator(nn.Module):
    """Generator for PGGAN.

    Args:
        noise_size (int): Size of the input noise vector.
        out_scale (int): Output scale for the generated image.
        label_size (int, optional): Size of the label vector.
            Defaults to 0.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this
            number. Defaults to 8192.
        channel_decay (float, optional): Decay for channels of feature maps.
            Defaults to 1.0.
        max_channels (int, optional): Maximum channels for the feature
            maps in the generator block. Defaults to 512.
        fused_upconv (bool, optional): Whether use fused upconv.
            Defaults to True.
        conv_module_cfg (dict, optional): Config for the convolution
            module used in this generator. Defaults to None.
        fused_upconv_cfg (dict, optional): Config for the fused upconv
            module used in this generator. Defaults to None.
        upsample_cfg (dict, optional): Config for the upsampling operation.
            Defaults to None.
    """
    _default_fused_upconv_cfg = dict(conv_cfg=dict(type='deconv'), kernel_size=3, stride=2, padding=1, bias=True, act_cfg=dict(type='LeakyReLU', negative_slope=0.2), norm_cfg=dict(type='PixelNorm'), order=('conv', 'act', 'norm'))
    _default_conv_module_cfg = dict(conv_cfg=None, kernel_size=3, stride=1, padding=1, bias=True, act_cfg=dict(type='LeakyReLU', negative_slope=0.2), norm_cfg=dict(type='PixelNorm'), order=('conv', 'act', 'norm'))
    _default_upsample_cfg = dict(type='nearest', scale_factor=2)

    def __init__(self, noise_size, out_scale, label_size=0, base_channels=8192, channel_decay=1.0, max_channels=512, fused_upconv=True, conv_module_cfg=None, fused_upconv_cfg=None, upsample_cfg=None):
        super().__init__()
        self.noise_size = noise_size if noise_size else min(base_channels, max_channels)
        self.out_scale = out_scale
        self.out_log2_scale = int(np.log2(out_scale))
        assert out_scale == 2 ** self.out_log2_scale and out_scale >= 4
        self.label_size = label_size
        self.base_channels = base_channels
        self.channel_decay = channel_decay
        self.max_channels = max_channels
        self.fused_upconv = fused_upconv
        self.conv_module_cfg = deepcopy(self._default_conv_module_cfg)
        if conv_module_cfg:
            self.conv_module_cfg.update(conv_module_cfg)
        if self.fused_upconv:
            self.fused_upconv_cfg = deepcopy(self._default_fused_upconv_cfg)
            if fused_upconv_cfg:
                self.fused_upconv_cfg.update(fused_upconv_cfg)
        self.upsample_cfg = deepcopy(self._default_upsample_cfg)
        if upsample_cfg is not None:
            self.upsample_cfg.update(upsample_cfg)
        self.noise2feat = PGGANNoiseTo2DFeat(noise_size + label_size, self._num_out_channels(1))
        self.torgb_layers = nn.ModuleList()
        self.conv_blocks = nn.ModuleList()
        for s in range(2, self.out_log2_scale + 1):
            in_ch = self._num_out_channels(s - 1) if s == 2 else self._num_out_channels(s - 2)
            self.torgb_layers.append(self._get_torgb_layer(self._num_out_channels(s - 1)))
            self.conv_blocks.extend(self._get_upconv_block(in_ch, s))
        self.upsample_layer = build_upsample_layer(self.upsample_cfg)

    def _get_torgb_layer(self, in_channels):
        return EqualizedLRConvModule(in_channels, 3, kernel_size=1, stride=1, equalized_lr_cfg=dict(gain=1), bias=True, norm_cfg=None, act_cfg=None)

    def _num_out_channels(self, log_scale):
        return min(int(self.base_channels / 2.0 ** (log_scale * self.channel_decay)), self.max_channels)

    def _get_upconv_block(self, in_channels, log_scale):
        modules = []
        if log_scale == 2:
            modules.append(EqualizedLRConvModule(in_channels, self._num_out_channels(log_scale - 1), **self.conv_module_cfg))
        else:
            if self.fused_upconv:
                cfg_ = dict(upsample=dict(type='fused_nn'))
                cfg_.update(self.fused_upconv_cfg)
            else:
                cfg_ = dict(upsample=self.upsample_cfg)
                cfg_.update(self.conv_module_cfg)
            modules.append(EqualizedLRConvUpModule(in_channels, self._num_out_channels(log_scale - 1), **cfg_))
            modules.append(EqualizedLRConvModule(self._num_out_channels(log_scale - 1), self._num_out_channels(log_scale - 1), **self.conv_module_cfg))
        return modules

    def forward(self, noise, label=None, num_batches=0, return_noise=False, transition_weight=1.0, curr_scale=-1):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            label (Tensor, optional): Label vector with shape [N, C]. Defaults
                to None.
            num_batches (int, optional): The number of batch size. Defaults to
                0.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            transition_weight (float, optional): The weight used in resolution
                transition. Defaults to 1.0.
            curr_scale (int, optional): The scale for the current inference or
                training. Defaults to -1.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output image
                will be returned. Otherwise, a dict contains ``fake_img`` and
                ``noise_batch`` will be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            assert noise.ndim == 2, f'The noise should be in shape of (n, c), but got {noise.shape}'
            noise_batch = noise
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size))
        noise_batch = noise_batch
        if label is not None:
            noise_batch = torch.cat([noise_batch, label], dim=1)
        x = self.noise2feat(noise_batch)
        curr_log2_scale = self.out_log2_scale if curr_scale < 0 else int(np.log2(curr_scale))
        x = self.conv_blocks[0](x)
        if curr_log2_scale <= 3:
            out_img = last_img = self.torgb_layers[0](x)
        for s in range(3, curr_log2_scale + 1):
            x = self.conv_blocks[2 * s - 5](x)
            x = self.conv_blocks[2 * s - 4](x)
            if s + 1 == curr_log2_scale:
                last_img = self.torgb_layers[s - 2](x)
            elif s == curr_log2_scale:
                out_img = self.torgb_layers[s - 2](x)
                residual_img = self.upsample_layer(last_img)
                out_img = residual_img + transition_weight * (out_img - residual_img)
        if return_noise:
            output = dict(fake_img=out_img, noise_batch=noise_batch, label=label)
            return output
        return out_img


class AllGatherLayer(autograd.Function):
    """All gather layer with backward propagation path.

    Indeed, this module is to make ``dist.all_gather()`` in the backward graph.
    Such kind of operation has been widely used in Moco and other contrastive
    learning algorithms.
    """

    @staticmethod
    def forward(ctx, x):
        """Forward function."""
        ctx.save_for_backward(x)
        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]
        dist.all_gather(output, x)
        return tuple(output)

    @staticmethod
    def backward(ctx, *grad_outputs):
        """Backward function."""
        x, = ctx.saved_tensors
        grad_out = torch.zeros_like(x)
        grad_out = grad_outputs[dist.get_rank()]
        return grad_out


class MiniBatchStddevLayer(nn.Module):
    """Minibatch standard deviation.

    Args:
        group_size (int, optional): The size of groups in batch dimension.
            Defaults to 4.
        eps (float, optional):  Epsilon value to avoid computation error.
            Defaults to 1e-8.
        gather_all_batch (bool, optional): Whether gather batch from all GPUs.
            Defaults to False.
    """

    def __init__(self, group_size=4, eps=1e-08, gather_all_batch=False):
        super().__init__()
        self.group_size = group_size
        self.eps = eps
        self.gather_all_batch = gather_all_batch
        if self.gather_all_batch:
            assert torch.distributed.is_initialized(), 'Only in distributed training can the tensors be all gathered.'

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        if self.gather_all_batch:
            x = torch.cat(AllGatherLayer.apply(x), dim=0)
        assert x.shape[0] <= self.group_size or x.shape[0] % self.group_size == 0, f'Batch size be smaller than or equal to group size. Otherwise, batch size should be divisible by the group size.But got batch size {x.shape[0]}, group size {self.group_size}'
        n, c, h, w = x.shape
        group_size = min(n, self.group_size)
        y = torch.reshape(x, (group_size, -1, c, h, w))
        y = y - y.mean(dim=0, keepdim=True)
        y = y.pow(2).mean(dim=0, keepdim=False)
        y = torch.sqrt(y + self.eps)
        y = y.mean(dim=(1, 2, 3), keepdim=True)
        y = y.repeat(group_size, 1, h, w)
        return torch.cat([x, y], dim=1)


class PGGANDecisionHead(nn.Module):

    def __init__(self, in_channels, mid_channels, out_channels, bias=True, equalized_lr_cfg=dict(gain=1), act_cfg=dict(type='LeakyReLU', negative_slope=0.2), out_act=None):
        super().__init__()
        self.in_channels = in_channels
        self.mid_channels = mid_channels
        self.out_channels = out_channels
        self.with_activation = act_cfg is not None
        self.with_out_activation = out_act is not None
        if equalized_lr_cfg:
            equalized_lr_cfg_ = dict(gain=2 ** 0.5)
        else:
            equalized_lr_cfg_ = None
        self.linear0 = EqualizedLRLinearModule(self.in_channels, self.mid_channels, bias=bias, equalized_lr_cfg=equalized_lr_cfg_)
        self.linear1 = EqualizedLRLinearModule(self.mid_channels, self.out_channels, bias=bias, equalized_lr_cfg=equalized_lr_cfg)
        if self.with_activation:
            self.activation = build_activation_layer(act_cfg)
        if self.with_out_activation:
            self.out_activation = build_activation_layer(out_act)

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        if x.ndim > 2:
            x = torch.reshape(x, (x.shape[0], -1))
        x = self.linear0(x)
        if self.with_activation:
            x = self.activation(x)
        x = self.linear1(x)
        if self.with_out_activation:
            x = self.out_activation(x)
        return x


class PGGANDiscriminator(nn.Module):
    """Discriminator for PGGAN.

    Args:
        in_scale (int): The scale of the input image.
        label_size (int, optional): Size of the label vector. Defaults to
            0.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this
            number. Defaults to 8192.
        max_channels (int, optional): Maximum channels for the feature
            maps in the discriminator block. Defaults to 512.
        in_channels (int, optional): Number of channels in input images.
            Defaults to 3.
        channel_decay (float, optional): Decay for channels of feature
            maps. Defaults to 1.0.
        mbstd_cfg (dict, optional): Configs for minibatch-stddev layer.
            Defaults to dict(group_size=4).
        fused_convdown (bool, optional): Whether use fused downconv.
            Defaults to True.
        conv_module_cfg (dict, optional): Config for the convolution
            module used in this generator. Defaults to None.
        fused_convdown_cfg (dict, optional): Config for the fused downconv
            module used in this discriminator. Defaults to None.
        fromrgb_layer_cfg (dict, optional): Config for the fromrgb layer.
            Defaults to None.
        downsample_cfg (dict, optional): Config for the downsampling
            operation. Defaults to None.
    """
    _default_fromrgb_cfg = dict(conv_cfg=None, kernel_size=1, stride=1, padding=0, bias=True, act_cfg=dict(type='LeakyReLU', negative_slope=0.2), norm_cfg=None, order=('conv', 'act', 'norm'))
    _default_conv_module_cfg = dict(kernel_size=3, padding=1, stride=1, norm_cfg=None, act_cfg=dict(type='LeakyReLU', negative_slope=0.2))
    _default_convdown_cfg = dict(kernel_size=3, padding=1, stride=2, norm_cfg=None, act_cfg=dict(type='LeakyReLU', negative_slope=0.2))

    def __init__(self, in_scale, label_size=0, base_channels=8192, max_channels=512, in_channels=3, channel_decay=1.0, mbstd_cfg=dict(group_size=4), fused_convdown=True, conv_module_cfg=None, fused_convdown_cfg=None, fromrgb_layer_cfg=None, downsample_cfg=None):
        super().__init__()
        self.in_scale = in_scale
        self.in_log2_scale = int(np.log2(self.in_scale))
        self.label_size = label_size
        self.base_channels = base_channels
        self.max_channels = max_channels
        self.in_channels = in_channels
        self.channel_decay = channel_decay
        self.with_mbstd = mbstd_cfg is not None
        self.fused_convdown = fused_convdown
        self.conv_module_cfg = deepcopy(self._default_conv_module_cfg)
        if conv_module_cfg is not None:
            self.conv_module_cfg.update(conv_module_cfg)
        if self.fused_convdown:
            self.fused_convdown_cfg = deepcopy(self._default_convdown_cfg)
            if fused_convdown_cfg is not None:
                self.fused_convdown_cfg.update(fused_convdown_cfg)
        self.fromrgb_layer_cfg = deepcopy(self._default_fromrgb_cfg)
        if fromrgb_layer_cfg:
            self.fromrgb_layer_cfg.update(fromrgb_layer_cfg)
        self.conv_blocks = nn.ModuleList()
        self.fromrgb_layers = nn.ModuleList()
        for s in range(2, self.in_log2_scale + 1):
            self.fromrgb_layers.append(self._get_fromrgb_layer(self.in_channels, s))
            self.conv_blocks.extend(self._get_convdown_block(self._num_out_channels(s - 1), s))
        self.downsample_cfg = deepcopy(downsample_cfg)
        if self.downsample_cfg is None or self.downsample_cfg.get('type', None) == 'avgpool':
            self.downsample = nn.AvgPool2d(kernel_size=2, stride=2)
        elif self.downsample_cfg.get('type', None) in ['nearest', 'bilinear']:
            self.downsample = partial(F.interpolate, mode=self.downsample_cfg.pop('type'), **self.downsample_cfg)
        else:
            raise NotImplementedError(f'We have not supported the downsampling with type {downsample_cfg}.')
        if self.with_mbstd:
            self.mbstd_layer = MiniBatchStddevLayer(**mbstd_cfg)
            decision_in_channels = self._num_out_channels(1) * 16 + 16
        else:
            decision_in_channels = self._num_out_channels(1) * 16
        self.decision = PGGANDecisionHead(decision_in_channels, self._num_out_channels(0), 1 + self.label_size)

    def _num_out_channels(self, log_scale):
        return min(int(self.base_channels / 2.0 ** (log_scale * self.channel_decay)), self.max_channels)

    def _get_fromrgb_layer(self, in_channels, log2_scale):
        return EqualizedLRConvModule(in_channels, self._num_out_channels(log2_scale - 1), **self.fromrgb_layer_cfg)

    def _get_convdown_block(self, in_channels, log2_scale):
        modules = []
        if log2_scale == 2:
            modules.append(EqualizedLRConvModule(in_channels, self._num_out_channels(log2_scale - 1), **self.conv_module_cfg))
        else:
            modules.append(EqualizedLRConvModule(in_channels, self._num_out_channels(log2_scale - 1), **self.conv_module_cfg))
            if self.fused_convdown:
                cfg_ = dict(downsample=dict(type='fused_pool'))
                cfg_.update(self.fused_convdown_cfg)
            else:
                cfg_ = dict(downsample=self.downsample)
                cfg_.update(self.conv_module_cfg)
            modules.append(EqualizedLRConvDownModule(self._num_out_channels(log2_scale - 1), self._num_out_channels(log2_scale - 2), **cfg_))
        return modules

    def forward(self, x, transition_weight=1.0, curr_scale=-1):
        """Forward function.

        Args:
            x (torch.Tensor): Input image tensor.
            transition_weight (float, optional): The weight used in resolution
                transition. Defaults to 1.0.
            curr_scale (int, optional): The scale for the current inference or
                training. Defaults to -1.

        Returns:
            Tensor: Predict score for the input image.
        """
        curr_log2_scale = self.in_log2_scale if curr_scale < 4 else int(np.log2(curr_scale))
        original_img = x
        x = self.fromrgb_layers[curr_log2_scale - 2](x)
        for s in range(curr_log2_scale, 2, -1):
            x = self.conv_blocks[2 * s - 5](x)
            x = self.conv_blocks[2 * s - 4](x)
            if s == curr_log2_scale:
                img_down = self.downsample(original_img)
                y = self.fromrgb_layers[curr_log2_scale - 3](img_down)
                x = y + transition_weight * (x - y)
        if self.with_mbstd:
            x = self.mbstd_layer(x)
        x = self.decision(x)
        if self.label_size > 0:
            return x[:, :1], x[:, 1:]
        return x


class PixelNorm(nn.Module):
    """Pixel Normalization.

    This module is proposed in:
    Progressive Growing of GANs for Improved Quality, Stability, and Variation

    Args:
        eps (float, optional): Epsilon value. Defaults to 1e-6.
    """
    _abbr_ = 'pn'

    def __init__(self, in_channels=None, eps=1e-06):
        super().__init__()
        self.eps = eps

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Tensor to be normalized.

        Returns:
            torch.Tensor: Normalized tensor.
        """
        return pixel_norm(x, self.eps)


class UnetSkipConnectionBlock(nn.Module):
    """Construct a Unet submodule with skip connections, with the following.

    structure: downsampling - `submodule` - upsampling.

    Args:
        outer_channels (int): Number of channels at the outer conv layer.
        inner_channels (int): Number of channels at the inner conv layer.
        in_channels (int): Number of channels in input images/features. If is
            None, equals to `outer_channels`. Default: None.
        submodule (UnetSkipConnectionBlock): Previously constructed submodule.
            Default: None.
        is_outermost (bool): Whether this module is the outermost module.
            Default: False.
        is_innermost (bool): Whether this module is the innermost module.
            Default: False.
        norm_cfg (dict): Config dict to build norm layer. Default:
            `dict(type='BN')`.
        use_dropout (bool): Whether to use dropout layers. Default: False.
    """

    def __init__(self, outer_channels, inner_channels, in_channels=None, submodule=None, is_outermost=False, is_innermost=False, norm_cfg=dict(type='BN'), use_dropout=False):
        super().__init__()
        assert not (is_outermost and is_innermost), "'is_outermost' and 'is_innermost' cannot be Trueat the same time."
        self.is_outermost = is_outermost
        assert isinstance(norm_cfg, dict), f"'norm_cfg' should be dict, butgot {type(norm_cfg)}"
        assert 'type' in norm_cfg, "'norm_cfg' must have key 'type'"
        use_bias = norm_cfg['type'] == 'IN'
        kernel_size = 4
        stride = 2
        padding = 1
        if in_channels is None:
            in_channels = outer_channels
        down_conv_cfg = dict(type='Conv2d')
        down_norm_cfg = norm_cfg
        down_act_cfg = dict(type='LeakyReLU', negative_slope=0.2)
        up_conv_cfg = dict(type='deconv')
        up_norm_cfg = norm_cfg
        up_act_cfg = dict(type='ReLU')
        up_in_channels = inner_channels * 2
        up_bias = use_bias
        middle = [submodule]
        upper = []
        if is_outermost:
            down_act_cfg = None
            down_norm_cfg = None
            up_bias = True
            up_norm_cfg = None
            upper = [nn.Tanh()]
        elif is_innermost:
            down_norm_cfg = None
            up_in_channels = inner_channels
            middle = []
        else:
            upper = [nn.Dropout(0.5)] if use_dropout else []
        down = [ConvModule(in_channels=in_channels, out_channels=inner_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=use_bias, conv_cfg=down_conv_cfg, norm_cfg=down_norm_cfg, act_cfg=down_act_cfg, order=('act', 'conv', 'norm'))]
        up = [ConvModule(in_channels=up_in_channels, out_channels=outer_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=up_bias, conv_cfg=up_conv_cfg, norm_cfg=up_norm_cfg, act_cfg=up_act_cfg, order=('act', 'conv', 'norm'))]
        model = down + middle + up + upper
        self.model = nn.Sequential(*model)

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        if self.is_outermost:
            return self.model(x)
        return torch.cat([x, self.model(x)], 1)


class UnetGenerator(nn.Module):
    """Construct the Unet-based generator from the innermost layer to the
    outermost layer, which is a recursive process.

    Args:
        in_channels (int): Number of channels in input images.
        out_channels (int): Number of channels in output images.
        num_down (int): Number of downsamplings in Unet. If `num_down` is 8,
            the image with size 256x256 will become 1x1 at the bottleneck.
            Default: 8.
        base_channels (int): Number of channels at the last conv layer.
            Default: 64.
        norm_cfg (dict): Config dict to build norm layer. Default:
            `dict(type='BN')`.
        use_dropout (bool): Whether to use dropout layers. Default: False.
        init_cfg (dict): Config dict for initialization.
            `type`: The name of our initialization method. Default: 'normal'.
            `gain`: Scaling factor for normal, xavier and orthogonal.
            Default: 0.02.
    """

    def __init__(self, in_channels, out_channels, num_down=8, base_channels=64, norm_cfg=dict(type='BN'), use_dropout=False, init_cfg=dict(type='normal', gain=0.02)):
        super().__init__()
        assert isinstance(norm_cfg, dict), f"'norm_cfg' should be dict, butgot {type(norm_cfg)}"
        assert 'type' in norm_cfg, "'norm_cfg' must have key 'type'"
        unet_block = UnetSkipConnectionBlock(base_channels * 8, base_channels * 8, in_channels=None, submodule=None, norm_cfg=norm_cfg, is_innermost=True)
        for _ in range(num_down - 5):
            unet_block = UnetSkipConnectionBlock(base_channels * 8, base_channels * 8, in_channels=None, submodule=unet_block, norm_cfg=norm_cfg, use_dropout=use_dropout)
        unet_block = UnetSkipConnectionBlock(base_channels * 4, base_channels * 8, in_channels=None, submodule=unet_block, norm_cfg=norm_cfg)
        unet_block = UnetSkipConnectionBlock(base_channels * 2, base_channels * 4, in_channels=None, submodule=unet_block, norm_cfg=norm_cfg)
        unet_block = UnetSkipConnectionBlock(base_channels, base_channels * 2, in_channels=None, submodule=unet_block, norm_cfg=norm_cfg)
        self.model = UnetSkipConnectionBlock(out_channels, base_channels, in_channels=in_channels, submodule=unet_block, is_outermost=True, norm_cfg=norm_cfg)
        self.init_type = 'normal' if init_cfg is None else init_cfg.get('type', 'normal')
        self.init_gain = 0.02 if init_cfg is None else init_cfg.get('gain', 0.02)

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        return self.model(x)

    def init_weights(self, pretrained=None, strict=True):
        """Initialize weights for the model.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Default: None.
            strict (bool, optional): Whether to allow different params for the
                model and checkpoint. Default: True.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=strict, logger=logger)
        elif pretrained is None:
            generation_init_weights(self, init_type=self.init_type, init_gain=self.init_gain)
        else:
            raise TypeError(f"'pretrained' must be a str or None. But received {type(pretrained)}.")


class PatchDiscriminator(nn.Module):
    """A PatchGAN discriminator.

    Args:
        in_channels (int): Number of channels in input images.
        base_channels (int): Number of channels at the first conv layer.
            Default: 64.
        num_conv (int): Number of stacked intermediate convs (excluding input
            and output conv). Default: 3.
        norm_cfg (dict): Config dict to build norm layer. Default:
            `dict(type='BN')`.
        init_cfg (dict): Config dict for initialization.
            `type`: The name of our initialization method. Default: 'normal'.
            `gain`: Scaling factor for normal, xavier and orthogonal.
            Default: 0.02.
    """

    def __init__(self, in_channels, base_channels=64, num_conv=3, norm_cfg=dict(type='BN'), init_cfg=dict(type='normal', gain=0.02)):
        super().__init__()
        assert isinstance(norm_cfg, dict), f"'norm_cfg' should be dict, butgot {type(norm_cfg)}"
        assert 'type' in norm_cfg, "'norm_cfg' must have key 'type'"
        use_bias = norm_cfg['type'] == 'IN'
        kernel_size = 4
        padding = 1
        sequence = [ConvModule(in_channels=in_channels, out_channels=base_channels, kernel_size=kernel_size, stride=2, padding=padding, bias=True, norm_cfg=None, act_cfg=dict(type='LeakyReLU', negative_slope=0.2))]
        multiple_now = 1
        multiple_prev = 1
        for n in range(1, num_conv):
            multiple_prev = multiple_now
            multiple_now = min(2 ** n, 8)
            sequence += [ConvModule(in_channels=base_channels * multiple_prev, out_channels=base_channels * multiple_now, kernel_size=kernel_size, stride=2, padding=padding, bias=use_bias, norm_cfg=norm_cfg, act_cfg=dict(type='LeakyReLU', negative_slope=0.2))]
        multiple_prev = multiple_now
        multiple_now = min(2 ** num_conv, 8)
        sequence += [ConvModule(in_channels=base_channels * multiple_prev, out_channels=base_channels * multiple_now, kernel_size=kernel_size, stride=1, padding=padding, bias=use_bias, norm_cfg=norm_cfg, act_cfg=dict(type='LeakyReLU', negative_slope=0.2))]
        sequence += [build_conv_layer(dict(type='Conv2d'), base_channels * multiple_now, 1, kernel_size=kernel_size, stride=1, padding=padding)]
        self.model = nn.Sequential(*sequence)
        self.init_type = 'normal' if init_cfg is None else init_cfg.get('type', 'normal')
        self.init_gain = 0.02 if init_cfg is None else init_cfg.get('gain', 0.02)

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        return self.model(x)

    def init_weights(self, pretrained=None):
        """Initialize weights for the model.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Default: None.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            generation_init_weights(self, init_type=self.init_type, init_gain=self.init_gain)
        else:
            raise TypeError(f"'pretrained' must be a str or None. But received {type(pretrained)}.")


class SinusoidalPositionalEmbedding(nn.Module):
    """Sinusoidal Positional Embedding 1D or 2D (SPE/SPE2d).

    This module is a modified from:
    https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sinusoidal_positional_embedding.py # noqa

    Based on the original SPE in single dimension, we implement a 2D sinusoidal
    positional encodding (SPE2d), as introduced in Positional Encoding as
    Spatial Inductive Bias in GANs, CVPR'2021.

    Args:
        embedding_dim (int): The number of dimensions for the positional
            encoding.
        padding_idx (int | list[int]): The index for the padding contents. The
            padding positions will obtain an encoding vector filling in zeros.
        init_size (int, optional): The initial size of the positional buffer.
            Defaults to 1024.
        div_half_dim (bool, optional): If true, the embedding will be divided
            by :math:`d/2`. Otherwise, it will be divided by
            :math:`(d/2 -1)`. Defaults to False.
        center_shift (int | None, optional): Shift the center point to some
            index. Defaults to None.
    """

    def __init__(self, embedding_dim, padding_idx, init_size=1024, div_half_dim=False, center_shift=None):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.padding_idx = padding_idx
        self.div_half_dim = div_half_dim
        self.center_shift = center_shift
        self.weights = SinusoidalPositionalEmbedding.get_embedding(init_size, embedding_dim, padding_idx, self.div_half_dim)
        self.register_buffer('_float_tensor', torch.FloatTensor(1))
        self.max_positions = int(100000.0)

    @staticmethod
    def get_embedding(num_embeddings, embedding_dim, padding_idx=None, div_half_dim=False):
        """Build sinusoidal embeddings.

        This matches the implementation in tensor2tensor, but differs slightly
        from the description in Section 3.5 of "Attention Is All You Need".
        """
        assert embedding_dim % 2 == 0, f'In this version, we request embedding_dim divisible by 2 but got {embedding_dim}'
        half_dim = embedding_dim // 2
        if not div_half_dim:
            emb = np.log(10000) / (half_dim - 1)
        else:
            emb = np.log(10000.0) / half_dim
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)
        if padding_idx is not None:
            emb[padding_idx, :] = 0
        return emb

    def forward(self, input, **kwargs):
        """Input is expected to be of size [bsz x seqlen].

        Returned tensor is expected to be of size  [bsz x seq_len x emb_dim]
        """
        assert input.dim() == 2 or input.dim() == 4, 'Input dimension should be 2 (1D) or 4(2D)'
        if input.dim() == 4:
            return self.make_grid2d_like(input, **kwargs)
        b, seq_len = input.shape
        max_pos = self.padding_idx + 1 + seq_len
        if self.weights is None or max_pos > self.weights.size(0):
            self.weights = SinusoidalPositionalEmbedding.get_embedding(max_pos, self.embedding_dim, self.padding_idx)
        self.weights = self.weights
        positions = self.make_positions(input, self.padding_idx)
        return self.weights.index_select(0, positions.view(-1)).view(b, seq_len, self.embedding_dim).detach()

    def make_positions(self, input, padding_idx):
        mask = input.ne(padding_idx).int()
        return (torch.cumsum(mask, dim=1).type_as(mask) * mask).long() + padding_idx

    def make_grid2d(self, height, width, num_batches=1, center_shift=None):
        h, w = height, width
        if center_shift is None:
            center_shift = self.center_shift
        h_shift = 0
        w_shift = 0
        if center_shift is not None:
            if h % 2 == 0:
                h_left_center = h // 2
                h_shift = center_shift - h_left_center
            else:
                h_center = h // 2 + 1
                h_shift = center_shift - h_center
            if w % 2 == 0:
                w_left_center = w // 2
                w_shift = center_shift - w_left_center
            else:
                w_center = w // 2 + 1
                w_shift = center_shift - w_center
        x_axis = torch.arange(1, w + 1).unsqueeze(0).repeat(num_batches, 1) + w_shift
        y_axis = torch.arange(1, h + 1).unsqueeze(0).repeat(num_batches, 1) + h_shift
        x_emb = self(x_axis).transpose(1, 2)
        y_emb = self(y_axis).transpose(1, 2)
        x_grid = x_emb.unsqueeze(2).repeat(1, 1, h, 1)
        y_grid = y_emb.unsqueeze(3).repeat(1, 1, 1, w)
        grid = torch.cat([x_grid, y_grid], dim=1)
        return grid.detach()

    def make_grid2d_like(self, x, center_shift=None):
        """Input tensor with shape of (b, ..., h, w) Return tensor with shape
        of (b, 2 x emb_dim, h, w)

        Note that the positional embedding highly depends on the the function,
        ``make_positions``.
        """
        h, w = x.shape[-2:]
        grid = self.make_grid2d(h, w, x.size(0), center_shift)
        return grid


class CatersianGrid(nn.Module):
    """Catersian Grid for 2d tensor.

    The Catersian Grid is a common-used positional encoding in deep learning.
    In this implementation, we follow the convention of ``grid_sample`` in
    PyTorch. In other words, ``[-1, -1]`` denotes the left-top corner while
    ``[1, 1]`` denotes the right-botton corner.
    """

    def forward(self, x, **kwargs):
        assert x.dim() == 4
        return self.make_grid2d_like(x, **kwargs)

    def make_grid2d(self, height, width, num_batches=1, requires_grad=False):
        h, w = height, width
        grid_y, grid_x = torch.meshgrid(torch.arange(0, h), torch.arange(0, w))
        grid_x = 2 * grid_x / max(float(w) - 1.0, 1.0) - 1.0
        grid_y = 2 * grid_y / max(float(h) - 1.0, 1.0) - 1.0
        grid = torch.stack((grid_x, grid_y), 0)
        grid.requires_grad = requires_grad
        grid = torch.unsqueeze(grid, 0)
        grid = grid.repeat(num_batches, 1, 1, 1)
        return grid

    def make_grid2d_like(self, x, requires_grad=False):
        h, w = x.shape[-2:]
        grid = self.make_grid2d(h, w, x.size(0), requires_grad=requires_grad)
        return grid


class GeneratorBlock(nn.Module):
    """Generator block used in SinGAN.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        num_scales (int): The number of scales/stages in generator. Note
            that this number is counted from zero, which is the same as the
            original paper.
        kernel_size (int, optional): Kernel size, same as :obj:`nn.Conv2d`.
            Defaults to 3.
        padding (int, optional): Padding for the convolutional layer, same as
            :obj:`nn.Conv2d`. Defaults to 0.
        num_layers (int, optional): The number of convolutional layers in each
            generator block. Defaults to 5.
        base_channels (int, optional): The basic channels for convolutional
            layers in the generator block. Defaults to 32.
        min_feat_channels (int, optional): Minimum channels for the feature
            maps in the generator block. Defaults to 32.
        out_act_cfg (dict | None, optional): Configs for output activation
            layer. Defaults to dict(type='Tanh').
        stride (int, optional): Same as :obj:`nn.Conv2d`. Defaults to 1.
        allow_no_residual (bool, optional): Whether to allow no residual link
            in this block. Defaults to False.
    """

    def __init__(self, in_channels, out_channels, kernel_size, padding, num_layers, base_channels, min_feat_channels, out_act_cfg=dict(type='Tanh'), stride=1, allow_no_residual=False, **kwargs):
        super().__init__()
        self.in_channels = in_channels
        self.base_channels = base_channels
        self.kernel_size = kernel_size
        self.num_layers = num_layers
        self.allow_no_residual = allow_no_residual
        self.head = ConvModule(in_channels=in_channels, out_channels=base_channels, kernel_size=kernel_size, padding=padding, stride=1, norm_cfg=dict(type='BN'), act_cfg=dict(type='LeakyReLU', negative_slope=0.2), **kwargs)
        self.body = nn.Sequential()
        for i in range(num_layers - 2):
            feat_channels_ = int(base_channels / pow(2, i + 1))
            block = ConvModule(max(2 * feat_channels_, min_feat_channels), max(feat_channels_, min_feat_channels), kernel_size=kernel_size, padding=padding, stride=stride, norm_cfg=dict(type='BN'), act_cfg=dict(type='LeakyReLU', negative_slope=0.2), **kwargs)
            self.body.add_module(f'block{i + 1}', block)
        self.tail = ConvModule(max(feat_channels_, min_feat_channels), out_channels, kernel_size=kernel_size, padding=padding, stride=1, norm_cfg=None, act_cfg=out_act_cfg, **kwargs)
        self.init_weights()

    def forward(self, x, prev):
        """Forward function.

        Args:
            x (Tensor): Input feature map.
            prev (Tensor): Previous feature map.

        Returns:
            Tensor: Output feature map with the shape of (N, C, H, W).
        """
        x = self.head(x)
        x = self.body(x)
        x = self.tail(x)
        if self.allow_no_residual and x.shape[1] != prev.shape[1]:
            return x
        return x + prev

    def init_weights(self, pretrained=None):
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, 0, 0.02)
                elif isinstance(m, (_BatchNorm, nn.InstanceNorm2d)):
                    constant_init(m, 1)
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class SinGANMultiScaleGenerator(nn.Module):
    """Multi-Scale Generator used in SinGAN.

    More details can be found in: Singan: Learning a Generative Model from a
    Single Natural Image, ICCV'19.

    Notes:

    - In this version, we adopt the interpolation function from the official
      PyTorch APIs, which is different from the original implementation by the
      authors. However, in our experiments, this influence can be ignored.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        num_scales (int): The number of scales/stages in generator. Note
            that this number is counted from zero, which is the same as the
            original paper.
        kernel_size (int, optional): Kernel size, same as :obj:`nn.Conv2d`.
            Defaults to 3.
        padding (int, optional): Padding for the convolutional layer, same as
            :obj:`nn.Conv2d`. Defaults to 0.
        num_layers (int, optional): The number of convolutional layers in each
            generator block. Defaults to 5.
        base_channels (int, optional): The basic channels for convolutional
            layers in the generator block. Defaults to 32.
        min_feat_channels (int, optional): Minimum channels for the feature
            maps in the generator block. Defaults to 32.
        out_act_cfg (dict | None, optional): Configs for output activation
            layer. Defaults to dict(type='Tanh').
    """

    def __init__(self, in_channels, out_channels, num_scales, kernel_size=3, padding=0, num_layers=5, base_channels=32, min_feat_channels=32, out_act_cfg=dict(type='Tanh'), **kwargs):
        super().__init__()
        self.pad_head = int((kernel_size - 1) / 2 * num_layers)
        self.blocks = nn.ModuleList()
        self.upsample = partial(F.interpolate, mode='bicubic', align_corners=True)
        for scale in range(num_scales + 1):
            base_ch = min(base_channels * pow(2, int(np.floor(scale / 4))), 128)
            min_feat_ch = min(min_feat_channels * pow(2, int(np.floor(scale / 4))), 128)
            self.blocks.append(GeneratorBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, num_layers=num_layers, base_channels=base_ch, min_feat_channels=min_feat_ch, out_act_cfg=out_act_cfg, **kwargs))
        self.noise_padding_layer = nn.ZeroPad2d(self.pad_head)
        self.img_padding_layer = nn.ZeroPad2d(self.pad_head)

    def forward(self, input_sample, fixed_noises, noise_weights, rand_mode, curr_scale, num_batches=1, get_prev_res=False, return_noise=False):
        """Forward function.

        Args:
            input_sample (Tensor | None): The input for generator. In the
                original implementation, a tensor filled with zeros is adopted.
                If None is given, we will construct it from the first fixed
                noises.
            fixed_noises (list[Tensor]): List of the fixed noises in SinGAN.
            noise_weights (list[float]): List of the weights for random noises.
            rand_mode (str): Choices from ['rand', 'recon']. In ``rand`` mode,
                it will sample from random noises. Otherwise, the
                reconstruction for the single image will be returned.
            curr_scale (int): The scale for the current inference or training.
            num_batches (int, optional): The number of batches. Defaults to 1.
            get_prev_res (bool, optional): Whether to return results from
                previous stages. Defaults to False.
            return_noise (bool, optional): Whether to return noises tensor.
                Defaults to False.

        Returns:
            Tensor | dict: Generated image tensor or dictionary containing                 more data.
        """
        if get_prev_res or return_noise:
            prev_res_list = []
            noise_list = []
        if input_sample is None:
            input_sample = torch.zeros((num_batches, 3, fixed_noises[0].shape[-2], fixed_noises[0].shape[-1]))
        g_res = input_sample
        for stage in range(curr_scale + 1):
            if rand_mode == 'recon':
                noise_ = fixed_noises[stage]
            else:
                noise_ = torch.randn(num_batches, *fixed_noises[stage].shape[1:])
            if return_noise:
                noise_list.append(noise_)
            pad_ = (self.pad_head,) * 4
            noise_ = F.pad(noise_, pad_)
            g_res_pad = F.pad(g_res, pad_)
            noise = noise_ * noise_weights[stage] + g_res_pad
            g_res = self.blocks[stage](noise.detach(), g_res)
            if get_prev_res and stage != curr_scale:
                prev_res_list.append(g_res)
            if stage != curr_scale:
                h_next, w_next = fixed_noises[stage + 1].shape[-2:]
                g_res = self.upsample(g_res, (h_next, w_next))
        if get_prev_res or return_noise:
            output_dict = dict(fake_img=g_res, prev_res_list=prev_res_list, noise_batch=noise_list)
            return output_dict
        return g_res

    def check_and_load_prev_weight(self, curr_scale):
        if curr_scale == 0:
            return
        prev_ch = self.blocks[curr_scale - 1].base_channels
        curr_ch = self.blocks[curr_scale].base_channels
        prev_in_ch = self.blocks[curr_scale - 1].in_channels
        curr_in_ch = self.blocks[curr_scale].in_channels
        if prev_ch == curr_ch and prev_in_ch == curr_in_ch:
            load_state_dict(self.blocks[curr_scale], self.blocks[curr_scale - 1].state_dict(), logger=get_root_logger())
            print_log('Successfully load pretrianed model from last scale.')
        else:
            print_log(f'Cannot load pretrained model from last scale since prev_ch({prev_ch}) != curr_ch({curr_ch}) or prev_in_ch({prev_in_ch}) != curr_in_ch({curr_in_ch})')


class DiscriminatorBlock(nn.Module):
    """Discriminator Block used in SinGAN.

    Args:
        in_channels (int): Input channels.
        base_channels (int): Base channels for this block.
        min_feat_channels (int): The minimum channels for feature map.
        kernel_size (int): Size of convolutional kernel, same as
            :obj:`nn.Conv2d`.
        padding (int): Padding for convolutional layer, same as
            :obj:`nn.Conv2d`.
        num_layers (int): The number of convolutional layers in this block.
        norm_cfg (dict | None, optional): Config for the normalization layer.
            Defaults to dict(type='BN').
        act_cfg (dict | None, optional): Config for the activation layer.
            Defaults to dict(type='LeakyReLU', negative_slope=0.2).
        stride (int, optional): The stride for the convolutional layer, same as
            :obj:`nn.Conv2d`. Defaults to 1.
    """

    def __init__(self, in_channels, base_channels, min_feat_channels, kernel_size, padding, num_layers, norm_cfg=dict(type='BN'), act_cfg=dict(type='LeakyReLU', negative_slope=0.2), stride=1, **kwargs):
        super().__init__()
        self.base_channels = base_channels
        self.stride = stride
        self.head = ConvModule(in_channels, base_channels, kernel_size=kernel_size, padding=padding, stride=1, norm_cfg=norm_cfg, act_cfg=act_cfg, **kwargs)
        self.body = nn.Sequential()
        for i in range(num_layers - 2):
            feat_channels_ = int(base_channels / pow(2, i + 1))
            block = ConvModule(max(2 * feat_channels_, min_feat_channels), max(feat_channels_, min_feat_channels), kernel_size=kernel_size, padding=padding, stride=stride, conv_cfg=None, norm_cfg=norm_cfg, act_cfg=act_cfg, **kwargs)
            self.body.add_module(f'block{i + 1}', block)
        self.tail = ConvModule(max(feat_channels_, min_feat_channels), 1, kernel_size=kernel_size, padding=padding, stride=1, norm_cfg=None, act_cfg=None, **kwargs)
        self.init_weights()

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input feature map with shape of (N, C, H, W).

        Returns:
            Tensor: Output feature map.
        """
        x = self.head(x)
        x = self.body(x)
        x = self.tail(x)
        return x

    def init_weights(self, pretrained=None):
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            for m in self.modules():
                if isinstance(m, nn.Conv2d):
                    normal_init(m, 0, 0.02)
                elif isinstance(m, (_BatchNorm, nn.InstanceNorm2d)):
                    constant_init(m, 1)
        else:
            raise TypeError(f'pretrained must be a str or None but got {type(pretrained)} instead.')


class SinGANMultiScaleDiscriminator(nn.Module):
    """Multi-Scale Discriminator used in SinGAN.

    More details can be found in: Singan: Learning a Generative Model from a
    Single Natural Image, ICCV'19.

    Args:
        in_channels (int): Input channels.
        num_scales (int): The number of scales/stages in generator. Note
            that this number is counted from zero, which is the same as the
            original paper.
        kernel_size (int, optional): Kernel size, same as :obj:`nn.Conv2d`.
            Defaults to 3.
        padding (int, optional): Padding for the convolutional layer, same as
            :obj:`nn.Conv2d`. Defaults to 0.
        num_layers (int, optional): The number of convolutional layers in each
            generator block. Defaults to 5.
        base_channels (int, optional): The basic channels for convolutional
            layers in the generator block. Defaults to 32.
        min_feat_channels (int, optional): Minimum channels for the feature
            maps in the generator block. Defaults to 32.
    """

    def __init__(self, in_channels, num_scales, kernel_size=3, padding=0, num_layers=5, base_channels=32, min_feat_channels=32, **kwargs):
        super().__init__()
        self.blocks = nn.ModuleList()
        for scale in range(num_scales + 1):
            base_ch = min(base_channels * pow(2, int(np.floor(scale / 4))), 128)
            min_feat_ch = min(min_feat_channels * pow(2, int(np.floor(scale / 4))), 128)
            self.blocks.append(DiscriminatorBlock(in_channels=in_channels, kernel_size=kernel_size, padding=padding, num_layers=num_layers, base_channels=base_ch, min_feat_channels=min_feat_ch, **kwargs))

    def forward(self, x, curr_scale):
        """Forward function.

        Args:
            x (Tensor): Input feature map.
            curr_scale (int): Current scale for discriminator. If in testing,
                you need to set it to the last scale.

        Returns:
            Tensor: Discriminative results.
        """
        out = self.blocks[curr_scale](x)
        return out

    def check_and_load_prev_weight(self, curr_scale):
        if curr_scale == 0:
            return
        prev_ch = self.blocks[curr_scale - 1].base_channels
        curr_ch = self.blocks[curr_scale].base_channels
        if prev_ch == curr_ch:
            self.blocks[curr_scale].load_state_dict(self.blocks[curr_scale - 1].state_dict())
            print_log('Successfully load pretrianed model from last scale.')
        else:
            print_log(f'Cannot load pretrained model from last scale since prev_ch({prev_ch}) != curr_ch({curr_ch})')


class SinGANMSGeneratorPE(SinGANMultiScaleGenerator):
    """Multi-Scale Generator used in SinGAN with positional encoding.

    More details can be found in: Positional Encoding as Spatial Inductvie Bias
    in GANs, CVPR'2021.

    Notes:

    - In this version, we adopt the interpolation function from the official
      PyTorch APIs, which is different from the original implementation by the
      authors. However, in our experiments, this influence can be ignored.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        num_scales (int): The number of scales/stages in generator. Note
            that this number is counted from zero, which is the same as the
            original paper.
        kernel_size (int, optional): Kernel size, same as :obj:`nn.Conv2d`.
            Defaults to 3.
        padding (int, optional): Padding for the convolutional layer, same as
            :obj:`nn.Conv2d`. Defaults to 0.
        num_layers (int, optional): The number of convolutional layers in each
            generator block. Defaults to 5.
        base_channels (int, optional): The basic channels for convolutional
            layers in the generator block. Defaults to 32.
        min_feat_channels (int, optional): Minimum channels for the feature
            maps in the generator block. Defaults to 32.
        out_act_cfg (dict | None, optional): Configs for output activation
            layer. Defaults to dict(type='Tanh').
        padding_mode (str, optional): The mode of convolutional padding, same
            as :obj:`nn.Conv2d`. Defaults to 'zero'.
        pad_at_head (bool, optional): Whether to add padding at head.
            Defaults to True.
        interp_pad (bool, optional): The padding value of interpolating feature
            maps. Defaults to False.
        noise_with_pad (bool, optional): Whether the input fixed noises are
            with explicit padding. Defaults to False.
        positional_encoding (dict | None, optional): Configs for the positional
            encoding. Defaults to None.
        first_stage_in_channels (int | None, optional): The input channel of
            the first generator block. If None, the first stage will adopt the
            same input channels as other stages. Defaults to None.
    """

    def __init__(self, in_channels, out_channels, num_scales, kernel_size=3, padding=0, num_layers=5, base_channels=32, min_feat_channels=32, out_act_cfg=dict(type='Tanh'), padding_mode='zero', pad_at_head=True, interp_pad=False, noise_with_pad=False, positional_encoding=None, first_stage_in_channels=None, **kwargs):
        super(SinGANMultiScaleGenerator, self).__init__()
        self.pad_at_head = pad_at_head
        self.interp_pad = interp_pad
        self.noise_with_pad = noise_with_pad
        self.with_positional_encode = positional_encoding is not None
        if self.with_positional_encode:
            self.head_position_encode = build_module(positional_encoding)
        self.pad_head = int((kernel_size - 1) / 2 * num_layers)
        self.blocks = nn.ModuleList()
        self.upsample = partial(F.interpolate, mode='bicubic', align_corners=True)
        for scale in range(num_scales + 1):
            base_ch = min(base_channels * pow(2, int(np.floor(scale / 4))), 128)
            min_feat_ch = min(min_feat_channels * pow(2, int(np.floor(scale / 4))), 128)
            if scale == 0:
                in_ch = first_stage_in_channels if first_stage_in_channels else in_channels
            else:
                in_ch = in_channels
            self.blocks.append(GeneratorBlock(in_channels=in_ch, out_channels=out_channels, kernel_size=kernel_size, padding=padding, num_layers=num_layers, base_channels=base_ch, min_feat_channels=min_feat_ch, out_act_cfg=out_act_cfg, padding_mode=padding_mode, **kwargs))
        if padding_mode == 'zero':
            self.noise_padding_layer = nn.ZeroPad2d(self.pad_head)
            self.img_padding_layer = nn.ZeroPad2d(self.pad_head)
            self.mask_padding_layer = nn.ReflectionPad2d(self.pad_head)
        elif padding_mode == 'reflect':
            self.noise_padding_layer = nn.ReflectionPad2d(self.pad_head)
            self.img_padding_layer = nn.ReflectionPad2d(self.pad_head)
            self.mask_padding_layer = nn.ReflectionPad2d(self.pad_head)
            mmcv.print_log('Using Reflection padding', 'mmgen')
        else:
            raise NotImplementedError(f'Padding mode {padding_mode} is not supported')

    def forward(self, input_sample, fixed_noises, noise_weights, rand_mode, curr_scale, num_batches=1, get_prev_res=False, return_noise=False):
        """Forward function.

        Args:
            input_sample (Tensor | None): The input for generator. In the
                original implementation, a tensor filled with zeros is adopted.
                If None is given, we will construct it from the first fixed
                noises.
            fixed_noises (list[Tensor]): List of the fixed noises in SinGAN.
            noise_weights (list[float]): List of the weights for random noises.
            rand_mode (str): Choices from ['rand', 'recon']. In ``rand`` mode,
                it will sample from random noises. Otherwise, the
                reconstruction for the single image will be returned.
            curr_scale (int): The scale for the current inference or training.
            num_batches (int, optional): The number of batches. Defaults to 1.
            get_prev_res (bool, optional): Whether to return results from
                previous stages. Defaults to False.
            return_noise (bool, optional): Whether to return noises tensor.
                Defaults to False.

        Returns:
            Tensor | dict: Generated image tensor or dictionary containing                 more data.
        """
        if get_prev_res or return_noise:
            prev_res_list = []
            noise_list = []
        if input_sample is None:
            input_sample = torch.zeros((num_batches, 3, fixed_noises[0].shape[-2], fixed_noises[0].shape[-1]))
        g_res = input_sample
        for stage in range(curr_scale + 1):
            if rand_mode == 'recon':
                noise_ = fixed_noises[stage]
            else:
                noise_ = torch.randn(num_batches, *fixed_noises[stage].shape[1:])
            if return_noise:
                noise_list.append(noise_)
            if self.with_positional_encode and stage == 0:
                head_grid = self.head_position_encode(fixed_noises[0])
                noise_ = noise_ + head_grid
            if self.pad_at_head:
                if self.interp_pad:
                    if self.noise_with_pad:
                        size = noise_.shape[-2:]
                    else:
                        size = noise_.size(2) + 2 * self.pad_head, noise_.size(3) + 2 * self.pad_head
                        noise_ = self.upsample(noise_, size)
                    g_res_pad = self.upsample(g_res, size)
                else:
                    if not self.noise_with_pad:
                        noise_ = self.noise_padding_layer(noise_)
                    g_res_pad = self.img_padding_layer(g_res)
            else:
                g_res_pad = g_res
            if stage == 0 and self.with_positional_encode:
                noise = noise_ * noise_weights[stage]
            else:
                noise = noise_ * noise_weights[stage] + g_res_pad
            g_res = self.blocks[stage](noise.detach(), g_res)
            if get_prev_res and stage != curr_scale:
                prev_res_list.append(g_res)
            if stage != curr_scale:
                h_next, w_next = fixed_noises[stage + 1].shape[-2:]
                if self.noise_with_pad:
                    h_next -= 2 * self.pad_head
                    w_next -= 2 * self.pad_head
                g_res = self.upsample(g_res, (h_next, w_next))
        if get_prev_res or return_noise:
            output_dict = dict(fake_img=g_res, prev_res_list=prev_res_list, noise_batch=noise_list)
            return output_dict
        return g_res


def check_dist_init():
    return dist.is_available() and dist.is_initialized()


class SNGANGenerator(nn.Module):
    """Generator for SNGAN / Proj-GAN. The implementation refers to
    https://github.com/pfnet-research/sngan_projection/tree/master/gen_models

    In our implementation, we have two notable design. Namely,
    ``channels_cfg`` and ``blocks_cfg``.

    ``channels_cfg``: In default config of SNGAN / Proj-GAN, the number of
        ResBlocks and the channels of those blocks are corresponding to the
        resolution of the output image. Therefore, we allow user to define
        ``channels_cfg`` to try their own models. We also provide a default
        config to allow users to build the model only from the output
        resolution.

    ``block_cfg``: In reference code, the generator consists of a group of
        ResBlock. However, in our implementation, to make this model more
        generalize, we support defining ``blocks_cfg`` by users and loading
        the blocks by calling the build_module method.

    Args:
        output_scale (int): Output scale for the generated image.
        num_classes (int, optional): The number classes you would like to
            generate. This arguments would influence the structure of the
            intermedia blocks and label sampling operation in ``forward``
            (e.g. If num_classes=0, ConditionalNormalization layers would
            degrade to unconditional ones.). This arguments would be passed
            to intermedia blocks by overwrite their config. Defaults to 0.
        base_channels (int, optional): The basic channel number of the
            generator. The other layers contains channels based on this number.
            Default to 64.
        out_channels (int, optional): Channels of the output images.
            Default to 3.
        input_scale (int, optional): Input scale for the features.
            Defaults to 4.
        noise_size (int, optional): Size of the input noise vector.
            Default to 128.
        attention_cfg (dict, optional): Config for the self-attention block.
            Default to ``dict(type='SelfAttentionBlock')``.
        attention_after_nth_block (int | list[int], optional): Self attention
            block would be added after which *ConvBlock*. If ``int`` is passed,
            only one attention block would be added. If ``list`` is passed,
            self-attention blocks would be added after multiple ConvBlocks.
            To be noted that if the input is smaller than ``1``,
            self-attention corresponding to this index would be ignored.
            Default to 0.
        channels_cfg (list | dict[list], optional): Config for input channels
            of the intermedia blocks. If list is passed, each element of the
            list means the input channels of current block is how many times
            compared to the ``base_channels``. For block ``i``, the input and
            output channels should be ``channels_cfg[i]`` and
            ``channels_cfg[i+1]`` If dict is provided, the key of the dict
            should be the output scale and corresponding value should be a list
            to define channels.  Default: Please refer to
            ``_defualt_channels_cfg``.
        blocks_cfg (dict, optional): Config for the intermedia blocks.
            Defaults to ``dict(type='SNGANGenResBlock')``
        act_cfg (dict, optional): Activation config for the final output
            layer. Defaults to ``dict(type='ReLU')``.
        use_cbn (bool, optional): Whether use conditional normalization. This
            argument would pass to norm layers. Defaults to True.
        auto_sync_bn (bool, optional): Whether convert Batch Norm to
            Synchronized ones when Distributed training is on. Defaults to
            True.
        with_spectral_norm (bool, optional): Whether use spectral norm for
            conv blocks or not. Default to False.
        with_embedding_spectral_norm (bool, optional): Whether use spectral
            norm for embedding layers in normalization blocks or not. If not
            specified (set as ``None``), ``with_embedding_spectral_norm`` would
            be set as the same value as ``with_spectral_norm``.
            Defaults to None.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `torch`.
        norm_eps (float, optional): eps for Normalization layers (both
            conditional and non-conditional ones). Default to `1e-4`.
        sn_eps (float, optional): eps for spectral normalization operation.
            Defaults to `1e-12`.
        init_cfg (string, optional): Config for weight initialization.
            Defaults to ``dict(type='BigGAN')``.
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict.  Defaults to None.
    """
    _default_channels_cfg = {(32): [1, 1, 1], (64): [16, 8, 4, 2], (128): [16, 16, 8, 4, 2]}

    def __init__(self, output_scale, num_classes=0, base_channels=64, out_channels=3, input_scale=4, noise_size=128, attention_cfg=dict(type='SelfAttentionBlock'), attention_after_nth_block=0, channels_cfg=None, blocks_cfg=dict(type='SNGANGenResBlock'), act_cfg=dict(type='ReLU'), use_cbn=True, auto_sync_bn=True, with_spectral_norm=False, with_embedding_spectral_norm=None, sn_style='torch', norm_eps=0.0001, sn_eps=1e-12, init_cfg=dict(type='BigGAN'), pretrained=None):
        super().__init__()
        self.input_scale = input_scale
        self.output_scale = output_scale
        self.noise_size = noise_size
        self.num_classes = num_classes
        self.init_type = init_cfg.get('type', None)
        self.blocks_cfg = deepcopy(blocks_cfg)
        self.blocks_cfg.setdefault('num_classes', num_classes)
        self.blocks_cfg.setdefault('act_cfg', act_cfg)
        self.blocks_cfg.setdefault('use_cbn', use_cbn)
        self.blocks_cfg.setdefault('auto_sync_bn', auto_sync_bn)
        self.blocks_cfg.setdefault('with_spectral_norm', with_spectral_norm)
        with_embedding_spectral_norm = with_embedding_spectral_norm if with_embedding_spectral_norm is not None else with_spectral_norm
        self.blocks_cfg.setdefault('with_embedding_spectral_norm', with_embedding_spectral_norm)
        self.blocks_cfg.setdefault('init_cfg', init_cfg)
        self.blocks_cfg.setdefault('sn_style', sn_style)
        self.blocks_cfg.setdefault('norm_eps', norm_eps)
        self.blocks_cfg.setdefault('sn_eps', sn_eps)
        channels_cfg = deepcopy(self._default_channels_cfg) if channels_cfg is None else deepcopy(channels_cfg)
        if isinstance(channels_cfg, dict):
            if output_scale not in channels_cfg:
                raise KeyError(f'`output_scale={output_scale} is not found in `channel_cfg`, only support configs for {[chn for chn in channels_cfg.keys()]}')
            self.channel_factor_list = channels_cfg[output_scale]
        elif isinstance(channels_cfg, list):
            self.channel_factor_list = channels_cfg
        else:
            raise ValueError(f'Only support list or dict for `channel_cfg`, receive {type(channels_cfg)}')
        self.noise2feat = nn.Linear(noise_size, input_scale ** 2 * base_channels * self.channel_factor_list[0])
        if with_spectral_norm:
            self.noise2feat = spectral_norm(self.noise2feat)
        if not isinstance(attention_after_nth_block, list):
            attention_after_nth_block = [attention_after_nth_block]
        if not is_list_of(attention_after_nth_block, int):
            raise ValueError('`attention_after_nth_block` only support int or a list of int. Please check your input type.')
        self.conv_blocks = nn.ModuleList()
        self.attention_block_idx = []
        for idx in range(len(self.channel_factor_list)):
            factor_input = self.channel_factor_list[idx]
            factor_output = self.channel_factor_list[idx + 1] if idx < len(self.channel_factor_list) - 1 else 1
            block_cfg_ = deepcopy(self.blocks_cfg)
            block_cfg_['in_channels'] = factor_input * base_channels
            block_cfg_['out_channels'] = factor_output * base_channels
            self.conv_blocks.append(build_module(block_cfg_))
            if idx + 1 in attention_after_nth_block:
                self.attention_block_idx.append(len(self.conv_blocks))
                attn_cfg_ = deepcopy(attention_cfg)
                attn_cfg_['in_channels'] = factor_output * base_channels
                attn_cfg_['sn_style'] = sn_style
                self.conv_blocks.append(build_module(attn_cfg_))
        to_rgb_norm_cfg = dict(type='BN', eps=norm_eps)
        if check_dist_init() and auto_sync_bn:
            to_rgb_norm_cfg['type'] = 'SyncBN'
        self.to_rgb = ConvModule(factor_output * base_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, norm_cfg=to_rgb_norm_cfg, act_cfg=act_cfg, order=('norm', 'act', 'conv'), with_spectral_norm=with_spectral_norm)
        self.final_act = build_activation_layer(dict(type='Tanh'))
        self.init_weights(pretrained)

    def forward(self, noise, num_batches=0, label=None, return_noise=False):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            label (torch.Tensor | callable | None): You can directly give a
                batch of label through a ``torch.Tensor`` or offer a callable
                function to sample a batch of label data. Otherwise, the
                ``None`` indicates to use the default label sampler.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output
                image will be returned. Otherwise, a dict contains
                ``fake_image``, ``noise_batch`` and ``label_batch``
                would be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            assert noise.ndim == 2, f'The noise should be in shape of (n, c), but got {noise.shape}'
            noise_batch = noise
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size))
        if isinstance(label, torch.Tensor):
            assert label.ndim == 1, f'The label shoube be in shape of (n, )but got {label.shape}.'
            label_batch = label
        elif callable(label):
            label_generator = label
            assert num_batches > 0
            label_batch = label_generator(num_batches)
        elif self.num_classes == 0:
            label_batch = None
        else:
            assert num_batches > 0
            label_batch = torch.randint(0, self.num_classes, (num_batches,))
        noise_batch = noise_batch
        if label_batch is not None:
            label_batch = label_batch
        x = self.noise2feat(noise_batch)
        x = x.reshape(x.size(0), -1, self.input_scale, self.input_scale)
        for idx, conv_block in enumerate(self.conv_blocks):
            if idx in self.attention_block_idx:
                x = conv_block(x)
            else:
                x = conv_block(x, label_batch)
        out_feat = self.to_rgb(x)
        out_img = self.final_act(out_feat)
        if return_noise:
            return dict(fake_img=out_img, noise_batch=noise_batch, label=label_batch)
        return out_img

    def init_weights(self, pretrained=None, strict=True):
        """Init weights for SNGAN-Proj and SAGAN. If ``pretrained=None``,
        weight initialization would follow the ``INIT_TYPE`` in
        ``init_cfg=dict(type=INIT_TYPE)``.

        For SNGAN-Proj,
        (``INIT_TYPE.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']``),
        we follow the initialization method in the official Chainer's
        implementation (https://github.com/pfnet-research/sngan_projection).

        For SAGAN (``INIT_TYPE.upper() == 'SAGAN'``), we follow the
        initialization method in official tensorflow's implementation
        (https://github.com/brain-research/self-attention-gan).

        Besides the reimplementation of the official code's initialization, we
        provide BigGAN's and Pytorch-StudioGAN's style initialization
        (``INIT_TYPE.upper() == BIGGAN`` and ``INIT_TYPE.upper() == STUDIO``).
        Please refer to https://github.com/ajbrock/BigGAN-PyTorch and
        https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.

        Args:
            pretrained (str | dict, optional): Path for the pretrained model or
                dict containing information for pretained models whose
                necessary key is 'ckpt_path'. Besides, you can also provide
                'prefix' to load the generator part from the whole state dict.
                Defaults to None.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=strict, logger=logger)
        elif isinstance(pretrained, dict):
            ckpt_path = pretrained.get('ckpt_path', None)
            assert ckpt_path is not None
            prefix = pretrained.get('prefix', '')
            map_location = pretrained.get('map_location', 'cpu')
            strict = pretrained.get('strict', True)
            state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
            self.load_state_dict(state_dict, strict=strict)
        elif pretrained is None:
            if self.init_type.upper() in 'STUDIO':
                for m in self.modules():
                    if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                        nn.init.orthogonal_(m.weight)
                        if hasattr(m, 'bias') and m.bias is not None:
                            m.bias.data.fill_(0.0)
            elif self.init_type.upper() == 'BIGGAN':
                for n, m in self.named_modules():
                    if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                        xavier_uniform_(m.weight, gain=1)
            elif self.init_type.upper() == 'SAGAN':
                for n, m in self.named_modules():
                    if isinstance(m, (nn.Conv2d, nn.Linear)):
                        xavier_init(m, gain=1, distribution='uniform')
                    if isinstance(m, nn.Embedding):
                        if 'weight' in n:
                            constant_init(m, 1)
                        if 'bias' in n:
                            constant_init(m, 0)
            elif self.init_type.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']:
                for n, m in self.named_modules():
                    if isinstance(m, nn.Conv2d):
                        if 'shortcut' in n or 'to_rgb' in n:
                            xavier_init(m, gain=1, distribution='uniform')
                        else:
                            xavier_init(m, gain=np.sqrt(2), distribution='uniform')
                    if isinstance(m, nn.Linear):
                        xavier_init(m, gain=1, distribution='uniform')
                    if isinstance(m, nn.Embedding):
                        if 'weight' in n:
                            constant_init(m, 1)
                        if 'bias' in n:
                            constant_init(m, 0)
            else:
                raise NotImplementedError(f"Unknown initialization method: '{self.init_type}'")
        else:
            raise TypeError(f"'pretrined' must be a str or None. But receive {type(pretrained)}.")


class ProjDiscriminator(nn.Module):
    """Discriminator for SNGAN / Proj-GAN. The implementation is refer to
    https://github.com/pfnet-research/sngan_projection/tree/master/dis_models

    The overall structure of the projection discriminator can be split into a
    ``from_rgb`` layer, a group of ResBlocks, a linear decision layer, and a
    projection layer. To support defining custom layers, we introduce
    ``from_rgb_cfg`` and ``blocks_cfg``.

    The design of the model structure is highly corresponding to the output
    resolution. Therefore, we provide `channels_cfg` and `downsample_cfg` to
    control the input channels and the downsample behavior of the intermedia
    blocks.

    ``downsample_cfg``: In default config of SNGAN / Proj-GAN, whether to apply
        downsample in each intermedia blocks is quite flexible and
        corresponding to the resolution of the output image. Therefore, we
        support user to define the ``downsample_cfg`` by themselves, and to
        control the structure of the discriminator.

    ``channels_cfg``: In default config of SNGAN / Proj-GAN, the number of
        ResBlocks and the channels of those blocks are corresponding to the
        resolution of the output image. Therefore, we allow user to define
        `channels_cfg` for try their own models.  We also provide a default
        config to allow users to build the model only from the output
        resolution.

    Args:
        input_scale (int): The scale of the input image.
        num_classes (int, optional): The number classes you would like to
            generate. If num_classes=0, no label projection would be used.
            Default to 0.
        base_channels (int, optional): The basic channel number of the
            discriminator. The other layers contains channels based on this
            number.  Defaults to 128.
        input_channels (int, optional): Channels of the input image.
            Defaults to 3.
        attention_cfg (dict, optional): Config for the self-attention block.
            Default to ``dict(type='SelfAttentionBlock')``.
        attention_after_nth_block (int | list[int], optional): Self-attention
            block would be added after which *ConvBlock* (including the head
            block). If ``int`` is passed, only one attention block would be
            added. If ``list`` is passed, self-attention blocks would be added
            after multiple ConvBlocks. To be noted that if the input is
            smaller than ``1``, self-attention corresponding to this index
            would be ignored. Default to 0.
        channels_cfg (list | dict[list], optional): Config for input channels
            of the intermedia blocks. If list is passed, each element of the
            list means the input channels of current block is how many times
            compared to the ``base_channels``. For block ``i``, the input and
            output channels should be ``channels_cfg[i]`` and
            ``channels_cfg[i+1]`` If dict is provided, the key of the dict
            should be the output scale and corresponding value should be a list
            to define channels.  Default: Please refer to
            ``_defualt_channels_cfg``.
        downsample_cfg (list[bool] | dict[list], optional): Config for
            downsample behavior of the intermedia layers. If a list is passed,
            ``downsample_cfg[idx] == True`` means apply downsample in idx-th
            block, and vice versa. If dict is provided, the key dict should
            be the input scale of the image and corresponding value should be
            a list ti define the downsample behavior. Default: Please refer
            to ``_default_downsample_cfg``.
        from_rgb_cfg (dict, optional): Config for the first layer to convert
            rgb image to feature map. Defaults to
            ``dict(type='SNGANDiscHeadResBlock')``.
        blocks_cfg (dict, optional): Config for the intermedia blocks.
            Defaults to ``dict(type='SNGANDiscResBlock')``
        act_cfg (dict, optional): Activation config for the final output
            layer. Defaults to ``dict(type='ReLU')``.
        with_spectral_norm (bool, optional): Whether use spectral norm for
            all conv blocks or not. Default to True.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `torch`.
        sn_eps (float, optional): eps for spectral normalization operation.
            Defaults to `1e-12`.
        init_cfg (dict, optional): Config for weight initialization.
            Default to ``dict(type='BigGAN')``.
        pretrained (str | dict , optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict.  Defaults to None.
    """
    _defualt_channels_cfg = {(32): [1, 1, 1], (64): [2, 4, 8, 16], (128): [2, 4, 8, 16, 16]}
    _defualt_downsample_cfg = {(32): [True, False, False], (64): [True, True, True, True], (128): [True, True, True, True, False]}

    def __init__(self, input_scale, num_classes=0, base_channels=128, input_channels=3, attention_cfg=dict(type='SelfAttentionBlock'), attention_after_nth_block=-1, channels_cfg=None, downsample_cfg=None, from_rgb_cfg=dict(type='SNGANDiscHeadResBlock'), blocks_cfg=dict(type='SNGANDiscResBlock'), act_cfg=dict(type='ReLU'), with_spectral_norm=True, sn_style='torch', sn_eps=1e-12, init_cfg=dict(type='BigGAN'), pretrained=None):
        super().__init__()
        self.init_type = init_cfg.get('type', None)
        self.from_rgb_cfg = deepcopy(from_rgb_cfg)
        self.from_rgb_cfg.setdefault('act_cfg', act_cfg)
        self.from_rgb_cfg.setdefault('with_spectral_norm', with_spectral_norm)
        self.from_rgb_cfg.setdefault('sn_style', sn_style)
        self.from_rgb_cfg.setdefault('init_cfg', init_cfg)
        self.blocks_cfg = deepcopy(blocks_cfg)
        self.blocks_cfg.setdefault('act_cfg', act_cfg)
        self.blocks_cfg.setdefault('with_spectral_norm', with_spectral_norm)
        self.blocks_cfg.setdefault('sn_style', sn_style)
        self.blocks_cfg.setdefault('sn_eps', sn_eps)
        self.blocks_cfg.setdefault('init_cfg', init_cfg)
        channels_cfg = deepcopy(self._defualt_channels_cfg) if channels_cfg is None else deepcopy(channels_cfg)
        if isinstance(channels_cfg, dict):
            if input_scale not in channels_cfg:
                raise KeyError(f'`input_scale={input_scale} is not found in `channel_cfg`, only support configs for {[chn for chn in channels_cfg.keys()]}')
            self.channel_factor_list = channels_cfg[input_scale]
        elif isinstance(channels_cfg, list):
            self.channel_factor_list = channels_cfg
        else:
            raise ValueError(f'Only support list or dict for `channel_cfg`, receive {type(channels_cfg)}')
        downsample_cfg = deepcopy(self._defualt_downsample_cfg) if downsample_cfg is None else deepcopy(downsample_cfg)
        if isinstance(downsample_cfg, dict):
            if input_scale not in downsample_cfg:
                raise KeyError(f'`output_scale={input_scale} is not found in `downsample_cfg`, only support configs for {[chn for chn in downsample_cfg.keys()]}')
            self.downsample_list = downsample_cfg[input_scale]
        elif isinstance(downsample_cfg, list):
            self.downsample_list = downsample_cfg
        else:
            raise ValueError(f'Only support list or dict for `channel_cfg`, receive {type(downsample_cfg)}')
        if len(self.downsample_list) != len(self.channel_factor_list):
            raise ValueError(f'`downsample_cfg` should have same length with `channels_cfg`, but receive {len(self.downsample_list)} and {len(self.channel_factor_list)}.')
        if not isinstance(attention_after_nth_block, list):
            attention_after_nth_block = [attention_after_nth_block]
        if not all([isinstance(idx, int) for idx in attention_after_nth_block]):
            raise ValueError('`attention_after_nth_block` only support int or a list of int. Please check your input type.')
        self.from_rgb = build_module(self.from_rgb_cfg, dict(in_channels=input_channels, out_channels=base_channels))
        self.conv_blocks = nn.ModuleList()
        if 1 in attention_after_nth_block:
            attn_cfg_ = deepcopy(attention_cfg)
            attn_cfg_['in_channels'] = base_channels
            attn_cfg_['sn_style'] = sn_style
            self.conv_blocks.append(build_module(attn_cfg_))
        for idx in range(len(self.downsample_list)):
            factor_input = 1 if idx == 0 else self.channel_factor_list[idx - 1]
            factor_output = self.channel_factor_list[idx]
            block_cfg_ = deepcopy(self.blocks_cfg)
            block_cfg_['downsample'] = self.downsample_list[idx]
            block_cfg_['in_channels'] = factor_input * base_channels
            block_cfg_['out_channels'] = factor_output * base_channels
            self.conv_blocks.append(build_module(block_cfg_))
            if idx + 2 in attention_after_nth_block:
                attn_cfg_ = deepcopy(attention_cfg)
                attn_cfg_['in_channels'] = factor_output * base_channels
                self.conv_blocks.append(build_module(attn_cfg_))
        self.decision = nn.Linear(factor_output * base_channels, 1)
        if with_spectral_norm:
            self.decision = spectral_norm(self.decision)
        self.num_classes = num_classes
        if num_classes > 0:
            self.proj_y = nn.Embedding(num_classes, factor_output * base_channels)
            if with_spectral_norm:
                self.proj_y = spectral_norm(self.proj_y)
        self.activate = build_activation_layer(act_cfg)
        self.init_weights(pretrained)

    def forward(self, x, label=None):
        """Forward function. If `self.num_classes` is larger than 0, label
        projection would be used.

        Args:
            x (torch.Tensor): Fake or real image tensor.
            label (torch.Tensor, options): Label correspond to the input image.
                Noted that, if `self.num_classed` is larger than 0,
                `label` should not be None.  Default to None.

        Returns:
            torch.Tensor: Prediction for the reality of the input image.
        """
        h = self.from_rgb(x)
        for conv_block in self.conv_blocks:
            h = conv_block(h)
        h = self.activate(h)
        h = torch.sum(h, dim=[2, 3])
        out = self.decision(h)
        if self.num_classes > 0:
            w_y = self.proj_y(label)
            out = out + torch.sum(w_y * h, dim=1, keepdim=True)
        return out.view(out.size(0), -1)

    def init_weights(self, pretrained=None, strict=True):
        """Init weights for SNGAN-Proj and SAGAN. If ``pretrained=None`` and
        weight initialization would follow the ``INIT_TYPE`` in
        ``init_cfg=dict(type=INIT_TYPE)``.

        For SNGAN-Proj
        (``INIT_TYPE.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']``),
        we follow the initialization method in the official Chainer's
        implementation (https://github.com/pfnet-research/sngan_projection).

        For SAGAN (``INIT_TYPE.upper() == 'SAGAN'``), we follow the
        initialization method in official tensorflow's implementation
        (https://github.com/brain-research/self-attention-gan).

        Besides the reimplementation of the official code's initialization, we
        provide BigGAN's and Pytorch-StudioGAN's style initialization
        (``INIT_TYPE.upper() == BIGGAN`` and ``INIT_TYPE.upper() == STUDIO``).
        Please refer to https://github.com/ajbrock/BigGAN-PyTorch and
        https://github.com/POSTECH-CVLab/PyTorch-StudioGAN.

        Args:
            pretrained (str | dict, optional): Path for the pretrained model or
                dict containing information for pretained models whose
                necessary key is 'ckpt_path'. Besides, you can also provide
                'prefix' to load the generator part from the whole state dict.
                Defaults to None.
        """
        if isinstance(pretrained, str):
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=strict, logger=logger)
        elif isinstance(pretrained, dict):
            ckpt_path = pretrained.get('ckpt_path', None)
            assert ckpt_path is not None
            prefix = pretrained.get('prefix', '')
            map_location = pretrained.get('map_location', 'cpu')
            strict = pretrained.get('strict', True)
            state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
            self.load_state_dict(state_dict, strict=strict)
        elif pretrained is None:
            if self.init_type.upper() == 'STUDIO':
                for m in self.modules():
                    if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                        nn.init.orthogonal_(m.weight, gain=1)
                        if hasattr(m, 'bias') and m.bias is not None:
                            m.bias.data.fill_(0.0)
            elif self.init_type.upper() == 'BIGGAN':
                for m in self.modules():
                    if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                        xavier_uniform_(m.weight, gain=1)
            elif self.init_type.upper() == 'SAGAN':
                for m in self.modules():
                    if isinstance(m, (nn.Conv2d, nn.Linear, nn.Embedding)):
                        xavier_init(m, gain=1, distribution='uniform')
            elif self.init_type.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']:
                for n, m in self.named_modules():
                    if isinstance(m, nn.Conv2d):
                        if 'shortcut' in n:
                            xavier_init(m, gain=1, distribution='uniform')
                        else:
                            xavier_init(m, gain=np.sqrt(2), distribution='uniform')
                    if isinstance(m, (nn.Linear, nn.Embedding)):
                        xavier_init(m, gain=1, distribution='uniform')
            else:
                raise NotImplementedError(f"Unknown initialization method: '{self.init_type}'")
        else:
            raise TypeError(f"'pretrained' must by a str or None. But receive {type(pretrained)}.")


class SNConditionNorm(nn.Module):
    """Conditional Normalization for SNGAN / Proj-GAN. The implementation
    refers to.

    https://github.com/pfnet-research/sngan_projection/blob/master/source/links/conditional_batch_normalization.py  # noda

    and

    https://github.com/POSTECH-CVLab/PyTorch-StudioGAN/blob/master/src/utils/model_ops.py  # noqa

    Args:
        in_channels (int): Number of the channels of the input feature map.
        num_classes (int): Number of the classes in the dataset. If ``use_cbn``
            is True, ``num_classes`` must larger than 0.
        use_cbn (bool, optional): Whether use conditional normalization. If
            ``use_cbn`` is True, two embedding layers would be used to mapping
            label to weight and bias used in normalization process.
        norm_cfg (dict, optional): Config for normalization method. Defaults
            to ``dict(type='BN')``.
        cbn_norm_affine (bool):  Whether set ``affine=True`` when use conditional batch norm.
            This argument only work when ``use_cbn`` is True. Defaults to False.
        auto_sync_bn (bool, optional): Whether convert Batch Norm to
            Synchronized ones when Distributed training is on. Defaults to True.
        with_spectral_norm (bool, optional): whether use spectral norm for
            conv blocks and norm layers. Defaults to true.
        norm_eps (float, optional): eps for Normalization layers (both
            conditional and non-conditional ones). Defaults to `1e-4`.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `torch`.
        sn_eps (float, optional): eps for spectral normalization operation.
            Defaults to `1e-12`.
        init_cfg (dict, optional): Config for weight initialization.
            Defaults to ``dict(type='BigGAN')``.
    """

    def __init__(self, in_channels, num_classes, use_cbn=True, norm_cfg=dict(type='BN'), cbn_norm_affine=False, auto_sync_bn=True, with_spectral_norm=False, sn_style='torch', norm_eps=0.0001, sn_eps=1e-12, init_cfg=dict(type='BigGAN')):
        super().__init__()
        self.use_cbn = use_cbn
        self.init_type = init_cfg.get('type', None)
        norm_cfg = deepcopy(norm_cfg)
        norm_type = norm_cfg['type']
        if norm_type not in ['IN', 'BN', 'SyncBN']:
            raise ValueError(f'Only support `IN` (InstanceNorm), `BN` (BatcnNorm) and `SyncBN` for Class-conditional bn. Receive norm_type: {norm_type}')
        if self.use_cbn:
            norm_cfg.setdefault('affine', cbn_norm_affine)
        norm_cfg.setdefault('eps', norm_eps)
        if check_dist_init() and auto_sync_bn and norm_type == 'BN':
            norm_cfg['type'] = 'SyncBN'
        _, self.norm = build_norm_layer(norm_cfg, in_channels)
        if self.use_cbn:
            if num_classes <= 0:
                raise ValueError('`num_classes` must be larger than 0 with `use_cbn=True`')
            self.reweight_embedding = self.init_type.upper() == 'BIGGAN' or self.init_type.upper() == 'STUDIO'
            if with_spectral_norm:
                if sn_style == 'torch':
                    self.weight_embedding = spectral_norm(nn.Embedding(num_classes, in_channels), eps=sn_eps)
                    self.bias_embedding = spectral_norm(nn.Embedding(num_classes, in_channels), eps=sn_eps)
                elif sn_style == 'ajbrock':
                    self.weight_embedding = SNEmbedding(num_classes, in_channels, eps=sn_eps)
                    self.bias_embedding = SNEmbedding(num_classes, in_channels, eps=sn_eps)
                else:
                    raise NotImplementedError(f'{sn_style} style spectral Norm is not supported yet')
            else:
                self.weight_embedding = nn.Embedding(num_classes, in_channels)
                self.bias_embedding = nn.Embedding(num_classes, in_channels)
        self.init_weights()

    def forward(self, x, y=None):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).
            y (Tensor, optional): Input label with shape (n, ).
                Default None.

        Returns:
            Tensor: Forward results.
        """
        out = self.norm(x)
        if self.use_cbn:
            weight = self.weight_embedding(y)[:, :, None, None]
            bias = self.bias_embedding(y)[:, :, None, None]
            if self.reweight_embedding:
                weight = weight + 1.0
            out = out * weight + bias
        return out

    def init_weights(self):
        if self.use_cbn:
            if self.init_type.upper() == 'STUDIO':
                nn.init.orthogonal_(self.weight_embedding.weight)
                nn.init.orthogonal_(self.bias_embedding.weight)
            elif self.init_type.upper() == 'BIGGAN':
                xavier_uniform_(self.weight_embedding.weight, gain=1)
                xavier_uniform_(self.bias_embedding.weight, gain=1)
            elif self.init_type.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ', 'SAGAN']:
                constant_init(self.weight_embedding, 1)
                constant_init(self.bias_embedding, 0)
            else:
                raise NotImplementedError(f"Unknown initialization method: '{self.init_type}'")


class SNGANGenResBlock(nn.Module):
    """ResBlock used in Generator of SNGAN / Proj-GAN.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        hidden_channels (int, optional): Input channels of the second Conv
            layer of the block. If ``None`` is given, would be set as
            ``out_channels``. Default to None.
        num_classes (int, optional): Number of classes would like to generate.
            This argument would pass to norm layers and influence the structure
            and behavior of the normalization process. Default to 0.
        use_cbn (bool, optional): Whether use conditional normalization. This
            argument would pass to norm layers. Default to True.
        use_norm_affine (bool, optional): Whether use learnable affine
            parameters in norm operation when cbn is off. Default False.
        act_cfg (dict, optional): Config for activate function. Default
            to ``dict(type='ReLU')``.
        upsample_cfg (dict, optional): Config for the upsample method.
            Default to ``dict(type='nearest', scale_factor=2)``.
        upsample (bool, optional): Whether apply upsample operation in this
            module. Default to True.
        auto_sync_bn (bool, optional): Whether convert Batch Norm to
            Synchronized ones when Distributed training is on. Default to True.
        conv_cfg (dict | None): Config for conv blocks of this module. If pass
            ``None``, would use ``_default_conv_cfg``. Default to ``None``.
        with_spectral_norm (bool, optional): Whether use spectral norm for
            conv blocks and norm layers. Default to True.
        with_embedding_spectral_norm (bool, optional): Whether use spectral
            norm for embedding layers in normalization blocks or not. If not
            specified (set as ``None``), ``with_embedding_spectral_norm`` would
            be set as the same value as ``with_spectral_norm``.
            Default to None.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `torch`.
        norm_eps (float, optional): eps for Normalization layers (both
            conditional and non-conditional ones). Default to `1e-4`.
        sn_eps (float, optional): eps for spectral normalization operation.
            Default to `1e-12`.
        init_cfg (dict, optional): Config for weight initialization.
            Default to ``dict(type='BigGAN')``.
    """
    _default_conv_cfg = dict(kernel_size=3, stride=1, padding=1, act_cfg=None)

    def __init__(self, in_channels, out_channels, hidden_channels=None, num_classes=0, use_cbn=True, use_norm_affine=False, act_cfg=dict(type='ReLU'), norm_cfg=dict(type='BN'), upsample_cfg=dict(type='nearest', scale_factor=2), upsample=True, auto_sync_bn=True, conv_cfg=None, with_spectral_norm=False, with_embedding_spectral_norm=None, sn_style='torch', norm_eps=0.0001, sn_eps=1e-12, init_cfg=dict(type='BigGAN')):
        super().__init__()
        self.learnable_sc = in_channels != out_channels or upsample
        self.with_upsample = upsample
        self.init_type = init_cfg.get('type', None)
        self.activate = build_activation_layer(act_cfg)
        hidden_channels = out_channels if hidden_channels is None else hidden_channels
        if self.with_upsample:
            self.upsample = build_upsample_layer(upsample_cfg)
        self.conv_cfg = deepcopy(self._default_conv_cfg)
        if conv_cfg is not None:
            self.conv_cfg.update(conv_cfg)
        with_embedding_spectral_norm = with_embedding_spectral_norm if with_embedding_spectral_norm is not None else with_spectral_norm
        sn_cfg = dict(eps=sn_eps, sn_style=sn_style)
        self.conv_1 = SNConvModule(in_channels, hidden_channels, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg, **self.conv_cfg)
        self.conv_2 = SNConvModule(hidden_channels, out_channels, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg, **self.conv_cfg)
        self.norm_1 = SNConditionNorm(in_channels, num_classes, use_cbn, norm_cfg, use_norm_affine, auto_sync_bn, with_embedding_spectral_norm, sn_style, norm_eps, sn_eps, init_cfg)
        self.norm_2 = SNConditionNorm(hidden_channels, num_classes, use_cbn, norm_cfg, use_norm_affine, auto_sync_bn, with_embedding_spectral_norm, sn_style, norm_eps, sn_eps, init_cfg)
        if self.learnable_sc:
            self.shortcut = SNConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg)
        self.init_weights()

    def forward(self, x, y=None):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).
            y (Tensor): Input label with shape (n, ).
                Default None.

        Returns:
            Tensor: Forward results.
        """
        out = self.norm_1(x, y)
        out = self.activate(out)
        if self.with_upsample:
            out = self.upsample(out)
        out = self.conv_1(out)
        out = self.norm_2(out, y)
        out = self.activate(out)
        out = self.conv_2(out)
        shortcut = self.forward_shortcut(x)
        return out + shortcut

    def forward_shortcut(self, x):
        out = x
        if self.learnable_sc:
            if self.with_upsample:
                out = self.upsample(out)
            out = self.shortcut(out)
        return out

    def init_weights(self):
        """Initialize weights for the model."""
        if self.init_type.upper() == 'STUDIO':
            nn.init.orthogonal_(self.conv_1.conv.weight)
            nn.init.orthogonal_(self.conv_2.conv.weight)
            self.conv_1.conv.bias.data.fill_(0.0)
            self.conv_2.conv.bias.data.fill_(0.0)
            if self.learnable_sc:
                nn.init.orthogonal_(self.shortcut.conv.weight)
                self.shortcut.conv.bias.data.fill_(0.0)
        elif self.init_type.upper() == 'BIGGAN':
            xavier_uniform_(self.conv_1.conv.weight, gain=1)
            xavier_uniform_(self.conv_2.conv.weight, gain=1)
            if self.learnable_sc:
                xavier_uniform_(self.shortcut.conv.weight, gain=1)
        elif self.init_type.upper() == 'SAGAN':
            xavier_init(self.conv_1, gain=1, distribution='uniform')
            xavier_init(self.conv_2, gain=1, distribution='uniform')
            if self.learnable_sc:
                xavier_init(self.shortcut, gain=1, distribution='uniform')
        elif self.init_type.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']:
            xavier_init(self.conv_1, gain=np.sqrt(2), distribution='uniform')
            xavier_init(self.conv_2, gain=np.sqrt(2), distribution='uniform')
            if self.learnable_sc:
                xavier_init(self.shortcut, gain=1, distribution='uniform')
        else:
            raise NotImplementedError(f"Unknown initialization method: '{self.init_type}'")


class SNGANDiscResBlock(nn.Module):
    """resblock used in discriminator of sngan / proj-gan.

    args:
        in_channels (int): input channels.
        out_channels (int): output channels.
        hidden_channels (int, optional): input channels of the second conv
            layer of the block. if ``none`` is given, would be set as
            ``out_channels``. Defaults to none.
        downsample (bool, optional): whether apply downsample operation in this
            module.  Defaults to false.
        act_cfg (dict, optional): config for activate function. default
            to ``dict(type='relu')``.
        conv_cfg (dict | none): config for conv blocks of this module. if pass
            ``none``, would use ``_default_conv_cfg``. default to ``none``.
        with_spectral_norm (bool, optional): whether use spectral norm for
            conv blocks and norm layers. Defaults to true.
        sn_eps (float, optional): eps for spectral normalization operation.
            Default to `1e-12`.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `torch`.
        init_cfg (dict, optional): Config for weight initialization.
            Defaults to ``dict(type='BigGAN')``.
    """
    _default_conv_cfg = dict(kernel_size=3, stride=1, padding=1, act_cfg=None)

    def __init__(self, in_channels, out_channels, hidden_channels=None, downsample=False, act_cfg=dict(type='ReLU'), conv_cfg=None, with_spectral_norm=True, sn_style='torch', sn_eps=1e-12, init_cfg=dict(type='BigGAN')):
        super().__init__()
        hidden_channels = out_channels if hidden_channels is None else hidden_channels
        self.with_downsample = downsample
        self.init_type = init_cfg.get('type', None)
        self.conv_cfg = deepcopy(self._default_conv_cfg)
        if conv_cfg is not None:
            self.conv_cfg.update(conv_cfg)
        self.activate = build_activation_layer(act_cfg)
        sn_cfg = dict(eps=sn_eps, sn_style=sn_style)
        self.conv_1 = SNConvModule(in_channels, hidden_channels, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg, **self.conv_cfg)
        self.conv_2 = SNConvModule(hidden_channels, out_channels, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg, **self.conv_cfg)
        if self.with_downsample:
            self.downsample = nn.AvgPool2d(2, 2)
        self.learnable_sc = in_channels != out_channels or downsample
        if self.learnable_sc:
            self.shortcut = SNConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg)
        self.init_weights()

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        out = self.activate(x)
        out = self.conv_1(out)
        out = self.activate(out)
        out = self.conv_2(out)
        if self.with_downsample:
            out = self.downsample(out)
        shortcut = self.forward_shortcut(x)
        return out + shortcut

    def forward_shortcut(self, x):
        out = x
        if self.learnable_sc:
            out = self.shortcut(out)
            if self.with_downsample:
                out = self.downsample(out)
        return out

    def init_weights(self):
        if self.init_type.upper() == 'STUDIO':
            nn.init.orthogonal_(self.conv_1.conv.weight)
            nn.init.orthogonal_(self.conv_2.conv.weight)
            self.conv_1.conv.bias.data.fill_(0.0)
            self.conv_2.conv.bias.data.fill_(0.0)
            if self.learnable_sc:
                nn.init.orthogonal_(self.shortcut.conv.weight)
                self.shortcut.conv.bias.data.fill_(0.0)
        elif self.init_type.upper() == 'BIGGAN':
            xavier_uniform_(self.conv_1.conv.weight, gain=1)
            xavier_uniform_(self.conv_2.conv.weight, gain=1)
            if self.learnable_sc:
                xavier_uniform_(self.shortcut.conv.weight, gain=1)
        elif self.init_type.upper() == 'SAGAN':
            xavier_init(self.conv_1, gain=1, distribution='uniform')
            xavier_init(self.conv_2, gain=1, distribution='uniform')
            if self.learnable_sc:
                xavier_init(self.shortcut, gain=1, distribution='uniform')
        elif self.init_type.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']:
            xavier_init(self.conv_1, gain=np.sqrt(2), distribution='uniform')
            xavier_init(self.conv_2, gain=np.sqrt(2), distribution='uniform')
            if self.learnable_sc:
                xavier_init(self.shortcut, gain=1, distribution='uniform')
        else:
            raise NotImplementedError(f"Unknown initialization method: '{self.init_type}'")


class SNGANDiscHeadResBlock(nn.Module):
    """The first ResBlock used in discriminator of sngan / proj-gan. Compared
    to ``SNGANDisResBlock``, this module has a different forward order.

    args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        downsample (bool, optional): whether apply downsample operation in this
            module.  default to false.
        conv_cfg (dict | none): config for conv blocks of this module. if pass
            ``none``, would use ``_default_conv_cfg``. default to ``none``.
        act_cfg (dict, optional): config for activate function. default
            to ``dict(type='relu')``.
        with_spectral_norm (bool, optional): whether use spectral norm for
            conv blocks and norm layers. default to true.
        sn_style (str, optional): The style of spectral normalization.
            If set to `ajbrock`, implementation by
            ajbrock(https://github.com/ajbrock/BigGAN-PyTorch/blob/master/layers.py)
            will be adopted.
            If set to `torch`, implementation by `PyTorch` will be adopted.
            Defaults to `torch`.
        sn_eps (float, optional): eps for spectral normalization operation.
            Default to `1e-12`.
        init_cfg (dict, optional): Config for weight initialization.
            Default to ``dict(type='BigGAN')``.
    """
    _default_conv_cfg = dict(kernel_size=3, stride=1, padding=1, act_cfg=None)

    def __init__(self, in_channels, out_channels, conv_cfg=None, act_cfg=dict(type='ReLU'), with_spectral_norm=True, sn_eps=1e-12, sn_style='torch', init_cfg=dict(type='BigGAN')):
        super().__init__()
        self.init_type = init_cfg.get('type', None)
        self.conv_cfg = deepcopy(self._default_conv_cfg)
        if conv_cfg is not None:
            self.conv_cfg.update(conv_cfg)
        self.activate = build_activation_layer(act_cfg)
        sn_cfg = dict(eps=sn_eps, sn_style=sn_style)
        self.conv_1 = SNConvModule(in_channels, out_channels, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg, **self.conv_cfg)
        self.conv_2 = SNConvModule(out_channels, out_channels, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg, **self.conv_cfg)
        self.downsample = nn.AvgPool2d(2, 2)
        self.shortcut = SNConvModule(in_channels, out_channels, kernel_size=1, stride=1, padding=0, act_cfg=None, with_spectral_norm=with_spectral_norm, spectral_norm_cfg=sn_cfg)
        self.init_weights()

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        out = self.conv_1(x)
        out = self.activate(out)
        out = self.conv_2(out)
        out = self.downsample(out)
        shortcut = self.forward_shortcut(x)
        return out + shortcut

    def forward_shortcut(self, x):
        out = self.downsample(x)
        out = self.shortcut(out)
        return out

    def init_weights(self):
        if self.init_type.upper() == 'STUDIO':
            for m in [self.conv_1, self.conv_2, self.shortcut]:
                nn.init.orthogonal_(m.conv.weight)
                m.conv.bias.data.fill_(0.0)
        elif self.init_type.upper() == 'BIGGAN':
            xavier_uniform_(self.conv_1.conv.weight, gain=1)
            xavier_uniform_(self.conv_2.conv.weight, gain=1)
            xavier_uniform_(self.shortcut.conv.weight, gain=1)
        elif self.init_type.upper() == 'SAGAN':
            xavier_init(self.conv_1, gain=1, distribution='uniform')
            xavier_init(self.conv_2, gain=1, distribution='uniform')
            xavier_init(self.shortcut, gain=1, distribution='uniform')
        elif self.init_type.upper() in ['SNGAN', 'SNGAN-PROJ', 'GAN-PROJ']:
            xavier_init(self.conv_1, gain=np.sqrt(2), distribution='uniform')
            xavier_init(self.conv_2, gain=np.sqrt(2), distribution='uniform')
            xavier_init(self.shortcut, gain=1, distribution='uniform')
        else:
            raise NotImplementedError(f"Unknown initialization method: '{self.init_type}'")


def matrix(*rows, device=None):
    """Constructing transformation matrices.

    Args:
        device (str|torch.device, optional): Matrix device. Defaults to None.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    assert all(len(row) == len(rows[0]) for row in rows)
    elems = [x for row in rows for x in row]
    ref = [x for x in elems if isinstance(x, torch.Tensor)]
    if len(ref) == 0:
        return misc.constant(np.asarray(rows), device=device)
    assert device is None or device == ref[0].device
    elems = [(x.float() if isinstance(x, torch.Tensor) else misc.constant(x, shape=ref[0].shape, device=ref[0].device)) for x in elems]
    return torch.stack(elems, dim=-1).reshape(ref[0].shape + (len(rows), -1))


def rotate2d(theta, **kwargs):
    """Construct 2d rotating matrix.

    Args:
        theta (float): Counter-clock wise rotation angle.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return matrix([torch.cos(theta), torch.sin(-theta), 0], [torch.sin(theta), torch.cos(theta), 0], [0, 0, 1], **kwargs)


def rotate2d_inv(theta, **kwargs):
    """Construct inverse matrix of 2d rotating matrix.

    Args:
        theta (float): Counter-clock wise rotation angle.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return rotate2d(-theta, **kwargs)


def rotate3d(v, theta, **kwargs):
    """Constructing 3d rotating matrix.

    Args:
        v (torch.Tensor): Luma axis.
        theta (float): Rotate theta counter-clock wise with ``v`` as the axis.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    vx = v[..., 0]
    vy = v[..., 1]
    vz = v[..., 2]
    s = torch.sin(theta)
    c = torch.cos(theta)
    cc = 1 - c
    return matrix([vx * vx * cc + c, vx * vy * cc - vz * s, vx * vz * cc + vy * s, 0], [vy * vx * cc + vz * s, vy * vy * cc + c, vy * vz * cc - vx * s, 0], [vz * vx * cc - vy * s, vz * vy * cc + vx * s, vz * vz * cc + c, 0], [0, 0, 0, 1], **kwargs)


def scale2d(sx, sy, **kwargs):
    """Construct 2d scaling matrix.

    Args:
        sx (float): X-direction scaling coefficient.
        sy (float): Y-direction scaling coefficient.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return matrix([sx, 0, 0], [0, sy, 0], [0, 0, 1], **kwargs)


def scale2d_inv(sx, sy, **kwargs):
    """Construct inverse matrix of 2d scaling matrix.

    Args:
        sx (float): X-direction scaling coefficient.
        sy (float): Y-direction scaling coefficient.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return scale2d(1 / sx, 1 / sy, **kwargs)


def scale3d(sx, sy, sz, **kwargs):
    """Construct 3d scaling matrix.

    Args:
        sx (float): X-direction scaling coefficient.
        sy (float): Y-direction scaling coefficient.
        sz (float): Z-direction scaling coefficient.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return matrix([sx, 0, 0, 0], [0, sy, 0, 0], [0, 0, sz, 0], [0, 0, 0, 1], **kwargs)


def translate2d(tx, ty, **kwargs):
    """Construct 2d translation matrix.

    Args:
        tx (float): X-direction translation amount.
        ty (float): Y-direction translation amount.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return matrix([1, 0, tx], [0, 1, ty], [0, 0, 1], **kwargs)


def translate2d_inv(tx, ty, **kwargs):
    """Construct inverse matrix of 2d translation matrix.

    Args:
        tx (float): X-direction translation amount.
        ty (float): Y-direction translation amount.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return translate2d(-tx, -ty, **kwargs)


def translate3d(tx, ty, tz, **kwargs):
    """Construct 3d translation matrix.

    Args:
        tx (float): X-direction translation amount.
        ty (float): Y-direction translation amount.
        tz (float): Z-direction translation amount.
    Returns:
        ndarry | Tensor : Transformation matrices in np.ndarry or torch.Tensor
            format.
    """
    return matrix([1, 0, 0, tx], [0, 1, 0, ty], [0, 0, 1, tz], [0, 0, 0, 1], **kwargs)


def _init():
    global _plugin
    if _plugin is None:
        _plugin = custom_ops.get_plugin(module_name='upfirdn2d_plugin', sources=['upfirdn2d.cpp', 'upfirdn2d.cu'], headers=['upfirdn2d.h'], source_dir=os.path.dirname(__file__), extra_cuda_cflags=['--use_fast_math'])
    return True


def _get_filter_size(f):
    """get width and height of filter kernel."""
    if f is None:
        return 1, 1
    assert isinstance(f, torch.Tensor) and f.ndim in [1, 2]
    fw = f.shape[-1]
    fh = f.shape[0]
    fw = int(fw)
    fh = int(fh)
    assert fw >= 1 and fh >= 1
    return fw, fh


def _parse_padding(padding):
    """parse padding into list [padx0, padx1, pady0, pady1]"""
    if isinstance(padding, int):
        padding = [padding, padding]
    assert isinstance(padding, (list, tuple))
    assert all(isinstance(x, int) for x in padding)
    if len(padding) == 2:
        padx, pady = padding
        padding = [padx, padx, pady, pady]
    padx0, padx1, pady0, pady1 = padding
    return padx0, padx1, pady0, pady1


def _parse_scaling(scaling):
    """parse scaling into list [x, y]"""
    if isinstance(scaling, int):
        scaling = [scaling, scaling]
    assert isinstance(scaling, (list, tuple))
    assert all(isinstance(x, int) for x in scaling)
    sx, sy = scaling
    assert sx >= 1 and sy >= 1
    return sx, sy


_plugin = None


_upfirdn2d_cuda_cache = dict()


def _upfirdn2d_cuda(up=1, down=1, padding=0, flip_filter=False, gain=1):
    """Fast CUDA implementation of `upfirdn2d()` using custom ops."""
    upx, upy = _parse_scaling(up)
    downx, downy = _parse_scaling(down)
    padx0, padx1, pady0, pady1 = _parse_padding(padding)
    key = upx, upy, downx, downy, padx0, padx1, pady0, pady1, flip_filter, gain
    if key in _upfirdn2d_cuda_cache:
        return _upfirdn2d_cuda_cache[key]


    class Upfirdn2dCuda(torch.autograd.Function):

        @staticmethod
        def forward(ctx, x, f):
            assert isinstance(x, torch.Tensor) and x.ndim == 4
            if f is None:
                f = torch.ones([1, 1], dtype=torch.float32, device=x.device)
            if f.ndim == 1 and f.shape[0] == 1:
                f = f.square().unsqueeze(0)
            assert isinstance(f, torch.Tensor) and f.ndim in [1, 2]
            y = x
            if f.ndim == 2:
                y = _plugin.upfirdn2d(y, f, upx, upy, downx, downy, padx0, padx1, pady0, pady1, flip_filter, gain)
            else:
                y = _plugin.upfirdn2d(y, f.unsqueeze(0), upx, 1, downx, 1, padx0, padx1, 0, 0, flip_filter, 1.0)
                y = _plugin.upfirdn2d(y, f.unsqueeze(1), 1, upy, 1, downy, 0, 0, pady0, pady1, flip_filter, gain)
            ctx.save_for_backward(f)
            ctx.x_shape = x.shape
            return y

        @staticmethod
        def backward(ctx, dy):
            f, = ctx.saved_tensors
            _, _, ih, iw = ctx.x_shape
            _, _, oh, ow = dy.shape
            fw, fh = _get_filter_size(f)
            p = [fw - padx0 - 1, iw * upx - ow * downx + padx0 - upx + 1, fh - pady0 - 1, ih * upy - oh * downy + pady0 - upy + 1]
            dx = None
            df = None
            if ctx.needs_input_grad[0]:
                dx = _upfirdn2d_cuda(up=down, down=up, padding=p, flip_filter=not flip_filter, gain=gain).apply(dy, f)
            assert not ctx.needs_input_grad[1]
            return dx, df
    _upfirdn2d_cuda_cache[key] = Upfirdn2dCuda
    return Upfirdn2dCuda


_conv2d_gradfix_cache = dict()


_null_tensor = torch.empty([0])


def _tuple_of_ints(xs, ndim):
    xs = tuple(xs) if isinstance(xs, (tuple, list)) else (xs,) * ndim
    assert len(xs) == ndim
    assert all(isinstance(x, int) for x in xs)
    return xs


weight_gradients_disabled = False


def _conv2d_gradfix(transpose, weight_shape, stride, padding, output_padding, dilation, groups):
    ndim = 2
    weight_shape = tuple(weight_shape)
    stride = _tuple_of_ints(stride, ndim)
    padding = _tuple_of_ints(padding, ndim)
    output_padding = _tuple_of_ints(output_padding, ndim)
    dilation = _tuple_of_ints(dilation, ndim)
    key = transpose, weight_shape, stride, padding, output_padding, dilation, groups
    if key in _conv2d_gradfix_cache:
        return _conv2d_gradfix_cache[key]
    assert groups >= 1
    assert len(weight_shape) == ndim + 2
    assert all(stride[i] >= 1 for i in range(ndim))
    assert all(padding[i] >= 0 for i in range(ndim))
    assert all(dilation[i] >= 0 for i in range(ndim))
    if not transpose:
        assert all(output_padding[i] == 0 for i in range(ndim))
    else:
        assert all(0 <= output_padding[i] < max(stride[i], dilation[i]) for i in range(ndim))
    common_kwargs = dict(stride=stride, padding=padding, dilation=dilation, groups=groups)

    def calc_output_padding(input_shape, output_shape):
        if transpose:
            return [0, 0]
        return [(input_shape[i + 2] - (output_shape[i + 2] - 1) * stride[i] - (1 - 2 * padding[i]) - dilation[i] * (weight_shape[i + 2] - 1)) for i in range(ndim)]


    class Conv2d(torch.autograd.Function):

        @staticmethod
        def forward(ctx, input, weight, bias):
            assert weight.shape == weight_shape
            ctx.save_for_backward(input if weight.requires_grad else _null_tensor, weight if input.requires_grad else _null_tensor)
            ctx.input_shape = input.shape
            if weight_shape[2:] == stride == dilation == (1, 1) and padding == (0, 0) and torch.cuda.get_device_capability(input.device) < (8, 0):
                a = weight.reshape(groups, weight_shape[0] // groups, weight_shape[1])
                b = input.reshape(input.shape[0], groups, input.shape[1] // groups, -1)
                c = (a.transpose(1, 2) if transpose else a) @ b.permute(1, 2, 0, 3).flatten(2)
                c = c.reshape(-1, input.shape[0], *input.shape[2:]).transpose(0, 1)
                c = c if bias is None else c + bias.unsqueeze(0).unsqueeze(2).unsqueeze(3)
                return c.contiguous(memory_format=torch.channels_last if input.stride(1) == 1 else torch.contiguous_format)
            if transpose:
                return torch.nn.functional.conv_transpose2d(input=input, weight=weight, bias=bias, output_padding=output_padding, **common_kwargs)
            return torch.nn.functional.conv2d(input=input, weight=weight, bias=bias, **common_kwargs)

        @staticmethod
        def backward(ctx, grad_output):
            input, weight = ctx.saved_tensors
            input_shape = ctx.input_shape
            grad_input = None
            grad_weight = None
            grad_bias = None
            if ctx.needs_input_grad[0]:
                p = calc_output_padding(input_shape=input_shape, output_shape=grad_output.shape)
                op = _conv2d_gradfix(transpose=not transpose, weight_shape=weight_shape, output_padding=p, **common_kwargs)
                grad_input = op.apply(grad_output, weight, None)
                assert grad_input.shape == input_shape
            if ctx.needs_input_grad[1] and not weight_gradients_disabled:
                grad_weight = Conv2dGradWeight.apply(grad_output, input)
                assert grad_weight.shape == weight_shape
            if ctx.needs_input_grad[2]:
                grad_bias = grad_output.sum([0, 2, 3])
            return grad_input, grad_weight, grad_bias


    class Conv2dGradWeight(torch.autograd.Function):

        @staticmethod
        def forward(ctx, grad_output, input):
            ctx.save_for_backward(grad_output if input.requires_grad else _null_tensor, input if grad_output.requires_grad else _null_tensor)
            ctx.grad_output_shape = grad_output.shape
            ctx.input_shape = input.shape
            if weight_shape[2:] == stride == dilation == (1, 1) and padding == (0, 0):
                a = grad_output.reshape(grad_output.shape[0], groups, grad_output.shape[1] // groups, -1).permute(1, 2, 0, 3).flatten(2)
                b = input.reshape(input.shape[0], groups, input.shape[1] // groups, -1).permute(1, 2, 0, 3).flatten(2)
                c = (b @ a.transpose(1, 2) if transpose else a @ b.transpose(1, 2)).reshape(weight_shape)
                return c.contiguous(memory_format=torch.channels_last if input.stride(1) == 1 else torch.contiguous_format)
            name = 'aten::cudnn_convolution_transpose_backward_weight' if transpose else 'aten::cudnn_convolution_backward_weight'
            flags = [torch.backends.cudnn.benchmark, torch.backends.cudnn.deterministic, torch.backends.cudnn.allow_tf32]
            return torch._C._jit_get_operation(name)(weight_shape, grad_output, input, padding, stride, dilation, groups, *flags)

        @staticmethod
        def backward(ctx, grad2_grad_weight):
            grad_output, input = ctx.saved_tensors
            grad_output_shape = ctx.grad_output_shape
            input_shape = ctx.input_shape
            grad2_grad_output = None
            grad2_input = None
            if ctx.needs_input_grad[0]:
                grad2_grad_output = Conv2d.apply(input, grad2_grad_weight, None)
                assert grad2_grad_output.shape == grad_output_shape
            if ctx.needs_input_grad[1]:
                p = calc_output_padding(input_shape=input_shape, output_shape=grad_output_shape)
                op = _conv2d_gradfix(transpose=not transpose, weight_shape=weight_shape, output_padding=p, **common_kwargs)
                grad2_input = op.apply(grad_output, grad2_grad_weight, None)
                assert grad2_input.shape == input_shape
            return grad2_grad_output, grad2_input
    _conv2d_gradfix_cache[key] = Conv2d
    return Conv2d


enabled = True


def _should_use_custom_op(input):
    assert isinstance(input, torch.Tensor)
    if not enabled or not torch.backends.cudnn.enabled:
        return False
    if input.device.type != 'cuda':
        return False
    return True


def conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):
    if _should_use_custom_op(input):
        return _conv2d_gradfix(transpose=False, weight_shape=weight.shape, stride=stride, padding=padding, output_padding=0, dilation=dilation, groups=groups).apply(input, weight, bias)
    return torch.nn.functional.conv2d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)


def _upfirdn2d_ref(x, f, up=1, down=1, padding=0, flip_filter=False, gain=1):
    """Slow reference implementation of `upfirdn2d()` using standard PyTorch
    ops."""
    assert isinstance(x, torch.Tensor) and x.ndim == 4
    if f is None:
        f = torch.ones([1, 1], dtype=torch.float32, device=x.device)
    assert isinstance(f, torch.Tensor) and f.ndim in [1, 2]
    assert f.dtype == torch.float32 and not f.requires_grad
    batch_size, num_channels, in_height, in_width = x.shape
    upx, upy = _parse_scaling(up)
    downx, downy = _parse_scaling(down)
    padx0, padx1, pady0, pady1 = _parse_padding(padding)
    upW = in_width * upx + padx0 + padx1
    upH = in_height * upy + pady0 + pady1
    assert upW >= f.shape[-1] and upH >= f.shape[0]
    x = x.reshape([batch_size, num_channels, in_height, 1, in_width, 1])
    x = torch.nn.functional.pad(x, [0, upx - 1, 0, 0, 0, upy - 1])
    x = x.reshape([batch_size, num_channels, in_height * upy, in_width * upx])
    x = torch.nn.functional.pad(x, [max(padx0, 0), max(padx1, 0), max(pady0, 0), max(pady1, 0)])
    x = x[:, :, max(-pady0, 0):x.shape[2] - max(-pady1, 0), max(-padx0, 0):x.shape[3] - max(-padx1, 0)]
    f = f * gain ** (f.ndim / 2)
    f = f
    if not flip_filter:
        f = f.flip(list(range(f.ndim)))
    f = f[np.newaxis, np.newaxis].repeat([num_channels, 1] + [1] * f.ndim)
    if f.ndim == 4:
        x = conv2d(input=x, weight=f, groups=num_channels)
    else:
        x = conv2d(input=x, weight=f.unsqueeze(2), groups=num_channels)
        x = conv2d(input=x, weight=f.unsqueeze(3), groups=num_channels)
    x = x[:, :, ::downy, ::downx]
    return x


def upfirdn2d(x, f, up=1, down=1, padding=0, flip_filter=False, gain=1, impl='cuda'):
    """Pad, upsample, filter, and downsample a batch of 2D images.

    Performs the following sequence of operations for each channel:

    1. Upsample the image by inserting N-1 zeros after each pixel (`up`).

    2. Pad the image with the specified number of zeros on each side
    (`padding`). Negative padding corresponds to cropping the image.

    3. Convolve the image with the specified 2D FIR filter (`f`), shrinking it
       so that the footprint of all output pixels lies within the input image.

    4. Downsample the image by keeping every Nth pixel (`down`).

    This sequence of operations bears close resemblance to
        scipy.signal.upfirdn().

    The fused op is considerably more efficient than performing the same
    calculation using standard PyTorch ops. It supports gradients of arbitrary
    order.

    Args:
        x:           Float32/float64/float16 input tensor of the shape
                     `[batch_size, num_channels, in_height, in_width]`.
        f:           Float32 FIR filter of the shape
                     `[filter_height, filter_width]` (non-separable),
                     `[filter_taps]` (separable), or
                     `None` (identity).
        up:          Integer upsampling factor. Can be a single int or a
        list/tuple
                     `[x, y]` (default: 1).
        down:        Integer downsampling factor. Can be a single int or a
                     list/tuple
                     `[x, y]` (default: 1).
        padding:     Padding with respect to the upsampled image. Can be a
                     single number or a list/tuple `[x, y]` or
                     `[x_before, x_after, y_before, y_after]` (default: 0).
        flip_filter: False = convolution, True = correlation (default: False).
        gain:        Overall scaling factor for signal magnitude (default: 1).
        impl:        Implementation to use. Can be `'ref'` or `'cuda'`
                     (default: `'cuda'`).

    Returns:
        Tensor of the shape `[batch_size, num_channels, out_height, out_width]`
        .
    """
    assert isinstance(x, torch.Tensor)
    assert impl in ['ref', 'cuda']
    if impl == 'cuda' and x.device.type == 'cuda' and _init():
        return _upfirdn2d_cuda(up=up, down=down, padding=padding, flip_filter=flip_filter, gain=gain).apply(x, f)
    return _upfirdn2d_ref(x, f, up=up, down=down, padding=padding, flip_filter=flip_filter, gain=gain)


wavelets = {'haar': [0.7071067811865476, 0.7071067811865476], 'db1': [0.7071067811865476, 0.7071067811865476], 'db2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025], 'db3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569], 'db4': [-0.010597401784997278, 0.032883011666982945, 0.030841381835986965, -0.18703481171888114, -0.02798376941698385, 0.6308807679295904, 0.7148465705525415, 0.23037781330885523], 'db5': [0.003335725285001549, -0.012580751999015526, -0.006241490213011705, 0.07757149384006515, -0.03224486958502952, -0.24229488706619015, 0.13842814590110342, 0.7243085284385744, 0.6038292697974729, 0.160102397974125], 'db6': [-0.00107730108499558, 0.004777257511010651, 0.0005538422009938016, -0.031582039318031156, 0.02752286553001629, 0.09750160558707936, -0.12976686756709563, -0.22626469396516913, 0.3152503517092432, 0.7511339080215775, 0.4946238903983854, 0.11154074335008017], 'db7': [0.0003537138000010399, -0.0018016407039998328, 0.00042957797300470274, 0.012550998556013784, -0.01657454163101562, -0.03802993693503463, 0.0806126091510659, 0.07130921926705004, -0.22403618499416572, -0.14390600392910627, 0.4697822874053586, 0.7291320908465551, 0.39653931948230575, 0.07785205408506236], 'db8': [-0.00011747678400228192, 0.0006754494059985568, -0.0003917403729959771, -0.00487035299301066, 0.008746094047015655, 0.013981027917015516, -0.04408825393106472, -0.01736930100202211, 0.128747426620186, 0.00047248457399797254, -0.2840155429624281, -0.015829105256023893, 0.5853546836548691, 0.6756307362980128, 0.3128715909144659, 0.05441584224308161], 'sym2': [-0.12940952255092145, 0.22414386804185735, 0.836516303737469, 0.48296291314469025], 'sym3': [0.035226291882100656, -0.08544127388224149, -0.13501102001039084, 0.4598775021193313, 0.8068915093133388, 0.3326705529509569], 'sym4': [-0.07576571478927333, -0.02963552764599851, 0.49761866763201545, 0.8037387518059161, 0.29785779560527736, -0.09921954357684722, -0.012603967262037833, 0.0322231006040427], 'sym5': [0.027333068345077982, 0.029519490925774643, -0.039134249302383094, 0.1993975339773936, 0.7234076904024206, 0.6339789634582119, 0.01660210576452232, -0.17532808990845047, -0.021101834024758855, 0.019538882735286728], 'sym6': [0.015404109327027373, 0.0034907120842174702, -0.11799011114819057, -0.048311742585633, 0.4910559419267466, 0.787641141030194, 0.3379294217276218, -0.07263752278646252, -0.021060292512300564, 0.04472490177066578, 0.0017677118642428036, -0.007800708325034148], 'sym7': [0.002681814568257878, -0.0010473848886829163, -0.01263630340325193, 0.03051551316596357, 0.0678926935013727, -0.049552834937127255, 0.017441255086855827, 0.5361019170917628, 0.767764317003164, 0.2886296317515146, -0.14004724044296152, -0.10780823770381774, 0.004010244871533663, 0.010268176708511255], 'sym8': [-0.0033824159510061256, -0.0005421323317911481, 0.03169508781149298, 0.007607487324917605, -0.1432942383508097, -0.061273359067658524, 0.4813596512583722, 0.7771857517005235, 0.3644418948353314, -0.05194583810770904, -0.027219029917056003, 0.049137179673607506, 0.003808752013890615, -0.01495225833704823, -0.0003029205147213668, 0.0018899503327594609]}


class AugmentPipe(torch.nn.Module):
    """Augmentation pipeline include multiple geometric and color
    transformations.

    Note: The meaning of arguments are written in the comments of
    ``__init__`` function.
    """

    def __init__(self, xflip=0, rotate90=0, xint=0, xint_max=0.125, scale=0, rotate=0, aniso=0, xfrac=0, scale_std=0.2, rotate_max=1, aniso_std=0.2, xfrac_std=0.125, brightness=0, contrast=0, lumaflip=0, hue=0, saturation=0, brightness_std=0.2, contrast_std=0.5, hue_max=1, saturation_std=1, imgfilter=0, imgfilter_bands=[1, 1, 1, 1], imgfilter_std=1, noise=0, cutout=0, noise_std=0.1, cutout_size=0.5):
        super().__init__()
        self.register_buffer('p', torch.ones([]))
        self.xflip = float(xflip)
        self.rotate90 = float(rotate90)
        self.xint = float(xint)
        self.xint_max = float(xint_max)
        self.scale = float(scale)
        self.rotate = float(rotate)
        self.aniso = float(aniso)
        self.xfrac = float(xfrac)
        self.scale_std = float(scale_std)
        self.rotate_max = float(rotate_max)
        self.aniso_std = float(aniso_std)
        self.xfrac_std = float(xfrac_std)
        self.brightness = float(brightness)
        self.contrast = float(contrast)
        self.lumaflip = float(lumaflip)
        self.hue = float(hue)
        self.saturation = float(saturation)
        self.brightness_std = float(brightness_std)
        self.contrast_std = float(contrast_std)
        self.hue_max = float(hue_max)
        self.saturation_std = float(saturation_std)
        self.imgfilter = float(imgfilter)
        self.imgfilter_bands = list(imgfilter_bands)
        self.imgfilter_std = float(imgfilter_std)
        self.noise = float(noise)
        self.cutout = float(cutout)
        self.noise_std = float(noise_std)
        self.cutout_size = float(cutout_size)
        self.register_buffer('Hz_geom', upfirdn2d.setup_filter(wavelets['sym6']))
        Hz_lo = np.asarray(wavelets['sym2'])
        Hz_hi = Hz_lo * (-1) ** np.arange(Hz_lo.size)
        Hz_lo2 = np.convolve(Hz_lo, Hz_lo[::-1]) / 2
        Hz_hi2 = np.convolve(Hz_hi, Hz_hi[::-1]) / 2
        Hz_fbank = np.eye(4, 1)
        for i in range(1, Hz_fbank.shape[0]):
            Hz_fbank = np.dstack([Hz_fbank, np.zeros_like(Hz_fbank)]).reshape(Hz_fbank.shape[0], -1)[:, :-1]
            Hz_fbank = scipy.signal.convolve(Hz_fbank, [Hz_lo2])
            Hz_fbank[i, (Hz_fbank.shape[1] - Hz_hi2.size) // 2:(Hz_fbank.shape[1] + Hz_hi2.size) // 2] += Hz_hi2
        self.register_buffer('Hz_fbank', torch.as_tensor(Hz_fbank, dtype=torch.float32))

    def forward(self, images, debug_percentile=None):
        assert isinstance(images, torch.Tensor) and images.ndim == 4
        batch_size, num_channels, height, width = images.shape
        device = images.device
        if debug_percentile is not None:
            debug_percentile = torch.as_tensor(debug_percentile, dtype=torch.float32, device=device)
        I_3 = torch.eye(3, device=device)
        G_inv = I_3
        if self.xflip > 0:
            i = torch.floor(torch.rand([batch_size], device=device) * 2)
            i = torch.where(torch.rand([batch_size], device=device) < self.xflip * self.p, i, torch.zeros_like(i))
            if debug_percentile is not None:
                i = torch.full_like(i, torch.floor(debug_percentile * 2))
            G_inv = G_inv @ scale2d_inv(1 - 2 * i, 1)
        if self.rotate90 > 0:
            i = torch.floor(torch.rand([batch_size], device=device) * 4)
            i = torch.where(torch.rand([batch_size], device=device) < self.rotate90 * self.p, i, torch.zeros_like(i))
            if debug_percentile is not None:
                i = torch.full_like(i, torch.floor(debug_percentile * 4))
            G_inv = G_inv @ rotate2d_inv(-np.pi / 2 * i)
        if self.xint > 0:
            t = (torch.rand([batch_size, 2], device=device) * 2 - 1) * self.xint_max
            t = torch.where(torch.rand([batch_size, 1], device=device) < self.xint * self.p, t, torch.zeros_like(t))
            if debug_percentile is not None:
                t = torch.full_like(t, (debug_percentile * 2 - 1) * self.xint_max)
            G_inv = G_inv @ translate2d_inv(torch.round(t[:, 0] * width), torch.round(t[:, 1] * height))
        _scalor_log2 = torch.log(torch.tensor(2.0, device=images.device, dtype=images.dtype))
        if self.scale > 0:
            s = torch.exp(torch.randn([batch_size], device=device) * self.scale_std * _scalor_log2)
            s = torch.where(torch.rand([batch_size], device=device) < self.scale * self.p, s, torch.ones_like(s))
            if debug_percentile is not None:
                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.scale_std))
            G_inv = G_inv @ scale2d_inv(s, s)
        p_rot = 1 - torch.sqrt((1 - self.rotate * self.p).clamp(0, 1))
        if self.rotate > 0:
            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max
            theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))
            if debug_percentile is not None:
                theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.rotate_max)
            G_inv = G_inv @ rotate2d_inv(-theta)
        if self.aniso > 0:
            s = torch.exp(torch.randn([batch_size], device=device) * self.aniso_std * _scalor_log2)
            s = torch.where(torch.rand([batch_size], device=device) < self.aniso * self.p, s, torch.ones_like(s))
            if debug_percentile is not None:
                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.aniso_std))
            G_inv = G_inv @ scale2d_inv(s, 1 / s)
        if self.rotate > 0:
            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.rotate_max
            theta = torch.where(torch.rand([batch_size], device=device) < p_rot, theta, torch.zeros_like(theta))
            if debug_percentile is not None:
                theta = torch.zeros_like(theta)
            G_inv = G_inv @ rotate2d_inv(-theta)
        if self.xfrac > 0:
            t = torch.randn([batch_size, 2], device=device) * self.xfrac_std
            t = torch.where(torch.rand([batch_size, 1], device=device) < self.xfrac * self.p, t, torch.zeros_like(t))
            if debug_percentile is not None:
                t = torch.full_like(t, torch.erfinv(debug_percentile * 2 - 1) * self.xfrac_std)
            G_inv = G_inv @ translate2d_inv(t[:, 0] * width, t[:, 1] * height)
        if G_inv is not I_3:
            cx = (width - 1) / 2
            cy = (height - 1) / 2
            cp = matrix([-cx, -cy, 1], [cx, -cy, 1], [cx, cy, 1], [-cx, cy, 1], device=device)
            cp = G_inv @ cp.t()
            Hz_pad = self.Hz_geom.shape[0] // 4
            margin = cp[:, :2, :].permute(1, 0, 2).flatten(1)
            margin = torch.cat([-margin, margin]).max(dim=1).values
            margin = margin + misc.constant([Hz_pad * 2 - cx, Hz_pad * 2 - cy] * 2, device=device)
            margin = margin.max(misc.constant([0, 0] * 2, device=device))
            margin = margin.min(misc.constant([width - 1, height - 1] * 2, device=device))
            mx0, my0, mx1, my1 = margin.ceil()
            images = torch.nn.functional.pad(input=images, pad=[mx0, mx1, my0, my1], mode='reflect')
            G_inv = translate2d(torch.true_divide(mx0 - mx1, 2), torch.true_divide(my0 - my1, 2)) @ G_inv
            images = upfirdn2d.upsample2d(x=images, f=self.Hz_geom, up=2)
            G_inv = scale2d(2, 2, device=device) @ G_inv @ scale2d_inv(2, 2, device=device)
            G_inv = translate2d(-0.5, -0.5, device=device) @ G_inv @ translate2d_inv(-0.5, -0.5, device=device)
            shape = [batch_size, num_channels, (height + Hz_pad * 2) * 2, (width + Hz_pad * 2) * 2]
            G_inv = scale2d(2 / images.shape[3], 2 / images.shape[2], device=device) @ G_inv @ scale2d_inv(2 / shape[3], 2 / shape[2], device=device)
            grid = torch.nn.functional.affine_grid(theta=G_inv[:, :2, :], size=shape, align_corners=False)
            images = grid_sample_gradfix.grid_sample(images, grid)
            images = upfirdn2d.downsample2d(x=images, f=self.Hz_geom, down=2, padding=-Hz_pad * 2, flip_filter=True)
        I_4 = torch.eye(4, device=device)
        C = I_4
        if self.brightness > 0:
            b = torch.randn([batch_size], device=device) * self.brightness_std
            b = torch.where(torch.rand([batch_size], device=device) < self.brightness * self.p, b, torch.zeros_like(b))
            if debug_percentile is not None:
                b = torch.full_like(b, torch.erfinv(debug_percentile * 2 - 1) * self.brightness_std)
            C = translate3d(b, b, b) @ C
        if self.contrast > 0:
            c = torch.exp(torch.randn([batch_size], device=device) * self.contrast_std * _scalor_log2)
            c = torch.where(torch.rand([batch_size], device=device) < self.contrast * self.p, c, torch.ones_like(c))
            if debug_percentile is not None:
                c = torch.full_like(c, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.contrast_std))
            C = scale3d(c, c, c) @ C
        v = misc.constant(np.asarray([1, 1, 1, 0]) / np.sqrt(3), device=device)
        if self.lumaflip > 0:
            i = torch.floor(torch.rand([batch_size, 1, 1], device=device) * 2)
            i = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.lumaflip * self.p, i, torch.zeros_like(i))
            if debug_percentile is not None:
                i = torch.full_like(i, torch.floor(debug_percentile * 2))
            C = (I_4 - 2 * v.ger(v) * i) @ C
        if self.hue > 0 and num_channels > 1:
            theta = (torch.rand([batch_size], device=device) * 2 - 1) * np.pi * self.hue_max
            theta = torch.where(torch.rand([batch_size], device=device) < self.hue * self.p, theta, torch.zeros_like(theta))
            if debug_percentile is not None:
                theta = torch.full_like(theta, (debug_percentile * 2 - 1) * np.pi * self.hue_max)
            C = rotate3d(v, theta) @ C
        if self.saturation > 0 and num_channels > 1:
            s = torch.exp(torch.randn([batch_size, 1, 1], device=device) * self.saturation_std * _scalor_log2)
            s = torch.where(torch.rand([batch_size, 1, 1], device=device) < self.saturation * self.p, s, torch.ones_like(s))
            if debug_percentile is not None:
                s = torch.full_like(s, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.saturation_std))
            C = (v.ger(v) + (I_4 - v.ger(v)) * s) @ C
        if C is not I_4:
            images = images.reshape([batch_size, num_channels, height * width])
            if num_channels == 3:
                images = C[:, :3, :3] @ images + C[:, :3, 3:]
            elif num_channels == 1:
                C = C[:, :3, :].mean(dim=1, keepdims=True)
                images = images * C[:, :, :3].sum(dim=2, keepdims=True) + C[:, :, 3:]
            else:
                raise ValueError('Image must be RGB (3 channels) or L (1 channel)')
            images = images.reshape([batch_size, num_channels, height, width])
        if self.imgfilter > 0:
            num_bands = self.Hz_fbank.shape[0]
            assert len(self.imgfilter_bands) == num_bands
            expected_power = misc.constant(np.array([10, 1, 1, 1]) / 13, device=device)
            g = torch.ones([batch_size, num_bands], device=device)
            for i, band_strength in enumerate(self.imgfilter_bands):
                t_i = torch.exp(torch.randn([batch_size], device=device) * self.imgfilter_std * _scalor_log2)
                t_i = torch.where(torch.rand([batch_size], device=device) < self.imgfilter * self.p * band_strength, t_i, torch.ones_like(t_i))
                if debug_percentile is not None:
                    t_i = torch.full_like(t_i, torch.exp2(torch.erfinv(debug_percentile * 2 - 1) * self.imgfilter_std)) if band_strength > 0 else torch.ones_like(t_i)
                t = torch.ones([batch_size, num_bands], device=device)
                t[:, i] = t_i
                t = t / (expected_power * t.square()).sum(dim=-1, keepdims=True).sqrt()
                g = g * t
            Hz_prime = g @ self.Hz_fbank
            Hz_prime = Hz_prime.unsqueeze(1).repeat([1, num_channels, 1])
            Hz_prime = Hz_prime.reshape([batch_size * num_channels, 1, -1])
            p = self.Hz_fbank.shape[1] // 2
            images = images.reshape([1, batch_size * num_channels, height, width])
            images = torch.nn.functional.pad(input=images, pad=[p, p, p, p], mode='reflect')
            images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(2), groups=batch_size * num_channels)
            images = conv2d_gradfix.conv2d(input=images, weight=Hz_prime.unsqueeze(3), groups=batch_size * num_channels)
            images = images.reshape([batch_size, num_channels, height, width])
        if self.noise > 0:
            sigma = torch.randn([batch_size, 1, 1, 1], device=device).abs() * self.noise_std
            sigma = torch.where(torch.rand([batch_size, 1, 1, 1], device=device) < self.noise * self.p, sigma, torch.zeros_like(sigma))
            if debug_percentile is not None:
                sigma = torch.full_like(sigma, torch.erfinv(debug_percentile) * self.noise_std)
            images = images + torch.randn([batch_size, num_channels, height, width], device=device) * sigma
        if self.cutout > 0:
            size = torch.full([batch_size, 2, 1, 1, 1], self.cutout_size, device=device)
            size = torch.where(torch.rand([batch_size, 1, 1, 1, 1], device=device) < self.cutout * self.p, size, torch.zeros_like(size))
            center = torch.rand([batch_size, 2, 1, 1, 1], device=device)
            if debug_percentile is not None:
                size = torch.full_like(size, self.cutout_size)
                center = torch.full_like(center, debug_percentile)
            coord_x = torch.arange(width, device=device).reshape([1, 1, 1, -1])
            coord_y = torch.arange(height, device=device).reshape([1, 1, -1, 1])
            mask_x = ((coord_x + 0.5) / width - center[:, 0]).abs() >= size[:, 0] / 2
            mask_y = ((coord_y + 0.5) / height - center[:, 1]).abs() >= size[:, 1] / 2
            mask = torch.logical_or(mask_x, mask_y)
            images = images * mask
        return images


class EqualLinearActModule(nn.Module):
    """Equalized LR Linear Module with Activation Layer.

    This module is modified from ``EqualizedLRLinearModule`` defined in PGGAN.
    The major features updated in this module is adding support for activation
    layers used in StyleGAN2.

    Args:
        equalized_lr_cfg (dict | None, optional): Config for equalized lr.
            Defaults to dict(gain=1., lr_mul=1.).
        bias (bool, optional): Whether to use bias item. Defaults to True.
        bias_init (float, optional): The value for bias initialization.
            Defaults to ``0.``.
        act_cfg (dict | None, optional): Config for activation layer.
            Defaults to None.
    """

    def __init__(self, *args, equalized_lr_cfg=dict(gain=1.0, lr_mul=1.0), bias=True, bias_init=0.0, act_cfg=None, **kwargs):
        super().__init__()
        self.with_activation = act_cfg is not None
        self.linear = EqualizedLRLinearModule(*args, bias=False, equalized_lr_cfg=equalized_lr_cfg, **kwargs)
        if equalized_lr_cfg is not None:
            self.lr_mul = equalized_lr_cfg.get('lr_mul', 1.0)
        else:
            self.lr_mul = 1.0
        if bias:
            self.bias = nn.Parameter(torch.zeros(self.linear.out_features).fill_(bias_init))
        else:
            self.bias = None
        if self.with_activation:
            act_cfg = deepcopy(act_cfg)
            if act_cfg['type'] == 'fused_bias':
                self.act_type = act_cfg.pop('type')
                assert self.bias is not None
                self.activate = partial(fused_bias_leakyrelu, **act_cfg)
            else:
                self.act_type = 'normal'
                self.activate = build_activation_layer(act_cfg)
        else:
            self.act_type = None

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input feature map with shape of (N, C, ...).

        Returns:
            Tensor: Output feature map.
        """
        if x.ndim >= 3:
            x = x.reshape(x.size(0), -1)
        x = self.linear(x)
        if self.with_activation and self.act_type == 'fused_bias':
            x = self.activate(x, self.bias * self.lr_mul)
        elif self.bias is not None and self.with_activation:
            x = self.activate(x + self.bias * self.lr_mul)
        elif self.bias is not None:
            x = x + self.bias * self.lr_mul
        elif self.with_activation:
            x = self.activate(x)
        return x


class AdaptiveInstanceNorm(nn.Module):
    """Adaptive Instance Normalization Module.

    Ref: https://github.com/rosinality/style-based-gan-pytorch/blob/master/model.py  # noqa

    Args:
        in_channel (int): The number of input's channel.
        style_dim (int): Style latent dimension.
    """

    def __init__(self, in_channel, style_dim):
        super().__init__()
        self.norm = nn.InstanceNorm2d(in_channel)
        self.affine = EqualizedLRLinearModule(style_dim, in_channel * 2)
        self.affine.bias.data[:in_channel] = 1
        self.affine.bias.data[in_channel:] = 0

    def forward(self, input, style):
        """Forward function.

        Args:
            input (Tensor): Input tensor with shape (n, c, h, w).
            style (Tensor): Input style tensor with shape (n, c).

        Returns:
            Tensor: Forward results.
        """
        style = self.affine(style).unsqueeze(2).unsqueeze(3)
        gamma, beta = style.chunk(2, 1)
        out = self.norm(input)
        out = gamma * out + beta
        return out


def _make_kernel(k):
    k = torch.tensor(k, dtype=torch.float32)
    if k.ndim == 1:
        k = k[None, :] * k[:, None]
    k /= k.sum()
    return k


class Blur(nn.Module):
    """Blur module.

    This module is adopted rightly after upsampling operation in StyleGAN2.

    Args:
        kernel (Array): Blur kernel/filter used in UpFIRDn.
        pad (list[int]): Padding for features.
        upsample_factor (int, optional): Upsampling factor. Defaults to 1.
    """

    def __init__(self, kernel, pad, upsample_factor=1):
        super().__init__()
        kernel = _make_kernel(kernel)
        if upsample_factor > 1:
            kernel = kernel * upsample_factor ** 2
        self.register_buffer('kernel', kernel)
        self.pad = pad

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input feature map with shape of (N, C, H, W).

        Returns:
            Tensor: Output feature map.
        """
        return upfirdn2d(x, self.kernel, pad=self.pad)


class ConstantInput(nn.Module):
    """Constant Input.

    In StyleGAN2, they substitute the original head noise input with such a
    constant input module.

    Args:
        channel (int): Channels for the constant input tensor.
        size (int, optional): Spatial size for the constant input.
            Defaults to 4.
    """

    def __init__(self, channel, size=4):
        super().__init__()
        if isinstance(size, int):
            size = [size, size]
        elif mmcv.is_seq_of(size, int):
            assert len(size) == 2, f'The length of size should be 2 but got {len(size)}'
        else:
            raise ValueError(f'Got invalid value in size, {size}')
        self.input = nn.Parameter(torch.randn(1, channel, *size))

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input feature map with shape of (N, C, ...).

        Returns:
            Tensor: Output feature map.
        """
        batch = x.shape[0]
        out = self.input.repeat(batch, 1, 1, 1)
        return out


class NoiseInjection(nn.Module):
    """Noise Injection Module.

    In StyleGAN2, they adopt this module to inject spatial random noise map in
    the generators.

    Args:
        noise_weight_init (float, optional): Initialization weight for noise
            injection. Defaults to ``0.``.
    """

    def __init__(self, noise_weight_init=0.0):
        super().__init__()
        self.weight = nn.Parameter(torch.zeros(1).fill_(noise_weight_init))

    def forward(self, image, noise=None, return_noise=False):
        """Forward Function.

        Args:
            image (Tensor): Spatial features with a shape of (N, C, H, W).
            noise (Tensor, optional): Noises from the outside.
                Defaults to None.
            return_noise (bool, optional): Whether to return noise tensor.
                Defaults to False.

        Returns:
            Tensor: Output features.
        """
        if noise is None:
            batch, _, height, width = image.shape
            noise = image.new_empty(batch, 1, height, width).normal_()
        noise = noise
        if return_noise:
            return image + self.weight * noise, noise
        return image + self.weight * noise


class StyleConv(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, style_channels, padding=1, initial=False, blur_kernel=[1, 2, 1], upsample=False, fused=False):
        """Convolutional style blocks composing of noise injector, AdaIN module
        and convolution layers.

        Args:
            in_channels (int): The channel number of the input tensor.
            out_channels (itn): The channel number of the output tensor.
            kernel_size (int): The kernel size of convolution layers.
            style_channels (int): The number of channels for style code.
            padding (int, optional): Padding of convolution layers.
                Defaults to 1.
            initial (bool, optional): Whether this is the first StyleConv of
                StyleGAN's generator. Defaults to False.
            blur_kernel (list, optional): The blurry kernel.
                Defaults to [1, 2, 1].
            upsample (bool, optional): Whether perform upsampling.
                Defaults to False.
            fused (bool, optional): Whether use fused upconv.
                Defaults to False.
        """
        super().__init__()
        if initial:
            self.conv1 = ConstantInput(in_channels)
        elif upsample:
            if fused:
                self.conv1 = nn.Sequential(EqualizedLRConvUpModule(in_channels, out_channels, kernel_size, padding=padding, act_cfg=dict(type='LeakyReLU', negative_slope=0.2)), Blur(blur_kernel, pad=(1, 1)))
            else:
                self.conv1 = nn.Sequential(nn.Upsample(scale_factor=2, mode='nearest'), EqualizedLRConvModule(in_channels, out_channels, kernel_size, padding=padding, act_cfg=None), Blur(blur_kernel, pad=(1, 1)))
        else:
            self.conv1 = EqualizedLRConvModule(in_channels, out_channels, kernel_size, padding=padding, act_cfg=None)
        self.noise_injector1 = NoiseInjection()
        self.activate1 = nn.LeakyReLU(0.2)
        self.adain1 = AdaptiveInstanceNorm(out_channels, style_channels)
        self.conv2 = EqualizedLRConvModule(out_channels, out_channels, kernel_size, padding=padding, act_cfg=None)
        self.noise_injector2 = NoiseInjection()
        self.activate2 = nn.LeakyReLU(0.2)
        self.adain2 = AdaptiveInstanceNorm(out_channels, style_channels)

    def forward(self, x, style1, style2, noise1=None, noise2=None, return_noise=False):
        """Forward function.

        Args:
            x (Tensor): Input tensor.
            style1 (Tensor): Input style tensor with shape (n, c).
            style2 (Tensor): Input style tensor with shape (n, c).
            noise1 (Tensor, optional): Noise tensor with shape (n, c, h, w).
                Defaults to None.
            noise2 (Tensor, optional): Noise tensor with shape (n, c, h, w).
                Defaults to None.
            return_noise (bool, optional): If True, ``noise1`` and ``noise2``
            will be returned with ``out``. Defaults to False.

        Returns:
            Tensor | tuple[Tensor]: Forward results.
        """
        out = self.conv1(x)
        if return_noise:
            out, noise1 = self.noise_injector1(out, noise=noise1, return_noise=return_noise)
        else:
            out = self.noise_injector1(out, noise=noise1, return_noise=return_noise)
        out = self.activate1(out)
        out = self.adain1(out, style1)
        out = self.conv2(out)
        if return_noise:
            out, noise2 = self.noise_injector2(out, noise=noise2, return_noise=return_noise)
        else:
            out = self.noise_injector2(out, noise=noise2, return_noise=return_noise)
        out = self.activate2(out)
        out = self.adain2(out, style2)
        if return_noise:
            return out, noise1, noise2
        return out


class StyleGANv1Generator(nn.Module):
    """StyleGAN1 Generator.

    In StyleGAN1, we use a progressive growing architecture composing of a
    style mapping module and number of convolutional style blocks. More details
    can be found in: A Style-Based Generator Architecture for Generative
    Adversarial Networks CVPR2019.

    Args:
        out_size (int): The output size of the StyleGAN1 generator.
        style_channels (int): The number of channels for style code.
        num_mlps (int, optional): The number of MLP layers. Defaults to 8.
        blur_kernel (list, optional): The blurry kernel. Defaults
            to [1, 2, 1].
        lr_mlp (float, optional): The learning rate for the style mapping
            layer. Defaults to 0.01.
        default_style_mode (str, optional): The default mode of style mixing.
            In training, we defaultly adopt mixing style mode. However, in the
            evaluation, we use 'single' style mode. `['mix', 'single']` are
            currently supported. Defaults to 'mix'.
        eval_style_mode (str, optional): The evaluation mode of style mixing.
            Defaults to 'single'.
        mix_prob (float, optional): Mixing probability. The value should be
            in range of [0, 1]. Defaults to 0.9.
    """

    def __init__(self, out_size, style_channels, num_mlps=8, blur_kernel=[1, 2, 1], lr_mlp=0.01, default_style_mode='mix', eval_style_mode='single', mix_prob=0.9):
        super().__init__()
        self.out_size = out_size
        self.style_channels = style_channels
        self.num_mlps = num_mlps
        self.lr_mlp = lr_mlp
        self._default_style_mode = default_style_mode
        self.default_style_mode = default_style_mode
        self.eval_style_mode = eval_style_mode
        self.mix_prob = mix_prob
        mapping_layers = [PixelNorm()]
        for _ in range(num_mlps):
            mapping_layers.append(EqualLinearActModule(style_channels, style_channels, equalized_lr_cfg=dict(lr_mul=lr_mlp, gain=1.0), act_cfg=dict(type='LeakyReLU', negative_slope=0.2)))
        self.style_mapping = nn.Sequential(*mapping_layers)
        self.channels = {(4): 512, (8): 512, (16): 512, (32): 512, (64): 256, (128): 128, (256): 64, (512): 32, (1024): 16}
        self.log_size = int(np.log2(self.out_size))
        self.convs = nn.ModuleList()
        self.to_rgbs = nn.ModuleList()
        in_channels_ = self.channels[4]
        for i in range(2, self.log_size + 1):
            out_channels_ = self.channels[2 ** i]
            self.convs.append(StyleConv(in_channels_, out_channels_, 3, style_channels, initial=i == 2, upsample=True, fused=True))
            self.to_rgbs.append(EqualizedLRConvModule(out_channels_, 3, 1, act_cfg=None))
            in_channels_ = out_channels_
        self.num_latents = self.log_size * 2 - 2
        self.num_injected_noises = self.num_latents
        for layer_idx in range(self.num_injected_noises):
            res = (layer_idx + 4) // 2
            shape = [1, 1, 2 ** res, 2 ** res]
            self.register_buffer(f'injected_noise_{layer_idx}', torch.randn(*shape))

    def train(self, mode=True):
        if mode:
            if self.default_style_mode != self._default_style_mode:
                mmcv.print_log(f'Switch to train style mode: {self._default_style_mode}', 'mmgen')
            self.default_style_mode = self._default_style_mode
        else:
            if self.default_style_mode != self.eval_style_mode:
                mmcv.print_log(f'Switch to evaluation style mode: {self.eval_style_mode}', 'mmgen')
            self.default_style_mode = self.eval_style_mode
        return super(StyleGANv1Generator, self).train(mode)

    def make_injected_noise(self):
        """make noises that will be injected into feature maps.

        Returns:
            list[Tensor]: List of layer-wise noise tensor.
        """
        device = get_module_device(self)
        noises = []
        for i in range(2, self.log_size + 1):
            for _ in range(2):
                noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))
        return noises

    def get_mean_latent(self, num_samples=4096, **kwargs):
        """Get mean latent of W space in this generator.

        Args:
            num_samples (int, optional): Number of sample times. Defaults
                to 4096.

        Returns:
            Tensor: Mean latent of this generator.
        """
        return get_mean_latent(self, num_samples, **kwargs)

    def style_mixing(self, n_source, n_target, inject_index=1, truncation_latent=None, truncation=0.7, curr_scale=-1, transition_weight=1):
        return style_mixing(self, n_source=n_source, n_target=n_target, inject_index=inject_index, truncation=truncation, truncation_latent=truncation_latent, style_channels=self.style_channels, curr_scale=curr_scale, transition_weight=transition_weight)

    def forward(self, styles, num_batches=-1, return_noise=False, return_latents=False, inject_index=None, truncation=1, truncation_latent=None, input_is_latent=False, injected_noise=None, randomize_noise=True, transition_weight=1.0, curr_scale=-1):
        """Forward function.

        This function has been integrated with the truncation trick. Please
        refer to the usage of `truncation` and `truncation_latent`.

        Args:
            styles (torch.Tensor | list[torch.Tensor] | callable | None): In
                StyleGAN1, you can provide noise tensor or latent tensor. Given
                a list containing more than one noise or latent tensors, style
                mixing trick will be used in training. Of course, You can
                directly give a batch of noise through a ``torch.Tensor`` or
                offer a callable function to sample a batch of noise data.
                Otherwise, the ``None`` indicates to use the default noise
                sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            return_latents (bool, optional): If True, ``latent`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            inject_index (int | None, optional): The index number for mixing
                style codes. Defaults to None.
            truncation (float, optional): Truncation factor. Give value less
                than 1., the truncation trick will be adopted. Defaults to 1.
            truncation_latent (torch.Tensor, optional): Mean truncation latent.
                Defaults to None.
            input_is_latent (bool, optional): If `True`, the input tensor is
                the latent tensor. Defaults to False.
            injected_noise (torch.Tensor | None, optional): Given a tensor, the
                random noise will be fixed as this input injected noise.
                Defaults to None.
            randomize_noise (bool, optional): If `False`, images are sampled
                with the buffered noise tensor injected to the style conv
                block. Defaults to True.
            transition_weight (float, optional): The weight used in resolution
                transition. Defaults to 1..
            curr_scale (int, optional): The resolution scale of generated image
                tensor. -1 means the max resolution scale of the StyleGAN1.
                Defaults to -1.

        Returns:
            torch.Tensor | dict: Generated image tensor or dictionary                 containing more data.
        """
        if isinstance(styles, torch.Tensor):
            assert styles.shape[1] == self.style_channels
            styles = [styles]
        elif mmcv.is_seq_of(styles, torch.Tensor):
            for t in styles:
                assert t.shape[-1] == self.style_channels
        elif callable(styles):
            device = get_module_device(self)
            noise_generator = styles
            assert num_batches > 0
            if self.default_style_mode == 'mix' and random.random() < self.mix_prob:
                styles = [noise_generator((num_batches, self.style_channels)) for _ in range(2)]
            else:
                styles = [noise_generator((num_batches, self.style_channels))]
            styles = [s for s in styles]
        else:
            device = get_module_device(self)
            assert num_batches > 0 and not input_is_latent
            if self.default_style_mode == 'mix' and random.random() < self.mix_prob:
                styles = [torch.randn((num_batches, self.style_channels)) for _ in range(2)]
            else:
                styles = [torch.randn((num_batches, self.style_channels))]
            styles = [s for s in styles]
        if not input_is_latent:
            noise_batch = styles
            styles = [self.style_mapping(s) for s in styles]
        else:
            noise_batch = None
        if injected_noise is None:
            if randomize_noise:
                injected_noise = [None] * self.num_injected_noises
            else:
                injected_noise = [getattr(self, f'injected_noise_{i}') for i in range(self.num_injected_noises)]
        if truncation < 1:
            style_t = []
            if truncation_latent is None and not hasattr(self, 'truncation_latent'):
                self.truncation_latent = self.get_mean_latent()
                truncation_latent = self.truncation_latent
            elif truncation_latent is None and hasattr(self, 'truncation_latent'):
                truncation_latent = self.truncation_latent
            for style in styles:
                style_t.append(truncation_latent + truncation * (style - truncation_latent))
            styles = style_t
        if len(styles) < 2:
            inject_index = self.num_latents
            if styles[0].ndim < 3:
                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)
            else:
                latent = styles[0]
        else:
            if inject_index is None:
                inject_index = random.randint(1, self.num_latents - 1)
            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)
            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latents - inject_index, 1)
            latent = torch.cat([latent, latent2], 1)
        curr_log_size = self.log_size if curr_scale < 0 else int(np.log2(curr_scale))
        step = curr_log_size - 2
        _index = 0
        out = latent
        for i, (conv, to_rgb) in enumerate(zip(self.convs, self.to_rgbs)):
            if i > 0 and step > 0:
                out_prev = out
            out = conv(out, latent[:, _index], latent[:, _index + 1], noise1=injected_noise[2 * i], noise2=injected_noise[2 * i + 1])
            if i == step:
                out = to_rgb(out)
                if i > 0 and 0 <= transition_weight < 1:
                    skip_rgb = self.to_rgbs[i - 1](out_prev)
                    skip_rgb = F.interpolate(skip_rgb, scale_factor=2, mode='nearest')
                    out = (1 - transition_weight) * skip_rgb + transition_weight * out
                break
            _index += 2
        img = out
        if return_latents or return_noise:
            output_dict = dict(fake_img=img, latent=latent, inject_index=inject_index, noise_batch=noise_batch)
            return output_dict
        return img


class StyleGAN1Discriminator(nn.Module):
    """StyleGAN1 Discriminator.

    The architecture of this discriminator is proposed in StyleGAN1. More
    details can be found in: A Style-Based Generator Architecture for
    Generative Adversarial Networks CVPR2019.

    Args:
        in_size (int): The input size of images.
        blur_kernel (list, optional): The blurry kernel. Defaults
            to [1, 2, 1].
        mbstd_cfg (dict, optional): Configs for minibatch-stddev layer.
            Defaults to dict(group_size=4).
    """

    def __init__(self, in_size, blur_kernel=[1, 2, 1], mbstd_cfg=dict(group_size=4)):
        super().__init__()
        self.with_mbstd = mbstd_cfg is not None
        channels = {(4): 512, (8): 512, (16): 512, (32): 512, (64): 256, (128): 128, (256): 64, (512): 32, (1024): 16}
        log_size = int(np.log2(in_size))
        self.log_size = log_size
        in_channels = channels[in_size]
        self.convs = nn.ModuleList()
        self.from_rgb = nn.ModuleList()
        for i in range(log_size, 2, -1):
            out_channel = channels[2 ** (i - 1)]
            self.from_rgb.append(EqualizedLRConvModule(3, in_channels, kernel_size=3, padding=1, act_cfg=dict(type='LeakyReLU', negative_slope=0.2)))
            self.convs.append(nn.Sequential(EqualizedLRConvModule(in_channels, out_channel, kernel_size=3, padding=1, bias=True, norm_cfg=None, act_cfg=dict(type='LeakyReLU', negative_slope=0.2)), Blur(blur_kernel, pad=(1, 1)), EqualizedLRConvDownModule(out_channel, out_channel, kernel_size=3, stride=2, padding=1, act_cfg=None), nn.LeakyReLU(negative_slope=0.2, inplace=True)))
            in_channels = out_channel
        self.from_rgb.append(EqualizedLRConvModule(3, in_channels, kernel_size=3, padding=0, act_cfg=dict(type='LeakyReLU', negative_slope=0.2)))
        self.convs.append(nn.Sequential(EqualizedLRConvModule(in_channels + 1, 512, kernel_size=3, padding=1, bias=True, norm_cfg=None, act_cfg=dict(type='LeakyReLU', negative_slope=0.2)), EqualizedLRConvModule(512, 512, kernel_size=4, padding=0, bias=True, norm_cfg=None, act_cfg=None)))
        if self.with_mbstd:
            self.mbstd_layer = MiniBatchStddevLayer(**mbstd_cfg)
        self.final_linear = nn.Sequential(EqualLinearActModule(channels[4], 1))
        self.n_layer = len(self.convs)

    def forward(self, input, transition_weight=1.0, curr_scale=-1):
        """Forward function.

        Args:
            input (torch.Tensor): Input image tensor.
            transition_weight (float, optional): The weight used in resolution
                transition. Defaults to 1..
            curr_scale (int, optional): The resolution scale of image tensor.
                -1 means the max resolution scale of the StyleGAN1.
                Defaults to -1.

        Returns:
            torch.Tensor: Predict score for the input image.
        """
        curr_log_size = self.log_size if curr_scale < 0 else int(np.log2(curr_scale))
        step = curr_log_size - 2
        for i in range(step, -1, -1):
            index = self.n_layer - i - 1
            if i == step:
                out = self.from_rgb[index](input)
            if i == 0:
                out = self.mbstd_layer(out)
            out = self.convs[index](out)
            if i > 0:
                if i == step and 0 <= transition_weight < 1:
                    skip_rgb = F.avg_pool2d(input, 2)
                    skip_rgb = self.from_rgb[index + 1](skip_rgb)
                    out = (1 - transition_weight) * skip_rgb + transition_weight * out
        out = out.view(out.shape[0], -1)
        out = self.final_linear(out)
        return out


def conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1):
    if _should_use_custom_op(input):
        return _conv2d_gradfix(transpose=True, weight_shape=weight.shape, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation).apply(input, weight, bias)
    return torch.nn.functional.conv_transpose2d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, output_padding=output_padding, groups=groups, dilation=dilation)


class ModulatedConv2d(nn.Module):
    """Modulated Conv2d in StyleGANv2.

    This module implements the modulated convolution layers proposed in
    StyleGAN2. Details can be found in Analyzing and Improving the Image
    Quality of StyleGAN, CVPR2020.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        kernel_size (int): Kernel size, same as :obj:`nn.Con2d`.
        style_channels (int): Channels for the style codes.
        demodulate (bool, optional): Whether to adopt demodulation.
            Defaults to True.
        upsample (bool, optional): Whether to adopt upsampling in features.
            Defaults to False.
        downsample (bool, optional): Whether to adopt downsampling in features.
            Defaults to False.
        blur_kernel (list[int], optional): Blurry kernel.
            Defaults to [1, 3, 3, 1].
        equalized_lr_cfg (dict | None, optional): Configs for equalized lr.
            Defaults to dict(mode='fan_in', lr_mul=1., gain=1.).
        style_mod_cfg (dict, optional): Configs for style modulation module.
            Defaults to dict(bias_init=1.).
        style_bias (float, optional): Bias value for style code.
            Defaults to 0..
        eps (float, optional): Epsilon value to avoid computation error.
            Defaults to 1e-8.
    """

    def __init__(self, in_channels, out_channels, kernel_size, style_channels, demodulate=True, upsample=False, downsample=False, blur_kernel=[1, 3, 3, 1], equalized_lr_cfg=dict(mode='fan_in', lr_mul=1.0, gain=1.0), style_mod_cfg=dict(bias_init=1.0), style_bias=0.0, padding=None, eps=1e-08):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.style_channels = style_channels
        self.demodulate = demodulate
        assert isinstance(self.kernel_size, int) and (self.kernel_size >= 1 and self.kernel_size % 2 == 1)
        self.upsample = upsample
        self.downsample = downsample
        self.style_bias = style_bias
        self.eps = eps
        style_mod_cfg = dict() if style_mod_cfg is None else style_mod_cfg
        self.style_modulation = EqualLinearActModule(style_channels, in_channels, **style_mod_cfg)
        lr_mul_ = 1.0
        if equalized_lr_cfg is not None:
            lr_mul_ = equalized_lr_cfg.get('lr_mul', 1.0)
        self.weight = nn.Parameter(torch.randn(1, out_channels, in_channels, kernel_size, kernel_size).div_(lr_mul_))
        if upsample:
            factor = 2
            p = len(blur_kernel) - factor - (kernel_size - 1)
            pad0 = (p + 1) // 2 + factor - 1
            pad1 = p // 2 + 1
            self.blur = Blur(blur_kernel, (pad0, pad1), upsample_factor=factor)
        if downsample:
            factor = 2
            p = len(blur_kernel) - factor + (kernel_size - 1)
            pad0 = (p + 1) // 2
            pad1 = p // 2
            self.blur = Blur(blur_kernel, pad=(pad0, pad1))
        if equalized_lr_cfg is not None:
            equalized_lr(self, **equalized_lr_cfg)
        self.padding = padding if padding else kernel_size // 2

    def forward(self, x, style, input_gain=None):
        n, c, h, w = x.shape
        weight = self.weight
        if x.dtype == torch.float16 and self.demodulate:
            weight = weight * (1 / np.sqrt(self.in_channels * self.kernel_size * self.kernel_size) / weight.norm(float('inf'), dim=[1, 2, 3], keepdim=True))
            style = style / style.norm(float('inf'), dim=1, keepdim=True)
        style = self.style_modulation(style).view(n, 1, c, 1, 1) + self.style_bias
        weight = weight * style
        if self.demodulate:
            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)
            weight = weight * demod.view(n, self.out_channels, 1, 1, 1)
        if input_gain is not None:
            input_gain = input_gain.expand(n, self.in_channels)
            weight = weight * input_gain.unsqueeze(1).unsqueeze(3).unsqueeze(4)
        weight = weight.view(n * self.out_channels, c, self.kernel_size, self.kernel_size)
        weight = weight
        if self.upsample:
            x = x.reshape(1, n * c, h, w)
            weight = weight.view(n, self.out_channels, c, self.kernel_size, self.kernel_size)
            weight = weight.transpose(1, 2).reshape(n * c, self.out_channels, self.kernel_size, self.kernel_size)
            x = conv_transpose2d(x, weight, padding=0, stride=2, groups=n)
            x = x.reshape(n, self.out_channels, *x.shape[-2:])
            x = self.blur(x)
        elif self.downsample:
            x = self.blur(x)
            x = x.view(1, n * self.in_channels, *x.shape[-2:])
            x = conv2d(x, weight, stride=2, padding=0, groups=n)
            x = x.view(n, self.out_channels, *x.shape[-2:])
        else:
            x = x.reshape(1, n * c, h, w)
            x = conv2d(x, weight, stride=1, padding=self.padding, groups=n)
            x = x.view(n, self.out_channels, *x.shape[-2:])
        return x


def cast_tensor_type(inputs, src_type, dst_type):
    """Recursively convert Tensor in inputs from src_type to dst_type.

    Args:
        inputs: Inputs that to be casted.
        src_type (torch.dtype): Source type..
        dst_type (torch.dtype): Destination type.

    Returns:
        The same type with inputs, but all contained Tensors have been cast.
    """
    if isinstance(inputs, torch.Tensor):
        return inputs
    if isinstance(inputs, nn.Module):
        return inputs
    elif isinstance(inputs, str):
        return inputs
    elif isinstance(inputs, np.ndarray):
        return inputs
    elif isinstance(inputs, abc.Mapping):
        return type(inputs)({k: cast_tensor_type(v, src_type, dst_type) for k, v in inputs.items()})
    elif isinstance(inputs, abc.Iterable):
        return type(inputs)(cast_tensor_type(item, src_type, dst_type) for item in inputs)
    else:
        return inputs


def auto_fp16(apply_to=None, out_fp32=False):
    """Decorator to enable fp16 training automatically.

    This decorator is useful when you write custom modules and want to support
    mixed precision training. If inputs arguments are fp32 tensors, they will
    be converted to fp16 automatically. Arguments other than fp32 tensors are
    ignored. If you are using PyTorch >= 1.6, torch.cuda.amp is used as the
    backend, otherwise, original mmcv implementation will be adopted.

    Args:
        apply_to (Iterable, optional): The argument names to be converted.
            `None` indicates all arguments.
        out_fp32 (bool): Whether to convert the output back to fp32.

    Example:

        >>> import torch.nn as nn
        >>> class MyModule1(nn.Module):
        >>>
        >>>     # Convert x and y to fp16
        >>>     @auto_fp16()
        >>>     def forward(self, x, y):
        >>>         pass

        >>> import torch.nn as nn
        >>> class MyModule2(nn.Module):
        >>>
        >>>     # convert pred to fp16
        >>>     @auto_fp16(apply_to=('pred', ))
        >>>     def do_something(self, pred, others):
        >>>         pass
    """

    def auto_fp16_wrapper(old_func):

        @functools.wraps(old_func)
        def new_func(*args, **kwargs):
            if not isinstance(args[0], torch.nn.Module):
                raise TypeError('@auto_fp16 can only be used to decorate the method of nn.Module')
            if not (hasattr(args[0], 'fp16_enabled') and args[0].fp16_enabled):
                return old_func(*args, **kwargs)
            if hasattr(args[0], 'out_fp32') and args[0].out_fp32:
                _out_fp32 = True
            else:
                _out_fp32 = False
            args_info = getfullargspec(old_func)
            args_to_cast = [] if apply_to is None else apply_to
            new_args = []
            if args:
                arg_names = args_info.args[:len(args)]
                for i, arg_name in enumerate(arg_names):
                    if arg_name in args_to_cast:
                        new_args.append(cast_tensor_type(args[i], torch.float, torch.half))
                    else:
                        new_args.append(args[i])
            new_kwargs = {}
            if kwargs:
                for arg_name, arg_value in kwargs.items():
                    if arg_name in args_to_cast:
                        new_kwargs[arg_name] = cast_tensor_type(arg_value, torch.float, torch.half)
                    else:
                        new_kwargs[arg_name] = arg_value
            if TORCH_VERSION != 'parrots' and TORCH_VERSION >= '1.6.0':
                output = autocast(enabled=True)(old_func)(*new_args, **new_kwargs)
            else:
                raise RuntimeError('Please use PyTorch >= 1.6.0')
            if out_fp32 or _out_fp32:
                output = cast_tensor_type(output, torch.half, torch.float)
            return output
        return new_func
    return auto_fp16_wrapper


class ModulatedStyleConv(nn.Module):
    """Modulated Style Convolution.

    In this module, we integrate the modulated conv2d, noise injector and
    activation layers into together.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        kernel_size (int): Kernel size, same as :obj:`nn.Con2d`.
        style_channels (int): Channels for the style codes.
        demodulate (bool, optional): Whether to adopt demodulation.
            Defaults to True.
        upsample (bool, optional): Whether to adopt upsampling in features.
            Defaults to False.
        downsample (bool, optional): Whether to adopt downsampling in features.
            Defaults to False.
        blur_kernel (list[int], optional): Blurry kernel.
            Defaults to [1, 3, 3, 1].
        equalized_lr_cfg (dict | None, optional): Configs for equalized lr.
            Defaults to dict(mode='fan_in', lr_mul=1., gain=1.).
        style_mod_cfg (dict, optional): Configs for style modulation module.
            Defaults to dict(bias_init=1.).
        style_bias (float, optional): Bias value for style code.
            Defaults to ``0.``.
        fp16_enabled (bool, optional): Whether to use fp16 training in this
            module. Defaults to False.
        conv_clamp (float, optional): Clamp the convolutional layer results to
            avoid gradient overflow. Defaults to `256.0`.
    """

    def __init__(self, in_channels, out_channels, kernel_size, style_channels, upsample=False, blur_kernel=[1, 3, 3, 1], demodulate=True, style_mod_cfg=dict(bias_init=1.0), style_bias=0.0, fp16_enabled=False, conv_clamp=256):
        super().__init__()
        self.fp16_enabled = fp16_enabled
        self.conv_clamp = float(conv_clamp)
        self.conv = ModulatedConv2d(in_channels, out_channels, kernel_size, style_channels, demodulate=demodulate, upsample=upsample, blur_kernel=blur_kernel, style_mod_cfg=style_mod_cfg, style_bias=style_bias)
        self.noise_injector = NoiseInjection()
        self.activate = _FusedBiasLeakyReLU(out_channels)

    @auto_fp16(apply_to=('x', 'noise'))
    def forward(self, x, style, noise=None, return_noise=False):
        """Forward Function.

        Args:
            x ([Tensor): Input features with shape of (N, C, H, W).
            style (Tensor): Style latent with shape of (N, C).
            noise (Tensor, optional): Noise for injection. Defaults to None.
            return_noise (bool, optional): Whether to return noise tensors.
                Defaults to False.

        Returns:
            Tensor: Output features with shape of (N, C, H, W)
        """
        out = self.conv(x, style)
        if return_noise:
            out, noise = self.noise_injector(out, noise=noise, return_noise=return_noise)
        else:
            out = self.noise_injector(out, noise=noise, return_noise=return_noise)
        out = self.activate(out)
        if self.fp16_enabled:
            out = torch.clamp(out, min=-self.conv_clamp, max=self.conv_clamp)
        if return_noise:
            return out, noise
        return out


class UpsampleUpFIRDn(nn.Module):
    """UpFIRDn for Upsampling.

    This module is used in the ``to_rgb`` layers in StyleGAN2 for upsampling
    the images.

    Args:
        kernel (Array): Blur kernel/filter used in UpFIRDn.
        factor (int, optional): Upsampling factor. Defaults to 2.
    """

    def __init__(self, kernel, factor=2):
        super().__init__()
        self.factor = factor
        kernel = _make_kernel(kernel) * factor ** 2
        self.register_buffer('kernel', kernel)
        p = kernel.shape[0] - factor
        pad0 = (p + 1) // 2 + factor - 1
        pad1 = p // 2
        self.pad = pad0, pad1

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input feature map with shape of (N, C, H, W).

        Returns:
            Tensor: Output feature map.
        """
        out = upfirdn2d(x, self.kernel, up=self.factor, down=1, pad=self.pad)
        return out


class ModulatedToRGB(nn.Module):
    """To RGB layer.

    This module is designed to output image tensor in StyleGAN2.

    Args:
        in_channels (int): Input channels.
        style_channels (int): Channels for the style codes.
        out_channels (int, optional): Output channels. Defaults to 3.
        upsample (bool, optional): Whether to adopt upsampling in features.
            Defaults to False.
        blur_kernel (list[int], optional): Blurry kernel.
            Defaults to [1, 3, 3, 1].
        style_mod_cfg (dict, optional): Configs for style modulation module.
            Defaults to dict(bias_init=1.).
        style_bias (float, optional): Bias value for style code.
            Defaults to 0..
        fp16_enabled (bool, optional): Whether to use fp16 training in this
            module. Defaults to False.
        conv_clamp (float, optional): Clamp the convolutional layer results to
            avoid gradient overflow. Defaults to `256.0`.
        out_fp32 (bool, optional): Whether to convert the output feature map to
            `torch.float32`. Defaults to `True`.
    """

    def __init__(self, in_channels, style_channels, out_channels=3, upsample=True, blur_kernel=[1, 3, 3, 1], style_mod_cfg=dict(bias_init=1.0), style_bias=0.0, fp16_enabled=False, conv_clamp=256, out_fp32=True):
        super().__init__()
        if upsample:
            self.upsample = UpsampleUpFIRDn(blur_kernel)
        self.fp16_enabled = fp16_enabled
        self.conv_clamp = float(conv_clamp)
        self.conv = ModulatedConv2d(in_channels, out_channels=out_channels, kernel_size=1, style_channels=style_channels, demodulate=False, style_mod_cfg=style_mod_cfg, style_bias=style_bias)
        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))
        self.out_fp32 = out_fp32

    @auto_fp16(apply_to=('x', 'style'))
    def forward(self, x, style, skip=None):
        """Forward Function.

        Args:
            x ([Tensor): Input features with shape of (N, C, H, W).
            style (Tensor): Style latent with shape of (N, C).
            skip (Tensor, optional): Tensor for skip link. Defaults to None.

        Returns:
            Tensor: Output features with shape of (N, C, H, W)
        """
        out = self.conv(x, style)
        out = out + self.bias
        if self.fp16_enabled:
            out = torch.clamp(out, min=-self.conv_clamp, max=self.conv_clamp)
        if skip is not None:
            skip = self.upsample(skip)
            out = out + skip
        return out


class StyleGANv2Generator(nn.Module):
    """StyleGAN2 Generator.

    In StyleGAN2, we use a static architecture composing of a style mapping
    module and number of convolutional style blocks. More details can be found
    in: Analyzing and Improving the Image Quality of StyleGAN CVPR2020.

    You can load pretrained model through passing information into
    ``pretrained`` argument. We have already offered official weights as
    follows:

    - stylegan2-ffhq-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-ffhq-config-f-official_20210327_171224-bce9310c.pth  # noqa
    - stylegan2-horse-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-horse-config-f-official_20210327_173203-ef3e69ca.pth  # noqa
    - stylegan2-car-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-car-config-f-official_20210327_172340-8cfe053c.pth  # noqa
    - stylegan2-cat-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-cat-config-f-official_20210327_172444-15bc485b.pth  # noqa
    - stylegan2-church-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-church-config-f-official_20210327_172657-1d42b7d1.pth  # noqa

    If you want to load the ema model, you can just use following codes:

    .. code-block:: python

        # ckpt_http is one of the valid path from http source
        generator = StyleGANv2Generator(1024, 512,
                                        pretrained=dict(
                                            ckpt_path=ckpt_http,
                                            prefix='generator_ema'))

    Of course, you can also download the checkpoint in advance and set
    ``ckpt_path`` with local path. If you just want to load the original
    generator (not the ema model), please set the prefix with 'generator'.

    Note that our implementation allows to generate BGR image, while the
    original StyleGAN2 outputs RGB images by default. Thus, we provide
    ``bgr2rgb`` argument to convert the image space.

    Args:
        out_size (int): The output size of the StyleGAN2 generator.
        style_channels (int): The number of channels for style code.
        num_mlps (int, optional): The number of MLP layers. Defaults to 8.
        channel_multiplier (int, optional): The multiplier factor for the
            channel number. Defaults to 2.
        blur_kernel (list, optional): The blurry kernel. Defaults
            to [1, 3, 3, 1].
        lr_mlp (float, optional): The learning rate for the style mapping
            layer. Defaults to 0.01.
        default_style_mode (str, optional): The default mode of style mixing.
            In training, we defaultly adopt mixing style mode. However, in the
            evaluation, we use 'single' style mode. `['mix', 'single']` are
            currently supported. Defaults to 'mix'.
        eval_style_mode (str, optional): The evaluation mode of style mixing.
            Defaults to 'single'.
        mix_prob (float, optional): Mixing probability. The value should be
            in range of [0, 1]. Defaults to ``0.9``.
        num_fp16_scales (int, optional): The number of resolutions to use auto
            fp16 training. Different from ``fp16_enabled``, this argument
            allows users to adopt FP16 training only in several blocks.
            This behaviour is much more similar to the official implementation
            by Tero. Defaults to 0.
        fp16_enabled (bool, optional): Whether to use fp16 training in this
            module. If this flag is `True`, the whole module will be wrapped
            with ``auto_fp16``. Defaults to False.
        pretrained (dict | None, optional): Information for pretained models.
            The necessary key is 'ckpt_path'. Besides, you can also provide
            'prefix' to load the generator part from the whole state dict.
            Defaults to None.
    """

    def __init__(self, out_size, style_channels, num_mlps=8, channel_multiplier=2, blur_kernel=[1, 3, 3, 1], lr_mlp=0.01, default_style_mode='mix', eval_style_mode='single', mix_prob=0.9, num_fp16_scales=0, fp16_enabled=False, pretrained=None):
        super().__init__()
        self.out_size = out_size
        self.style_channels = style_channels
        self.num_mlps = num_mlps
        self.channel_multiplier = channel_multiplier
        self.lr_mlp = lr_mlp
        self._default_style_mode = default_style_mode
        self.default_style_mode = default_style_mode
        self.eval_style_mode = eval_style_mode
        self.mix_prob = mix_prob
        self.num_fp16_scales = num_fp16_scales
        self.fp16_enabled = fp16_enabled
        mapping_layers = [PixelNorm()]
        for _ in range(num_mlps):
            mapping_layers.append(EqualLinearActModule(style_channels, style_channels, equalized_lr_cfg=dict(lr_mul=lr_mlp, gain=1.0), act_cfg=dict(type='fused_bias')))
        self.style_mapping = nn.Sequential(*mapping_layers)
        self.channels = {(4): 512, (8): 512, (16): 512, (32): 512, (64): 256 * channel_multiplier, (128): 128 * channel_multiplier, (256): 64 * channel_multiplier, (512): 32 * channel_multiplier, (1024): 16 * channel_multiplier}
        self.constant_input = ConstantInput(self.channels[4])
        self.conv1 = ModulatedStyleConv(self.channels[4], self.channels[4], kernel_size=3, style_channels=style_channels, blur_kernel=blur_kernel)
        self.to_rgb1 = ModulatedToRGB(self.channels[4], style_channels, upsample=False, fp16_enabled=fp16_enabled)
        self.log_size = int(np.log2(self.out_size))
        self.convs = nn.ModuleList()
        self.upsamples = nn.ModuleList()
        self.to_rgbs = nn.ModuleList()
        in_channels_ = self.channels[4]
        for i in range(3, self.log_size + 1):
            out_channels_ = self.channels[2 ** i]
            _use_fp16 = self.log_size - i < num_fp16_scales or fp16_enabled
            self.convs.append(ModulatedStyleConv(in_channels_, out_channels_, 3, style_channels, upsample=True, blur_kernel=blur_kernel, fp16_enabled=_use_fp16))
            self.convs.append(ModulatedStyleConv(out_channels_, out_channels_, 3, style_channels, upsample=False, blur_kernel=blur_kernel, fp16_enabled=_use_fp16))
            self.to_rgbs.append(ModulatedToRGB(out_channels_, style_channels, upsample=True, fp16_enabled=_use_fp16))
            in_channels_ = out_channels_
        self.num_latents = self.log_size * 2 - 2
        self.num_injected_noises = self.num_latents - 1
        for layer_idx in range(self.num_injected_noises):
            res = (layer_idx + 5) // 2
            shape = [1, 1, 2 ** res, 2 ** res]
            self.register_buffer(f'injected_noise_{layer_idx}', torch.randn(*shape))
        if pretrained is not None:
            self._load_pretrained_model(**pretrained)

    def _load_pretrained_model(self, ckpt_path, prefix='', map_location='cpu', strict=True):
        state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
        self.load_state_dict(state_dict, strict=strict)
        mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')

    def train(self, mode=True):
        if mode:
            if self.default_style_mode != self._default_style_mode:
                mmcv.print_log(f'Switch to train style mode: {self._default_style_mode}', 'mmgen')
            self.default_style_mode = self._default_style_mode
        else:
            if self.default_style_mode != self.eval_style_mode:
                mmcv.print_log(f'Switch to evaluation style mode: {self.eval_style_mode}', 'mmgen')
            self.default_style_mode = self.eval_style_mode
        return super(StyleGANv2Generator, self).train(mode)

    def make_injected_noise(self):
        """make noises that will be injected into feature maps.

        Returns:
            list[Tensor]: List of layer-wise noise tensor.
        """
        device = get_module_device(self)
        noises = [torch.randn(1, 1, 2 ** 2, 2 ** 2, device=device)]
        for i in range(3, self.log_size + 1):
            for _ in range(2):
                noises.append(torch.randn(1, 1, 2 ** i, 2 ** i, device=device))
        return noises

    def get_mean_latent(self, num_samples=4096, **kwargs):
        """Get mean latent of W space in this generator.

        Args:
            num_samples (int, optional): Number of sample times. Defaults
                to 4096.

        Returns:
            Tensor: Mean latent of this generator.
        """
        return get_mean_latent(self, num_samples, **kwargs)

    def style_mixing(self, n_source, n_target, inject_index=1, truncation_latent=None, truncation=0.7):
        return style_mixing(self, n_source=n_source, n_target=n_target, inject_index=inject_index, truncation=truncation, truncation_latent=truncation_latent, style_channels=self.style_channels)

    @auto_fp16()
    def forward(self, styles, num_batches=-1, return_noise=False, return_latents=False, inject_index=None, truncation=1, truncation_latent=None, input_is_latent=False, injected_noise=None, randomize_noise=True):
        """Forward function.

        This function has been integrated with the truncation trick. Please
        refer to the usage of `truncation` and `truncation_latent`.

        Args:
            styles (torch.Tensor | list[torch.Tensor] | callable | None): In
                StyleGAN2, you can provide noise tensor or latent tensor. Given
                a list containing more than one noise or latent tensors, style
                mixing trick will be used in training. Of course, You can
                directly give a batch of noise through a ``torch.Tensor`` or
                offer a callable function to sample a batch of noise data.
                Otherwise, the ``None`` indicates to use the default noise
                sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            return_latents (bool, optional): If True, ``latent`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            inject_index (int | None, optional): The index number for mixing
                style codes. Defaults to None.
            truncation (float, optional): Truncation factor. Give value less
                than 1., the truncation trick will be adopted. Defaults to 1.
            truncation_latent (torch.Tensor, optional): Mean truncation latent.
                Defaults to None.
            input_is_latent (bool, optional): If `True`, the input tensor is
                the latent tensor. Defaults to False.
            injected_noise (torch.Tensor | None, optional): Given a tensor, the
                random noise will be fixed as this input injected noise.
                Defaults to None.
            randomize_noise (bool, optional): If `False`, images are sampled
                with the buffered noise tensor injected to the style conv
                block. Defaults to True.

        Returns:
            torch.Tensor | dict: Generated image tensor or dictionary                 containing more data.
        """
        if isinstance(styles, torch.Tensor):
            assert styles.shape[1] == self.style_channels
            styles = [styles]
        elif mmcv.is_seq_of(styles, torch.Tensor):
            for t in styles:
                assert t.shape[-1] == self.style_channels
        elif callable(styles):
            device = get_module_device(self)
            noise_generator = styles
            assert num_batches > 0
            if self.default_style_mode == 'mix' and random.random() < self.mix_prob:
                styles = [noise_generator((num_batches, self.style_channels)) for _ in range(2)]
            else:
                styles = [noise_generator((num_batches, self.style_channels))]
            styles = [s for s in styles]
        else:
            device = get_module_device(self)
            assert num_batches > 0 and not input_is_latent
            if self.default_style_mode == 'mix' and random.random() < self.mix_prob:
                styles = [torch.randn((num_batches, self.style_channels)) for _ in range(2)]
            else:
                styles = [torch.randn((num_batches, self.style_channels))]
            styles = [s for s in styles]
        if not input_is_latent:
            noise_batch = styles
            styles = [self.style_mapping(s) for s in styles]
        else:
            noise_batch = None
        if injected_noise is None:
            if randomize_noise:
                injected_noise = [None] * self.num_injected_noises
            else:
                injected_noise = [getattr(self, f'injected_noise_{i}') for i in range(self.num_injected_noises)]
        if truncation < 1:
            style_t = []
            if truncation_latent is None and not hasattr(self, 'truncation_latent'):
                self.truncation_latent = self.get_mean_latent()
                truncation_latent = self.truncation_latent
            elif truncation_latent is None and hasattr(self, 'truncation_latent'):
                truncation_latent = self.truncation_latent
            for style in styles:
                style_t.append(truncation_latent + truncation * (style - truncation_latent))
            styles = style_t
        if len(styles) < 2:
            inject_index = self.num_latents
            if styles[0].ndim < 3:
                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)
            else:
                latent = styles[0]
        else:
            if inject_index is None:
                inject_index = random.randint(1, self.num_latents - 1)
            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)
            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latents - inject_index, 1)
            latent = torch.cat([latent, latent2], 1)
        out = self.constant_input(latent)
        out = self.conv1(out, latent[:, 0], noise=injected_noise[0])
        skip = self.to_rgb1(out, latent[:, 1])
        _index = 1
        for up_conv, conv, noise1, noise2, to_rgb in zip(self.convs[::2], self.convs[1::2], injected_noise[1::2], injected_noise[2::2], self.to_rgbs):
            out = up_conv(out, latent[:, _index], noise=noise1)
            out = conv(out, latent[:, _index + 1], noise=noise2)
            skip = to_rgb(out, latent[:, _index + 2], skip)
            _index += 2
        img = skip
        if return_latents or return_noise:
            output_dict = dict(fake_img=img, latent=latent, inject_index=inject_index, noise_batch=noise_batch)
            return output_dict
        return img


class ConvDownLayer(nn.Sequential):
    """Convolution and Downsampling layer.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        kernel_size (int): Kernel size, same as :obj:`nn.Con2d`.
        downsample (bool, optional): Whether to adopt downsampling in features.
            Defaults to False.
        blur_kernel (list[int], optional): Blurry kernel.
            Defaults to [1, 3, 3, 1].
        bias (bool, optional): Whether to use bias parameter. Defaults to True.
        act_cfg (dict, optional): Activation configs.
            Defaults to dict(type='fused_bias').
        fp16_enabled (bool, optional): Whether to use fp16 training in this
            module. Defaults to False.
        conv_clamp (float, optional): Clamp the convolutional layer results to
            avoid gradient overflow. Defaults to `256.0`.
    """

    def __init__(self, in_channels, out_channels, kernel_size, downsample=False, blur_kernel=[1, 3, 3, 1], bias=True, act_cfg=dict(type='fused_bias'), fp16_enabled=False, conv_clamp=256.0):
        self.fp16_enabled = fp16_enabled
        self.conv_clamp = float(conv_clamp)
        layers = []
        if downsample:
            factor = 2
            p = len(blur_kernel) - factor + (kernel_size - 1)
            pad0 = (p + 1) // 2
            pad1 = p // 2
            layers.append(Blur(blur_kernel, pad=(pad0, pad1)))
            stride = 2
            self.padding = 0
        else:
            stride = 1
            self.padding = kernel_size // 2
        self.with_fused_bias = act_cfg is not None and act_cfg.get('type') == 'fused_bias'
        if self.with_fused_bias:
            conv_act_cfg = None
        else:
            conv_act_cfg = act_cfg
        layers.append(EqualizedLRConvModule(in_channels, out_channels, kernel_size, padding=self.padding, stride=stride, bias=bias and not self.with_fused_bias, norm_cfg=None, act_cfg=conv_act_cfg, equalized_lr_cfg=dict(mode='fan_in', gain=1.0)))
        if self.with_fused_bias:
            layers.append(_FusedBiasLeakyReLU(out_channels))
        super(ConvDownLayer, self).__init__(*layers)

    @auto_fp16(apply_to=('x',))
    def forward(self, x):
        x = super().forward(x)
        if self.fp16_enabled:
            x = torch.clamp(x, min=-self.conv_clamp, max=self.conv_clamp)
        return x


class ModMBStddevLayer(nn.Module):
    """Modified MiniBatch Stddev Layer.

    This layer is modified from ``MiniBatchStddevLayer`` used in PGGAN. In
    StyleGAN2, the authors add a new feature, `channel_groups`, into this
    layer.

    Note that to accelerate the training procedure, we also add a new feature
    of ``sync_std`` to achieve multi-nodes/machine training. This feature is
    still in beta version and we have tested it on 256 scales.

    Args:
        group_size (int, optional): The size of groups in batch dimension.
            Defaults to 4.
        channel_groups (int, optional): The size of groups in channel
            dimension. Defaults to 1.
        sync_std (bool, optional): Whether to use synchronized std feature.
            Defaults to False.
        sync_groups (int | None, optional): The size of groups in node
            dimension. Defaults to None.
        eps (float, optional): Epsilon value to avoid computation error.
            Defaults to 1e-8.
    """

    def __init__(self, group_size=4, channel_groups=1, sync_std=False, sync_groups=None, eps=1e-08):
        super().__init__()
        self.group_size = group_size
        self.eps = eps
        self.channel_groups = channel_groups
        self.sync_std = sync_std
        self.sync_groups = group_size if sync_groups is None else sync_groups
        if self.sync_std:
            assert torch.distributed.is_initialized(), 'Only in distributed training can the sync_std be activated.'
            mmcv.print_log('Adopt synced minibatch stddev layer', 'mmgen')

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input feature map with shape of (N, C, H, W).

        Returns:
            Tensor: Output feature map with shape of (N, C+1, H, W).
        """
        if self.sync_std:
            all_features = torch.cat(AllGatherLayer.apply(x), dim=0)
            rank, ws = get_dist_info()
            local_bs = all_features.shape[0] // ws
            start_idx = local_bs * rank
            if start_idx + self.sync_groups > all_features.shape[0]:
                start_idx = all_features.shape[0] - self.sync_groups
            end_idx = min(local_bs * rank + self.sync_groups, all_features.shape[0])
            x = all_features[start_idx:end_idx]
        assert x.shape[0] <= self.group_size or x.shape[0] % self.group_size == 0, f'Batch size be smaller than or equal to group size. Otherwise, batch size should be divisible by the group size.But got batch size {x.shape[0]}, group size {self.group_size}'
        assert x.shape[1] % self.channel_groups == 0, f'"channel_groups" must be divided by the feature channels. channel_groups: {self.channel_groups}, feature channels: {x.shape[1]}'
        n, c, h, w = x.shape
        group_size = min(n, self.group_size)
        y = torch.reshape(x, (group_size, -1, self.channel_groups, c // self.channel_groups, h, w))
        y = torch.var(y, dim=0, unbiased=False)
        y = torch.sqrt(y + self.eps)
        y = y.mean(dim=(2, 3, 4), keepdim=True).squeeze(2)
        y = y.repeat(group_size, 1, h, w)
        return torch.cat([x, y], dim=1)


class ResBlock(nn.Module):
    """Residual block used in the discriminator of StyleGAN2.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        kernel_size (int): Kernel size, same as :obj:`nn.Con2d`.
        fp16_enabled (bool, optional): Whether to use fp16 training in this
            module. Defaults to False.
        convert_input_fp32 (bool, optional): Whether to convert input type to
            fp32 if not `fp16_enabled`. This argument is designed to deal with
            the cases where some modules are run in FP16 and others in FP32.
            Defaults to True.
    """

    def __init__(self, in_channels, out_channels, blur_kernel=[1, 3, 3, 1], fp16_enabled=False, convert_input_fp32=True):
        super().__init__()
        self.fp16_enabled = fp16_enabled
        self.convert_input_fp32 = convert_input_fp32
        self.conv1 = ConvDownLayer(in_channels, in_channels, 3, blur_kernel=blur_kernel)
        self.conv2 = ConvDownLayer(in_channels, out_channels, 3, downsample=True, blur_kernel=blur_kernel)
        self.skip = ConvDownLayer(in_channels, out_channels, 1, downsample=True, act_cfg=None, bias=False, blur_kernel=blur_kernel)

    @auto_fp16()
    def forward(self, input):
        """Forward function.

        Args:
            input (Tensor): Input feature map with shape of (N, C, H, W).

        Returns:
            Tensor: Output feature map.
        """
        if not self.fp16_enabled and self.convert_input_fp32:
            input = input
        out = self.conv1(input)
        out = self.conv2(out)
        skip = self.skip(input)
        out = (out + skip) / np.sqrt(2)
        return out


class StyleGAN2Discriminator(nn.Module):
    """StyleGAN2 Discriminator.

    The architecture of this discriminator is proposed in StyleGAN2. More
    details can be found in: Analyzing and Improving the Image Quality of
    StyleGAN CVPR2020.

    You can load pretrained model through passing information into
    ``pretrained`` argument. We have already offered official weights as
    follows:

    - stylegan2-ffhq-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-ffhq-config-f-official_20210327_171224-bce9310c.pth  # noqa
    - stylegan2-horse-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-horse-config-f-official_20210327_173203-ef3e69ca.pth  # noqa
    - stylegan2-car-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-car-config-f-official_20210327_172340-8cfe053c.pth  # noqa
    - stylegan2-cat-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-cat-config-f-official_20210327_172444-15bc485b.pth  # noqa
    - stylegan2-church-config-f: https://download.openmmlab.com/mmgen/stylegan2/official_weights/stylegan2-church-config-f-official_20210327_172657-1d42b7d1.pth  # noqa

    If you want to load the ema model, you can just use following codes:

    .. code-block:: python

        # ckpt_http is one of the valid path from http source
        discriminator = StyleGAN2Discriminator(1024, 512,
                                               pretrained=dict(
                                                   ckpt_path=ckpt_http,
                                                   prefix='discriminator'))

    Of course, you can also download the checkpoint in advance and set
    ``ckpt_path`` with local path.

    Note that our implementation adopts BGR image as input, while the
    original StyleGAN2 provides RGB images to the discriminator. Thus, we
    provide ``bgr2rgb`` argument to convert the image space. If your images
    follow the RGB order, please set it to ``True`` accordingly.

    Args:
        in_size (int): The input size of images.
        channel_multiplier (int, optional): The multiplier factor for the
            channel number. Defaults to 2.
        blur_kernel (list, optional): The blurry kernel. Defaults
            to [1, 3, 3, 1].
        mbstd_cfg (dict, optional): Configs for minibatch-stddev layer.
            Defaults to dict(group_size=4, channel_groups=1).
        num_fp16_scales (int, optional): The number of resolutions to use auto
            fp16 training. Defaults to 0.
        fp16_enabled (bool, optional): Whether to use fp16 training in this
            module. Defaults to False.
        out_fp32 (bool, optional): Whether to convert the output feature map to
            `torch.float32`. Defaults to `True`.
        convert_input_fp32 (bool, optional): Whether to convert input type to
            fp32 if not `fp16_enabled`. This argument is designed to deal with
            the cases where some modules are run in FP16 and others in FP32.
            Defaults to True.
        input_bgr2rgb (bool, optional): Whether to reformat the input channels
            with order `rgb`. Since we provide several converted weights,
            whose input order is `rgb`. You can set this argument to True if
            you want to finetune on converted weights. Defaults to False.
        pretrained (dict | None, optional): Information for pretained models.
            The necessary key is 'ckpt_path'. Besides, you can also provide
            'prefix' to load the generator part from the whole state dict.
            Defaults to None.
    """

    def __init__(self, in_size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1], mbstd_cfg=dict(group_size=4, channel_groups=1), num_fp16_scales=0, fp16_enabled=False, out_fp32=True, convert_input_fp32=True, input_bgr2rgb=False, pretrained=None):
        super().__init__()
        self.num_fp16_scale = num_fp16_scales
        self.fp16_enabled = fp16_enabled
        self.convert_input_fp32 = convert_input_fp32
        self.out_fp32 = out_fp32
        channels = {(4): 512, (8): 512, (16): 512, (32): 512, (64): 256 * channel_multiplier, (128): 128 * channel_multiplier, (256): 64 * channel_multiplier, (512): 32 * channel_multiplier, (1024): 16 * channel_multiplier}
        log_size = int(np.log2(in_size))
        in_channels = channels[in_size]
        _use_fp16 = num_fp16_scales > 0
        convs = [ConvDownLayer(3, channels[in_size], 1, fp16_enabled=_use_fp16)]
        for i in range(log_size, 2, -1):
            out_channel = channels[2 ** (i - 1)]
            _use_fp16 = log_size - i < num_fp16_scales or fp16_enabled
            convs.append(ResBlock(in_channels, out_channel, blur_kernel, fp16_enabled=_use_fp16, convert_input_fp32=convert_input_fp32))
            in_channels = out_channel
        self.convs = nn.Sequential(*convs)
        self.mbstd_layer = ModMBStddevLayer(**mbstd_cfg)
        self.final_conv = ConvDownLayer(in_channels + 1, channels[4], 3)
        self.final_linear = nn.Sequential(EqualLinearActModule(channels[4] * 4 * 4, channels[4], act_cfg=dict(type='fused_bias')), EqualLinearActModule(channels[4], 1))
        self.input_bgr2rgb = input_bgr2rgb
        if pretrained is not None:
            self._load_pretrained_model(**pretrained)

    def _load_pretrained_model(self, ckpt_path, prefix='', map_location='cpu', strict=True):
        state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
        self.load_state_dict(state_dict, strict=strict)
        mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')

    @auto_fp16()
    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Input image tensor.

        Returns:
            torch.Tensor: Predict score for the input image.
        """
        if self.input_bgr2rgb:
            x = x[:, [2, 1, 0], ...]
        x = self.convs(x)
        x = self.mbstd_layer(x)
        if not self.final_conv.fp16_enabled and self.convert_input_fp32:
            x = x
        x = self.final_conv(x)
        x = x.view(x.shape[0], -1)
        x = self.final_linear(x)
        return x


class ADAStyleGAN2Discriminator(StyleGAN2Discriminator):

    def __init__(self, in_size, *args, data_aug=None, **kwargs):
        """StyleGANv2 Discriminator with adaptive augmentation.

        Args:
            in_size (int): The input size of images.
            data_aug (dict, optional): Config for data
                augmentation. Defaults to None.
        """
        super().__init__(in_size, *args, **kwargs)
        self.with_ada = data_aug is not None
        if self.with_ada:
            self.ada_aug = build_module(data_aug)
            self.ada_aug.requires_grad = False
        self.log_size = int(np.log2(in_size))

    def forward(self, x):
        """Forward function."""
        if self.with_ada:
            x = self.ada_aug.aug_pipeline(x)
        return super().forward(x)


_constant_cache = dict()


def constant(value, shape=None, dtype=None, device=None, memory_format=None):
    value = np.asarray(value)
    if shape is not None:
        shape = tuple(shape)
    if dtype is None:
        dtype = torch.get_default_dtype()
    if device is None:
        device = torch.device('cpu')
    if memory_format is None:
        memory_format = torch.contiguous_format
    key = value.shape, value.dtype, value.tobytes(), shape, dtype, device, memory_format
    tensor = _constant_cache.get(key, None)
    if tensor is None:
        tensor = torch.as_tensor(value.copy(), dtype=dtype, device=device)
        if shape is not None:
            tensor, _ = torch.broadcast_tensors(tensor, torch.empty(shape))
        tensor = tensor.contiguous(memory_format=memory_format)
        _constant_cache[key] = tensor
    return tensor


class ADAAug(nn.Module):
    """Data Augmentation Module for Adaptive Discriminator augmentation.

    Args:
        aug_pipeline (dict, optional): Config for augmentation pipeline.
            Defaults to None.
        update_interval (int, optional): Interval for updating
            augmentation probability. Defaults to 4.
        augment_initial_p (float, optional): Initial augmentation
            probability. Defaults to 0..
        ada_target (float, optional): ADA target. Defaults to 0.6.
        ada_kimg (int, optional): ADA training duration. Defaults to 500.
    """

    def __init__(self, aug_pipeline=None, update_interval=4, augment_initial_p=0.0, ada_target=0.6, ada_kimg=500):
        super().__init__()
        self.aug_pipeline = AugmentPipe(**aug_pipeline)
        self.update_interval = update_interval
        self.ada_kimg = ada_kimg
        self.ada_target = ada_target
        self.aug_pipeline.p.copy_(torch.tensor(augment_initial_p))
        self.register_buffer('log_buffer', torch.zeros((2,)))

    def update(self, iteration=0, num_batches=0):
        """Update Augment probability.

        Args:
            iteration (int, optional): Training iteration.
                Defaults to 0.
            num_batches (int, optional): The number of reals batches.
                Defaults to 0.
        """
        if (iteration + 1) % self.update_interval == 0:
            adjust_step = float(num_batches * self.update_interval) / float(self.ada_kimg * 1000.0)
            ada_heuristic = self.log_buffer[1] / self.log_buffer[0]
            adjust = np.sign(ada_heuristic.item() - self.ada_target) * adjust_step
            self.aug_pipeline.p.copy_((self.aug_pipeline.p + adjust).max(constant(0, device=self.log_buffer.device)))
            self.log_buffer = self.log_buffer * 0.0


class StyleGANv3Generator(nn.Module):
    """StyleGAN3 Generator.

    In StyleGAN3, we make several changes to StyleGANv2's generator which
    include transformed fourier features, filtered nonlinearities and
    non-critical sampling, etc. More details can be found in: Alias-Free
    Generative Adversarial Networks NeurIPS'2021.

    Ref: https://github.com/NVlabs/stylegan3

    Args:
        out_size (int): The output size of the StyleGAN3 generator.
        style_channels (int): The number of channels for style code.
        img_channels (int): The number of output's channels.
        noise_size (int, optional): Size of the input noise vector.
            Defaults to 512.
        rgb2bgr (bool, optional): Whether to reformat the output channels
                with order `bgr`. We provide several pre-trained StyleGAN3
                weights whose output channels order is `rgb`. You can set
                this argument to True to use the weights.
        pretrained (str | dict, optional): Path for the pretrained model or
            dict containing information for pretained models whose necessary
            key is 'ckpt_path'. Besides, you can also provide 'prefix' to load
            the generator part from the whole state dict. Defaults to None.
        synthesis_cfg (dict, optional): Config for synthesis network. Defaults
            to dict(type='SynthesisNetwork').
        mapping_cfg (dict, optional): Config for mapping network. Defaults to
            dict(type='MappingNetwork').
    """

    def __init__(self, out_size, style_channels, img_channels, noise_size=512, rgb2bgr=False, pretrained=None, synthesis_cfg=dict(type='SynthesisNetwork'), mapping_cfg=dict(type='MappingNetwork')):
        super().__init__()
        self.noise_size = noise_size
        self.style_channels = style_channels
        self.out_size = out_size
        self.img_channels = img_channels
        self.rgb2bgr = rgb2bgr
        self._synthesis_cfg = deepcopy(synthesis_cfg)
        self._synthesis_cfg.setdefault('style_channels', style_channels)
        self._synthesis_cfg.setdefault('out_size', out_size)
        self._synthesis_cfg.setdefault('img_channels', img_channels)
        self.synthesis = build_module(self._synthesis_cfg)
        self.num_ws = self.synthesis.num_ws
        self._mapping_cfg = deepcopy(mapping_cfg)
        self._mapping_cfg.setdefault('noise_size', noise_size)
        self._mapping_cfg.setdefault('style_channels', style_channels)
        self._mapping_cfg.setdefault('num_ws', self.num_ws)
        self.style_mapping = build_module(self._mapping_cfg)
        if pretrained is not None:
            self._load_pretrained_model(**pretrained)

    def _load_pretrained_model(self, ckpt_path, prefix='', map_location='cpu', strict=True):
        state_dict = _load_checkpoint_with_prefix(prefix, ckpt_path, map_location)
        self.load_state_dict(state_dict, strict=strict)
        mmcv.print_log(f'Load pretrained model from {ckpt_path}', 'mmgen')

    def forward(self, noise, num_batches=0, input_is_latent=False, truncation=1, num_truncation_layer=None, update_emas=False, force_fp32=True, return_noise=False, return_latents=False):
        """Forward Function for stylegan3.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            input_is_latent (bool, optional): If `True`, the input tensor is
                the latent tensor. Defaults to False.
            truncation (float, optional): Truncation factor. Give value less
                than 1., the truncation trick will be adopted. Defaults to 1.
            num_truncation_layer (int, optional): Number of layers use
                truncated latent. Defaults to None.
            update_emas (bool, optional): Whether update moving average of
                mean latent. Defaults to False.
            force_fp32 (bool, optional): Force fp32 ignore the weights.
                Defaults to True.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            return_latents (bool, optional): If True, ``latent`` will be
                returned in a dict with ``fake_img``. Defaults to False.
        Returns:
            torch.Tensor | dict: Generated image tensor or dictionary                 containing more data.
        """
        noise_size = self.style_channels if input_is_latent else self.noise_size
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == noise_size
            assert noise.ndim == 2, f'The noise should be in shape of (n, c), but got {noise.shape}'
            noise_batch = noise
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, noise_size))
        device = get_module_device(self)
        noise_batch = noise_batch
        if input_is_latent:
            ws = noise_batch.unsqueeze(1).repeat([1, self.num_ws, 1])
        else:
            ws = self.style_mapping(noise_batch, truncation=truncation, num_truncation_layer=num_truncation_layer, update_emas=update_emas)
        out_img = self.synthesis(ws, update_emas=update_emas, force_fp32=force_fp32)
        if self.rgb2bgr:
            out_img = out_img[:, [2, 1, 0], ...]
        if return_noise or return_latents:
            output = dict(fake_img=out_img, noise_batch=noise_batch, latent=ws)
            return output
        return out_img

    def get_mean_latent(self, num_samples=4096, **kwargs):
        """Get mean latent of W space in this generator.

        Args:
            num_samples (int, optional): Number of sample times. Defaults
                to 4096.

        Returns:
            Tensor: Mean latent of this generator.
        """
        if hasattr(self.style_mapping, 'w_avg'):
            return self.style_mapping.w_avg
        return get_mean_latent(self, num_samples, **kwargs)

    def get_training_kwargs(self, phase):
        """Get training kwargs. In StyleGANv3, we enable fp16, and update
        mangitude ema during training of discriminator. This function is used
        to pass related arguments.

        Args:
            phase (str): Current training phase.

        Returns:
            dict: Training kwargs.
        """
        if phase == 'disc':
            return dict(update_emas=True, force_fp32=False)
        if phase == 'gen':
            return dict(force_fp32=False)
        return {}


class DownsampleUpFIRDn(nn.Module):
    """UpFIRDn for Downsampling.

    This module is mentioned in StyleGAN2 for dowampling the feature maps.

    Args:
        kernel (Array): Blur kernel/filter used in UpFIRDn.
        factor (int, optional): Downsampling factor. Defaults to 2.
    """

    def __init__(self, kernel, factor=2):
        super().__init__()
        self.factor = factor
        kernel = _make_kernel(kernel)
        self.register_buffer('kernel', kernel)
        p = kernel.shape[0] - factor
        pad0 = (p + 1) // 2
        pad1 = p // 2
        self.pad = pad0, pad1

    def forward(self, input):
        """Forward function.

        Args:
            input (Tensor): Input feature map with shape of (N, C, H, W).

        Returns:
            Tensor: Output feature map.
        """
        out = upfirdn2d(input, self.kernel, up=1, down=self.factor, pad=self.pad)
        return out


class ModulatedPEConv2d(nn.Module):
    """Modulated Conv2d in StyleGANv2 with Positional Encoding (PE).

    This module is modified from the ``ModulatedConv2d`` in StyleGAN2 to
    support the experiments in: Positional Encoding as Spatial Inductive Bias
    in GANs, CVPR'2021.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        kernel_size (int): Kernel size, same as :obj:`nn.Con2d`.
        style_channels (int): Channels for the style codes.
        demodulate (bool, optional): Whether to adopt demodulation.
            Defaults to True.
        upsample (bool, optional): Whether to adopt upsampling in features.
            Defaults to False.
        downsample (bool, optional): Whether to adopt downsampling in features.
            Defaults to False.
        blur_kernel (list[int], optional): Blurry kernel.
            Defaults to [1, 3, 3, 1].
        equalized_lr_cfg (dict | None, optional): Configs for equalized lr.
            Defaults to dict(mode='fan_in', lr_mul=1., gain=1.).
        style_mod_cfg (dict, optional): Configs for style modulation module.
            Defaults to dict(bias_init=1.).
        style_bias (float, optional): Bias value for style code.
            Defaults to 0..
        eps (float, optional): Epsilon value to avoid computation error.
            Defaults to 1e-8.
        no_pad (bool, optional): Whether to removing the padding in
            convolution. Defaults to False.
        deconv2conv (bool, optional): Whether to substitute the transposed conv
            with (conv2d, upsampling). Defaults to False.
        interp_pad (int | None, optional): The padding number of interpolation
            pad. Defaults to None.
        up_config (dict, optional): Upsampling config.
            Defaults to dict(scale_factor=2, mode='nearest').
        up_after_conv (bool, optional): Whether to adopt upsampling after
            convolution. Defaults to False.
    """

    def __init__(self, in_channels, out_channels, kernel_size, style_channels, demodulate=True, upsample=False, downsample=False, blur_kernel=[1, 3, 3, 1], equalized_lr_cfg=dict(mode='fan_in', lr_mul=1.0, gain=1.0), style_mod_cfg=dict(bias_init=1.0), style_bias=0.0, eps=1e-08, no_pad=False, deconv2conv=False, interp_pad=None, up_config=dict(scale_factor=2, mode='nearest'), up_after_conv=False):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = kernel_size
        self.style_channels = style_channels
        self.demodulate = demodulate
        assert isinstance(self.kernel_size, int) and (self.kernel_size >= 1 and self.kernel_size % 2 == 1)
        self.upsample = upsample
        self.downsample = downsample
        self.style_bias = style_bias
        self.eps = eps
        self.no_pad = no_pad
        self.deconv2conv = deconv2conv
        self.interp_pad = interp_pad
        self.with_interp_pad = interp_pad is not None
        self.up_config = deepcopy(up_config)
        self.up_after_conv = up_after_conv
        style_mod_cfg = dict() if style_mod_cfg is None else style_mod_cfg
        self.style_modulation = EqualLinearActModule(style_channels, in_channels, **style_mod_cfg)
        lr_mul_ = 1.0
        if equalized_lr_cfg is not None:
            lr_mul_ = equalized_lr_cfg.get('lr_mul', 1.0)
        self.weight = nn.Parameter(torch.randn(1, out_channels, in_channels, kernel_size, kernel_size).div_(lr_mul_))
        if upsample and not self.deconv2conv:
            factor = 2
            p = len(blur_kernel) - factor - (kernel_size - 1)
            pad0 = (p + 1) // 2 + factor - 1
            pad1 = p // 2 + 1
            self.blur = Blur(blur_kernel, (pad0, pad1), upsample_factor=factor)
        if downsample:
            factor = 2
            p = len(blur_kernel) - factor + (kernel_size - 1)
            pad0 = (p + 1) // 2
            pad1 = p // 2
            self.blur = Blur(blur_kernel, pad=(pad0, pad1))
        if equalized_lr_cfg is not None:
            equalized_lr(self, **equalized_lr_cfg)
        self.padding = kernel_size // 2 if not no_pad else 0

    def forward(self, x, style):
        """Forward function.

        Args:
            x ([Tensor): Input features with shape of (N, C, H, W).
            style (Tensor): Style latent with shape of (N, C).

        Returns:
            Tensor: Output feature with shape of (N, C, H, W).
        """
        n, c, h, w = x.shape
        style = self.style_modulation(style).view(n, 1, c, 1, 1) + self.style_bias
        weight = self.weight * style
        if self.demodulate:
            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)
            weight = weight * demod.view(n, self.out_channels, 1, 1, 1)
        weight = weight.view(n * self.out_channels, c, self.kernel_size, self.kernel_size)
        if self.upsample and not self.deconv2conv:
            x = x.reshape(1, n * c, h, w)
            weight = weight.view(n, self.out_channels, c, self.kernel_size, self.kernel_size)
            weight = weight.transpose(1, 2).reshape(n * c, self.out_channels, self.kernel_size, self.kernel_size)
            x = conv_transpose2d(x, weight, padding=0, stride=2, groups=n)
            x = x.reshape(n, self.out_channels, *x.shape[-2:])
            x = self.blur(x)
        elif self.upsample and self.deconv2conv:
            if self.up_after_conv:
                x = x.reshape(1, n * c, h, w)
                x = conv2d(x, weight, padding=self.padding, groups=n)
                x = x.view(n, self.out_channels, *x.shape[2:4])
            if self.with_interp_pad:
                h_, w_ = x.shape[-2:]
                up_cfg_ = deepcopy(self.up_config)
                up_scale = up_cfg_.pop('scale_factor')
                size_ = h_ * up_scale + self.interp_pad, w_ * up_scale + self.interp_pad
                x = F.interpolate(x, size=size_, **up_cfg_)
            else:
                x = F.interpolate(x, **self.up_config)
            if not self.up_after_conv:
                h_, w_ = x.shape[-2:]
                x = x.view(1, n * c, h_, w_)
                x = conv2d(x, weight, padding=self.padding, groups=n)
                x = x.view(n, self.out_channels, *x.shape[2:4])
        elif self.downsample:
            x = self.blur(x)
            x = x.view(1, n * self.in_channels, *x.shape[-2:])
            x = conv2d(x, weight, stride=2, padding=0, groups=n)
            x = x.view(n, self.out_channels, *x.shape[-2:])
        else:
            x = x.view(1, n * c, h, w)
            x = conv2d(x, weight, stride=1, padding=self.padding, groups=n)
            x = x.view(n, self.out_channels, *x.shape[-2:])
        return x


class ModulatedPEStyleConv(nn.Module):
    """Modulated Style Convolution with Positional Encoding.

    This module is modified from the ``ModulatedStyleConv`` in StyleGAN2 to
    support the experiments in: Positional Encoding as Spatial Inductive Bias
    in GANs, CVPR'2021.

    Args:
        in_channels (int): Input channels.
        out_channels (int): Output channels.
        kernel_size (int): Kernel size, same as :obj:`nn.Con2d`.
        style_channels (int): Channels for the style codes.
        demodulate (bool, optional): Whether to adopt demodulation.
            Defaults to True.
        upsample (bool, optional): Whether to adopt upsampling in features.
            Defaults to False.
        downsample (bool, optional): Whether to adopt downsampling in features.
            Defaults to False.
        blur_kernel (list[int], optional): Blurry kernel.
            Defaults to [1, 3, 3, 1].
        equalized_lr_cfg (dict | None, optional): Configs for equalized lr.
            Defaults to dict(mode='fan_in', lr_mul=1., gain=1.).
        style_mod_cfg (dict, optional): Configs for style modulation module.
            Defaults to dict(bias_init=1.).
        style_bias (float, optional): Bias value for style code.
            Defaults to 0..
    """

    def __init__(self, in_channels, out_channels, kernel_size, style_channels, upsample=False, blur_kernel=[1, 3, 3, 1], demodulate=True, style_mod_cfg=dict(bias_init=1.0), style_bias=0.0, **kwargs):
        super().__init__()
        self.conv = ModulatedPEConv2d(in_channels, out_channels, kernel_size, style_channels, demodulate=demodulate, upsample=upsample, blur_kernel=blur_kernel, style_mod_cfg=style_mod_cfg, style_bias=style_bias, **kwargs)
        self.noise_injector = NoiseInjection()
        self.activate = _FusedBiasLeakyReLU(out_channels)

    def forward(self, x, style, noise=None, return_noise=False):
        """Forward Function.

        Args:
            x ([Tensor): Input features with shape of (N, C, H, W).
            style (Tensor): Style latent with shape of (N, C).
            noise (Tensor, optional): Noise for injection. Defaults to None.
            return_noise (bool, optional): Whether to return noise tensors.
                Defaults to False.

        Returns:
            Tensor: Output features with shape of (N, C, H, W)
        """
        out = self.conv(x, style)
        if return_noise:
            out, noise = self.noise_injector(out, noise=noise, return_noise=return_noise)
        else:
            out = self.noise_injector(out, noise=noise, return_noise=return_noise)
        out = self.activate(out)
        if return_noise:
            return out, noise
        return out


_bias_act_cuda_cache = dict()


class EasyDict(dict):
    """Convenience class that behaves like a dict but allows access with the
    attribute syntax."""

    def __getattr__(self, name: str) ->Any:
        try:
            return self[name]
        except KeyError:
            raise AttributeError(name)

    def __setattr__(self, name: str, value: Any) ->None:
        self[name] = value

    def __delattr__(self, name: str) ->None:
        del self[name]


activation_funcs = {'linear': EasyDict(func=lambda x, **_: x, def_alpha=0, def_gain=1, cuda_idx=1, ref='', has_2nd_grad=False), 'relu': EasyDict(func=lambda x, **_: torch.nn.functional.relu(x), def_alpha=0, def_gain=np.sqrt(2), cuda_idx=2, ref='y', has_2nd_grad=False), 'lrelu': EasyDict(func=lambda x, alpha, **_: torch.nn.functional.leaky_relu(x, alpha), def_alpha=0.2, def_gain=np.sqrt(2), cuda_idx=3, ref='y', has_2nd_grad=False), 'tanh': EasyDict(func=lambda x, **_: torch.tanh(x), def_alpha=0, def_gain=1, cuda_idx=4, ref='y', has_2nd_grad=True), 'sigmoid': EasyDict(func=lambda x, **_: torch.sigmoid(x), def_alpha=0, def_gain=1, cuda_idx=5, ref='y', has_2nd_grad=True), 'elu': EasyDict(func=lambda x, **_: torch.nn.functional.elu(x), def_alpha=0, def_gain=1, cuda_idx=6, ref='y', has_2nd_grad=True), 'selu': EasyDict(func=lambda x, **_: torch.nn.functional.selu(x), def_alpha=0, def_gain=1, cuda_idx=7, ref='y', has_2nd_grad=True), 'softplus': EasyDict(func=lambda x, **_: torch.nn.functional.softplus(x), def_alpha=0, def_gain=1, cuda_idx=8, ref='y', has_2nd_grad=True), 'swish': EasyDict(func=lambda x, **_: torch.sigmoid(x) * x, def_alpha=0, def_gain=np.sqrt(2), cuda_idx=9, ref='x', has_2nd_grad=True)}


def _bias_act_cuda(dim=1, act='linear', alpha=None, gain=None, clamp=None):
    """Fast CUDA implementation of `bias_act()` using custom ops."""
    assert clamp is None or clamp >= 0
    spec = activation_funcs[act]
    alpha = float(alpha if alpha is not None else spec.def_alpha)
    gain = float(gain if gain is not None else spec.def_gain)
    clamp = float(clamp if clamp is not None else -1)
    key = dim, act, alpha, gain, clamp
    if key in _bias_act_cuda_cache:
        return _bias_act_cuda_cache[key]


    class BiasActCuda(torch.autograd.Function):

        @staticmethod
        def forward(ctx, x, b):
            ctx.memory_format = torch.channels_last if x.ndim > 2 and x.stride(1) == 1 else torch.contiguous_format
            x = x.contiguous(memory_format=ctx.memory_format)
            b = b.contiguous() if b is not None else _null_tensor
            y = x
            if act != 'linear' or gain != 1 or clamp >= 0 or b is not _null_tensor:
                y = _plugin.bias_act(x, b, _null_tensor, _null_tensor, _null_tensor, 0, dim, spec.cuda_idx, alpha, gain, clamp)
            ctx.save_for_backward(x if 'x' in spec.ref or spec.has_2nd_grad else _null_tensor, b if 'x' in spec.ref or spec.has_2nd_grad else _null_tensor, y if 'y' in spec.ref else _null_tensor)
            return y

        @staticmethod
        def backward(ctx, dy):
            dy = dy.contiguous(memory_format=ctx.memory_format)
            x, b, y = ctx.saved_tensors
            dx = None
            db = None
            if ctx.needs_input_grad[0] or ctx.needs_input_grad[1]:
                dx = dy
                if act != 'linear' or gain != 1 or clamp >= 0:
                    dx = BiasActCudaGrad.apply(dy, x, b, y)
            if ctx.needs_input_grad[1]:
                db = dx.sum([i for i in range(dx.ndim) if i != dim])
            return dx, db


    class BiasActCudaGrad(torch.autograd.Function):

        @staticmethod
        def forward(ctx, dy, x, b, y):
            ctx.memory_format = torch.channels_last if dy.ndim > 2 and dy.stride(1) == 1 else torch.contiguous_format
            dx = _plugin.bias_act(dy, b, x, y, _null_tensor, 1, dim, spec.cuda_idx, alpha, gain, clamp)
            ctx.save_for_backward(dy if spec.has_2nd_grad else _null_tensor, x, b, y)
            return dx

        @staticmethod
        def backward(ctx, d_dx):
            d_dx = d_dx.contiguous(memory_format=ctx.memory_format)
            dy, x, b, y = ctx.saved_tensors
            d_dy = None
            d_x = None
            d_b = None
            d_y = None
            if ctx.needs_input_grad[0]:
                d_dy = BiasActCudaGrad.apply(d_dx, x, b, y)
            if spec.has_2nd_grad and (ctx.needs_input_grad[1] or ctx.needs_input_grad[2]):
                d_x = _plugin.bias_act(d_dx, b, x, y, dy, 2, dim, spec.cuda_idx, alpha, gain, clamp)
            if spec.has_2nd_grad and ctx.needs_input_grad[2]:
                d_b = d_x.sum([i for i in range(d_x.ndim) if i != dim])
            return d_dy, d_x, d_b, d_y
    _bias_act_cuda_cache[key] = BiasActCuda
    return BiasActCuda


def _bias_act_ref(x, b=None, dim=1, act='linear', alpha=None, gain=None, clamp=None):
    """Slow reference implementation of `bias_act()` using standard TensorFlow
    ops."""
    assert isinstance(x, torch.Tensor)
    assert clamp is None or clamp >= 0
    spec = activation_funcs[act]
    alpha = float(alpha if alpha is not None else spec.def_alpha)
    gain = float(gain if gain is not None else spec.def_gain)
    clamp = float(clamp if clamp is not None else -1)
    if b is not None:
        assert isinstance(b, torch.Tensor) and b.ndim == 1
        assert 0 <= dim < x.ndim
        assert b.shape[0] == x.shape[dim]
        x = x + b.reshape([(-1 if i == dim else 1) for i in range(x.ndim)])
    alpha = float(alpha)
    x = spec.func(x, alpha=alpha)
    gain = float(gain)
    if gain != 1:
        x = x * gain
    if clamp >= 0:
        x = x.clamp(-clamp, clamp)
    return x


def bias_act(x, b=None, dim=1, act='linear', alpha=None, gain=None, clamp=None, impl='cuda'):
    """Fused bias and activation function.
    Adds bias `b` to activation tensor `x`, evaluates activation function
    `act`, and scales the result by `gain`. Each of the steps is optional.
    In most cases, the fused op is considerably more efficient than performing
    the same calculation using standard PyTorch ops. It supports first and
    second order gradients, but not third order gradients.

    Args:
        x:      Input activation tensor. Can be of any shape.
        b:      Bias vector, or `None` to disable. Must be a 1D tensor of the
                same type as `x`. The shape must be known, and it must match
                the dimension of `x` corresponding to `dim`.
        dim:    The dimension in `x` corresponding to the elements of `b`.
                The value of `dim` is ignored if `b` is not specified.
        act:    Name of the activation function to evaluate, or `"linear"` to
                disable. Can be e.g. `"relu"`, `"lrelu"`, `"tanh"`,
                `"sigmoid"`, `"swish"`, etc. See `activation_funcs` for a full
                list. `None` is not allowed.
        alpha:  Shape parameter for the activation function, or `None` to use
                the default.
        gain:   Scaling factor for the output tensor, or `None` to use default.
                See `activation_funcs` for the default scaling of each
                activation function. If unsure, consider specifying 1.
        clamp:  Clamp the output values to `[-clamp, +clamp]`, or `None` to
                disable the clamping (default).
        impl:   Name of the implementation to use. Can be `"ref"` or `"cuda"`
                (default).

    Returns:
        Tensor of the same shape and datatype as `x`.
    """
    assert isinstance(x, torch.Tensor)
    assert impl in ['ref', 'cuda']
    if impl == 'cuda' and x.device.type == 'cuda' and _init():
        return _bias_act_cuda(dim=dim, act=act, alpha=alpha, gain=gain, clamp=clamp).apply(x, b)
    return _bias_act_ref(x=x, b=b, dim=dim, act=act, alpha=alpha, gain=gain, clamp=clamp)


class FullyConnectedLayer(nn.Module):
    """Fully connected layer used in StyleGANv3.

    Args:
        in_features (int): Number of channels in the input feature.
        out_features (int): Number of channels in the out feature.
        activation (str, optional): Activation function with choices 'relu',
            'lrelu', 'linear'. 'linear' means no extra activation.
            Defaults to 'linear'.
        bias (bool, optional): Whether to use additive bias. Defaults to True.
        lr_multiplier (float, optional): Equalized learning rate multiplier.
            Defaults to 1..
        weight_init (float, optional): Weight multiplier for initialization.
            Defaults to 1..
        bias_init (float, optional): Initial bias. Defaults to 0..
    """

    def __init__(self, in_features, out_features, activation='linear', bias=True, lr_multiplier=1.0, weight_init=1.0, bias_init=0.0):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.activation = activation
        self.weight = torch.nn.Parameter(torch.randn([out_features, in_features]) * (weight_init / lr_multiplier))
        bias_init = np.broadcast_to(np.asarray(bias_init, dtype=np.float32), [out_features])
        self.bias = torch.nn.Parameter(torch.from_numpy(bias_init / lr_multiplier)) if bias else None
        self.weight_gain = lr_multiplier / np.sqrt(in_features)
        self.bias_gain = lr_multiplier

    def forward(self, x):
        """Forward function."""
        w = self.weight * self.weight_gain
        b = self.bias
        if b is not None:
            b = b
            if self.bias_gain != 1:
                b = b * self.bias_gain
        if self.activation == 'linear' and b is not None:
            x = torch.addmm(b.unsqueeze(0), x, w.t())
        else:
            x = x.matmul(w.t())
            x = bias_act.bias_act(x, b, act=self.activation)
        return x


class MappingNetwork(nn.Module):
    """Style mapping network used in StyleGAN3. The main difference between it
    and styleganv1,v2 is that mean latent is registered as a buffer and dynamic
    updated during training.

    Args:
        noise_size (int, optional): Size of the input noise vector.
        c_dim (int, optional): Size of the input noise vector.
        style_channels (int): The number of channels for style code.
        num_ws (int): The repeat times of w latent.
        num_layers (int, optional): The number of layers of mapping network.
            Defaults to 2.
        lr_multiplier (float, optional): Equalized learning rate multiplier.
            Defaults to 0.01.
        w_avg_beta (float, optional): The value used for update `w_avg`.
            Defaults to 0.998.
    """

    def __init__(self, noise_size, style_channels, num_ws, c_dim=0, num_layers=2, lr_multiplier=0.01, w_avg_beta=0.998):
        super().__init__()
        self.noise_size = noise_size
        self.c_dim = c_dim
        self.style_channels = style_channels
        self.num_ws = num_ws
        self.num_layers = num_layers
        self.w_avg_beta = w_avg_beta
        self.embed = FullyConnectedLayer(self.c_dim, self.style_channels) if self.c_dim > 0 else None
        features = [self.noise_size + (self.style_channels if self.c_dim > 0 else 0)] + [self.style_channels] * self.num_layers
        for idx, in_features, out_features in zip(range(num_layers), features[:-1], features[1:]):
            layer = FullyConnectedLayer(in_features, out_features, activation='lrelu', lr_multiplier=lr_multiplier)
            setattr(self, f'fc{idx}', layer)
        self.register_buffer('w_avg', torch.zeros([style_channels]))

    def forward(self, z, c=None, truncation=1, num_truncation_layer=None, update_emas=False):
        """Style mapping function.

        Args:
            z (torch.Tensor): Input noise tensor.
            c (torch.Tensor, optional): Input label tensor. Defaults to None.
            truncation (float, optional): Truncation factor. Give value less
                than 1., the truncation trick will be adopted. Defaults to 1.
            num_truncation_layer (int, optional): Number of layers use
                truncated latent. Defaults to None.
            update_emas (bool, optional): Whether update moving average of
                mean latent. Defaults to False.

        Returns:
            torch.Tensor: W-plus latent.
        """
        if num_truncation_layer is None:
            num_truncation_layer = self.num_ws
        x = z
        x = x * (x.square().mean(1, keepdim=True) + 1e-08).rsqrt()
        if self.c_dim > 0:
            y = self.embed(c)
            y = y * (y.square().mean(1, keepdim=True) + 1e-08).rsqrt()
            x = torch.cat([x, y], dim=1) if x is not None else y
        for idx in range(self.num_layers):
            x = getattr(self, f'fc{idx}')(x)
        if update_emas:
            self.w_avg.copy_(x.detach().mean(dim=0).lerp(self.w_avg, self.w_avg_beta))
        x = x.unsqueeze(1).repeat([1, self.num_ws, 1])
        if truncation != 1:
            x[:, :num_truncation_layer] = self.w_avg.lerp(x[:, :num_truncation_layer], truncation)
        return x


class SynthesisInput(nn.Module):
    """Module which generate input for synthesis layer.

    Args:
        style_channels (int): The number of channels for style code.
        channels (int): The number of output channel.
        size (int): The size of sampling grid.
        sampling_rate (int): Sampling rate for construct sampling grid.
        bandwidth (float): Bandwidth of random frequencies.
    """

    def __init__(self, style_channels, channels, size, sampling_rate, bandwidth):
        super().__init__()
        self.style_channels = style_channels
        self.channels = channels
        self.size = np.broadcast_to(np.asarray(size), [2])
        self.sampling_rate = sampling_rate
        self.bandwidth = bandwidth
        freqs = torch.randn([self.channels, 2])
        radii = freqs.square().sum(dim=1, keepdim=True).sqrt()
        freqs /= radii * radii.square().exp().pow(0.25)
        freqs *= bandwidth
        phases = torch.rand([self.channels]) - 0.5
        self.weight = torch.nn.Parameter(torch.randn([self.channels, self.channels]))
        self.affine = FullyConnectedLayer(style_channels, 4, weight_init=0, bias_init=[1, 0, 0, 0])
        self.register_buffer('transform', torch.eye(3, 3))
        self.register_buffer('freqs', freqs)
        self.register_buffer('phases', phases)

    def forward(self, w):
        """Forward function."""
        transforms = self.transform.unsqueeze(0)
        freqs = self.freqs.unsqueeze(0)
        phases = self.phases.unsqueeze(0)
        t = self.affine(w)
        t = t / t[:, :2].norm(dim=1, keepdim=True)
        m_r = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1])
        m_r[:, 0, 0] = t[:, 0]
        m_r[:, 0, 1] = -t[:, 1]
        m_r[:, 1, 0] = t[:, 1]
        m_r[:, 1, 1] = t[:, 0]
        m_t = torch.eye(3, device=w.device).unsqueeze(0).repeat([w.shape[0], 1, 1])
        m_t[:, 0, 2] = -t[:, 2]
        m_t[:, 1, 2] = -t[:, 3]
        transforms = m_r @ m_t @ transforms
        phases = phases + (freqs @ transforms[:, :2, 2:]).squeeze(2)
        freqs = freqs @ transforms[:, :2, :2]
        amplitudes = (1 - (freqs.norm(dim=2) - self.bandwidth) / (self.sampling_rate / 2 - self.bandwidth)).clamp(0, 1)
        theta = torch.eye(2, 3, device=w.device)
        theta[0, 0] = 0.5 * self.size[0] / self.sampling_rate
        theta[1, 1] = 0.5 * self.size[1] / self.sampling_rate
        grids = torch.nn.functional.affine_grid(theta.unsqueeze(0), [1, 1, self.size[1], self.size[0]], align_corners=False)
        x = (grids.unsqueeze(3) @ freqs.permute(0, 2, 1).unsqueeze(1).unsqueeze(2)).squeeze(3)
        x = x + phases.unsqueeze(1).unsqueeze(2)
        x = torch.sin(x * (np.pi * 2))
        x = x * amplitudes.unsqueeze(1).unsqueeze(2)
        weight = self.weight / np.sqrt(self.channels)
        x = x @ weight.t()
        x = x.permute(0, 3, 1, 2)
        return x


_filtered_lrelu_cuda_cache = dict()


def _filtered_lrelu_cuda(up=1, down=1, padding=0, gain=np.sqrt(2), slope=0.2, clamp=None, flip_filter=False):
    """Fast CUDA implementation of `filtered_lrelu()` using custom ops."""
    assert isinstance(up, int) and up >= 1
    assert isinstance(down, int) and down >= 1
    px0, px1, py0, py1 = _parse_padding(padding)
    assert gain == float(gain) and gain > 0
    gain = float(gain)
    assert slope == float(slope) and slope >= 0
    slope = float(slope)
    assert clamp is None or clamp == float(clamp) and clamp >= 0
    clamp = float(clamp if clamp is not None else 'inf')
    key = up, down, px0, px1, py0, py1, gain, slope, clamp, flip_filter
    if key in _filtered_lrelu_cuda_cache:
        return _filtered_lrelu_cuda_cache[key]


    class FilteredLReluCuda(torch.autograd.Function):

        @staticmethod
        def forward(ctx, x, fu, fd, b, si, sx, sy):
            assert isinstance(x, torch.Tensor) and x.ndim == 4
            if fu is None:
                fu = torch.ones([1, 1], dtype=torch.float32, device=x.device)
            if fd is None:
                fd = torch.ones([1, 1], dtype=torch.float32, device=x.device)
            assert 1 <= fu.ndim <= 2
            assert 1 <= fd.ndim <= 2
            if up == 1 and fu.ndim == 1 and fu.shape[0] == 1:
                fu = fu.square()[None]
            if down == 1 and fd.ndim == 1 and fd.shape[0] == 1:
                fd = fd.square()[None]
            if si is None:
                si = torch.empty([0])
            if b is None:
                b = torch.zeros([x.shape[1]], dtype=x.dtype, device=x.device)
            write_signs = si.numel() == 0 and (x.requires_grad or b.requires_grad)
            strides = [x.stride(i) for i in range(x.ndim) if x.size(i) > 1]
            if any(a < b for a, b in zip(strides[:-1], strides[1:])):
                warnings.warn('low-performance memory layout detected in filtered_lrelu input', RuntimeWarning)
            if x.dtype in [torch.float16, torch.float32]:
                if torch.cuda.current_stream(x.device) != torch.cuda.default_stream(x.device):
                    warnings.warn('filtered_lrelu called with non-default cuda stream but concurrent execution is not supported', RuntimeWarning)
                y, so, return_code = _plugin.filtered_lrelu(x, fu, fd, b, si, up, down, px0, px1, py0, py1, sx, sy, gain, slope, clamp, flip_filter, write_signs)
            else:
                return_code = -1
            if return_code < 0:
                warnings.warn('filtered_lrelu called with parameters that have no optimized CUDA kernel, using generic fallback', RuntimeWarning)
                y = x.add(b.unsqueeze(-1).unsqueeze(-1))
                y = upfirdn2d.upfirdn2d(x=y, f=fu, up=up, padding=[px0, px1, py0, py1], gain=up ** 2, flip_filter=flip_filter)
                so = _plugin.filtered_lrelu_act_(y, si, sx, sy, gain, slope, clamp, write_signs)
                y = upfirdn2d.upfirdn2d(x=y, f=fd, down=down, flip_filter=flip_filter)
            ctx.save_for_backward(fu, fd, si if si.numel() else so)
            ctx.x_shape = x.shape
            ctx.y_shape = y.shape
            ctx.s_ofs = sx, sy
            return y

        @staticmethod
        def backward(ctx, dy):
            fu, fd, si = ctx.saved_tensors
            _, _, xh, xw = ctx.x_shape
            _, _, yh, yw = ctx.y_shape
            sx, sy = ctx.s_ofs
            dx = None
            dfu = None
            assert not ctx.needs_input_grad[1]
            dfd = None
            assert not ctx.needs_input_grad[2]
            db = None
            dsi = None
            assert not ctx.needs_input_grad[4]
            dsx = None
            assert not ctx.needs_input_grad[5]
            dsy = None
            assert not ctx.needs_input_grad[6]
            if ctx.needs_input_grad[0] or ctx.needs_input_grad[3]:
                pp = [fu.shape[-1] - 1 + (fd.shape[-1] - 1) - px0, xw * up - yw * down + px0 - (up - 1), fu.shape[0] - 1 + (fd.shape[0] - 1) - py0, xh * up - yh * down + py0 - (up - 1)]
                gg = gain * up ** 2 / down ** 2
                ff = not flip_filter
                sx = sx - (fu.shape[-1] - 1) + px0
                sy = sy - (fu.shape[0] - 1) + py0
                dx = _filtered_lrelu_cuda(up=down, down=up, padding=pp, gain=gg, slope=slope, clamp=None, flip_filter=ff).apply(dy, fd, fu, None, si, sx, sy)
            if ctx.needs_input_grad[3]:
                db = dx.sum([0, 2, 3])
            return dx, dfu, dfd, db, dsi, dsx, dsy
    _filtered_lrelu_cuda_cache[key] = FilteredLReluCuda
    return FilteredLReluCuda


def _filtered_lrelu_ref(x, fu=None, fd=None, b=None, up=1, down=1, padding=0, gain=np.sqrt(2), slope=0.2, clamp=None, flip_filter=False):
    """Slow and memory-inefficient reference implementation of
    `filtered_lrelu()` using existing `upfirdn2n()` and `bias_act()` ops."""
    assert isinstance(x, torch.Tensor) and x.ndim == 4
    fu_w, fu_h = _get_filter_size(fu)
    fd_w, fd_h = _get_filter_size(fd)
    if b is not None:
        assert isinstance(b, torch.Tensor) and b.dtype == x.dtype
    assert isinstance(up, int) and up >= 1
    assert isinstance(down, int) and down >= 1
    px0, px1, py0, py1 = _parse_padding(padding)
    assert gain == float(gain) and gain > 0
    assert slope == float(slope) and slope >= 0
    assert clamp is None or clamp == float(clamp) and clamp >= 0
    batch_size, channels, in_h, in_w = x.shape
    in_dtype = x.dtype
    out_w = (in_w * up + (px0 + px1) - (fu_w - 1) - (fd_w - 1) + (down - 1)) // down
    out_h = (in_h * up + (py0 + py1) - (fu_h - 1) - (fd_h - 1) + (down - 1)) // down
    x = bias_act.bias_act(x=x, b=b)
    x = upfirdn2d.upfirdn2d(x=x, f=fu, up=up, padding=[px0, px1, py0, py1], gain=up ** 2, flip_filter=flip_filter)
    x = bias_act.bias_act(x=x, act='lrelu', alpha=slope, gain=gain, clamp=clamp)
    x = upfirdn2d.upfirdn2d(x=x, f=fd, down=down, flip_filter=flip_filter)
    assert x.shape == (batch_size, channels, out_h, out_w)
    assert x.dtype == in_dtype
    return x


def filtered_lrelu(x, fu=None, fd=None, b=None, up=1, down=1, padding=0, gain=np.sqrt(2), slope=0.2, clamp=None, flip_filter=False, impl='cuda'):
    """Filtered leaky ReLU for a batch of 2D images.

    Performs the following sequence of operations for each channel:

    1. Add channel-specific bias if provided (`b`).

    2. Upsample the image by inserting N-1 zeros after each pixel (`up`).

    3. Pad the image with the specified number of zeros on each side
      (`padding`). Negative padding corresponds to cropping the image.

    4. Convolve the image with the specified upsampling FIR filter (`fu`),
       shrinking it so that the footprint of all output pixels lies within the
       input image.

    5. Multiply each value by the provided gain factor (`gain`).

    6. Apply leaky ReLU activation function to each value.

    7. Clamp each value between -clamp and +clamp, if `clamp` parameter is
    provided.

    8. Convolve the image with the specified downsampling FIR filter (`fd`),
       shrinking it so that the footprint of all output pixels lies within the
       input image.

    9. Downsample the image by keeping every Nth pixel (`down`).

    The fused op is considerably more efficient than performing the same
    calculation using standard PyTorch ops. It supports gradients of arbitrary
    order.

    Args:
        x:           Float32/float16/float64 input tensor of the shape
                     `[batch_size, num_channels, in_height, in_width]`.
        fu:          Float32 upsampling FIR filter of the shape
                     `[filter_height, filter_width]` (non-separable),
                     `[filter_taps]` (separable), or
                     `None` (identity).
        fd:          Float32 downsampling FIR filter of the shape
                     `[filter_height, filter_width]` (non-separable),
                     `[filter_taps]` (separable), or
                     `None` (identity).
        b:           Bias vector, or `None` to disable. Must be a 1D tensor of
                     the same type as `x`. The length of vector must must match
                     the channel dimension of `x`.
        up:          Integer upsampling factor (default: 1).
        down:        Integer downsampling factor. (default: 1).
        padding:     Padding with respect to the upsampled image. Can be a
                     single number or a list/tuple `[x, y]` or `[x_before,
                     x_after, y_before, y_after]` (default: 0).
        gain:        Overall scaling factor for signal magnitude (default:
                     sqrt(2)).
        slope:       Slope on the negative side of leaky ReLU (default: 0.2).
        clamp:       Maximum magnitude for leaky ReLU output (default: None).
        flip_filter: False = convolution, True = correlation (default: False).
        impl:        Implementation to use. Can be `'ref'` or `'cuda'`
                     (default: `'cuda'`).

    Returns:
        Tensor of the shape `[batch_size, num_channels, out_height,
                out_width]`.
    """
    assert isinstance(x, torch.Tensor)
    assert impl in ['ref', 'cuda']
    if impl == 'cuda' and x.device.type == 'cuda' and _init():
        return _filtered_lrelu_cuda(up=up, down=down, padding=padding, gain=gain, slope=slope, clamp=clamp, flip_filter=flip_filter).apply(x, fu, fd, b, None, 0, 0)
    return _filtered_lrelu_ref(x, fu=fu, fd=fd, b=b, up=up, down=down, padding=padding, gain=gain, slope=slope, clamp=clamp, flip_filter=flip_filter)


def modulated_conv2d(x, w, s, demodulate=True, padding=0, input_gain=None):
    """Modulated Conv2d in StyleGANv3.

    Args:
        x (torch.Tensor): Input tensor with shape (batch_size, in_channels,
            height, width).
        w (torch.Tensor): Weight of modulated convolution with shape
            (out_channels, in_channels, kernel_height, kernel_width).
        s (torch.Tensor): Style tensor with shape (batch_size, in_channels).
        demodulate (bool): Whether apply weight demodulation. Defaults to True.
        padding (int or list[int]): Convolution padding. Defaults to 0.
        input_gain (list[int]): Scaling factors for input. Defaults to None.

    Returns:
        torch.Tensor: Convolution Output.
    """
    batch_size = int(x.shape[0])
    _, in_channels, kh, kw = w.shape
    if demodulate:
        w = w * w.square().mean([1, 2, 3], keepdim=True).rsqrt()
        s = s * s.square().mean().rsqrt()
    w = w.unsqueeze(0)
    w = w * s.unsqueeze(1).unsqueeze(3).unsqueeze(4)
    if demodulate:
        dcoefs = (w.square().sum(dim=[2, 3, 4]) + 1e-08).rsqrt()
        w = w * dcoefs.unsqueeze(2).unsqueeze(3).unsqueeze(4)
    if input_gain is not None:
        input_gain = input_gain.expand(batch_size, in_channels)
        w = w * input_gain.unsqueeze(1).unsqueeze(3).unsqueeze(4)
    x = x.reshape(1, -1, *x.shape[2:])
    w = w.reshape(-1, in_channels, kh, kw)
    x = conv2d_gradfix.conv2d(input=x, weight=w, padding=padding, groups=batch_size)
    x = x.reshape(batch_size, -1, *x.shape[2:])
    return x


class SynthesisLayer(nn.Module):
    """Layer of Synthesis network for stylegan3.

    Args:
        style_channels (int): The number of channels for style code.
        is_torgb (bool): Whether output of this layer is transformed to
            rgb image.
        is_critically_sampled (bool): Whether filter cutoff is set exactly
            at the bandlimit.
        use_fp16 (bool, optional): Whether to use fp16 training in this
            module. If this flag is `True`, the whole module will be wrapped
            with ``auto_fp16``.
        in_channels (int): The channel number of the input feature map.
        out_channels (int): The channel number of the output feature map.
        in_size (int): The input size of feature map.
        out_size (int): The output size of feature map.
        in_sampling_rate (int): Sampling rate for upsampling filter.
        out_sampling_rate (int): Sampling rate for downsampling filter.
        in_cutoff (float): Cutoff frequency for upsampling filter.
        out_cutoff (float): Cutoff frequency for downsampling filter.
        in_half_width (float): The approximate width of the transition region
            for upsampling filter.
        out_half_width (float): The approximate width of the transition region
            for downsampling filter.
        conv_kernel (int, optional): The kernel of modulated convolution.
            Defaults to 3.
        filter_size (int, optional): Base filter size. Defaults to 6.
        lrelu_upsampling (int, optional): Upsamling rate for `filtered_lrelu`.
            Defaults to 2.
        use_radial_filters (bool, optional): Whether use radially symmetric
            jinc-based filter in downsamping filter. Defaults to False.
        conv_clamp (int, optional): Clamp bound for convolution.
            Defaults to 256.
        magnitude_ema_beta (float, optional): Beta coefficient for calculating
            input magnitude ema. Defaults to 0.999.
    """

    def __init__(self, style_channels, is_torgb, is_critically_sampled, use_fp16, in_channels, out_channels, in_size, out_size, in_sampling_rate, out_sampling_rate, in_cutoff, out_cutoff, in_half_width, out_half_width, conv_kernel=3, filter_size=6, lrelu_upsampling=2, use_radial_filters=False, conv_clamp=256, magnitude_ema_beta=0.999):
        super().__init__()
        self.style_channels = style_channels
        self.is_torgb = is_torgb
        self.is_critically_sampled = is_critically_sampled
        self.use_fp16 = use_fp16
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.in_size = np.broadcast_to(np.asarray(in_size), [2])
        self.out_size = np.broadcast_to(np.asarray(out_size), [2])
        self.in_sampling_rate = in_sampling_rate
        self.out_sampling_rate = out_sampling_rate
        self.tmp_sampling_rate = max(in_sampling_rate, out_sampling_rate) * (1 if is_torgb else lrelu_upsampling)
        self.in_cutoff = in_cutoff
        self.out_cutoff = out_cutoff
        self.in_half_width = in_half_width
        self.out_half_width = out_half_width
        self.conv_kernel = 1 if is_torgb else conv_kernel
        self.conv_clamp = conv_clamp
        self.magnitude_ema_beta = magnitude_ema_beta
        self.affine = FullyConnectedLayer(self.style_channels, self.in_channels, bias_init=1)
        self.weight = torch.nn.Parameter(torch.randn([self.out_channels, self.in_channels, self.conv_kernel, self.conv_kernel]))
        self.bias = torch.nn.Parameter(torch.zeros([self.out_channels]))
        self.register_buffer('magnitude_ema', torch.ones([]))
        self.up_factor = int(np.rint(self.tmp_sampling_rate / self.in_sampling_rate))
        assert self.in_sampling_rate * self.up_factor == self.tmp_sampling_rate
        self.up_taps = filter_size * self.up_factor if self.up_factor > 1 and not self.is_torgb else 1
        self.register_buffer('up_filter', self.design_lowpass_filter(numtaps=self.up_taps, cutoff=self.in_cutoff, width=self.in_half_width * 2, fs=self.tmp_sampling_rate))
        self.down_factor = int(np.rint(self.tmp_sampling_rate / self.out_sampling_rate))
        assert self.out_sampling_rate * self.down_factor == self.tmp_sampling_rate
        self.down_taps = filter_size * self.down_factor if self.down_factor > 1 and not self.is_torgb else 1
        self.down_radial = use_radial_filters and not self.is_critically_sampled
        self.register_buffer('down_filter', self.design_lowpass_filter(numtaps=self.down_taps, cutoff=self.out_cutoff, width=self.out_half_width * 2, fs=self.tmp_sampling_rate, radial=self.down_radial))
        pad_total = (self.out_size - 1) * self.down_factor + 1
        pad_total -= (self.in_size + self.conv_kernel - 1) * self.up_factor
        pad_total += self.up_taps + self.down_taps - 2
        pad_lo = (pad_total + self.up_factor) // 2
        pad_hi = pad_total - pad_lo
        self.padding = [int(pad_lo[0]), int(pad_hi[0]), int(pad_lo[1]), int(pad_hi[1])]

    def forward(self, x, w, force_fp32=False, update_emas=False):
        """Forward function for synthesis layer.

        Args:
            x (torch.Tensor): Input feature map tensor.
            w (torch.Tensor): Input style tensor.
            force_fp32 (bool, optional): Force fp32 ignore the weights.
                Defaults to True.
            update_emas (bool, optional): Whether update moving average of
                input magnitude. Defaults to False.

        Returns:
            torch.Tensor: Output feature map tensor.
        """
        if update_emas:
            with torch.autograd.profiler.record_function('update_magnitude_ema'):
                magnitude_cur = x.detach().square().mean()
                self.magnitude_ema.copy_(magnitude_cur.lerp(self.magnitude_ema, self.magnitude_ema_beta))
        input_gain = self.magnitude_ema.rsqrt()
        styles = self.affine(w)
        if self.is_torgb:
            weight_gain = 1 / np.sqrt(self.in_channels * self.conv_kernel ** 2)
            styles = styles * weight_gain
        dtype = torch.float16 if self.use_fp16 and not force_fp32 and x.device.type == 'cuda' else torch.float32
        x = modulated_conv2d(x=x, w=self.weight, s=styles, padding=self.conv_kernel - 1, demodulate=not self.is_torgb, input_gain=input_gain)
        gain = 1 if self.is_torgb else np.sqrt(2)
        slope = 1 if self.is_torgb else 0.2
        x = filtered_lrelu.filtered_lrelu(x=x, fu=self.up_filter, fd=self.down_filter, b=self.bias, up=self.up_factor, down=self.down_factor, padding=self.padding, gain=gain, slope=slope, clamp=self.conv_clamp)
        assert x.dtype == dtype
        return x

    @staticmethod
    def design_lowpass_filter(numtaps, cutoff, width, fs, radial=False):
        """Design lowpass filter giving related arguments.

        Args:
            numtaps (int): Length of the filter. `numtaps` must be odd if a
                passband includes the Nyquist frequency.
            cutoff (float): Cutoff frequency of filter
            width (float): The approximate width of the transition region.
            fs (float): The sampling frequency of the signal.
            radial (bool, optional):  Whether use radially symmetric jinc-based
                filter. Defaults to False.

        Returns:
            torch.Tensor: Kernel of lowpass filter.
        """
        assert numtaps >= 1
        if numtaps == 1:
            return None
        if not radial:
            f = scipy.signal.firwin(numtaps=numtaps, cutoff=cutoff, width=width, fs=fs)
            return torch.as_tensor(f, dtype=torch.float32)
        x = (np.arange(numtaps) - (numtaps - 1) / 2) / fs
        r = np.hypot(*np.meshgrid(x, x))
        f = scipy.special.j1(2 * cutoff * (np.pi * r)) / (np.pi * r)
        beta = scipy.signal.kaiser_beta(scipy.signal.kaiser_atten(numtaps, width / (fs / 2)))
        w = np.kaiser(numtaps, beta)
        f *= np.outer(w, w)
        f /= np.sum(f)
        return torch.as_tensor(f, dtype=torch.float32)


class SynthesisNetwork(nn.Module):
    """Synthesis network for stylegan3.

    Args:
        style_channels (int): The number of channels for style code.
        out_size (int): The resolution of output image.
        img_channels (int): The number of channels for output image.
        channel_base (int, optional): Overall multiplier for the number of
            channels. Defaults to 32768.
        channel_max (int, optional): Maximum number of channels in any layer.
            Defaults to 512.
        num_layers (int, optional): Total number of layers, excluding Fourier
            features and ToRGB. Defaults to 14.
        num_critical (int, optional):  Number of critically sampled layers at
            the end. Defaults to 2.
        first_cutoff (int, optional): Cutoff frequency of the first layer.
            Defaults to 2.
        first_stopband (int, optional): Minimum stopband of the first layer.
            Defaults to 2**2.1.
        last_stopband_rel (float, optional): Minimum stopband of the last
            layer, expressed relative to the cutoff. Defaults to 2**0.3.
        margin_size (int, optional): Number of additional pixels outside the
            image. Defaults to 10.
        output_scale (float, optional): Scale factor for output value.
            Defaults to 0.25.
        num_fp16_res (int, optional): Number of first few layers use fp16.
            Defaults to 4.
    """

    def __init__(self, style_channels, out_size, img_channels, channel_base=32768, channel_max=512, num_layers=14, num_critical=2, first_cutoff=2, first_stopband=2 ** 2.1, last_stopband_rel=2 ** 0.3, margin_size=10, output_scale=0.25, num_fp16_res=4, **layer_kwargs):
        super().__init__()
        self.style_channels = style_channels
        self.num_ws = num_layers + 2
        self.out_size = out_size
        self.img_channels = img_channels
        self.num_layers = num_layers
        self.num_critical = num_critical
        self.margin_size = margin_size
        self.output_scale = output_scale
        self.num_fp16_res = num_fp16_res
        last_cutoff = self.out_size / 2
        last_stopband = last_cutoff * last_stopband_rel
        exponents = np.minimum(np.arange(self.num_layers + 1) / (self.num_layers - self.num_critical), 1)
        cutoffs = first_cutoff * (last_cutoff / first_cutoff) ** exponents
        stopbands = first_stopband * (last_stopband / first_stopband) ** exponents
        sampling_rates = np.exp2(np.ceil(np.log2(np.minimum(stopbands * 2, self.out_size))))
        half_widths = np.maximum(stopbands, sampling_rates / 2) - cutoffs
        sizes = sampling_rates + self.margin_size * 2
        sizes[-2:] = self.out_size
        channels = np.rint(np.minimum(channel_base / 2 / cutoffs, channel_max))
        channels[-1] = self.img_channels
        self.input = SynthesisInput(style_channels=self.style_channels, channels=int(channels[0]), size=int(sizes[0]), sampling_rate=sampling_rates[0], bandwidth=cutoffs[0])
        self.layer_names = []
        for idx in range(self.num_layers + 1):
            prev = max(idx - 1, 0)
            is_torgb = idx == self.num_layers
            is_critically_sampled = idx >= self.num_layers - self.num_critical
            use_fp16 = sampling_rates[idx] * 2 ** self.num_fp16_res > self.out_size
            layer = SynthesisLayer(style_channels=self.style_channels, is_torgb=is_torgb, is_critically_sampled=is_critically_sampled, use_fp16=use_fp16, in_channels=int(channels[prev]), out_channels=int(channels[idx]), in_size=int(sizes[prev]), out_size=int(sizes[idx]), in_sampling_rate=int(sampling_rates[prev]), out_sampling_rate=int(sampling_rates[idx]), in_cutoff=cutoffs[prev], out_cutoff=cutoffs[idx], in_half_width=half_widths[prev], out_half_width=half_widths[idx], **layer_kwargs)
            name = f'L{idx}_{layer.out_size[0]}_{layer.out_channels}'
            setattr(self, name, layer)
            self.layer_names.append(name)

    def forward(self, ws, **layer_kwargs):
        """Forward function."""
        ws = ws.unbind(dim=1)
        x = self.input(ws[0])
        for name, w in zip(self.layer_names, ws[1:]):
            x = getattr(self, name)(x, w, **layer_kwargs)
        if self.output_scale != 1:
            x = x * self.output_scale
        x = x
        return x


class MSStyleGANv2Generator(nn.Module):
    """StyleGAN2 Generator.

    In StyleGAN2, we use a static architecture composing of a style mapping
    module and number of convolutional style blocks. More details can be found
    in: Analyzing and Improving the Image Quality of StyleGAN CVPR2020.

    Args:
        out_size (int): The output size of the StyleGAN2 generator.
        style_channels (int): The number of channels for style code.
        num_mlps (int, optional): The number of MLP layers. Defaults to 8.
        channel_multiplier (int, optional): The multiplier factor for the
            channel number. Defaults to 2.
        blur_kernel (list, optional): The blurry kernel. Defaults
            to [1, 3, 3, 1].
        lr_mlp (float, optional): The learning rate for the style mapping
            layer. Defaults to 0.01.
        default_style_mode (str, optional): The default mode of style mixing.
            In training, we defaultly adopt mixing style mode. However, in the
            evaluation, we use 'single' style mode. `['mix', 'single']` are
            currently supported. Defaults to 'mix'.
        eval_style_mode (str, optional): The evaluation mode of style mixing.
            Defaults to 'single'.
        mix_prob (float, optional): Mixing probability. The value should be
            in range of [0, 1]. Defaults to 0.9.
    """

    def __init__(self, out_size, style_channels, num_mlps=8, channel_multiplier=2, blur_kernel=[1, 3, 3, 1], lr_mlp=0.01, default_style_mode='mix', eval_style_mode='single', mix_prob=0.9, no_pad=False, deconv2conv=False, interp_pad=None, up_config=dict(scale_factor=2, mode='nearest'), up_after_conv=False, head_pos_encoding=None, head_pos_size=(4, 4), interp_head=False):
        super().__init__()
        self.out_size = out_size
        self.style_channels = style_channels
        self.num_mlps = num_mlps
        self.channel_multiplier = channel_multiplier
        self.lr_mlp = lr_mlp
        self._default_style_mode = default_style_mode
        self.default_style_mode = default_style_mode
        self.eval_style_mode = eval_style_mode
        self.mix_prob = mix_prob
        self.no_pad = no_pad
        self.deconv2conv = deconv2conv
        self.interp_pad = interp_pad
        self.with_interp_pad = interp_pad is not None
        self.up_config = deepcopy(up_config)
        self.up_after_conv = up_after_conv
        self.head_pos_encoding = head_pos_encoding
        self.head_pos_size = head_pos_size
        self.interp_head = interp_head
        mapping_layers = [PixelNorm()]
        for _ in range(num_mlps):
            mapping_layers.append(EqualLinearActModule(style_channels, style_channels, equalized_lr_cfg=dict(lr_mul=lr_mlp, gain=1.0), act_cfg=dict(type='fused_bias')))
        self.style_mapping = nn.Sequential(*mapping_layers)
        self.channels = {(4): 512, (8): 512, (16): 512, (32): 512, (64): 256 * channel_multiplier, (128): 128 * channel_multiplier, (256): 64 * channel_multiplier, (512): 32 * channel_multiplier, (1024): 16 * channel_multiplier}
        in_ch = self.channels[4]
        if self.head_pos_encoding:
            if self.head_pos_encoding['type'] in ['CatersianGrid', 'CSG', 'CSG2d']:
                in_ch = 2
            self.head_pos_enc = build_module(self.head_pos_encoding)
        else:
            size_ = 4
            if self.no_pad:
                size_ += 2
            self.constant_input = ConstantInput(self.channels[4], size=size_)
        self.conv1 = ModulatedPEStyleConv(in_ch, self.channels[4], kernel_size=3, style_channels=style_channels, blur_kernel=blur_kernel, deconv2conv=self.deconv2conv, no_pad=self.no_pad, up_config=self.up_config, interp_pad=self.interp_pad)
        self.to_rgb1 = ModulatedToRGB(self.channels[4], style_channels, upsample=False)
        self.log_size = int(np.log2(self.out_size))
        self.convs = nn.ModuleList()
        self.upsamples = nn.ModuleList()
        self.to_rgbs = nn.ModuleList()
        in_channels_ = self.channels[4]
        for i in range(3, self.log_size + 1):
            out_channels_ = self.channels[2 ** i]
            self.convs.append(ModulatedPEStyleConv(in_channels_, out_channels_, 3, style_channels, upsample=True, blur_kernel=blur_kernel, deconv2conv=self.deconv2conv, no_pad=self.no_pad, up_config=self.up_config, interp_pad=self.interp_pad, up_after_conv=self.up_after_conv))
            self.convs.append(ModulatedPEStyleConv(out_channels_, out_channels_, 3, style_channels, upsample=False, blur_kernel=blur_kernel, deconv2conv=self.deconv2conv, no_pad=self.no_pad, up_config=self.up_config, interp_pad=self.interp_pad, up_after_conv=self.up_after_conv))
            self.to_rgbs.append(ModulatedToRGB(out_channels_, style_channels, upsample=True))
            in_channels_ = out_channels_
        self.num_latents = self.log_size * 2 - 2
        self.num_injected_noises = self.num_latents - 1
        noises = self.make_injected_noise()
        for layer_idx in range(self.num_injected_noises):
            self.register_buffer(f'injected_noise_{layer_idx}', noises[layer_idx])

    def train(self, mode=True):
        if mode:
            if self.default_style_mode != self._default_style_mode:
                mmcv.print_log(f'Switch to train style mode: {self._default_style_mode}', 'mmgen')
            self.default_style_mode = self._default_style_mode
        else:
            if self.default_style_mode != self.eval_style_mode:
                mmcv.print_log(f'Switch to evaluation style mode: {self.eval_style_mode}', 'mmgen')
            self.default_style_mode = self.eval_style_mode
        return super(MSStyleGANv2Generator, self).train(mode)

    def make_injected_noise(self, chosen_scale=0):
        device = get_module_device(self)
        base_scale = 2 ** 2 + chosen_scale
        noises = [torch.randn(1, 1, base_scale, base_scale, device=device)]
        for i in range(3, self.log_size + 1):
            for n in range(2):
                _pad = 0
                if self.no_pad and not self.up_after_conv and n == 0:
                    _pad = 2
                noises.append(torch.randn(1, 1, base_scale * 2 ** (i - 2) + _pad, base_scale * 2 ** (i - 2) + _pad, device=device))
        return noises

    def get_mean_latent(self, num_samples=4096, **kwargs):
        """Get mean latent of W space in this generator.

        Args:
            num_samples (int, optional): Number of sample times. Defaults
                to 4096.

        Returns:
            Tensor: Mean latent of this generator.
        """
        return get_mean_latent(self, num_samples, **kwargs)

    def style_mixing(self, n_source, n_target, inject_index=1, truncation_latent=None, truncation=0.7, chosen_scale=0):
        return style_mixing(self, n_source=n_source, n_target=n_target, inject_index=inject_index, truncation_latent=truncation_latent, truncation=truncation, style_channels=self.style_channels, chosen_scale=chosen_scale)

    def forward(self, styles, num_batches=-1, return_noise=False, return_latents=False, inject_index=None, truncation=1, truncation_latent=None, input_is_latent=False, injected_noise=None, randomize_noise=True, chosen_scale=0):
        """Forward function.

        This function has been integrated with the truncation trick. Please
        refer to the usage of `truncation` and `truncation_latent`.

        Args:
            styles (torch.Tensor | list[torch.Tensor] | callable | None): In
                StyleGAN2, you can provide noise tensor or latent tensor. Given
                a list containing more than one noise or latent tensors, style
                mixing trick will be used in training. Of course, You can
                directly give a batch of noise through a ``torch.Tensor`` or
                offer a callable function to sample a batch of noise data.
                Otherwise, the ``None`` indicates to use the default noise
                sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            return_noise (bool, optional): If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            return_latents (bool, optional): If True, ``latent`` will be
                returned in a dict with ``fake_img``. Defaults to False.
            inject_index (int | None, optional): The index number for mixing
                style codes. Defaults to None.
            truncation (float, optional): Truncation factor. Give value less
                than 1., the truncation trick will be adopted. Defaults to 1.
            truncation_latent (torch.Tensor, optional): Mean truncation latent.
                Defaults to None.
            input_is_latent (bool, optional): If `True`, the input tensor is
                the latent tensor. Defaults to False.
            injected_noise (torch.Tensor | None, optional): Given a tensor, the
                random noise will be fixed as this input injected noise.
                Defaults to None.
            randomize_noise (bool, optional): If `False`, images are sampled
                with the buffered noise tensor injected to the style conv
                block. Defaults to True.

        Returns:
            torch.Tensor | dict: Generated image tensor or dictionary                 containing more data.
        """
        if isinstance(styles, torch.Tensor):
            assert styles.shape[1] == self.style_channels
            styles = [styles]
        elif mmcv.is_seq_of(styles, torch.Tensor):
            for t in styles:
                assert t.shape[-1] == self.style_channels
        elif callable(styles):
            device = get_module_device(self)
            noise_generator = styles
            assert num_batches > 0
            if self.default_style_mode == 'mix' and random.random() < self.mix_prob:
                styles = [noise_generator((num_batches, self.style_channels)) for _ in range(2)]
            else:
                styles = [noise_generator((num_batches, self.style_channels))]
            styles = [s for s in styles]
        else:
            device = get_module_device(self)
            assert num_batches > 0 and not input_is_latent
            if self.default_style_mode == 'mix' and random.random() < self.mix_prob:
                styles = [torch.randn((num_batches, self.style_channels)) for _ in range(2)]
            else:
                styles = [torch.randn((num_batches, self.style_channels))]
            styles = [s for s in styles]
        if not input_is_latent:
            noise_batch = styles
            styles = [self.style_mapping(s) for s in styles]
        else:
            noise_batch = None
        if injected_noise is None:
            if randomize_noise:
                injected_noise = [None] * self.num_injected_noises
            elif chosen_scale > 0:
                if not hasattr(self, f'injected_noise_{chosen_scale}_0'):
                    noises_ = self.make_injected_noise(chosen_scale)
                    for i in range(self.num_injected_noises):
                        setattr(self, f'injected_noise_{chosen_scale}_{i}', noises_[i])
                injected_noise = [getattr(self, f'injected_noise_{chosen_scale}_{i}') for i in range(self.num_injected_noises)]
            else:
                injected_noise = [getattr(self, f'injected_noise_{i}') for i in range(self.num_injected_noises)]
        if truncation < 1:
            style_t = []
            if truncation_latent is None and not hasattr(self, 'truncation_latent'):
                self.truncation_latent = self.get_mean_latent()
                truncation_latent = self.truncation_latent
            elif truncation_latent is None and hasattr(self, 'truncation_latent'):
                truncation_latent = self.truncation_latent
            for style in styles:
                style_t.append(truncation_latent + truncation * (style - truncation_latent))
            styles = style_t
        if len(styles) < 2:
            inject_index = self.num_latents
            if styles[0].ndim < 3:
                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)
            else:
                latent = styles[0]
        else:
            if inject_index is None:
                inject_index = random.randint(1, self.num_latents - 1)
            latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)
            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latents - inject_index, 1)
            latent = torch.cat([latent, latent2], 1)
        if isinstance(chosen_scale, int):
            chosen_scale = chosen_scale, chosen_scale
        if self.head_pos_encoding:
            if self.interp_head:
                out = self.head_pos_enc.make_grid2d(self.head_pos_size[0], self.head_pos_size[1], latent.size(0))
                h_in = self.head_pos_size[0] + chosen_scale[0]
                w_in = self.head_pos_size[1] + chosen_scale[1]
                out = F.interpolate(out, size=(h_in, w_in), mode='bilinear', align_corners=True)
            else:
                out = self.head_pos_enc.make_grid2d(self.head_pos_size[0] + chosen_scale[0], self.head_pos_size[1] + chosen_scale[1], latent.size(0))
            out = out
        else:
            out = self.constant_input(latent)
            if chosen_scale[0] != 0 or chosen_scale[1] != 0:
                out = F.interpolate(out, size=(out.shape[2] + chosen_scale[0], out.shape[3] + chosen_scale[1]), mode='bilinear', align_corners=True)
        out = self.conv1(out, latent[:, 0], noise=injected_noise[0])
        skip = self.to_rgb1(out, latent[:, 1])
        _index = 1
        for up_conv, conv, noise1, noise2, to_rgb in zip(self.convs[::2], self.convs[1::2], injected_noise[1::2], injected_noise[2::2], self.to_rgbs):
            out = up_conv(out, latent[:, _index], noise=noise1)
            out = conv(out, latent[:, _index + 1], noise=noise2)
            skip = to_rgb(out, latent[:, _index + 2], skip)
            _index += 2
        img = skip
        if return_latents or return_noise:
            output_dict = dict(fake_img=img, latent=latent, inject_index=inject_index, noise_batch=noise_batch, injected_noise=injected_noise)
            return output_dict
        return img


class MSStyleGAN2Discriminator(nn.Module):
    """StyleGAN2 Discriminator.

    The architecture of this discriminator is proposed in StyleGAN2. More
    details can be found in: Analyzing and Improving the Image Quality of
    StyleGAN CVPR2020.

    Args:
        in_size (int): The input size of images.
        channel_multiplier (int, optional): The multiplier factor for the
            channel number. Defaults to 2.
        blur_kernel (list, optional): The blurry kernel. Defaults
            to [1, 3, 3, 1].
        mbstd_cfg (dict, optional): Configs for minibatch-stddev layer.
            Defaults to dict(group_size=4, channel_groups=1).
    """

    def __init__(self, in_size, channel_multiplier=2, blur_kernel=[1, 3, 3, 1], mbstd_cfg=dict(group_size=4, channel_groups=1), with_adaptive_pool=False, pool_size=(2, 2)):
        super().__init__()
        self.with_adaptive_pool = with_adaptive_pool
        self.pool_size = pool_size
        channels = {(4): 512, (8): 512, (16): 512, (32): 512, (64): 256 * channel_multiplier, (128): 128 * channel_multiplier, (256): 64 * channel_multiplier, (512): 32 * channel_multiplier, (1024): 16 * channel_multiplier}
        log_size = int(np.log2(in_size))
        in_channels = channels[in_size]
        convs = [ConvDownLayer(3, channels[in_size], 1)]
        for i in range(log_size, 2, -1):
            out_channel = channels[2 ** (i - 1)]
            convs.append(ResBlock(in_channels, out_channel, blur_kernel))
            in_channels = out_channel
        self.convs = nn.Sequential(*convs)
        self.mbstd_layer = ModMBStddevLayer(**mbstd_cfg)
        self.final_conv = ConvDownLayer(in_channels + 1, channels[4], 3)
        if self.with_adaptive_pool:
            self.adaptive_pool = nn.AdaptiveAvgPool2d(pool_size)
            linear_in_channels = channels[4] * pool_size[0] * pool_size[1]
        else:
            linear_in_channels = channels[4] * 4 * 4
        self.final_linear = nn.Sequential(EqualLinearActModule(linear_in_channels, channels[4], act_cfg=dict(type='fused_bias')), EqualLinearActModule(channels[4], 1))

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Input image tensor.

        Returns:
            torch.Tensor: Predict score for the input image.
        """
        x = self.convs(x)
        x = self.mbstd_layer(x)
        x = self.final_conv(x)
        if self.with_adaptive_pool:
            x = self.adaptive_pool(x)
        x = x.view(x.shape[0], -1)
        x = self.final_linear(x)
        return x


class WGANNoiseTo2DFeat(nn.Module):
    """Module used in WGAN-GP to transform 1D noise tensor in order [N, C] to
    2D shape feature tensor in order [N, C, H, W].

    Args:
        noise_size (int): Size of the input noise vector.
        out_channels (int): The channel number of the output feature.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU').
        norm_cfg (dict, optional): Config dict to build norm layer. Defaults to
            dict(type='BN').
        order (tuple, optional): The order of conv/norm/activation layers. It
            is a sequence of "conv", "norm" and "act". Common examples are
            ("conv", "norm", "act") and ("act", "conv", "norm"). Defaults to
            ('linear', 'act', 'norm').
    """

    def __init__(self, noise_size, out_channels, act_cfg=dict(type='ReLU'), norm_cfg=dict(type='BN'), order=('linear', 'act', 'norm')):
        super().__init__()
        self.noise_size = noise_size
        self.out_channels = out_channels
        self.with_activation = act_cfg is not None
        self.with_norm = norm_cfg is not None
        self.order = order
        assert len(order) == 3 and set(order) == set(['linear', 'act', 'norm'])
        self.linear = nn.Linear(noise_size, out_channels * 16, bias=False)
        if self.with_activation:
            self.activation = build_activation_layer(act_cfg)
        self.register_parameter('bias', nn.Parameter(torch.zeros(1, out_channels, 1, 1)))
        if self.with_norm:
            _, self.norm = build_norm_layer(norm_cfg, out_channels)
        self._init_weight()

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input noise tensor with shape (n, c).

        Returns:
            Tensor: Forward results with shape (n, c, 4, 4).
        """
        assert x.ndim == 2
        for order in self.order:
            if order == 'linear':
                x = self.linear(x)
                x = torch.reshape(x, (-1, self.out_channels, 4, 4))
                x = x + self.bias
            elif order == 'act' and self.with_activation:
                x = self.activation(x)
            elif order == 'norm' and self.with_norm:
                x = self.norm(x)
        return x

    def _init_weight(self):
        """Initialize weights for the model."""
        nn.init.normal_(self.linear.weight, 0.0, 1.0)
        if self.bias is not None:
            nn.init.constant_(self.bias, 0.0)
        if self.with_norm:
            constant_init(self.norm, 1, bias=0)


class WGANGPGenerator(nn.Module):
    """Generator for WGANGP.

    Implementation Details for WGANGP generator the same as training
    configuration (a) described in PGGAN paper:
    PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION
    https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf # noqa

    #. Adopt convolution architecture specified in appendix A.2;
    #. Use batchnorm in the generator except for the final output layer;
    #. Use ReLU in the generator except for the final output layer;
    #. Use Tanh in the last layer;
    #. Initialize all weights using He’s initializer.

    Args:
        noise_size (int): Size of the input noise vector.
        out_scale (int): Output scale for the generated image.
        conv_module_cfg (dict, optional): Config for the convolution
            module used in this generator. Defaults to None.
        upsample_cfg (dict, optional): Config for the upsampling operation.
            Defaults to None.
    """
    _default_channels_per_scale = {'4': 512, '8': 512, '16': 256, '32': 128, '64': 64, '128': 32}
    _default_conv_module_cfg = dict(conv_cfg=None, kernel_size=3, stride=1, padding=1, bias=True, act_cfg=dict(type='ReLU'), norm_cfg=dict(type='BN'), order=('conv', 'norm', 'act'))
    _default_upsample_cfg = dict(type='nearest', scale_factor=2)

    def __init__(self, noise_size, out_scale, conv_module_cfg=None, upsample_cfg=None):
        super().__init__()
        self.noise_size = noise_size
        self.out_scale = out_scale
        self.conv_module_cfg = deepcopy(self._default_conv_module_cfg)
        if conv_module_cfg is not None:
            self.conv_module_cfg.update(conv_module_cfg)
        self.upsample_cfg = upsample_cfg if upsample_cfg else deepcopy(self._default_upsample_cfg)
        self.noise2feat = WGANNoiseTo2DFeat(self.noise_size, self._default_channels_per_scale['4'])
        self.conv_blocks = nn.ModuleList()
        self.conv_blocks.append(ConvModule(512, 512, **self.conv_module_cfg))
        log2scale = int(np.log2(self.out_scale))
        for i in range(3, log2scale + 1):
            self.conv_blocks.append(build_upsample_layer(self._default_upsample_cfg))
            self.conv_blocks.append(ConvModule(self._default_channels_per_scale[str(2 ** (i - 1))], self._default_channels_per_scale[str(2 ** i)], **self.conv_module_cfg))
            self.conv_blocks.append(ConvModule(self._default_channels_per_scale[str(2 ** i)], self._default_channels_per_scale[str(2 ** i)], **self.conv_module_cfg))
        self.to_rgb = ConvModule(self._default_channels_per_scale[str(self.out_scale)], kernel_size=1, out_channels=3, act_cfg=dict(type='Tanh'))

    def forward(self, noise, num_batches=0, return_noise=False):
        """Forward function.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size. Defaults to
                0.
            return_noise (bool, optional):  If True, ``noise_batch`` will be
                returned in a dict with ``fake_img``. Defaults to False.

        Returns:
            torch.Tensor | dict: If not ``return_noise``, only the output image
                will be returned. Otherwise, a dict contains ``fake_img`` and
                ``noise_batch`` will be returned.
        """
        if isinstance(noise, torch.Tensor):
            assert noise.shape[1] == self.noise_size
            assert noise.ndim == 2, f'The noise should be in shape of (n, c), but got {noise.shape}'
            noise_batch = noise
        elif callable(noise):
            noise_generator = noise
            assert num_batches > 0
            noise_batch = noise_generator((num_batches, self.noise_size))
        else:
            assert num_batches > 0
            noise_batch = torch.randn((num_batches, self.noise_size))
        noise_batch = noise_batch
        x = self.noise2feat(noise_batch)
        for conv in self.conv_blocks:
            x = conv(x)
        out_img = self.to_rgb(x)
        if return_noise:
            output = dict(fake_img=out_img, noise_batch=noise_batch)
            return output
        return out_img


class WGANDecisionHead(nn.Module):
    """Module used in WGAN-GP to get the final prediction result with 4x4
    resolution input tensor in the bottom of the discriminator.

    Args:
        in_channels (int): Number of channels in input feature map.
        mid_channels (int): Number of channels in feature map after
            convolution.
        out_channels (int): The channel number of the final output layer.
        bias (bool, optional): Whether to use bias parameter. Defaults to True.
        act_cfg (dict, optional): Config for the activation layer. Defaults to
            dict(type='ReLU').
        out_act (dict, optional): Config for the activation layer of output
            layer. Defaults to None.
        norm_cfg (dict, optional): Config dict to build norm layer. Defaults to
            dict(type='LN2d').
    """

    def __init__(self, in_channels, mid_channels, out_channels, bias=True, act_cfg=dict(type='ReLU'), out_act=None, norm_cfg=dict(type='LN2d')):
        super().__init__()
        self.in_channels = in_channels
        self.mid_channels = mid_channels
        self.out_channels = out_channels
        self.with_out_activation = out_act is not None
        self.conv = ConvLNModule(in_channels, feature_shape=(mid_channels, 1, 1), kernel_size=4, out_channels=mid_channels, act_cfg=act_cfg, norm_cfg=norm_cfg, order=('conv', 'norm', 'act'))
        self.linear = nn.Linear(self.mid_channels, self.out_channels, bias=bias)
        if self.with_out_activation:
            self.out_activation = build_activation_layer(out_act)
        self._init_weight()

    def forward(self, x):
        """Forward function.

        Args:
            x (Tensor): Input tensor with shape (n, c, h, w).

        Returns:
            Tensor: Forward results.
        """
        x = self.conv(x)
        x = torch.reshape(x, (x.shape[0], -1))
        x = self.linear(x)
        if self.with_out_activation:
            x = self.out_activation(x)
        return x

    def _init_weight(self):
        """Initialize weights for the model."""
        nn.init.normal_(self.linear.weight, 0.0, 1.0)
        nn.init.constant_(self.linear.bias, 0.0)


class WGANGPDiscriminator(nn.Module):
    """Discriminator for WGANGP.

    Implementation Details for WGANGP discriminator the same as training
    configuration (a) described in PGGAN paper:
    PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION
    https://research.nvidia.com/sites/default/files/pubs/2017-10_Progressive-Growing-of/karras2018iclr-paper.pdf # noqa

    #. Adopt convolution architecture specified in appendix A.2;
    #. Add layer normalization to all conv3x3 and conv4x4 layers;
    #. Use LeakyReLU in the discriminator except for the final output layer;
    #. Initialize all weights using He’s initializer.

    Args:
        in_channel (int): The channel number of the input image.
        in_scale (int): The scale of the input image.
        conv_module_cfg (dict, optional): Config for the convolution module
            used in this discriminator. Defaults to None.
    """
    _default_channels_per_scale = {'4': 512, '8': 512, '16': 256, '32': 128, '64': 64, '128': 32}
    _default_conv_module_cfg = dict(conv_cfg=None, kernel_size=3, stride=1, padding=1, bias=True, act_cfg=dict(type='LeakyReLU', negative_slope=0.2), norm_cfg=dict(type='LN2d'), order=('conv', 'norm', 'act'))
    _default_upsample_cfg = dict(type='nearest', scale_factor=2)

    def __init__(self, in_channel, in_scale, conv_module_cfg=None):
        super().__init__()
        self.in_channel = in_channel
        self.in_scale = in_scale
        self.conv_module_cfg = deepcopy(self._default_conv_module_cfg)
        if conv_module_cfg is not None:
            self.conv_module_cfg.update(conv_module_cfg)
        self.from_rgb = ConvModule(3, kernel_size=1, out_channels=self._default_channels_per_scale[str(self.in_scale)], act_cfg=dict(type='LeakyReLU', negative_slope=0.2))
        self.conv_blocks = nn.ModuleList()
        log2scale = int(np.log2(self.in_scale))
        for i in range(log2scale, 2, -1):
            self.conv_blocks.append(ConvLNModule(self._default_channels_per_scale[str(2 ** i)], self._default_channels_per_scale[str(2 ** i)], feature_shape=(self._default_channels_per_scale[str(2 ** i)], 2 ** i, 2 ** i), **self.conv_module_cfg))
            self.conv_blocks.append(ConvLNModule(self._default_channels_per_scale[str(2 ** i)], self._default_channels_per_scale[str(2 ** (i - 1))], feature_shape=(self._default_channels_per_scale[str(2 ** (i - 1))], 2 ** i, 2 ** i), **self.conv_module_cfg))
            self.conv_blocks.append(nn.AvgPool2d(kernel_size=2, stride=2))
        self.decision = WGANDecisionHead(self._default_channels_per_scale['4'], self._default_channels_per_scale['4'], 1, act_cfg=dict(type='LeakyReLU', negative_slope=0.2), norm_cfg=self.conv_module_cfg['norm_cfg'])

    def forward(self, x):
        """Forward function.

        Args:
            x (torch.Tensor): Fake or real image tensor.

        Returns:
            torch.Tensor: Prediction for the reality of the input image.
        """
        x = self.from_rgb(x)
        for conv in self.conv_blocks:
            x = conv(x)
        x = self.decision(x)
        return x


def _get_label_batch(label, num_timesteps=0, num_classes=0, num_batches=0, timesteps_noise=False):
    """Get label batch. Support get sequeue of label along timesteps.

    We support the following use cases ('bz' denotes ```num_batches`` and 'n'
    denotes ``num_timesteps``):

    If num_classes <= 0, return None.

    If timesteps_noise is True, we output label which dimension is 2.
    - Input is [bz, ]: Expand to [n, bz]
    - Input is [n, ]: Expand to [n, bz]
    - Input is [n*bz, ]: View to [n, bz]
    - Dim of the input is 2: Return the input, ignore ``num_batches`` and
      ``num_timesteps``
    - Callable or None: Generate label shape as [n, bz]
    - Otherwise: Raise error

    If timesteps_noise is False, we output label which dimension is 1 and
    ignore ``num_timesteps``.
    - Dim of the input is 1: Unsqueeze to [1, ], ignore ``num_batches``
    - Dim of the input is 2: Return the input. ignore ``num_batches``
    - Callable or None: Generate label shape as [bz, ]
    - Otherwise: Raise error

    It's to be noted that, we do not move the generated label to target device
    in this function because we can not get which device the noise should move
    to.

    Args:
        label (torch.Tensor | callable | None): You can directly give a
            batch of noise through a ``torch.Tensor`` or offer a callable
            function to sample a batch of noise data. Otherwise, the
            ``None`` indicates to use the default noise sampler.
        num_timesteps (int, optional): Total timestpes of the diffusion and
            denoising process. Defaults to 0.
        num_batches (int, optional): The number of batch size. To be noted that
            this argument only work when the input ``noise`` is callable or
            ``None``. Defaults to 0.
        timesteps_noise (bool, optional): If True, returned noise will shape
            as [n, bz, c, h, w], otherwise shape as [bz, c, h, w].
            Defaults to False.
    Returns:
        torch.Tensor: Generated label with desired shape.
    """
    if num_classes == 0:
        assert label is None, "'label' should be None if 'num_classes == 0'."
        return None
    if isinstance(label, torch.Tensor):
        if timesteps_noise:
            if label.ndim == 1:
                assert num_batches > 0 and num_timesteps > 0
                if label.shape[0] == num_timesteps:
                    label_batch = label.view(num_timesteps, 1)
                    label_batch = label_batch.expand(-1, num_batches)
                elif label.shape[0] == num_batches:
                    label_batch = label.view(1, num_batches)
                    label_batch = label_batch.expand(num_timesteps, -1)
                elif label.shape[0] == num_timesteps * num_batches:
                    label_batch = label.view(num_timesteps, -1)
                else:
                    raise ValueError(f'The timesteps label should be in shape of (n, ), (bz,), (n*bz, ) or (n, bz, ). But receive {label.shape}.')
            elif label.ndim == 2:
                label_batch = label
            else:
                raise ValueError(f'The timesteps label should be in shape of (n, ), (bz,), (n*bz, ) or (n, bz, ). But receive {label.shape}.')
        elif label.ndim == 0:
            label_batch = label[None, ...]
        elif label.ndim == 1:
            label_batch = label
        else:
            raise ValueError(f'The label should be in shape of (bz, ) orzero-dimension tensor, but got {label.shape}')
    elif callable(label):
        assert num_batches > 0
        label_generator = label
        if timesteps_noise:
            assert num_timesteps > 0
            label_batch = label_generator((num_timesteps, num_batches))
        else:
            label_batch = label_generator((num_batches,))
    else:
        assert num_batches > 0
        if timesteps_noise:
            assert num_timesteps > 0
            label_batch = torch.randint(0, num_classes, (num_timesteps, num_batches))
        else:
            label_batch = torch.randint(0, num_classes, (num_batches,))
    return label_batch


def _get_noise_batch(noise, image_shape, num_timesteps=0, num_batches=0, timesteps_noise=False):
    """Get noise batch. Support get sequeue of noise along timesteps.

    We support the following use cases ('bz' denotes ```num_batches`` and 'n'
    denotes ``num_timesteps``):

    If timesteps_noise is True, we output noise which dimension is 5.
    - Input is [bz, c, h, w]: Expand to [n, bz, c, h, w]
    - Input is [n, c, h, w]: Expand to [n, bz, c, h, w]
    - Input is [n*bz, c, h, w]: View to [n, bz, c, h, w]
    - Dim of the input is 5: Return the input, ignore ``num_batches`` and
      ``num_timesteps``
    - Callable or None: Generate noise shape as [n, bz, c, h, w]
    - Otherwise: Raise error

    If timestep_noise is False, we output noise which dimension is 4 and
    ignore ``num_timesteps``.
    - Dim of the input is 3: Unsqueeze to [1, c, h, w], ignore ``num_batches``
    - Dim of the input is 4: Return input, ignore ``num_batches``
    - Callable or None: Generate noise shape as [bz, c, h, w]
    - Otherwise: Raise error

    It's to be noted that, we do not move the generated label to target device
    in this function because we can not get which device the noise should move
    to.

    Args:
        noise (torch.Tensor | callable | None): You can directly give a
            batch of noise through a ``torch.Tensor`` or offer a callable
            function to sample a batch of noise data. Otherwise, the
            ``None`` indicates to use the default noise sampler.
        image_shape (torch.Size): Size of images in the diffusion process.
        num_timesteps (int, optional): Total timestpes of the diffusion and
            denoising process. Defaults to 0.
        num_batches (int, optional): The number of batch size. To be noted that
            this argument only work when the input ``noise`` is callable or
            ``None``. Defaults to 0.
        timesteps_noise (bool, optional): If True, returned noise will shape
            as [n, bz, c, h, w], otherwise shape as [bz, c, h, w].
            Defaults to False.
        device (str, optional): If not ``None``, move the generated noise to
            corresponding device.
    Returns:
        torch.Tensor: Generated noise with desired shape.
    """
    if isinstance(noise, torch.Tensor):
        assert noise.shape[-3:] == image_shape
        if timesteps_noise:
            if noise.ndim == 4:
                assert num_batches > 0 and num_timesteps > 0
                if noise.shape[0] == num_timesteps:
                    noise_batch = noise.view(num_timesteps, 1, *image_shape)
                    noise_batch = noise_batch.expand(-1, num_batches, -1, -1, -1)
                elif noise.shape[0] == num_batches:
                    noise_batch = noise.view(1, num_batches, *image_shape)
                    noise_batch = noise_batch.expand(num_timesteps, -1, -1, -1, -1)
                elif noise.shape[0] == num_timesteps * num_batches:
                    noise_batch = noise.view(num_timesteps, -1, *image_shape)
                else:
                    raise ValueError(f'The timesteps noise should be in shape of (n, c, h, w), (bz, c, h, w), (n*bz, c, h, w) or (n, bz, c, h, w). But receive {noise.shape}.')
            elif noise.ndim == 5:
                noise_batch = noise
            else:
                raise ValueError(f'The timesteps noise should be in shape of (n, c, h, w), (bz, c, h, w), (n*bz, c, h, w) or (n, bz, c, h, w). But receive {noise.shape}.')
        elif noise.ndim == 3:
            noise_batch = noise[None, ...]
        elif noise.ndim == 4:
            noise_batch = noise
        else:
            raise ValueError(f'The noise should be in shape of (n, c, h, w) or(c, h, w), but got {noise.shape}')
    elif callable(noise):
        assert num_batches > 0
        noise_generator = noise
        if timesteps_noise:
            assert num_timesteps > 0
            noise_batch = noise_generator((num_timesteps, num_batches, *image_shape))
        else:
            noise_batch = noise_generator((num_batches, *image_shape))
    else:
        assert num_batches > 0
        if timesteps_noise:
            assert num_timesteps > 0
            noise_batch = torch.randn((num_timesteps, num_batches, *image_shape))
        else:
            noise_batch = torch.randn((num_batches, *image_shape))
    return noise_batch


def var_to_tensor(var, index, target_shape=None, device=None):
    """Function used to extract variables by given index, and convert into
    tensor as given shape.
    Args:
        var (np.array): Variables to be extracted.
        index (torch.Tensor): Target index to extract.
        target_shape (torch.Size, optional): If given, the indexed variable
            will expand to the given shape. Defaults to None.
        device (str): If given, the indexed variable will move to the target
            device. Otherwise, indexed variable will on cpu. Defaults to None.

    Returns:
        torch.Tensor: Converted variable.
    """
    var_indexed = torch.from_numpy(var)[index.cpu()].float()
    if device is not None:
        var_indexed = var_indexed
    while len(var_indexed.shape) < len(target_shape):
        var_indexed = var_indexed[..., None]
    return var_indexed


class BasicGaussianDiffusion(nn.Module, metaclass=ABCMeta):
    """Basic module for gaussian Diffusion Denoising Probabilistic Models. A
    diffusion probabilistic model (which we will call a 'diffusion model' for
    brevity) is a parameterized Markov chain trained using variational
    inference to produce samples matching the data after finite time.

    The design of this module implements DDPM and improve-DDPM according to
    "Denoising Diffusion Probabilistic Models" (2020) and "Improved Denoising
    Diffusion Probabilistic Models" (2021).

    Args:
        denoising (dict): Config for denoising model.
        ddpm_loss (dict): Config for losses of DDPM.
        betas_cfg (dict): Config for betas in diffusion process.
        num_timesteps (int, optional): The number of timesteps of the diffusion
            process. Defaults to 1000.
        num_classes (int | None, optional): The number of conditional classes.
            Defaults to None.
        sample_method (string, optional): Sample method for the denoising
            process. Support 'DDPM' and 'DDIM'. Defaults to 'DDPM'.
        timesteps_sampler (string, optional): How to sample timesteps in
            training process. Defaults to `UniformTimeStepSampler`.
        train_cfg (dict | None, optional): Config for training schedule.
            Defaults to None.
        test_cfg (dict | None, optional): Config for testing schedule. Defaults
            to None.
    """

    def __init__(self, denoising, ddpm_loss, betas_cfg, num_timesteps=1000, num_classes=0, sample_method='DDPM', timestep_sampler='UniformTimeStepSampler', train_cfg=None, test_cfg=None):
        super().__init__()
        self.fp16_enable = False
        self.num_classes = num_classes
        self.num_timesteps = num_timesteps
        self.sample_method = sample_method
        self._denoising_cfg = deepcopy(denoising)
        self.denoising = build_module(denoising, default_args=dict(num_classes=num_classes, num_timesteps=num_timesteps))
        self.denoising_var_mode = self.denoising.var_mode
        self.denoising_mean_mode = self.denoising.mean_mode
        image_channels = self._denoising_cfg['in_channels']
        image_size = self.denoising.image_size
        image_shape = torch.Size([image_channels, image_size, image_size])
        self.image_shape = image_shape
        self.get_noise = partial(_get_noise_batch, image_shape=image_shape, num_timesteps=self.num_timesteps)
        self.get_label = partial(_get_label_batch, num_timesteps=self.num_timesteps)
        if timestep_sampler is not None:
            self.sampler = build_module(timestep_sampler, default_args=dict(num_timesteps=num_timesteps))
        else:
            self.sampler = None
        if ddpm_loss is not None:
            self.ddpm_loss = build_module(ddpm_loss, default_args=dict(sampler=self.sampler))
            if not isinstance(self.ddpm_loss, nn.ModuleList):
                self.ddpm_loss = nn.ModuleList([self.ddpm_loss])
        else:
            self.ddpm_loss = None
        self.betas_cfg = deepcopy(betas_cfg)
        self.train_cfg = deepcopy(train_cfg) if train_cfg else None
        self.test_cfg = deepcopy(test_cfg) if test_cfg else None
        self._parse_train_cfg()
        if test_cfg is not None:
            self._parse_test_cfg()
        self.prepare_diffusion_vars()

    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""
        if self.train_cfg is None:
            self.train_cfg = dict()
        self.use_ema = self.train_cfg.get('use_ema', False)
        if self.use_ema:
            self.denoising_ema = deepcopy(self.denoising)
        self.real_img_key = self.train_cfg.get('real_img_key', 'real_img')

    def _parse_test_cfg(self):
        """Parsing test config and set some attributes for testing."""
        if self.test_cfg is None:
            self.test_cfg = dict()
        self.use_ema = self.test_cfg.get('use_ema', False)
        if self.use_ema:
            self.denoising_ema = deepcopy(self.denoising)

    def _get_loss(self, outputs_dict):
        losses_dict = {}
        for loss_fn in self.ddpm_loss:
            losses_dict[loss_fn.loss_name()] = loss_fn(outputs_dict)
        loss, log_vars = self._parse_losses(losses_dict)
        for loss_fn in self.ddpm_loss:
            if hasattr(loss_fn, 'log_vars'):
                log_vars.update(loss_fn.log_vars)
        return loss, log_vars

    def _parse_losses(self, losses):
        """Parse the raw outputs (losses) of the network.

        Args:
            losses (dict): Raw output of the network, which usually contain
                losses and other necessary information.

        Returns:
            tuple[Tensor, dict]: (loss, log_vars), loss is the loss tensor                 which may be a weighted sum of all losses, log_vars contains                 all the variables to be sent to the logger.
        """
        log_vars = OrderedDict()
        for loss_name, loss_value in losses.items():
            if isinstance(loss_value, torch.Tensor):
                log_vars[loss_name] = loss_value.mean()
            elif isinstance(loss_value, list):
                log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)
            else:
                raise TypeError(f'{loss_name} is not a tensor or list of tensor')
        loss = sum(_value for _key, _value in log_vars.items() if 'loss' in _key)
        log_vars['loss'] = loss
        for loss_name, loss_value in log_vars.items():
            if dist.is_available() and dist.is_initialized():
                loss_value = loss_value.data.clone()
                dist.all_reduce(loss_value.div_(dist.get_world_size()))
            log_vars[loss_name] = loss_value.item()
        return loss, log_vars

    def train_step(self, data, optimizer, ddp_reducer=None, loss_scaler=None, use_apex_amp=False, running_status=None):
        """The iteration step during training.

        This method defines an iteration step during training. Different from
        other repo in **MM** series, we allow the back propagation and
        optimizer updating to directly follow the iterative training schedule
        of DDPMs.
        Of course, we will show that you can also move the back
        propagation outside of this method, and then optimize the parameters
        in the optimizer hook. But this will cause extra GPU memory cost as a
        result of retaining computational graph. Otherwise, the training
        schedule should be modified in the detailed implementation.


        Args:
            optimizer (dict): Dict contains optimizer for denoising network.
            running_status (dict | None, optional): Contains necessary basic
                information for training, e.g., iteration number. Defaults to
                None.
        """
        if running_status is not None:
            curr_iter = running_status['iteration']
        else:
            if not hasattr(self, 'iteration'):
                self.iteration = 0
            curr_iter = self.iteration
        real_imgs = data[self.real_img_key]
        optimizer['denoising'].zero_grad()
        denoising_dict_ = self.reconstruction_step(data, timesteps=self.sampler, sample_model='orig', return_noise=True)
        denoising_dict_['iteration'] = curr_iter
        denoising_dict_['real_imgs'] = real_imgs
        denoising_dict_['loss_scaler'] = loss_scaler
        loss, log_vars = self._get_loss(denoising_dict_)
        if ddp_reducer is not None:
            ddp_reducer.prepare_for_backward(_find_tensors(loss))
        if loss_scaler:
            loss_scaler.scale(loss).backward()
        elif use_apex_amp:
            with amp.scale_loss(loss, optimizer['denoising'], loss_id=0) as scaled_loss_disc:
                scaled_loss_disc.backward()
        else:
            loss.backward()
        if loss_scaler:
            loss_scaler.unscale_(optimizer['denoising'])
            loss_scaler.step(optimizer['denoising'])
        else:
            optimizer['denoising'].step()
        results = dict(real_imgs=real_imgs, x_0_pred=denoising_dict_['x_0_pred'], x_t=denoising_dict_['diffusion_batches'], x_t_1=denoising_dict_['fake_img'])
        outputs = dict(log_vars=log_vars, num_samples=real_imgs.shape[0], results=results)
        if hasattr(self, 'iteration'):
            self.iteration += 1
        return outputs

    def reconstruction_step(self, data_batch, noise=None, label=None, timesteps=None, sample_model='orig', return_noise=False, **kwargs):
        """Reconstruction step at corresponding `timestep`. To be noted that,
        denoisint target ``x_t`` for each timestep are all generated from real
        images, but not the denoising result from denoising network.

        ``sample_from_noise`` focus on generate samples start from **random
        (or given) noise**. Therefore, we design this function to realize a
        reconstruction process for the given images.

        If `timestep` is None, automatically perform reconstruction at all
        timesteps.

        Args:
            data_batch (dict): Input data from dataloader.
            noise (torch.Tensor | callable | None): Noise used in diffusion
                process. You can directly give a batch of noise through a
                ``torch.Tensor`` or offer a callable function to sample a
                batch of noise data. Otherwise, the ``None`` indicates to use
                the default noise sampler. Defaults to None.
            label (torch.Tensor | None , optional): The conditional label of
                the input image. Defaults to None.
            timestep (int | list | torch.Tensor | callable | None): Target
                timestep to perform reconstruction.
            sampel_model (str, optional): Use which model to sample fake
                images. Defaults to `'orig'`.
            return_noise (bool, optional): If True,``noise_batch``, ``label``
                and all other intermedia variables will be returned together
                with ``fake_img`` in a dict. Defaults to False.

        Returns:
            torch.Tensor | dict: The output may be the direct synthesized
                images in ``torch.Tensor``. Otherwise, a dict with required
                data , including generated images, will be returned.
        """
        assert sample_model in ['orig', 'ema'], f"We only support 'orig' and 'ema' for 'reconstruction_step', but receive '{sample_model}'."
        denoising_model = self.denoising if sample_model == 'orig' else self.denoising_ema
        device = get_module_device(self)
        real_imgs = data_batch[self.real_img_key]
        num_batches = real_imgs.shape[0]
        if timesteps is None:
            timesteps = torch.LongTensor([t for t in range(self.num_timesteps)]).view(self.num_timesteps, 1)
            timesteps = timesteps.repeat([1, num_batches])
        if isinstance(timesteps, (int, list)):
            timesteps = torch.LongTensor(timesteps)
        elif callable(timesteps):
            timestep_generator = timesteps
            timesteps = timestep_generator(num_batches)
        else:
            assert isinstance(timesteps, torch.Tensor), 'we only support int list tensor or a callable function'
        if timesteps.ndim == 1:
            timesteps = timesteps.unsqueeze(0)
        timesteps = timesteps
        if noise is not None:
            assert 'noise' not in data_batch, "Receive 'noise' in both data_batch and passed arguments."
        if noise is None:
            noise = data_batch['noise'] if 'noise' in data_batch else None
        if self.num_classes > 0:
            if label is not None:
                assert 'label' not in data_batch, "Receive 'label' in both data_batch and passed arguments."
            if label is None:
                label = data_batch['label'] if 'label' in data_batch else None
            label_batches = self.get_label(label, num_batches=num_batches)
        else:
            label_batches = None
        output_dict = defaultdict(list)
        for timestep in timesteps:
            noise_batches = self.get_noise(noise, num_batches=num_batches)
            diffusion_batches = self.q_sample(real_imgs, timestep, noise_batches)
            denoising_batches = self.denoising_step(denoising_model, diffusion_batches, timestep, label=label_batches, return_noise=return_noise, clip_denoised=not self.training)
            target_batches = self.q_posterior_mean_variance(real_imgs, diffusion_batches, timestep, logvar=True)
            if return_noise:
                output_dict_ = dict(timesteps=timestep, noise=noise_batches, diffusion_batches=diffusion_batches)
                if self.num_classes > 0:
                    output_dict_['label'] = label_batches
                output_dict_.update(denoising_batches)
                output_dict_.update(target_batches)
            else:
                output_dict_ = dict(fake_img=denoising_batches)
            for k, v in output_dict_.items():
                if k in output_dict:
                    output_dict[k].append(v)
                else:
                    output_dict[k] = [v]
        for k, v in output_dict.items():
            output_dict[k] = torch.cat(v, dim=0)
        if return_noise:
            return output_dict
        return output_dict['fake_img']

    def sample_from_noise(self, noise, num_batches=0, sample_model='ema/orig', label=None, **kwargs):
        """Sample images from noises by using Denoising model.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional):  The number of batch size.
                Defaults to 0.
            sample_model (str, optional): The model to sample. If ``ema/orig``
                is passed, this method will try to sample from ema (if
                ``self.use_ema == True``) and orig model. Defaults to
                'ema/orig'.
            label (torch.Tensor | None , optional): The conditional label.
                Defaults to None.

        Returns:
            torch.Tensor | dict: The output may be the direct synthesized
                images in ``torch.Tensor``. Otherwise, a dict with queried
                data, including generated images, will be returned.
        """
        sample_fn_name = f'{self.sample_method.upper()}_sample'
        if not hasattr(self, sample_fn_name):
            raise AttributeError(f'Cannot find sample method [{sample_fn_name}] correspond to [{self.sample_method}].')
        sample_fn = getattr(self, sample_fn_name)
        if sample_model == 'ema':
            assert self.use_ema
            _model = self.denoising_ema
        elif sample_model == 'ema/orig' and self.use_ema:
            _model = self.denoising_ema
        else:
            _model = self.denoising
        outputs = sample_fn(_model, noise=noise, num_batches=num_batches, label=label, **kwargs)
        if isinstance(outputs, dict) and 'noise_batch' in outputs:
            noise = outputs['x_t']
            label = outputs['label']
            kwargs['timesteps_noise'] = outputs['noise_batch']
            fake_img = outputs['fake_img']
        else:
            fake_img = outputs
        if sample_model == 'ema/orig' and self.use_ema:
            _model = self.denoising
            outputs_ = sample_fn(_model, noise=noise, num_batches=num_batches, **kwargs)
            if isinstance(outputs_, dict) and 'noise_batch' in outputs_:
                fake_img_ = outputs_['fake_img']
            else:
                fake_img_ = outputs_
            if isinstance(fake_img, dict):
                fake_img = {k: torch.cat([fake_img[k], fake_img_[k]], dim=0) for k in fake_img.keys()}
            else:
                fake_img = torch.cat([fake_img, fake_img_], dim=0)
        return fake_img

    @torch.no_grad()
    def DDPM_sample(self, model, noise=None, num_batches=0, label=None, save_intermedia=False, timesteps_noise=None, return_noise=False, show_pbar=False, **kwargs):
        """DDPM sample from random noise.
        Args:
            model (torch.nn.Module): Denoising model used to sample images.
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            label (torch.Tensor | None , optional): The conditional label.
                Defaults to None.
            save_intermedia (bool, optional): Whether to save denoising result
                of intermedia timesteps. If set as True, will return a dict
                which key and value are denoising timestep and denoising
                result. Otherwise, only the final denoising result will be
                returned. Defaults to False.
            timesteps_noise (torch.Tensor, optional): Noise term used in each
                denoising timestep. If given, the input noise will be shaped to
                [num_timesteps, b, c, h, w]. If set as None, noise of each
                denoising timestep will be randomly sampled. Default as None.
            return_noise (bool, optional): If True, a dict contains
                ``noise_batch``, ``x_t`` and ``label`` will be returned
                together with the denoising results, and the key of denoising
                results is ``fake_img``. To be noted that ``noise_batches``
                will shape as [num_timesteps, b, c, h, w]. Defaults to False.
            show_pbar (bool, optional): If True, a progress bar will be
                displayed. Defaults to False.
        Returns:
            torch.Tensor | dict: If ``save_intermedia``, a dict contains
                denoising results of each timestep will be returned.
                Otherwise, only the final denoising result will be returned.
        """
        device = get_module_device(self)
        noise = self.get_noise(noise, num_batches=num_batches)
        x_t = noise.clone()
        if save_intermedia:
            intermedia = {self.num_timesteps: x_t.clone()}
        if timesteps_noise is not None:
            timesteps_noise = self.get_noise(timesteps_noise, num_batches=num_batches, timesteps_noise=True)
        batched_timesteps = torch.arange(self.num_timesteps - 1, -1, -1).long()
        if show_pbar:
            pbar = mmcv.ProgressBar(self.num_timesteps)
        for t in batched_timesteps:
            batched_t = t.expand(x_t.shape[0])
            step_noise = timesteps_noise[t, ...] if timesteps_noise is not None else None
            x_t = self.denoising_step(model, x_t, batched_t, noise=step_noise, label=label, **kwargs)
            if save_intermedia:
                intermedia[int(t)] = x_t.cpu().clone()
            if show_pbar:
                pbar.update()
        denoising_results = intermedia if save_intermedia else x_t
        if show_pbar:
            sys.stdout.write('\n')
        if return_noise:
            return dict(noise_batch=timesteps_noise, x_t=noise, label=label, fake_img=denoising_results)
        return denoising_results

    def prepare_diffusion_vars(self):
        """Prepare for variables used in the diffusion process."""
        self.betas = self.get_betas()
        self.alphas = 1.0 - self.betas
        self.alphas_bar = np.cumproduct(self.alphas, axis=0)
        self.alphas_bar_prev = np.append(1.0, self.alphas_bar[:-1])
        self.alphas_bar_next = np.append(self.alphas_bar[1:], 0.0)
        self.sqrt_alphas_bar = np.sqrt(self.alphas_bar)
        self.sqrt_one_minus_alphas_bar = np.sqrt(1.0 - self.alphas_bar)
        self.log_one_minus_alphas_bar = np.log(1.0 - self.alphas_bar)
        self.sqrt_recip_alplas_bar = np.sqrt(1.0 / self.alphas_bar)
        self.sqrt_recipm1_alphas_bar = np.sqrt(1.0 / self.alphas_bar - 1)
        self.tilde_betas_t = self.betas * (1 - self.alphas_bar_prev) / (1 - self.alphas_bar)
        self.log_tilde_betas_t_clipped = np.log(np.append(self.tilde_betas_t[1], self.tilde_betas_t[1:]))
        self.tilde_mu_t_coef1 = np.sqrt(self.alphas_bar_prev) / (1 - self.alphas_bar) * self.betas
        self.tilde_mu_t_coef2 = np.sqrt(self.alphas) * (1 - self.alphas_bar_prev) / (1 - self.alphas_bar)

    def get_betas(self):
        """Get betas by defined schedule method in diffusion process."""
        self.betas_schedule = self.betas_cfg.pop('type')
        if self.betas_schedule == 'linear':
            return self.linear_beta_schedule(self.num_timesteps, **self.betas_cfg)
        elif self.betas_schedule == 'cosine':
            return self.cosine_beta_schedule(self.num_timesteps, **self.betas_cfg)
        else:
            raise AttributeError(f'Unknown method name {self.beta_schedule}for beta schedule.')

    @staticmethod
    def linear_beta_schedule(diffusion_timesteps, beta_0=0.0001, beta_T=0.02):
        """Linear schedule from Ho et al, extended to work for any number of
        diffusion steps.

        Args:
            diffusion_timesteps (int): The number of betas to produce.
            beta_0 (float, optional): `\\beta` at timestep 0. Defaults to 1e-4.
            beta_T (float, optional): `\\beta` at timestep `T` (the final
                diffusion timestep). Defaults to 2e-2.

        Returns:
            np.ndarray: Betas used in diffusion process.
        """
        scale = 1000 / diffusion_timesteps
        beta_0 = scale * beta_0
        beta_T = scale * beta_T
        return np.linspace(beta_0, beta_T, diffusion_timesteps, dtype=np.float64)

    @staticmethod
    def cosine_beta_schedule(diffusion_timesteps, max_beta=0.999, s=0.008):
        """Create a beta schedule that discretizes the given alpha_t_bar
        function, which defines the cumulative product of `(1-\\beta)` over time
        from `t = [0, 1]`.

        Args:
            diffusion_timesteps (int): The number of betas to produce.
            max_beta (float, optional): The maximum beta to use; use values
                lower than 1 to prevent singularities. Defaults to 0.999.
            s (float, optional): Small offset to prevent `\\beta` from being too
                small near `t = 0` Defaults to 0.008.

        Returns:
            np.ndarray: Betas used in diffusion process.
        """

        def f(t, T, s):
            return np.cos((t / T + s) / (1 + s) * np.pi / 2) ** 2
        betas = []
        for t in range(diffusion_timesteps):
            alpha_bar_t = f(t + 1, diffusion_timesteps, s)
            alpha_bar_t_1 = f(t, diffusion_timesteps, s)
            betas_t = 1 - alpha_bar_t / alpha_bar_t_1
            betas.append(min(betas_t, max_beta))
        return np.array(betas)

    def q_sample(self, x_0, t, noise=None):
        """Get diffusion result at timestep `t` by `q(x_t | x_0)`.

        Args:
            x_0 (torch.Tensor): Original image without diffusion.
            t (torch.Tensor): Target diffusion timestep.
            noise (torch.Tensor, optional): Noise used in reparameteration
                trick. Default to None.

        Returns:
            torch.tensor: Diffused image `x_t`.
        """
        device = get_module_device(self)
        num_batches = x_0.shape[0]
        tar_shape = x_0.shape
        noise = self.get_noise(noise, num_batches=num_batches)
        mean = var_to_tensor(self.sqrt_alphas_bar, t, tar_shape, device)
        std = var_to_tensor(self.sqrt_one_minus_alphas_bar, t, tar_shape, device)
        return x_0 * mean + noise * std

    def q_mean_log_variance(self, x_0, t):
        """Get mean and log_variance of diffusion process `q(x_t | x_0)`.

        Args:
            x_0 (torch.tensor): The original image before diffusion, shape as
                [bz, ch, H, W].
            t (torch.tensor): Target timestep, shape as [bz, ].

        Returns:
            Tuple(torch.tensor): Tuple contains mean and log variance.
        """
        device = get_module_device(self)
        tar_shape = x_0.shape
        mean = var_to_tensor(self.sqrt_alphas_bar, t, tar_shape, device) * x_0
        logvar = var_to_tensor(self.log_one_minus_alphas_bar, t, tar_shape, device)
        return mean, logvar

    def q_posterior_mean_variance(self, x_0, x_t, t, need_var=True, logvar=False):
        """Get mean and variance of diffusion posterior
            `q(x_{t-1} | x_t, x_0)`.

        Args:
            x_0 (torch.tensor): The original image before diffusion, shape as
                [bz, ch, H, W].
            t (torch.tensor): Target timestep, shape as [bz, ].
            need_var (bool, optional): If set as ``True``, this function will
                return a dict contains ``var``. Otherwise, only mean will be
                returned, ``logvar`` will be ignored. Defaults to True.
            logvar (bool, optional): If set as ``True``, the returned dict
                will additionally contain ``logvar``. This argument will be
                considered only if ``var == True``. Defaults to False.

        Returns:
            torch.Tensor | dict: If ``var``, will return a dict contains
                ``mean`` and ``var``. Otherwise, only mean will be returned.
                If ``var`` and ``logvar`` set at as True simultaneously, the
                returned dict will additional contain ``logvar``.
        """
        device = get_module_device(self)
        tar_shape = x_0.shape
        tilde_mu_t_coef1 = var_to_tensor(self.tilde_mu_t_coef1, t, tar_shape, device)
        tilde_mu_t_coef2 = var_to_tensor(self.tilde_mu_t_coef2, t, tar_shape, device)
        posterior_mean = tilde_mu_t_coef1 * x_0 + tilde_mu_t_coef2 * x_t
        if not need_var:
            return posterior_mean
        posterior_var = var_to_tensor(self.tilde_betas_t, t, tar_shape, device)
        out_dict = dict(mean_posterior=posterior_mean, var_posterior=posterior_var)
        if logvar:
            posterior_logvar = var_to_tensor(self.log_tilde_betas_t_clipped, t, tar_shape, device)
            out_dict['logvar_posterior'] = posterior_logvar
        return out_dict

    def p_mean_variance(self, denoising_output, x_t, t, clip_denoised=True, denoised_fn=None):
        """Get mean, variance, log variance of denoising process
        `p(x_{t-1} | x_{t})` and predicted `x_0`.

        Args:
            denoising_output (dict[torch.Tensor]): The output from denoising
                model.
            x_t (torch.Tensor): Diffused image at timestep `t` to denoising.
            t (torch.Tensor): Current timestep.
            clip_denoised (bool, optional): Whether cliped sample results into
                [-1, 1]. Defaults to True.
            denoised_fn (callable, optional): If not None, a function which
                applies to the predicted ``x_0`` before it is passed to the
                following sampling procedure. Noted that this function will be
                applies before ``clip_denoised``. Defaults to None.

        Returns:
            dict: A dict contains ``var_pred``, ``logvar_pred``, ``mean_pred``
                and ``x_0_pred``.
        """
        target_shape = x_t.shape
        device = get_module_device(self)
        if self.denoising_var_mode.upper() == 'LEARNED':
            logvar_pred = denoising_output['logvar']
            varpred = torch.exp(logvar_pred)
        elif self.denoising_var_mode.upper() == 'LEARNED_RANGE':
            var_factor = denoising_output['factor']
            lower_bound_logvar = var_to_tensor(self.log_tilde_betas_t_clipped, t, target_shape, device)
            upper_bound_logvar = var_to_tensor(np.log(self.betas), t, target_shape, device)
            logvar_pred = var_factor * upper_bound_logvar + (1 - var_factor) * lower_bound_logvar
            varpred = torch.exp(logvar_pred)
        elif self.denoising_var_mode.upper() == 'FIXED_LARGE':
            varpred = var_to_tensor(np.append(self.tilde_betas_t[1], self.betas), t, target_shape, device)
            logvar_pred = torch.log(varpred)
        elif self.denoising_var_mode.upper() == 'FIXED_SMALL':
            varpred = var_to_tensor(self.tilde_betas_t, t, target_shape, device)
            logvar_pred = var_to_tensor(self.log_tilde_betas_t_clipped, t, target_shape, device)
        else:
            raise AttributeError(f'Unknown denoising var output type [{self.denoising_var_mode}].')

        def process_x_0(x):
            if denoised_fn is not None and callable(denoised_fn):
                x = denoised_fn(x)
            return x.clamp(-1, 1) if clip_denoised else x
        if self.denoising_mean_mode.upper() == 'EPS':
            eps_pred = denoising_output['eps_t_pred']
            x_0_pred = process_x_0(self.pred_x_0_from_eps(eps_pred, x_t, t))
            mean_pred = self.q_posterior_mean_variance(x_0_pred, x_t, t, need_var=False)
        elif self.denoising_mean_mode.upper() == 'START_X':
            x_0_pred = process_x_0(denoising_output['x_0_pred'])
            mean_pred = self.q_posterior_mean_variance(x_0_pred, x_t, t, need_var=False)
        elif self.denoising_mean_mode.upper() == 'PREVIOUS_X':
            mean_pred = denoising_output['x_tm1_pred']
            x_0_pred = process_x_0(self.pred_x_0_from_x_tm1(mean_pred, x_t, t))
        else:
            raise AttributeError(f'Unknown denoising mean output type [{self.denoising_mean_mode}].')
        output_dict = dict(var_pred=varpred, logvar_pred=logvar_pred, mean_pred=mean_pred, x_0_pred=x_0_pred)
        return {k: output_dict[k] for k in output_dict.keys() if k not in denoising_output}

    def denoising_step(self, model, x_t, t, noise=None, label=None, clip_denoised=True, denoised_fn=None, model_kwargs=None, return_noise=False):
        """Single denoising step. Get `x_{t-1}` from ``x_t`` and ``t``.

        Args:
            model (torch.nn.Module): Denoising model used to sample images.
            x_t (torch.Tensor): Input diffused image.
            t (torch.Tensor): Current timestep.
            noise (torch.Tensor | callable | None): Noise for
                reparameterization trick. You can directly give a batch of
                noise through a ``torch.Tensor`` or offer a callable function
                to sample a batch of noise data. Otherwise, the ``None``
                indicates to use the default noise sampler.
            label (torch.Tensor | callable | None): You can directly give a
                batch of label through a ``torch.Tensor`` or offer a callable
                function to sample a batch of label data. Otherwise, the
                ``None`` indicates to use the default label sampler.
            clip_denoised (bool, optional): Whether to clip sample results into
                [-1, 1]. Defaults to False.
            denoised_fn (callable, optional): If not None, a function which
                applies to the predicted ``x_0`` prediction before it is used
                to sample. Applies before ``clip_denoised``. Defaults to None.
            model_kwargs (dict, optional): Arguments passed to denoising model.
                Defaults to None.
            return_noise (bool, optional): If True, ``noise_batch``, outputs
                from denoising model and ``p_mean_variance`` will be returned
                in a dict with ``fake_img``. Defaults to False.

        Return:
            torch.Tensor | dict: If not ``return_noise``, only the denoising
                image will be returned. Otherwise, the dict contains
                ``fake_image``, ``noise_batch`` and outputs from denoising
                model and ``p_mean_variance`` will be returned.
        """
        if model_kwargs is None:
            model_kwargs = dict()
        model_kwargs.update(dict(return_noise=return_noise))
        denoising_output = model(x_t, t, label=label, **model_kwargs)
        p_output = self.p_mean_variance(denoising_output, x_t, t, clip_denoised, denoised_fn)
        mean_pred = p_output['mean_pred']
        var_pred = p_output['var_pred']
        num_batches = x_t.shape[0]
        device = get_module_device(self)
        noise = self.get_noise(noise, num_batches=num_batches)
        nonzero_mask = (t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1)))
        sample = mean_pred + nonzero_mask * torch.sqrt(var_pred) * noise
        if return_noise:
            return dict(fake_img=sample, noise_repar=noise, **denoising_output, **p_output)
        return sample

    def pred_x_0_from_eps(self, eps, x_t, t):
        """Predict x_0 from eps by Equ 15 in DDPM paper:

        .. math::
            x_0 = \\frac{(x_t - \\sqrt{(1-\\bar{\\alpha}_t)} * eps)}
            {\\sqrt{\\bar{\\alpha}_t}}

        Args:
            eps (torch.Tensor)
            x_t (torch.Tensor)
            t (torch.Tensor)

        Returns:
            torch.tensor: Predicted ``x_0``.
        """
        device = get_module_device(self)
        tar_shape = x_t.shape
        coef1 = var_to_tensor(self.sqrt_recip_alplas_bar, t, tar_shape, device)
        coef2 = var_to_tensor(self.sqrt_recipm1_alphas_bar, t, tar_shape, device)
        return x_t * coef1 - eps * coef2

    def pred_x_0_from_x_tm1(self, x_tm1, x_t, t):
        """
        Predict `x_0` from `x_{t-1}`. (actually from `\\mu_{\\theta}`).
        `(\\mu_{\\theta} - coef2 * x_t) / coef1`, where `coef1` and `coef2`
        are from Eq 6 of the DDPM paper.

        NOTE: This function actually predict ``x_0`` from ``mu_theta`` (mean
        of ``x_{t-1}``).

        Args:
            x_tm1 (torch.Tensor): `x_{t-1}` used to predict `x_0`.
            x_t (torch.Tensor): `x_{t}` used to predict `x_0`.
            t (torch.Tensor): Current timestep.

        Returns:
            torch.Tensor: Predicted `x_0`.

        """
        device = get_module_device(self)
        tar_shape = x_t.shape
        coef1 = var_to_tensor(self.tilde_mu_t_coef1, t, tar_shape, device)
        coef2 = var_to_tensor(self.tilde_mu_t_coef2, t, tar_shape, device)
        x_0 = (x_tm1 - coef2 * x_t) / coef1
        return x_0

    def forward_train(self, data, **kwargs):
        """Deprecated forward function in training."""
        raise NotImplementedError('In MMGeneration, we do NOT recommend users to callthis function, because the train_step function is designed for the training process.')

    def forward_test(self, data, **kwargs):
        """Testing function for Diffusion Denosing Probability Models.

        Args:
            data (torch.Tensor | dict | None): Input data. This data will be
                passed to different methods.
        """
        mode = kwargs.pop('mode', 'sampling')
        if mode == 'sampling':
            return self.sample_from_noise(data, **kwargs)
        elif mode == 'reconstruction':
            return self.reconstruction_step(data, **kwargs)
        raise NotImplementedError('Other specific testing functions should be implemented by the sub-classes.')

    def forward(self, data, return_loss=False, **kwargs):
        """Forward function.

        Args:
            data (dict | torch.Tensor): Input data dictionary.
            return_loss (bool, optional): Whether in training or testing.
                Defaults to False.

        Returns:
            dict: Output dictionary.
        """
        if return_loss:
            return self.forward_train(data, **kwargs)
        return self.forward_test(data, **kwargs)


class BaseGAN(nn.Module, metaclass=ABCMeta):
    """BaseGAN Module."""

    def __init__(self):
        super().__init__()
        self.fp16_enabled = False

    @property
    def with_disc(self):
        """Whether with dicriminator."""
        return hasattr(self, 'discriminator') and self.discriminator is not None

    @property
    def with_ema_gen(self):
        """bool: whether the GAN adopts exponential moving average."""
        return hasattr(self, 'gen_ema') and self.gen_ema is not None

    @property
    def with_gen_auxiliary_loss(self):
        """bool: whether the GAN adopts auxiliary loss in the generator."""
        return hasattr(self, 'gen_auxiliary_losses') and self.gen_auxiliary_losses is not None

    @property
    def with_disc_auxiliary_loss(self):
        """bool: whether the GAN adopts auxiliary loss in the discriminator."""
        return hasattr(self, 'disc_auxiliary_losses') and self.disc_auxiliary_losses is not None

    def _get_disc_loss(self, outputs_dict):
        losses_dict = {}
        losses_dict['loss_disc_fake'] = self.gan_loss(outputs_dict['disc_pred_fake'], target_is_real=False, is_disc=True)
        losses_dict['loss_disc_real'] = self.gan_loss(outputs_dict['disc_pred_real'], target_is_real=True, is_disc=True)
        if self.with_disc_auxiliary_loss:
            for loss_module in self.disc_auxiliary_losses:
                loss_ = loss_module(outputs_dict)
                if loss_ is None:
                    continue
                if loss_module.loss_name() in losses_dict:
                    losses_dict[loss_module.loss_name()] = losses_dict[loss_module.loss_name()] + loss_
                else:
                    losses_dict[loss_module.loss_name()] = loss_
        loss, log_var = self._parse_losses(losses_dict)
        return loss, log_var

    def _get_gen_loss(self, outputs_dict):
        losses_dict = {}
        losses_dict['loss_disc_fake_g'] = self.gan_loss(outputs_dict['disc_pred_fake_g'], target_is_real=True, is_disc=False)
        if self.with_gen_auxiliary_loss:
            for loss_module in self.gen_auxiliary_losses:
                loss_ = loss_module(outputs_dict)
                if loss_ is None:
                    continue
                if loss_module.loss_name() in losses_dict:
                    losses_dict[loss_module.loss_name()] = losses_dict[loss_module.loss_name()] + loss_
                else:
                    losses_dict[loss_module.loss_name()] = loss_
        loss, log_var = self._parse_losses(losses_dict)
        return loss, log_var

    @abstractmethod
    def train_step(self, data, optimizer, ddp_reducer=None):
        """The iteration step during training.

        This method defines an iteration step during training. Different from
        other repo in **MM** series, we allow the back propagation and
        optimizer updating to directly follow the iterative training schedule
        of GAN. Of course, we will show that you can also move the back
        propagation outside of this method, and then optimize the parameters
        in the optimizer hook. But this will cause extra GPU memory cost as a
        result of retaining computational graph. Otherwise, the training
        schedule should be modified in the detailed implementation.

        TODO: Give an example of removing bp outside ``train_step``.
        TODO: Try the synchronized back propagation.

        Args:
            data (dict): The output of dataloader.
            optimizer (:obj:`torch.optim.Optimizer` | dict): The optimizer of
                runner is passed to ``train_step()``. This argument is unused
                and reserved.
            ddp_reducer (:obj:`Reducer` | None, optional): This reducer is used
                to dynamically collect used parameters in the distributed
                training. If given an initialized ``Reducer``, we will call its
                ``prepare_for_backward()`` function just before calling
                ``.backward()``.

        Returns:
            dict: It should contain at least 3 keys: ``loss``, ``log_vars``,                 ``num_samples``.

                - ``loss`` is a tensor for back propagation, which can be a                 weighted sum of multiple losses.
                - ``log_vars`` contains all the variables to be sent to the
                logger.
                - ``num_samples`` indicates the batch size (when the model is                 DDP, it means the batch size on each GPU), which is used for                 averaging the logs.
        """

    def sample_from_noise(self, noise, num_batches=0, sample_model='ema/orig', **kwargs):
        """Sample images from noises by using the generator.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional):  The number of batch size.
                Defaults to 0.

        Returns:
            torch.Tensor | dict: The output may be the direct synthesized
                images in ``torch.Tensor``. Otherwise, a dict with queried
                data, including generated images, will be returned.
        """
        if sample_model == 'ema':
            assert self.use_ema
            _model = self.generator_ema
        elif sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator_ema
        else:
            _model = self.generator
        outputs = _model(noise, num_batches=num_batches, **kwargs)
        if isinstance(outputs, dict) and 'noise_batch' in outputs:
            noise = outputs['noise_batch']
        if sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator
            outputs_ = _model(noise, num_batches=num_batches, **kwargs)
            if isinstance(outputs_, dict):
                outputs['fake_img'] = torch.cat([outputs['fake_img'], outputs_['fake_img']], dim=0)
            else:
                outputs = torch.cat([outputs, outputs_], dim=0)
        return outputs

    def forward_train(self, data, **kwargs):
        """Deprecated forward function in training."""
        raise NotImplementedError('In MMGeneration, we do NOT recommend users to callthis function, because the train_step function is designed for the training process.')

    def forward_test(self, data, **kwargs):
        """Testing function for GANs.

        Args:
            data (torch.Tensor | dict | None): Input data. This data will be
                passed to different methods.
        """
        if kwargs.pop('mode', 'sampling') == 'sampling':
            return self.sample_from_noise(data, **kwargs)
        raise NotImplementedError('Other specific testing functions should be implemented by the sub-classes.')

    def forward(self, data, return_loss=False, **kwargs):
        """Forward function.

        Args:
            data (dict | torch.Tensor): Input data dictionary.
            return_loss (bool, optional): Whether in training or testing.
                Defaults to False.

        Returns:
            dict: Output dictionary.
        """
        if return_loss:
            return self.forward_train(data, **kwargs)
        return self.forward_test(data, **kwargs)

    def _parse_losses(self, losses):
        """Parse the raw outputs (losses) of the network.

        Args:
            losses (dict): Raw output of the network, which usually contain
                losses and other necessary information.

        Returns:
            tuple[Tensor, dict]: (loss, log_vars), loss is the loss tensor                 which may be a weighted sum of all losses, log_vars contains                 all the variables to be sent to the logger.
        """
        log_vars = OrderedDict()
        for loss_name, loss_value in losses.items():
            if isinstance(loss_value, torch.Tensor):
                log_vars[loss_name] = loss_value.mean()
            elif isinstance(loss_value, list):
                log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)
            elif loss_value is None:
                continue
            else:
                raise TypeError(f'{loss_name} is not a tensor or list of tensors')
        loss = sum(_value for _key, _value in log_vars.items() if 'loss' in _key)
        log_vars['loss'] = loss
        for loss_name, loss_value in log_vars.items():
            if dist.is_available() and dist.is_initialized():
                loss_value = loss_value.data.clone()
                dist.all_reduce(loss_value.div_(dist.get_world_size()))
            log_vars[loss_name] = loss_value.item()
        return loss, log_vars


def set_requires_grad(nets, requires_grad=False):
    """Set requires_grad for all the networks.

    Args:
        nets (nn.Module | list[nn.Module]): A list of networks or a single
            network.
        requires_grad (bool): Whether the networks require gradients or not
    """
    if not isinstance(nets, list):
        nets = [nets]
    for net in nets:
        if net is not None:
            for param in net.parameters():
                param.requires_grad = requires_grad


class BasicConditionalGAN(BaseGAN):
    """Basic conditional GANs.

    This is the conditional GAN model containing standard adversarial training
    schedule. To fulfill the requirements of various GAN algorithms,
    ``disc_auxiliary_loss`` and ``gen_auxiliary_loss`` are provided to
    customize auxiliary losses, e.g., gradient penalty loss, and discriminator
    shift loss. In addition, ``train_cfg`` and ``test_cfg`` aims at setuping
    training schedule.

    Args:
        generator (dict): Config for generator.
        discriminator (dict): Config for discriminator.
        gan_loss (dict): Config for generative adversarial loss.
        disc_auxiliary_loss (dict): Config for auxiliary loss to
            discriminator.
        gen_auxiliary_loss (dict | None, optional): Config for auxiliary loss
            to generator. Defaults to None.
        train_cfg (dict | None, optional): Config for training schedule.
            Defaults to None.
        test_cfg (dict | None, optional): Config for testing schedule. Defaults
            to None.
        num_classes (int | None, optional): The number of conditional classes.
            Defaults to None.
    """

    def __init__(self, generator, discriminator, gan_loss, disc_auxiliary_loss=None, gen_auxiliary_loss=None, train_cfg=None, test_cfg=None, num_classes=None):
        super().__init__()
        self.num_classes = num_classes
        self._gen_cfg = deepcopy(generator)
        self.generator = build_module(generator, default_args=dict(num_classes=num_classes))
        if discriminator is not None:
            self.discriminator = build_module(discriminator, default_args=dict(num_classes=num_classes))
        else:
            self.discriminator = None
        if gan_loss is not None:
            self.gan_loss = build_module(gan_loss)
        else:
            self.gan_loss = None
        if disc_auxiliary_loss:
            self.disc_auxiliary_losses = build_module(disc_auxiliary_loss)
            if not isinstance(self.disc_auxiliary_losses, nn.ModuleList):
                self.disc_auxiliary_losses = nn.ModuleList([self.disc_auxiliary_losses])
        else:
            self.disc_auxiliary_loss = None
        if gen_auxiliary_loss:
            self.gen_auxiliary_losses = build_module(gen_auxiliary_loss)
            if not isinstance(self.gen_auxiliary_losses, nn.ModuleList):
                self.gen_auxiliary_losses = nn.ModuleList([self.gen_auxiliary_losses])
        else:
            self.gen_auxiliary_losses = None
        self.train_cfg = deepcopy(train_cfg) if train_cfg else None
        self.test_cfg = deepcopy(test_cfg) if test_cfg else None
        self._parse_train_cfg()
        if test_cfg is not None:
            self._parse_test_cfg()

    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""
        if self.train_cfg is None:
            self.train_cfg = dict()
        self.disc_steps = self.train_cfg.get('disc_steps', 1)
        self.gen_steps = self.train_cfg.get('gen_steps', 1)
        self.batch_accumulation_steps = self.train_cfg.get('batch_accumulation_steps', 1)
        self.use_ema = self.train_cfg.get('use_ema', False)
        if self.use_ema:
            self.generator_ema = deepcopy(self.generator)

    def _parse_test_cfg(self):
        """Parsing test config and set some attributes for testing."""
        if self.test_cfg is None:
            self.test_cfg = dict()
        self.batch_size = self.test_cfg.get('batch_size', 1)
        self.use_ema = self.test_cfg.get('use_ema', False)

    def train_step(self, data_batch, optimizer, ddp_reducer=None, loss_scaler=None, use_apex_amp=False, running_status=None):
        """Train step function.

        This function implements the standard training iteration for
        asynchronous adversarial training. Namely, in each iteration, we first
        update discriminator and then compute loss for generator with the newly
        updated discriminator.

        As for distributed training, we use the ``reducer`` from ddp to
        synchronize the necessary params in current computational graph.

        Args:
            data_batch (dict): Input data from dataloader.
            optimizer (dict): Dict contains optimizer for generator and
                discriminator.
            ddp_reducer (:obj:`Reducer` | None, optional): Reducer from ddp.
                It is used to prepare for ``backward()`` in ddp. Defaults to
                None.
            loss_scaler (:obj:`torch.cuda.amp.GradScaler` | None, optional):
                The loss/gradient scaler used for auto mixed-precision
                training. Defaults to ``None``.
            use_apex_amp (bool, optional). Whether to use apex.amp. Defaults to
                ``False``.
            running_status (dict | None, optional): Contains necessary basic
                information for training, e.g., iteration number. Defaults to
                None.

        Returns:
            dict: Contains 'log_vars', 'num_samples', and 'results'.
        """
        real_imgs = data_batch['img']
        gt_label = data_batch['gt_label']
        batch_size = real_imgs.shape[0]
        if running_status is not None:
            curr_iter = running_status['iteration']
        else:
            if not hasattr(self, 'iteration'):
                self.iteration = 0
            curr_iter = self.iteration
        set_requires_grad(self.discriminator, True)
        if curr_iter % self.batch_accumulation_steps == 0:
            optimizer['discriminator'].zero_grad()
        with torch.no_grad():
            fake_data = self.generator(None, num_batches=batch_size, label=None, return_noise=True)
            fake_imgs, fake_label = fake_data['fake_img'], fake_data['label']
        disc_pred_fake = self.discriminator(fake_imgs, label=fake_label)
        disc_pred_real = self.discriminator(real_imgs, label=gt_label)
        data_dict_ = dict(gen=self.generator, disc=self.discriminator, disc_pred_fake=disc_pred_fake, disc_pred_real=disc_pred_real, fake_imgs=fake_imgs, real_imgs=real_imgs, iteration=curr_iter, batch_size=batch_size, gt_label=gt_label, fake_label=fake_label, loss_scaler=loss_scaler)
        loss_disc, log_vars_disc = self._get_disc_loss(data_dict_)
        loss_disc = loss_disc / float(self.batch_accumulation_steps)
        if ddp_reducer is not None:
            ddp_reducer.prepare_for_backward(_find_tensors(loss_disc))
        if loss_scaler:
            loss_scaler.scale(loss_disc).backward()
        elif use_apex_amp:
            with amp.scale_loss(loss_disc, optimizer['discriminator'], loss_id=0) as scaled_loss_disc:
                scaled_loss_disc.backward()
        else:
            loss_disc.backward()
        if (curr_iter + 1) % self.batch_accumulation_steps == 0:
            if loss_scaler:
                loss_scaler.unscale_(optimizer['discriminator'])
                loss_scaler.step(optimizer['discriminator'])
            else:
                optimizer['discriminator'].step()
        if (curr_iter + 1) % self.disc_steps != 0:
            results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=real_imgs.cpu())
            outputs = dict(log_vars=log_vars_disc, num_samples=batch_size, results=results)
            if hasattr(self, 'iteration'):
                self.iteration += 1
            return outputs
        set_requires_grad(self.discriminator, False)
        for _ in range(self.gen_steps):
            optimizer['generator'].zero_grad()
            for _ in range(self.batch_accumulation_steps):
                fake_data = self.generator(None, num_batches=batch_size, return_noise=True)
                fake_imgs, fake_label = fake_data['fake_img'], fake_data['label']
                disc_pred_fake_g = self.discriminator(fake_imgs, label=fake_label)
                data_dict_ = dict(gen=self.generator, disc=self.discriminator, fake_imgs=fake_imgs, disc_pred_fake_g=disc_pred_fake_g, iteration=curr_iter, batch_size=batch_size, fake_label=fake_label, loss_scaler=loss_scaler)
                loss_gen, log_vars_g = self._get_gen_loss(data_dict_)
                loss_gen = loss_gen / float(self.batch_accumulation_steps)
                if ddp_reducer is not None:
                    ddp_reducer.prepare_for_backward(_find_tensors(loss_gen))
                if loss_scaler:
                    loss_scaler.scale(loss_gen).backward()
                elif use_apex_amp:
                    with amp.scale_loss(loss_gen, optimizer['generator'], loss_id=1) as scaled_loss_disc:
                        scaled_loss_disc.backward()
                else:
                    loss_gen.backward()
            if loss_scaler:
                loss_scaler.unscale_(optimizer['generator'])
                loss_scaler.step(optimizer['generator'])
            else:
                optimizer['generator'].step()
        log_vars = {}
        log_vars.update(log_vars_g)
        log_vars.update(log_vars_disc)
        results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=real_imgs.cpu())
        outputs = dict(log_vars=log_vars, num_samples=batch_size, results=results)
        if hasattr(self, 'iteration'):
            self.iteration += 1
        return outputs

    def sample_from_noise(self, noise, num_batches=0, sample_model='ema/orig', label=None, **kwargs):
        """Sample images from noises by using the generator.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional): The number of batch size.
                Defaults to 0.
            sampel_model (str, optional): Use which model to sample fake
                images. Defaults to `'ema/orig'`.
            label (torch.Tensor | None , optional): The conditional label.
                Defaults to None.

        Returns:
            torch.Tensor | dict: The output may be the direct synthesized
                images in ``torch.Tensor``. Otherwise, a dict with queried
                data, including generated images, will be returned.
        """
        if sample_model == 'ema':
            assert self.use_ema
            _model = self.generator_ema
        elif sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator_ema
        else:
            _model = self.generator
        outputs = _model(noise, num_batches=num_batches, label=label, **kwargs)
        if isinstance(outputs, dict) and 'noise_batch' in outputs:
            noise = outputs['noise_batch']
            label = outputs['label']
        if sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator
            outputs_ = _model(noise, num_batches=num_batches, label=label, **kwargs)
            if isinstance(outputs_, dict):
                outputs['fake_img'] = torch.cat([outputs['fake_img'], outputs_['fake_img']], dim=0)
            else:
                outputs = torch.cat([outputs, outputs_], dim=0)
        return outputs


def build_optimizers(model, cfgs):
    """Build multiple optimizers from configs.

    If `cfgs` contains several dicts for optimizers, then a dict for each
    constructed optimizers will be returned.
    If `cfgs` only contains one optimizer config, the constructed optimizer
    itself will be returned.

    For example,

    1) Multiple optimizer configs:

    .. code-block:: python

        optimizer_cfg = dict(
            model1=dict(type='SGD', lr=lr),
            model2=dict(type='SGD', lr=lr))

    The return dict is
    ``dict('model1': torch.optim.Optimizer, 'model2': torch.optim.Optimizer)``

    2) Single optimizer config:

    .. code-block:: python

        optimizer_cfg = dict(type='SGD', lr=lr)

    The return is ``torch.optim.Optimizer``.

    Args:
        model (:obj:`nn.Module`): The model with parameters to be optimized.
        cfgs (dict): The config dict of the optimizer.

    Returns:
        dict[:obj:`torch.optim.Optimizer`] | :obj:`torch.optim.Optimizer`:
            The initialized optimizers.
    """
    optimizers = {}
    if hasattr(model, 'module'):
        model = model.module
    is_dict_of_dict = True
    for key, cfg in cfgs.items():
        if not isinstance(cfg, dict):
            is_dict_of_dict = False
    if is_dict_of_dict:
        for key, cfg in cfgs.items():
            cfg_ = cfg.copy()
            module = getattr(model, key)
            optimizers[key] = build_optimizer(module, cfg_)
        return optimizers
    return build_optimizer(model, cfgs)


class ProgressiveGrowingGAN(BaseGAN):
    """Progressive Growing Unconditional GAN.

    In this GAN model, we implement progressive growing training schedule,
    which is proposed in Progressive Growing of GANs for improved Quality,
    Stability and Variation, ICLR 2018.

    We highly recommend to use ``GrowScaleImgDataset`` for saving computational
    load in data pre-processing.

    Notes for **using PGGAN**:

    #. In official implementation, Tero uses gradient penalty with
       ``norm_mode="HWC"``
    #. We do not implement ``minibatch_repeats`` where has been used in
       official Tensorflow implementation.

    Notes for resuming progressive growing GANs:
    Users should specify the ``prev_stage`` in ``train_cfg``. Otherwise, the
    model is possible to reset the optimizer status, which will bring
    inferior performance. For example, if your model is resumed from the
    `256` stage, you should set ``train_cfg=dict(prev_stage=256)``.

    Args:
        generator (dict): Config for generator.
        discriminator (dict): Config for discriminator.
        gan_loss (dict): Config for generative adversarial loss.
        disc_auxiliary_loss (dict): Config for auxiliary loss to
            discriminator.
        gen_auxiliary_loss (dict | None, optional): Config for auxiliary loss
            to generator. Defaults to None.
        train_cfg (dict | None, optional): Config for training schedule.
            Defaults to None.
        test_cfg (dict | None, optional): Config for testing schedule. Defaults
            to None.
    """

    def __init__(self, generator, discriminator, gan_loss, disc_auxiliary_loss, gen_auxiliary_loss=None, train_cfg=None, test_cfg=None):
        super().__init__()
        self._gen_cfg = deepcopy(generator)
        self.generator = build_module(generator)
        if discriminator is not None:
            self.discriminator = build_module(discriminator)
        else:
            self.discriminator = None
        if gan_loss is not None:
            self.gan_loss = build_module(gan_loss)
        else:
            self.gan_loss = None
        if disc_auxiliary_loss:
            self.disc_auxiliary_losses = build_module(disc_auxiliary_loss)
            if not isinstance(self.disc_auxiliary_losses, nn.ModuleList):
                self.disc_auxiliary_losses = nn.ModuleList([self.disc_auxiliary_losses])
        else:
            self.disc_auxiliary_losses = None
        if gen_auxiliary_loss:
            self.gen_auxiliary_losses = build_module(gen_auxiliary_loss)
            if not isinstance(self.gen_auxiliary_losses, nn.ModuleList):
                self.gen_auxiliary_losses = nn.ModuleList([self.gen_auxiliary_losses])
        else:
            self.gen_auxiliary_losses = None
        self.register_buffer('shown_nkimg', torch.tensor(0.0))
        self.register_buffer('_curr_transition_weight', torch.tensor(1.0))
        self.train_cfg = deepcopy(train_cfg) if train_cfg else None
        self.test_cfg = deepcopy(test_cfg) if test_cfg else None
        self._parse_train_cfg()
        self.register_buffer('_next_scale_int', torch.tensor(self.scales[0][0], dtype=torch.int32))
        self.register_buffer('_curr_scale_int', torch.tensor(self.scales[-1][0], dtype=torch.int32))
        if test_cfg is not None:
            self._parse_test_cfg()

    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""
        if self.train_cfg is None:
            self.train_cfg = dict()
        self.disc_steps = self.train_cfg.get('disc_steps', 1)
        self.use_ema = self.train_cfg.get('use_ema', False)
        if self.use_ema:
            self.generator_ema = deepcopy(self.generator)
        interp_real_cfg = deepcopy(self.train_cfg.get('interp_real', None))
        if interp_real_cfg is None:
            interp_real_cfg = dict(mode='bilinear', align_corners=True)
        self.interp_real_to = partial(F.interpolate, **interp_real_cfg)
        assert isinstance(self.train_cfg['nkimgs_per_scale'], dict), 'Please provide "nkimgs_per_scale" to schedule the training procedure.'
        nkimgs_per_scale = deepcopy(self.train_cfg['nkimgs_per_scale'])
        self.scales = []
        self.nkimgs = []
        for k, v in nkimgs_per_scale.items():
            if isinstance(k, str):
                k = int(k), int(k)
            elif isinstance(k, int):
                k = k, k
            else:
                assert mmcv.is_tuple_of(k, int)
            assert len(self.scales) == 0 or k[0] > self.scales[-1][0]
            self.scales.append(k)
            self.nkimgs.append(v)
        self.cum_nkimgs = np.cumsum(self.nkimgs)
        self.curr_stage = 0
        self.prev_stage = 0
        self._actual_nkimgs = []
        self.transition_kimgs = self.train_cfg.get('transition_kimgs', 600)
        self.optimizer = build_optimizers(self, deepcopy(self.train_cfg['optimizer_cfg']))
        self.g_lr_base = self.train_cfg['g_lr_base']
        self.d_lr_base = self.train_cfg['d_lr_base']
        self.g_lr_schedule = self.train_cfg.get('g_lr_schedule', dict())
        self.d_lr_schedule = self.train_cfg.get('d_lr_schedule', dict())
        self.reset_optim_for_new_scale = self.train_cfg.get('reset_optim_for_new_scale', True)
        self.prev_stage = self.train_cfg.get('prev_stage', self.prev_stage)

    def _parse_test_cfg(self):
        """Parsing train config and set some attributes for testing."""
        if self.test_cfg is None:
            self.test_cfg = dict()
        self.batch_size = self.test_cfg.get('batch_size', 1)
        self.use_ema = self.test_cfg.get('use_ema', False)

    def sample_from_noise(self, noise, num_batches=0, curr_scale=None, transition_weight=None, sample_model='ema/orig', **kwargs):
        """Sample images from noises by using the generator.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional):  The number of batch size.
                Defaults to 0.

        Returns:
            torch.Tensor | dict: The output may be the direct synthesized                 images in ``torch.Tensor``. Otherwise, a dict with queried                 data, including generated images, will be returned.
        """
        if curr_scale is None:
            if hasattr(self, 'curr_scale'):
                curr_scale = self.curr_scale[0]
            else:
                curr_scale = self._curr_scale_int.item()
        if transition_weight is None:
            transition_weight = self._curr_transition_weight.item()
        if sample_model == 'ema':
            assert self.use_ema
            _model = self.generator_ema
        elif sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator_ema
        else:
            _model = self.generator
        outputs = _model(noise, num_batches=num_batches, curr_scale=curr_scale, transition_weight=transition_weight, **kwargs)
        if isinstance(outputs, dict) and 'noise_batch' in outputs:
            noise = outputs['noise_batch']
        if sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator
            outputs_ = _model(noise, num_batches=num_batches, curr_scale=curr_scale, transition_weight=transition_weight, **kwargs)
            if isinstance(outputs_, dict):
                outputs['fake_img'] = torch.cat([outputs['fake_img'], outputs_['fake_img']], dim=0)
            else:
                outputs = torch.cat([outputs, outputs_], dim=0)
        return outputs

    def train_step(self, data_batch, optimizer, ddp_reducer=None, running_status=None):
        """Train step function.

        This function implements the standard training iteration for
        asynchronous adversarial training. Namely, in each iteration, we first
        update discriminator and then compute loss for generator with the newly
        updated discriminator.

        As for distributed training, we use the ``reducer`` from ddp to
        synchronize the necessary params in current computational graph.

        Args:
            data_batch (dict): Input data from dataloader.
            optimizer (dict): Dict contains optimizer for generator and
                discriminator.
            ddp_reducer (:obj:`Reducer` | None, optional): Reducer from ddp.
                It is used to prepare for ``backward()`` in ddp. Defaults to
                None.
            running_status (dict | None, optional): Contains necessary basic
                information for training, e.g., iteration number. Defaults to
                None.

        Returns:
            dict: Contains 'log_vars', 'num_samples', and 'results'.
        """
        real_imgs = data_batch['real_img']
        batch_size = real_imgs.shape[0]
        if running_status is not None:
            curr_iter = running_status['iteration']
        else:
            if not hasattr(self, 'iteration'):
                self.iteration = 0
            curr_iter = self.iteration
        if hasattr(self, 'optimizer'):
            optimizer = self.optimizer
        self.curr_stage = int(min(sum(self.cum_nkimgs <= self.shown_nkimg.item()), len(self.scales) - 1))
        self.curr_scale = self.scales[self.curr_stage]
        self._curr_scale_int = self._next_scale_int.clone()
        if self.curr_stage != self.prev_stage:
            self.prev_stage = self.curr_stage
            self._actual_nkimgs.append(self.shown_nkimg.item())
            if self.reset_optim_for_new_scale:
                optim_cfg = deepcopy(self.train_cfg['optimizer_cfg'])
                optim_cfg['generator']['lr'] = self.g_lr_schedule.get(str(self.curr_scale[0]), self.g_lr_base)
                optim_cfg['discriminator']['lr'] = self.d_lr_schedule.get(str(self.curr_scale[0]), self.d_lr_base)
                self.optimizer = build_optimizers(self, optim_cfg)
                optimizer = self.optimizer
                mmcv.print_log('Reset optimizer for new scale', logger='mmgen')
        if self.curr_stage == 0:
            transition_weight = 1.0
        else:
            transition_weight = (self.shown_nkimg.item() - self._actual_nkimgs[-1]) / self.transition_kimgs
            transition_weight = min(max(transition_weight, 0.0), 1.0)
        self._curr_transition_weight = torch.tensor(transition_weight)
        if real_imgs.shape[2:] == self.curr_scale:
            pass
        elif real_imgs.shape[2] >= self.curr_scale[0] and real_imgs.shape[3] >= self.curr_scale[1]:
            real_imgs = self.interp_real_to(real_imgs, size=self.curr_scale)
        else:
            raise RuntimeError(f'The scale of real image {real_imgs.shape[2:]} is smaller than current scale {self.curr_scale}.')
        set_requires_grad(self.discriminator, True)
        optimizer['discriminator'].zero_grad()
        with torch.no_grad():
            fake_imgs = self.generator(None, num_batches=batch_size, curr_scale=self.curr_scale[0], transition_weight=transition_weight)
        disc_pred_fake = self.discriminator(fake_imgs, curr_scale=self.curr_scale[0], transition_weight=transition_weight)
        disc_pred_real = self.discriminator(real_imgs, curr_scale=self.curr_scale[0], transition_weight=transition_weight)
        data_dict_ = dict(iteration=curr_iter, gen=self.generator, disc=self.discriminator, disc_pred_fake=disc_pred_fake, disc_pred_real=disc_pred_real, fake_imgs=fake_imgs, real_imgs=real_imgs, curr_scale=self.curr_scale[0], transition_weight=transition_weight, gen_partial=partial(self.generator, curr_scale=self.curr_scale[0], transition_weight=transition_weight), disc_partial=partial(self.discriminator, curr_scale=self.curr_scale[0], transition_weight=transition_weight))
        loss_disc, log_vars_disc = self._get_disc_loss(data_dict_)
        if ddp_reducer is not None:
            ddp_reducer.prepare_for_backward(_find_tensors(loss_disc))
        loss_disc.backward()
        optimizer['discriminator'].step()
        if dist.is_initialized():
            _batch_size = batch_size * dist.get_world_size()
        else:
            if 'batch_size' not in running_status:
                raise RuntimeError('You should offer "batch_size" in running status for PGGAN')
            _batch_size = running_status['batch_size']
        self.shown_nkimg += _batch_size / 1000.0
        log_vars_disc.update(dict(shown_nkimg=self.shown_nkimg.item(), curr_scale=self.curr_scale[0], transition_weight=transition_weight))
        if (curr_iter + 1) % self.disc_steps != 0:
            results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=real_imgs.cpu())
            outputs = dict(log_vars=log_vars_disc, num_samples=batch_size, results=results)
            if hasattr(self, 'iteration'):
                self.iteration += 1
            return outputs
        set_requires_grad(self.discriminator, False)
        optimizer['generator'].zero_grad()
        fake_imgs = self.generator(None, num_batches=batch_size, curr_scale=self.curr_scale[0], transition_weight=transition_weight)
        disc_pred_fake_g = self.discriminator(fake_imgs, curr_scale=self.curr_scale[0], transition_weight=transition_weight)
        data_dict_ = dict(iteration=curr_iter, gen=self.generator, disc=self.discriminator, fake_imgs=fake_imgs, disc_pred_fake_g=disc_pred_fake_g)
        loss_gen, log_vars_g = self._get_gen_loss(data_dict_)
        if ddp_reducer is not None:
            ddp_reducer.prepare_for_backward(_find_tensors(loss_gen))
        loss_gen.backward()
        optimizer['generator'].step()
        log_vars = {}
        log_vars.update(log_vars_g)
        log_vars.update(log_vars_disc)
        log_vars.update({'batch_size': batch_size})
        results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=real_imgs.cpu())
        outputs = dict(log_vars=log_vars, num_samples=batch_size, results=results)
        if hasattr(self, 'iteration'):
            self.iteration += 1
        _curr_stage = int(min(sum(self.cum_nkimgs <= self.shown_nkimg.item()), len(self.scales) - 1))
        if _curr_stage != self.curr_stage:
            self._next_scale_int = self._next_scale_int * 2
        return outputs


class SinGAN(BaseGAN):
    """SinGAN.

    This model implement the single image generative adversarial model proposed
    in: Singan: Learning a Generative Model from a Single Natural Image,
    ICCV'19.

    Notes for training:

    - This model should be trained with our dataset ``SinGANDataset``.
    - In training, the ``total_iters`` arguments is related to the number of
      scales in the image pyramid and ``iters_per_scale`` in the ``train_cfg``.
      You should set it carefully in the training config file.

    Notes for model architectures:

    - The generator and discriminator need ``num_scales`` in initialization.
      However, this arguments is generated by ``create_real_pyramid`` function
      from the ``singan_dataset.py``. The last element in the returned list
      (``stop_scale``) is the value for ``num_scales``. Pay attention that this
      scale is counted from zero. Please see our tutorial for SinGAN to obtain
      more details or our standard config for reference.

    Args:
        generator (dict): Config for generator.
        discriminator (dict): Config for discriminator.
        gan_loss (dict): Config for generative adversarial loss.
        disc_auxiliary_loss (dict): Config for auxiliary loss to
            discriminator.
        gen_auxiliary_loss (dict | None, optional): Config for auxiliary loss
            to generator. Defaults to None.
        train_cfg (dict | None, optional): Config for training schedule.
            Defaults to None.
        test_cfg (dict | None, optional): Config for testing schedule. Defaults
            to None.
    """

    def __init__(self, generator, discriminator, gan_loss, disc_auxiliary_loss, gen_auxiliary_loss=None, train_cfg=None, test_cfg=None):
        super().__init__()
        self._gen_cfg = deepcopy(generator)
        self.generator = build_module(generator)
        if discriminator is not None:
            self.discriminator = build_module(discriminator)
        else:
            self.discriminator = None
        if gan_loss is not None:
            self.gan_loss = build_module(gan_loss)
        else:
            self.gan_loss = None
        if disc_auxiliary_loss:
            self.disc_auxiliary_losses = build_module(disc_auxiliary_loss)
            if not isinstance(self.disc_auxiliary_losses, nn.ModuleList):
                self.disc_auxiliary_losses = nn.ModuleList([self.disc_auxiliary_losses])
        else:
            self.disc_auxiliary_losses = None
        if gen_auxiliary_loss:
            self.gen_auxiliary_losses = build_module(gen_auxiliary_loss)
            if not isinstance(self.gen_auxiliary_losses, nn.ModuleList):
                self.gen_auxiliary_losses = nn.ModuleList([self.gen_auxiliary_losses])
        else:
            self.gen_auxiliary_losses = None
        self.curr_stage = -1
        self.noise_weights = [1]
        self.fixed_noises = []
        self.reals = []
        self.train_cfg = deepcopy(train_cfg) if train_cfg else None
        self.test_cfg = deepcopy(test_cfg) if test_cfg else None
        self._parse_train_cfg()
        if test_cfg is not None:
            self._parse_test_cfg()

    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""
        if self.train_cfg is None:
            self.train_cfg = dict()
        self.use_ema = self.train_cfg.get('use_ema', False)
        if self.use_ema:
            self.generator_ema = deepcopy(self.generator)

    def _parse_test_cfg(self):
        if self.test_cfg.get('pkl_data', None) is not None:
            with open(self.test_cfg.pkl_data, 'rb') as f:
                data = pickle.load(f)
                self.fixed_noises = self._from_numpy(data['fixed_noises'])
                self.noise_weights = self._from_numpy(data['noise_weights'])
                self.curr_stage = data['curr_stage']
            mmcv.print_log(f'Load pkl data from {self.test_cfg.pkl_data}', 'mmgen')

    def _from_numpy(self, data):
        if isinstance(data, list):
            return [self._from_numpy(x) for x in data]
        if isinstance(data, np.ndarray):
            data = torch.from_numpy(data)
            device = get_module_device(self.generator)
            data = data
            return data
        return data

    def get_module(self, model, module_name):
        """Get an inner module from model.

        Since we will wrapper DDP for some model, we have to judge whether the
        module can be indexed directly.

        Args:
            model (nn.Module): This model may wrapped with DDP or not.
            module_name (str): The name of specific module.

        Return:
            nn.Module: Returned sub module.
        """
        if isinstance(model, (DataParallel, DistributedDataParallel)):
            return getattr(model.module, module_name)
        return getattr(model, module_name)

    def sample_from_noise(self, noise, num_batches=0, curr_scale=None, sample_model='ema/orig', **kwargs):
        """Sample images from noises by using the generator.

        Args:
            noise (torch.Tensor | callable | None): You can directly give a
                batch of noise through a ``torch.Tensor`` or offer a callable
                function to sample a batch of noise data. Otherwise, the
                ``None`` indicates to use the default noise sampler.
            num_batches (int, optional):  The number of batch size.
                Defaults to 0.

        Returns:
            torch.Tensor | dict: The output may be the direct synthesized                 images in ``torch.Tensor``. Otherwise, a dict with queried                 data, including generated images, will be returned.
        """
        if curr_scale is None:
            curr_scale = self.curr_stage
        if sample_model == 'ema':
            assert self.use_ema
            _model = self.generator_ema
        elif sample_model == 'ema/orig' and self.use_ema:
            _model = self.generator_ema
        else:
            _model = self.generator
        if not self.fixed_noises[0].is_cuda and torch.cuda.is_available():
            self.fixed_noises = [x for x in self.fixed_noises]
        outputs = _model(None, fixed_noises=self.fixed_noises, noise_weights=self.noise_weights, rand_mode='rand', num_batches=num_batches, curr_scale=curr_scale, **kwargs)
        return outputs

    def construct_fixed_noises(self):
        """Construct the fixed noises list used in SinGAN."""
        for i, real in enumerate(self.reals):
            h, w = real.shape[-2:]
            if i == 0:
                noise = torch.randn(1, 1, h, w)
                self.fixed_noises.append(noise)
            else:
                noise = torch.zeros_like(real)
                self.fixed_noises.append(noise)

    def train_step(self, data_batch, optimizer, ddp_reducer=None, running_status=None):
        """Train step function.

        This function implements the standard training iteration for
        asynchronous adversarial training. Namely, in each iteration, we first
        update discriminator and then compute loss for generator with the newly
        updated discriminator.

        As for distributed training, we use the ``reducer`` from ddp to
        synchronize the necessary params in current computational graph.

        Args:
            data_batch (dict): Input data from dataloader.
            optimizer (dict): Dict contains optimizer for generator and
                discriminator.
            ddp_reducer (:obj:`Reducer` | None, optional): Reducer from ddp.
                It is used to prepare for ``backward()`` in ddp. Defaults to
                None.
            running_status (dict | None, optional): Contains necessary basic
                information for training, e.g., iteration number. Defaults to
                None.

        Returns:
            dict: Contains 'log_vars', 'num_samples', and 'results'.
        """
        if running_status is not None:
            curr_iter = running_status['iteration']
        else:
            if not hasattr(self, 'iteration'):
                self.iteration = 0
            curr_iter = self.iteration
        if curr_iter % self.train_cfg['iters_per_scale'] == 0:
            self.curr_stage += 1
            self.get_module(self.generator, 'check_and_load_prev_weight')(self.curr_stage)
            self.get_module(self.discriminator, 'check_and_load_prev_weight')(self.curr_stage)
            g_module = self.get_module(self.generator, 'blocks')
            param_list = g_module[self.curr_stage].parameters()
            self.g_optim = torch.optim.Adam(param_list, lr=self.train_cfg['lr_g'], betas=(0.5, 0.999))
            d_module = self.get_module(self.discriminator, 'blocks')
            self.d_optim = torch.optim.Adam(d_module[self.curr_stage].parameters(), lr=self.train_cfg['lr_d'], betas=(0.5, 0.999))
            self.optimizer = dict(generator=self.g_optim, discriminator=self.d_optim)
            self.g_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=self.g_optim, **self.train_cfg['lr_scheduler_args'])
            self.d_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=self.d_optim, **self.train_cfg['lr_scheduler_args'])
        optimizer = self.optimizer
        if curr_iter == 0 or len(self.reals) == 0:
            keys = [k for k in data_batch.keys() if 'real_scale' in k]
            scales = len(keys)
            self.reals = [data_batch[f'real_scale{s}'] for s in range(scales)]
            self.construct_fixed_noises()
        set_requires_grad(self.discriminator, True)
        for _ in range(self.train_cfg['disc_steps']):
            optimizer['discriminator'].zero_grad()
            with torch.no_grad():
                fake_imgs = self.generator(data_batch['input_sample'], self.fixed_noises, self.noise_weights, rand_mode='rand', curr_scale=self.curr_stage)
            disc_pred_fake = self.discriminator(fake_imgs.detach(), self.curr_stage)
            disc_pred_real = self.discriminator(self.reals[self.curr_stage], self.curr_stage)
            data_dict_ = dict(iteration=curr_iter, gen=self.generator, disc=self.discriminator, disc_pred_fake=disc_pred_fake, disc_pred_real=disc_pred_real, fake_imgs=fake_imgs, real_imgs=self.reals[self.curr_stage], disc_partial=partial(self.discriminator, curr_scale=self.curr_stage))
            loss_disc, log_vars_disc = self._get_disc_loss(data_dict_)
            if ddp_reducer is not None:
                ddp_reducer.prepare_for_backward(_find_tensors(loss_disc))
            loss_disc.backward()
            optimizer['discriminator'].step()
        log_vars_disc.update(dict(curr_stage=self.curr_stage))
        set_requires_grad(self.discriminator, False)
        for _ in range(self.train_cfg['generator_steps']):
            optimizer['generator'].zero_grad()
            fake_imgs = self.generator(data_batch['input_sample'], self.fixed_noises, self.noise_weights, rand_mode='rand', curr_scale=self.curr_stage)
            disc_pred_fake_g = self.discriminator(fake_imgs, curr_scale=self.curr_stage)
            recon_imgs = self.generator(data_batch['input_sample'], self.fixed_noises, self.noise_weights, rand_mode='recon', curr_scale=self.curr_stage)
            data_dict_ = dict(iteration=curr_iter, gen=self.generator, disc=self.discriminator, fake_imgs=fake_imgs, recon_imgs=recon_imgs, real_imgs=self.reals[self.curr_stage], disc_pred_fake_g=disc_pred_fake_g)
            loss_gen, log_vars_g = self._get_gen_loss(data_dict_)
            if ddp_reducer is not None:
                ddp_reducer.prepare_for_backward(_find_tensors(loss_gen))
            loss_gen.backward()
            optimizer['generator'].step()
        if curr_iter % self.train_cfg['iters_per_scale'] == 0 and self.curr_stage < len(self.reals) - 1:
            with torch.no_grad():
                g_recon = self.generator(data_batch['input_sample'], self.fixed_noises, self.noise_weights, rand_mode='recon', curr_scale=self.curr_stage)
                if isinstance(g_recon, dict):
                    g_recon = g_recon['fake_img']
                g_recon = F.interpolate(g_recon, self.reals[self.curr_stage + 1].shape[-2:])
            mse = F.mse_loss(g_recon.detach(), self.reals[self.curr_stage + 1])
            rmse = torch.sqrt(mse)
            self.noise_weights.append(self.train_cfg.get('noise_weight_init', 0.1) * rmse.item())
            torch.cuda.empty_cache()
        log_vars = {}
        log_vars.update(log_vars_g)
        log_vars.update(log_vars_disc)
        results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=self.reals[self.curr_stage].cpu(), recon_imgs=recon_imgs.cpu(), curr_stage=self.curr_stage, fixed_noises=self.fixed_noises, noise_weights=self.noise_weights)
        outputs = dict(log_vars=log_vars, num_samples=1, results=results)
        self.d_scheduler.step()
        self.g_scheduler.step()
        if hasattr(self, 'iteration'):
            self.iteration += 1
        return outputs


class PESinGAN(SinGAN):
    """Positional Encoding in SinGAN.

    This modified SinGAN is used to reimplement the experiments in: Positional
    Encoding as Spatial Inductive Bias in GANs, CVPR2021.
    """

    def _parse_train_cfg(self):
        super(PESinGAN, self)._parse_train_cfg()
        self.fixed_noise_with_pad = self.train_cfg.get('fixed_noise_with_pad', False)
        self.first_fixed_noises_ch = self.train_cfg.get('first_fixed_noises_ch', 1)

    def construct_fixed_noises(self):
        """Construct the fixed noises list used in SinGAN."""
        for i, real in enumerate(self.reals):
            h, w = real.shape[-2:]
            if self.fixed_noise_with_pad:
                pad_ = self.get_module(self, 'generator').pad_head
                h += 2 * pad_
                w += 2 * pad_
            if i == 0:
                noise = torch.randn(1, self.first_fixed_noises_ch, h, w)
                self.fixed_noises.append(noise)
            else:
                noise = torch.zeros((1, 1, h, w))
                self.fixed_noises.append(noise)


class StaticUnconditionalGAN(BaseGAN):
    """Unconditional GANs with static architecture in training.

    This is the standard GAN model containing standard adversarial training
    schedule. To fulfill the requirements of various GAN algorithms,
    ``disc_auxiliary_loss`` and ``gen_auxiliary_loss`` are provided to
    customize auxiliary losses, e.g., gradient penalty loss, and discriminator
    shift loss. In addition, ``train_cfg`` and ``test_cfg`` aims at setuping
    training schedule.

    Args:
        generator (dict): Config for generator.
        discriminator (dict): Config for discriminator.
        gan_loss (dict): Config for generative adversarial loss.
        disc_auxiliary_loss (dict): Config for auxiliary loss to
            discriminator.
        gen_auxiliary_loss (dict | None, optional): Config for auxiliary loss
            to generator. Defaults to None.
        train_cfg (dict | None, optional): Config for training schedule.
            Defaults to None.
        test_cfg (dict | None, optional): Config for testing schedule. Defaults
            to None.
    """

    def __init__(self, generator, discriminator, gan_loss, disc_auxiliary_loss=None, gen_auxiliary_loss=None, train_cfg=None, test_cfg=None):
        super().__init__()
        self._gen_cfg = deepcopy(generator)
        self.generator = build_module(generator)
        if discriminator is not None:
            self.discriminator = build_module(discriminator)
        else:
            self.discriminator = None
        if gan_loss is not None:
            self.gan_loss = build_module(gan_loss)
        else:
            self.gan_loss = None
        if disc_auxiliary_loss:
            self.disc_auxiliary_losses = build_module(disc_auxiliary_loss)
            if not isinstance(self.disc_auxiliary_losses, nn.ModuleList):
                self.disc_auxiliary_losses = nn.ModuleList([self.disc_auxiliary_losses])
        else:
            self.disc_auxiliary_loss = None
        if gen_auxiliary_loss:
            self.gen_auxiliary_losses = build_module(gen_auxiliary_loss)
            if not isinstance(self.gen_auxiliary_losses, nn.ModuleList):
                self.gen_auxiliary_losses = nn.ModuleList([self.gen_auxiliary_losses])
        else:
            self.gen_auxiliary_losses = None
        self.train_cfg = deepcopy(train_cfg) if train_cfg else None
        self.test_cfg = deepcopy(test_cfg) if test_cfg else None
        self._parse_train_cfg()
        if test_cfg is not None:
            self._parse_test_cfg()

    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""
        if self.train_cfg is None:
            self.train_cfg = dict()
        self.disc_steps = self.train_cfg.get('disc_steps', 1)
        self.use_ema = self.train_cfg.get('use_ema', False)
        if self.use_ema:
            self.generator_ema = deepcopy(self.generator)
        self.real_img_key = self.train_cfg.get('real_img_key', 'real_img')

    def _parse_test_cfg(self):
        """Parsing test config and set some attributes for testing."""
        if self.test_cfg is None:
            self.test_cfg = dict()
        self.batch_size = self.test_cfg.get('batch_size', 1)
        self.use_ema = self.test_cfg.get('use_ema', False)

    def train_step(self, data_batch, optimizer, ddp_reducer=None, loss_scaler=None, use_apex_amp=False, running_status=None):
        """Train step function.

        This function implements the standard training iteration for
        asynchronous adversarial training. Namely, in each iteration, we first
        update discriminator and then compute loss for generator with the newly
        updated discriminator.

        As for distributed training, we use the ``reducer`` from ddp to
        synchronize the necessary params in current computational graph.

        Args:
            data_batch (dict): Input data from dataloader.
            optimizer (dict): Dict contains optimizer for generator and
                discriminator.
            ddp_reducer (:obj:`Reducer` | None, optional): Reducer from ddp.
                It is used to prepare for ``backward()`` in ddp. Defaults to
                None.
            loss_scaler (:obj:`torch.cuda.amp.GradScaler` | None, optional):
                The loss/gradient scaler used for auto mixed-precision
                training. Defaults to ``None``.
            use_apex_amp (bool, optional). Whether to use apex.amp. Defaults to
                ``False``.
            running_status (dict | None, optional): Contains necessary basic
                information for training, e.g., iteration number. Defaults to
                None.

        Returns:
            dict: Contains 'log_vars', 'num_samples', and 'results'.
        """
        real_imgs = data_batch[self.real_img_key]
        batch_size = real_imgs.shape[0]
        if running_status is not None:
            curr_iter = running_status['iteration']
        else:
            if not hasattr(self, 'iteration'):
                self.iteration = 0
            curr_iter = self.iteration
        set_requires_grad(self.discriminator, True)
        optimizer['discriminator'].zero_grad()
        g_training_kwargs = {}
        if hasattr(self.generator, 'get_training_kwargs'):
            g_training_kwargs.update(self.generator.get_training_kwargs(phase='disc'))
        with torch.no_grad():
            fake_imgs = self.generator(None, num_batches=batch_size, **g_training_kwargs)
        disc_pred_fake = self.discriminator(fake_imgs)
        disc_pred_real = self.discriminator(real_imgs)
        data_dict_ = dict(gen=self.generator, disc=self.discriminator, disc_pred_fake=disc_pred_fake, disc_pred_real=disc_pred_real, fake_imgs=fake_imgs, real_imgs=real_imgs, iteration=curr_iter, batch_size=batch_size, loss_scaler=loss_scaler)
        loss_disc, log_vars_disc = self._get_disc_loss(data_dict_)
        if ddp_reducer is not None:
            ddp_reducer.prepare_for_backward(_find_tensors(loss_disc))
        if loss_scaler:
            loss_scaler.scale(loss_disc).backward()
        elif use_apex_amp:
            with amp.scale_loss(loss_disc, optimizer['discriminator'], loss_id=0) as scaled_loss_disc:
                scaled_loss_disc.backward()
        else:
            loss_disc.backward()
        if loss_scaler:
            loss_scaler.unscale_(optimizer['discriminator'])
            loss_scaler.step(optimizer['discriminator'])
        else:
            optimizer['discriminator'].step()
        if (curr_iter + 1) % self.disc_steps != 0:
            results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=real_imgs.cpu())
            outputs = dict(log_vars=log_vars_disc, num_samples=batch_size, results=results)
            if hasattr(self, 'iteration'):
                self.iteration += 1
            return outputs
        set_requires_grad(self.discriminator, False)
        optimizer['generator'].zero_grad()
        g_training_kwargs = {}
        if hasattr(self.generator, 'get_training_kwargs'):
            g_training_kwargs.update(self.generator.get_training_kwargs(phase='gen'))
        fake_imgs = self.generator(None, num_batches=batch_size, **g_training_kwargs)
        disc_pred_fake_g = self.discriminator(fake_imgs)
        data_dict_ = dict(gen=self.generator, disc=self.discriminator, fake_imgs=fake_imgs, disc_pred_fake_g=disc_pred_fake_g, iteration=curr_iter, batch_size=batch_size, loss_scaler=loss_scaler)
        loss_gen, log_vars_g = self._get_gen_loss(data_dict_)
        if ddp_reducer is not None:
            ddp_reducer.prepare_for_backward(_find_tensors(loss_gen))
        if loss_scaler:
            loss_scaler.scale(loss_gen).backward()
        elif use_apex_amp:
            with amp.scale_loss(loss_gen, optimizer['generator'], loss_id=1) as scaled_loss_disc:
                scaled_loss_disc.backward()
        else:
            loss_gen.backward()
        if loss_scaler:
            loss_scaler.unscale_(optimizer['generator'])
            loss_scaler.step(optimizer['generator'])
        else:
            optimizer['generator'].step()
        if hasattr(self.discriminator, 'with_ada') and self.discriminator.with_ada:
            self.discriminator.ada_aug.log_buffer[0] += batch_size
            self.discriminator.ada_aug.log_buffer[1] += disc_pred_real.sign().sum()
            self.discriminator.ada_aug.update(iteration=curr_iter, num_batches=batch_size)
            log_vars_disc['augment'] = self.discriminator.ada_aug.aug_pipeline.p.data.cpu()
        log_vars = {}
        log_vars.update(log_vars_g)
        log_vars.update(log_vars_disc)
        results = dict(fake_imgs=fake_imgs.cpu(), real_imgs=real_imgs.cpu())
        outputs = dict(log_vars=log_vars, num_samples=batch_size, results=results)
        if hasattr(self, 'iteration'):
            self.iteration += 1
        return outputs


_reduction_modes = ['none', 'mean', 'sum', 'batchmean', 'flatmean']


def digit_version(version_str):
    digit_version = []
    for x in version_str.split('.'):
        if x.isdigit():
            digit_version.append(int(x))
        elif x.find('rc') != -1:
            patch_version = x.split('rc')
            digit_version.append(int(patch_version[0]) - 1)
            digit_version.append(int(patch_version[1]))
    return digit_version


class DDPMLoss(nn.Module):
    """Base module for DDPM losses. We support loss weight rescale and log
    collection for DDPM models in this module.

    We support two kinds of loss rescale methods, which can be
    controlled by ``rescale_mode`` and ``rescale_cfg``:
    1. ``rescale_mode == 'constant'``: ``constant_rescale`` would be called,
        and ``rescale_cfg`` should be passed as ``dict(scale=SCALE)``,
        e.g., ``dict(scale=1.2)``. Then, all loss terms would be rescaled by
        multiply with ``SCALE``
    2. ``rescale_mode == timestep_weight``: ``timestep_weight_rescale`` would
        be called, and ``weight`` or ``sampler`` who contains attribute of
        weight must be passed. Then, loss at timestep `t` would be multiplied
        with `weight[t]`. We also support users further apply a constant
        rescale factor to all loss terms, e.g.
        ``rescale_cfg=dict(scale=SCALE)``. The overall rescale function for
        loss at timestep ``t`` can be formulated as
        `loss[t] := weight[t] * loss[t] * SCALE`. To be noted that, ``weight``
        or ``sampler.weight`` would be inplace modified in the outer code.
        e.g.,

        .. code-blocks:: python
            :linenos:

            # 1. define weight
            weight = torch.randn(10, )

            # 2. define loss function
            loss_fn = DDPMLoss(rescale_mode='timestep_weight', weight=weight)

            # 3 update weight
            # wrong usage: `weight` in `loss_fn` is not accessible from now
            # because you assign a new tensor to variable `weight`
            # weight = torch.randn(10, )

            # correct usage: update `weight` inplace
            weight[2] = 2

    If ``rescale_mode`` is not passed, ``rescale_cfg`` would be ignored, and
    all loss terms would not be rescaled.

    For loss log collection, we support users to pass a list of (or single)
    config by ``log_cfgs`` argument to define how they want to collect loss
    terms and show them in the log. Each log collection returns a dict which
    key and value are the name and value of collected loss terms. And the dict
    will be merged into  ``log_vars`` after the loss used for parameter
    optimization is calculated. The log updating process for the class which
    uses ddpm_loss can be referred to the following pseudo-code:

    .. code-block:: python
        :linenos:

        # 1. loss dict for parameter optimization
        losses_dict = {}

        # 2. calculate losses
        for loss_fn in self.ddpm_loss:
            losses_dict[loss_fn.loss_name()] = loss_fn(outputs_dict)

        # 3. init log_vars
        log_vars = OrderedDict()

        # 4. update log_vars with loss terms used for parameter optimization
        for loss_name, loss_value in losses_dict.items():
            log_vars[loss_name] = loss_value.mean()

        # 5. sum all loss terms used for backward
        loss = sum(_value for _key, _value in log_vars.items()
                   if 'loss' in _key)

        # 6. update log_var with log collection functions
        for loss_fn in self.ddpm_loss:
            if hasattr(loss_fn, 'log_vars'):
                log_vars.update(loss_fn.log_vars)

    Each log configs must contain ``type`` keyword, and may contain ``prefix``
    and ``reduction`` keywords.

    ``type``: Use to get the corresponding collection function. Functions would
        be named as ``f'{type}_log_collect'``. In `DDPMLoss`, we only support
        ``type=='quartile'``, but users may define their log collection
        functions and use them in this way.
    ``prefix``: This keyword is set for avoiding the name of displayed loss
        terms being too long. The name of each loss term will set as
        ``'{prefix}_{log_coll_fn_spec_name}'``, where
        ``{log_coll_fn_spec_name}`` is name specific to the log collection
        function. If passed, it must start with ``'loss_'``. If not passed,
        ``'loss_'`` would be used.
    ``reduction``: Control the reduction method of the collected loss terms.

    We implement ``quartile_log_collection`` in this module. In detail, we
    divide total timesteps into four parts and collect the loss in the
    corresponding timestep intervals.

    To use those collection methods, users may pass ``log_cfgs`` as the
    following example:

    .. code-block:: python
        :linenos:

        log_cfgs = [
            dict(type='quartile', reduction=REUCTION, prefix_name=PREFIX),
            ...
        ]

    Args:
        rescale_mode (str, optional): Mode of the loss rescale method.
            Defaults to None.
        rescale_cfg (dict, optional): Config of the loss rescale method.
        log_cfgs (list[dict] | dict | optional): Configs to collect logs.
            Defaults to None.
        sampler (object): Weight sampler. Defaults to None.
        weight (torch.Tensor, optional): Weight used for rescale losses.
            Defaults to None.
        reduction (str, optional): Same as built-in losses of PyTorch.
            Defaults to 'mean'.
        loss_name (str, optional): Name of the loss item. Defaults to None.
    """

    def __init__(self, rescale_mode=None, rescale_cfg=None, log_cfgs=None, weight=None, sampler=None, reduction='mean', loss_name=None):
        super().__init__()
        if reduction not in _reduction_modes:
            raise ValueError(f'Unsupported reduction mode: {reduction}. Supported ones are: {_reduction_modes}')
        self.reduction = reduction
        self._loss_name = loss_name
        self.log_fn_list = []
        log_cfgs_ = deepcopy(log_cfgs)
        if log_cfgs_ is not None:
            if not isinstance(log_cfgs_, list):
                log_cfgs_ = [log_cfgs_]
            assert mmcv.is_list_of(log_cfgs_, dict)
            for log_cfg_ in log_cfgs_:
                log_type = log_cfg_.pop('type')
                log_collect_fn = f'{log_type}_log_collect'
                assert hasattr(self, log_collect_fn)
                log_collect_fn = getattr(self, log_collect_fn)
                log_cfg_.setdefault('prefix_name', 'loss')
                assert log_cfg_['prefix_name'].startswith('loss')
                log_cfg_.setdefault('reduction', reduction)
                self.log_fn_list.append(partial(log_collect_fn, **log_cfg_))
        self.log_vars = dict()
        if not rescale_mode:
            self.rescale_fn = lambda loss, t: loss
        else:
            rescale_fn_name = f'{rescale_mode}_rescale'
            assert hasattr(self, rescale_fn_name)
            if rescale_mode == 'timestep_weight':
                if sampler is not None and hasattr(sampler, 'weight'):
                    weight = sampler.weight
                else:
                    assert weight is not None and isinstance(weight, torch.Tensor), "'weight' or a 'sampler' contains weight attribute is must be 'torch.Tensor' for 'timestep_weight' rescale_mode."
                mmcv.print_log(f"Apply 'timestep_weight' rescale_mode for {self._loss_name}. Please make sure the passed weight can be updated by external functions.", 'mmgen')
                rescale_cfg = dict(weight=weight)
            self.rescale_fn = partial(getattr(self, rescale_fn_name), **rescale_cfg)

    @staticmethod
    def constant_rescale(loss, timesteps, scale):
        """Rescale losses at all timesteps with a constant factor.

        Args:
            loss (torch.Tensor): Losses to rescale.
            timesteps (torch.Tensor): Timesteps of each loss items.
            scale (int): Rescale factor.

        Returns:
            torch.Tensor: Rescaled losses.
        """
        return loss * scale

    @staticmethod
    def timestep_weight_rescale(loss, timesteps, weight, scale=1):
        """Rescale losses corresponding to timestep.

        Args:
            loss (torch.Tensor): Losses to rescale.
            timesteps (torch.Tensor): Timesteps of each loss items.
            weight (torch.Tensor): Weight corresponding to each timestep.
            scale (int): Rescale factor.

        Returns:
            torch.Tensor: Rescaled losses.
        """
        return loss * weight[timesteps] * scale

    @torch.no_grad()
    def collect_log(self, loss, timesteps):
        """Collect logs.

        Args:
            loss (torch.Tensor): Losses to collect.
            timesteps (torch.Tensor): Timesteps of each loss items.
        """
        if not self.log_fn_list:
            return
        if dist.is_initialized():
            ws = dist.get_world_size()
            placeholder_l = [torch.zeros_like(loss) for _ in range(ws)]
            placeholder_t = [torch.zeros_like(timesteps) for _ in range(ws)]
            dist.all_gather(placeholder_l, loss)
            dist.all_gather(placeholder_t, timesteps)
            loss = torch.cat(placeholder_l, dim=0)
            timesteps = torch.cat(placeholder_t, dim=0)
        log_vars = dict()
        if dist.is_initialized() and dist.get_rank() == 0 or not dist.is_initialized():
            for log_fn in self.log_fn_list:
                log_vars.update(log_fn(loss, timesteps))
        self.log_vars = log_vars

    @torch.no_grad()
    def quartile_log_collect(self, loss, timesteps, total_timesteps, prefix_name, reduction='mean'):
        """Collect loss logs by quartile timesteps.

        Args:
            loss (torch.Tensor): Loss value of each input. Each loss tensor
                should be shape as [bz, ]
            timesteps (torch.Tensor): Timesteps corresponding to each loss.
                Each loss tensor should be shape as [bz, ].
            total_timesteps (int): Total timesteps of diffusion process.
            prefix_name (str): Prefix want to show in logs.
            reduction (str, optional): Specifies the reduction to apply to the
                output losses. Defaults to `mean`.

        Returns:
            dict: Collected log variables.
        """
        if digit_version(torch.__version__) <= digit_version('1.6.0'):
            quartile = torch.true_divide(timesteps, total_timesteps) * 4
        else:
            quartile = timesteps / total_timesteps * 4
        quartile = quartile.type(torch.LongTensor)
        log_vars = dict()
        for idx in range(4):
            if not (quartile == idx).any():
                loss_quartile = torch.zeros((1,))
            else:
                loss_quartile = reduce_loss(loss[quartile == idx], reduction)
            log_vars[f'{prefix_name}_quartile_{idx}'] = loss_quartile.item()
        return log_vars

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function, ``mse_loss``.
        """
        if len(args) == 1:
            assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
            output_dict = args[0]
        elif 'output_dict' in kwargs:
            assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
            output_dict = kwargs.pop('outputs_dict')
        else:
            raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
        assert 'timesteps' in output_dict, f"'timesteps' is must for DDPM-based losses, but found{output_dict.keys()} in 'output_dict'"
        timesteps = output_dict['timesteps']
        loss = self._forward_loss(output_dict)
        self.collect_log(loss, timesteps=timesteps)
        loss_rescaled = self.rescale_fn(loss, timesteps)
        return reduce_loss(loss_rescaled, self.reduction)

    @abstractmethod
    def _forward_loss(self, output_dict):
        """Forward function for loss calculation. This method should be
        implemented by each subclasses.

        Args:
            outputs_dict (dict): Outputs of the model used to calculate losses.

        Returns:
            torch.Tensor: Calculated loss.
        """
        raise NotImplementedError("'self._forward_loss' must be implemented.")

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


def approx_gaussian_cdf(x):
    """Approximate the cumulative distribution function of the gaussian distribution.

    Refers to:
    Approximations to the Cumulative Normal Function and its Inverse for Use on a Pocket Calculator  # noqa

    https://www.jstor.org/stable/2346872?origin=crossref

    .. math::
        :nowrap:
        \\begin{eqnarray}
            \\Phi(x) &\\approx \\frac{1}{2} \\left ( 1 + \\tanh(y) \\right ) \\\\
            y &= \\sqrt{\\frac{2}{\\pi}}(x+0.044715 x^3)
        \\end{eqnarray}

    Args:
        x (torch.Tensor): Input data.

    Returns:
        torch.Tensor: Calculated cumulative distribution.

    """
    factor = np.sqrt(2.0 / np.pi)
    y = factor * (x + 0.044715 * torch.pow(x, 3))
    phi = 0.5 * (1 + torch.tanh(y))
    return phi


@weighted_loss
def discretized_gaussian_log_likelihood(x, mean, logvar, base='e'):
    """Calculate gaussian log-likelihood for a discretized input. We assume
    that the input `x` are ranged in [-1, 1], the likelihood term can be
    calculated as the following equation:

    .. math::
     :nowrap:
        \\begin{equarray}
            p_{\\theta}(\\mathbf{x}_0 | \\mathbf{x}_1) =
                \\prod_{i=1}^{D} \\int_{\\delta_{-}(x_0^i)}^{\\delta_{+}(x_0^i)}
                {\\mathcal{N}(x; \\mu_{\\theta}^i(\\mathbf{x}_1, 1),
                \\sigma_{1}^2)}dx\\\\
            \\delta_{+}(x)= \\begin{cases}
                \\infty & \\text{if } x = 1 \\\\
                x + \\frac{1}{255} & \\text{if } x < 1
            \\end{cases}
            \\quad
            \\delta_{-}(x)= \\begin{cases}
                -\\infty & \\text{if } x = -1 \\\\
                x - \\frac{1}{255} & \\text{if } x > -1
            \\end{cases}
        \\end{equarray}

    When calculating this loss term, we first normalize `x` to normal
    distribution and calculate the above integral by the cumulative
    distribution function of normal distribution. Then rescale results to the
    target ones.

    Args:
        x (torch.Tensor): Target `x_0` to be modeled. Range in [-1, 1].
        mean (torch.Tensor): Predicted mean of `x_0`.
        logvar (torch.Tensor): Predicted log variance of `x_0`.
        base (str, optional): The log base of calculated KLD. Support ``'e'``
            and ``'2'``. Defaults to ``'e'``.

    Returns:
        torch.Tensor: Calculated log likelihood.
    """
    if base not in ['e', '2']:
        raise ValueError(f'Only support 2 and e for log base, but receive {base}')
    inv_std = torch.exp(-logvar * 0.5)
    x_centered = x - mean
    lower_bound = (x_centered - 1.0 / 255.0) * inv_std
    upper_bound = (x_centered + 1.0 / 255.0) * inv_std
    cdf_to_lower = approx_gaussian_cdf(lower_bound)
    cdf_to_upper = approx_gaussian_cdf(upper_bound)
    log_cdf_upper = torch.log(cdf_to_upper.clamp(min=1e-12))
    log_one_minus_cdf_lower = torch.log((1.0 - cdf_to_lower).clamp(min=1e-12))
    log_cdf_delta = torch.log((cdf_to_upper - cdf_to_lower).clamp(min=1e-12))
    log_probs = torch.where(x < -0.999, log_cdf_upper, torch.where(x > 0.999, log_one_minus_cdf_lower, log_cdf_delta))
    if base == '2':
        return log_probs / np.log(2.0)
    return log_probs


class DiscretizedGaussianLogLikelihoodLoss(nn.Module):
    """Discretized-Gaussian-Log-Likelihood Loss.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from BaseDiffusion, train_step
        :linenos:

        data_dict_ = dict(
            denoising=denoising,
            real_imgs=torch.Tensor([N, C, H, W]),
            mean_pred=torch.Tensor([N, C, H, W]),
            mean_target=torch.Tensor([N, C, H, W]),
            logvar_pred=torch.Tensor([N, C, H, W]),
            logvar_target=torch.Tensor([N, C, H, W]),
            timesteps=torch.Tensor([N,]),
            iteration=curr_iter,
            batch_size=batch_size)

    In this loss, we may need to provide ``mean``, ``logvar`` and ``x``. Thus,
    an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:
        data_info = dict(
            x='real_imgs',
            mean='mean_pred',
            logvar='logvar_pred')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        reduction (str, optional): Same as built-in losses of PyTorch.
            Defaults to 'mean'.
        avg_factor (float | None, optional): Average factor when computing the
            mean of losses. Defaults to ``None``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If not passed,
            ``_default_data_info`` would be used. Defaults to None.
        base (str, optional): The log base of calculated KLD. Support
            ``'e'`` and ``'2'``. Defaults to ``'e'``.
        only_update_var (bool, optional): If true, only `logvar_pred` will be
            updated and variable in output_dict corresponding to `mean_pred`
            will be detached. Defaults to False.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_l1'.
    """
    _default_data_info = dict(x='real_imgs', mean='mean_pred', logvar='logvar_pred')

    def __init__(self, loss_weight=1.0, reduction='mean', avg_factor=None, data_info=None, base='e', only_update_var=False, loss_name='loss_DiscGaussianLogLikelihood'):
        super().__init__()
        if reduction not in _reduction_modes:
            raise ValueError(f'Unsupported reduction mode: {reduction}. Supported ones are: {_reduction_modes}')
        self.loss_weight = loss_weight
        self.reduction = reduction
        self.avg_factor = avg_factor
        self.data_info = self._default_data_info if data_info is None else data_info
        self.base = base
        self.only_update_var = only_update_var
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``gaussian_kld_loss``.
        """
        if len(args) == 1:
            assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
            outputs_dict = args[0]
        elif 'outputs_dict' in kwargs:
            assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
            outputs_dict = kwargs.pop('outputs_dict')
        else:
            raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
        loss_input_dict = dict()
        for k, v in self.data_info.items():
            if k == 'mean' and self.only_update_var:
                loss_input_dict[k] = outputs_dict[v].detach()
            else:
                loss_input_dict[k] = outputs_dict[v]
        kwargs.update(loss_input_dict)
        kwargs.update(dict(weight=self.loss_weight, reduction=self.reduction, base=self.base))
        return discretized_gaussian_log_likelihood(**kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


@weighted_loss
def gaussian_kld(mean_target, mean_pred, logvar_target, logvar_pred, base='e'):
    """Calculate KLD (Kullback-Leibler divergence) of two gaussian
    distribution.
    To be noted that in this function, KLD is calcuated in base `e`.

    .. math::
        :nowrap:

        \\begin{align}
        KLD(p||q) &= -\\int{p(x)\\log{q(x)} dx} + \\int{p(x)\\log{p(x)} dx} \\\\
            &= \\frac{1}{2}\\log{(2\\pi \\sigma_2^2)} +
            \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} -
            \\frac{1}{2}(1 + \\log{2\\pi \\sigma_1^2}) \\\\
            &= \\log{\\frac{\\sigma_2}{\\sigma_1}} +
            \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}
        \\end{align}

    Args:
        mean_target (torch.Tensor): Mean of the target (or the first)
            distribution.
        mean_pred (torch.Tensor): Mean of the predicted (or the second)
            distribution.
        logvar_target (torch.Tensor): Log variance of the target (or the first)
            distribution
        logvar_pred (torch.Tensor): Log variance of the predicted (or the
            second) distribution.
        base (str, optional): The log base of calculated KLD. We support
            ``'e'`` (for ln) and ``'2'`` (for log_2). Defaults to ``'e'``.

    Returns:
        torch.Tensor: KLD between two given distribution.
    """
    if base not in ['e', '2']:
        raise ValueError(f'Only support 2 and e for log base, but receive {base}')
    kld = 0.5 * (-1.0 + logvar_pred - logvar_target + torch.exp(logvar_target - logvar_pred) + (mean_target - mean_pred) ** 2 * torch.exp(-logvar_pred))
    if base == '2':
        return kld / np.log(2.0)
    return kld


class GaussianKLDLoss(nn.Module):
    """GaussianKLD loss.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from BaseDiffusion, train_step
        :linenos:

        data_dict_ = dict(
            denoising=denoising,
            real_imgs=torch.Tensor([N, C, H, W]),
            mean_pred=torch.Tensor([N, C, H, W]),
            mean_target=torch.Tensor([N, C, H, W]),
            logvar_pred=torch.Tensor([N, C, H, W]),
            logvar_target=torch.Tensor([N, C, H, W]),
            timesteps=torch.Tensor([N,]),
            iteration=curr_iter,
            batch_size=batch_size)

    In this loss, we may need to provide ``mean_pred``, ``mean_target``,
    ``logvar_pred`` and ``logvar_target`` as input. Thus, an example of the
    ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            mean_pred='mean_pred',
            mean_target='mean_target',
            logvar_pred='logvar_pred',
            logvar_target='logvar_target')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        reduction (str, optional): Same as built-in losses of PyTorch. Noted
            that 'batchmean' mode given the correct KL divergence where losses
            are averaged over batch dimension only. Defaults to 'mean'.
        avg_factor (float | None, optional): Average factor when computing the
            mean of losses. Defaults to ``None``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If not passed,
            ``_default_data_info`` would be used. Defaults to None.
        base (str, optional): The log base of calculated KLD. Support
            ``'e'`` and ``'2'``. Defaults to ``'e'``.
        only_update_var (bool, optional): If true, only `logvar_pred` will be
            updated and variable in output_dict corresponding to `mean_pred`
            will be detached. Defaults to False.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_l1'.
    """
    _default_data_info = dict(mean_pred='mean_pred', mean_target='mean_target', logvar_pred='logvar_pred', logvar_target='logvar_target')

    def __init__(self, loss_weight=1.0, reduction='mean', avg_factor=None, data_info=None, base='e', only_update_var=False, loss_name='loss_GaussianKLD'):
        super().__init__()
        if reduction not in _reduction_modes:
            raise ValueError(f'Unsupported reduction mode: {reduction}. Supported ones are: {_reduction_modes}')
        self.loss_weight = loss_weight
        self.reduction = reduction
        self.avg_factor = avg_factor
        self.data_info = self._default_data_info if data_info is None else data_info
        self.base = base
        self.only_update_var = only_update_var
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``gaussian_kld_loss``.
        """
        if len(args) == 1:
            assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
            outputs_dict = args[0]
        elif 'outputs_dict' in kwargs:
            assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
            outputs_dict = kwargs.pop('outputs_dict')
        else:
            raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
        loss_input_dict = dict()
        for k, v in self.data_info.items():
            if 'mean_pred' == k and self.only_update_var:
                loss_input_dict[k] = outputs_dict[v].detach()
            else:
                loss_input_dict[k] = outputs_dict[v]
        kwargs.update(loss_input_dict)
        kwargs.update(dict(weight=self.loss_weight, reduction=self.reduction, base=self.base))
        return gaussian_kld(**kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


class DDPMVLBLoss(DDPMLoss):
    """Variational lower-bound loss for DDPM-based models.
    In this loss, we calculate VLB of different timesteps with different
    method. In detail, ``DiscretizedGaussianLogLikelihoodLoss`` is used at
    timesteps = 0 and ``GaussianKLDLoss`` at other timesteps.
    To control the data flow for loss calculation, users should define
    ``data_info`` and ``data_info_t_0`` for ``GaussianKLDLoss`` and
    ``DiscretizedGaussianLogLikelihoodLoss`` respectively. If not passed
    ``_default_data_info`` and ``_default_data_info_t_0`` would be used.
    To be noted that, we only penalize 'variance' in this loss term, and
    tensors in output dict corresponding to 'mean' would be detached.

    Additionally, we support another log collection function called
    ``name_log_collection``. In this collection method, we would directly
    collect loss terms calculated by different methods.
    To use this collection methods, users may passed ``log_cfgs`` as the
    following example:

    .. code-block:: python
        :linenos:

        log_cfgs = [
            dict(type='name', reduction=REUCTION, prefix_name=PREFIX),
            ...
        ]

    Args:
        rescale_mode (str, optional): Mode of the loss rescale method.
            Defaults to None.
        rescale_cfg (dict, optional): Config of the loss rescale method.
        sampler (object): Weight sampler. Defaults to None.
        weight (torch.Tensor, optional): Weight used for rescale losses.
            Defaults to None.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary for ``timesteps != 0``.
            Defaults to None.
        data_info_t_0 (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary for ``timesteps == 0``.
            Defaults to None.
        log_cfgs (list[dict] | dict | optional): Configs to collect logs.
            Defaults to None.
        reduction (str, optional): Same as built-in losses of PyTorch.
            Defaults to 'mean'.
        loss_name (str, optional): Name of the loss item. Defaults to
            'loss_ddpm_vlb'.
    """
    _default_data_info = dict(mean_pred='mean_pred', mean_target='mean_target', logvar_pred='logvar_pred', logvar_target='logvar_target')
    _default_data_info_t_0 = dict(x='real_imgs', mean='mean_pred', logvar='logvar_pred')

    def __init__(self, rescale_mode=None, rescale_cfg=None, sampler=None, weight=None, data_info=None, data_info_t_0=None, log_cfgs=None, reduction='mean', loss_name='loss_ddpm_vlb'):
        super().__init__(rescale_mode, rescale_cfg, log_cfgs, weight, sampler, reduction, loss_name)
        self.data_info = self._default_data_info if data_info is None else data_info
        self.data_info_t_0 = self._default_data_info_t_0 if data_info_t_0 is None else data_info_t_0
        self.loss_list = [DiscretizedGaussianLogLikelihoodLoss(reduction='flatmean', data_info=self.data_info_t_0, base='2', loss_weight=-1, only_update_var=True), GaussianKLDLoss(reduction='flatmean', data_info=self.data_info, base='2', only_update_var=True)]
        self.loss_select_fn_list = [lambda t: t == 0, lambda t: t != 0]

    @torch.no_grad()
    def name_log_collect(self, loss, timesteps, prefix_name, reduction='mean'):
        """Collect loss logs by name (GaissianKLD and
        DiscGaussianLogLikelihood).

        Args:
            loss (torch.Tensor): Loss value of each input. Each loss tensor
                should be in the shape of [bz, ].
            timesteps (torch.Tensor): Timesteps corresponding to each losses.
                Each loss tensor should be in the shape of [bz, ].
            prefix_name (str): Prefix want to show in logs.
            reduction (str, optional): Specifies the reduction to apply to the
                output losses. Defaults to `mean`.

        Returns:
            dict: Collected log variables.
        """
        log_vars = dict()
        for select_fn, loss_fn in zip(self.loss_select_fn_list, self.loss_list):
            mask = select_fn(timesteps)
            if not mask.any():
                loss_reduced = torch.zeros((1,))
            else:
                loss_reduced = reduce_loss(loss[mask], reduction)
            loss_term_name = loss_fn.loss_name().replace('loss_', '')
            log_vars[f'{prefix_name}_{loss_term_name}'] = loss_reduced.item()
        return log_vars

    def _forward_loss(self, outputs_dict):
        """Forward function for loss calculation.
        Args:
            outputs_dict (dict): Outputs of the model used to calculate losses.

        Returns:
            torch.Tensor: Calculated loss.
        """
        timesteps = outputs_dict['timesteps']
        loss = torch.zeros_like(timesteps).float()
        for select_fn, loss_fn in zip(self.loss_select_fn_list, self.loss_list):
            mask = select_fn(timesteps)
            outputs_dict_ = {}
            for k, v in outputs_dict.items():
                if v is None or not isinstance(v, (torch.Tensor, list)):
                    outputs_dict_[k] = v
                elif isinstance(v, list):
                    outputs_dict_[k] = [v[idx] for idx, m in enumerate(mask) if m]
                else:
                    outputs_dict_[k] = v[mask]
            loss[mask] = loss_fn(outputs_dict_)
        return loss


class DDPMMSELoss(DDPMLoss):
    """Mean square loss for DDPM-based models.

    Args:
        rescale_mode (str, optional): Mode of the loss rescale method.
            Defaults to None.
        rescale_cfg (dict, optional): Config of the loss rescale method.
        sampler (object): Weight sampler. Defaults to None.
        weight (torch.Tensor, optional): Weight used for rescale losses.
            Defaults to None.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary for ``timesteps != 0``.
            Defaults to None.
        log_cfgs (list[dict] | dict | optional): Configs to collect logs.
            Defaults to None.
        reduction (str, optional): Same as built-in losses of PyTorch.
            Defaults to 'mean'.
        loss_name (str, optional): Name of the loss item. Defaults to
            'loss_ddpm_vlb'.
    """
    _default_data_info = dict(pred='eps_t_pred', target='noise')

    def __init__(self, rescale_mode=None, rescale_cfg=None, sampler=None, weight=None, log_cfgs=None, reduction='mean', data_info=None, loss_name='loss_ddpm_mse'):
        super().__init__(rescale_mode, rescale_cfg, log_cfgs, weight, sampler, reduction, loss_name)
        self.data_info = self._default_data_info if data_info is None else data_info
        self.loss_fn = partial(mse_loss, reduction='flatmean')

    def _forward_loss(self, outputs_dict):
        """Forward function for loss calculation.
        Args:
            outputs_dict (dict): Outputs of the model used to calculate losses.

        Returns:
            torch.Tensor: Calculated loss.
        """
        loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
        loss = self.loss_fn(**loss_input_dict)
        return loss


@weighted_loss
def disc_shift_loss(pred):
    """Disc Shift loss.

    This loss is proposed in PGGAN as an auxiliary loss for discriminator.

    Args:
        pred (Tensor): Input tensor.

    Returns:
        torch.Tensor: loss tensor.
    """
    return pred ** 2


class DiscShiftLoss(nn.Module):
    """Disc Shift Loss.

    This loss is proposed in PGGAN as an auxiliary loss for discriminator.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            disc_pred_fake=disc_pred_fake,
            disc_pred_real=disc_pred_real,
            fake_imgs=fake_imgs,
            real_imgs=real_imgs,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we will need to provide ``pred`` as input. Thus, an
    example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            pred='disc_pred_fake')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    In addition, in general, ``disc_shift_loss`` will be applied over real and
    fake data. In this case, users just need to add this loss module twice, but
    with different ``data_info``. Our model will automatically add these two
    items.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_disc_shift'.
    """

    def __init__(self, loss_weight=1.0, data_info=None, loss_name='loss_disc_shift'):
        super().__init__()
        self.loss_weight = loss_weight
        self.data_info = data_info
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function, ``disc_shift_loss``.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight))
            return disc_shift_loss(**kwargs)
        else:
            return disc_shift_loss(*args, weight=self.loss_weight, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


@weighted_loss
def gradient_penalty_loss(discriminator, real_data, fake_data, mask=None, norm_mode='pixel'):
    """Calculate gradient penalty for wgan-gp.

    In the detailed implementation, there are two streams where one uses the
    pixel-wise gradient norm, but the other adopts normalization along instance
    (HWC) dimensions. Thus, ``norm_mode`` are offered to define which mode you
    want.

    Args:
        discriminator (nn.Module): Network for the discriminator.
        real_data (Tensor): Real input data.
        fake_data (Tensor): Fake input data.
        mask (Tensor): Masks for inpainting. Default: None.
        norm_mode (str): This argument decides along which dimension the norm
            of the gradients will be calculated. Currently, we support ["pixel"
            , "HWC"]. Defaults to "pixel".

    Returns:
        Tensor: A tensor for gradient penalty.
    """
    batch_size = real_data.size(0)
    alpha = torch.rand(batch_size, 1, 1, 1)
    interpolates = alpha * real_data + (1.0 - alpha) * fake_data
    interpolates = autograd.Variable(interpolates, requires_grad=True)
    disc_interpolates = discriminator(interpolates)
    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones_like(disc_interpolates), create_graph=True, retain_graph=True, only_inputs=True)[0]
    if mask is not None:
        gradients = gradients * mask
    if norm_mode == 'pixel':
        gradients_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    elif norm_mode == 'HWC':
        gradients_penalty = ((gradients.reshape(batch_size, -1).norm(2, dim=1) - 1) ** 2).mean()
    else:
        raise NotImplementedError(f'Currently, we only support ["pixel", "HWC"] norm mode but got {norm_mode}.')
    if mask is not None:
        gradients_penalty /= torch.mean(mask)
    return gradients_penalty


class GradientPenaltyLoss(nn.Module):
    """Gradient Penalty for WGAN-GP.

    In the detailed implementation, there are two streams where one uses the
    pixel-wise gradient norm, but the other adopts normalization along instance
    (HWC) dimensions. Thus, ``norm_mode`` are offered to define which mode you
    want.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            disc_pred_fake=disc_pred_fake,
            disc_pred_real=disc_pred_real,
            fake_imgs=fake_imgs,
            real_imgs=real_imgs,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we will need to provide ``discriminator``, ``real_data``,
    and ``fake_data`` as input. Thus, an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            discriminator='disc',
            real_data='real_imgs',
            fake_data='fake_imgs')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        norm_mode (str): This argument decides along which dimension the norm
            of the gradients will be calculated. Currently, we support ["pixel"
            , "HWC"]. Defaults to "pixel".
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_gp'.
    """

    def __init__(self, loss_weight=1.0, norm_mode='pixel', data_info=None, loss_name='loss_gp'):
        super().__init__()
        self.loss_weight = loss_weight
        self.norm_mode = norm_mode
        self.data_info = data_info
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``gradient_penalty_loss``.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight, norm_mode=self.norm_mode))
            return gradient_penalty_loss(**kwargs)
        else:
            return gradient_penalty_loss(*args, weight=self.loss_weight, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


@weighted_loss
def r1_gradient_penalty_loss(discriminator, real_data, mask=None, norm_mode='pixel', loss_scaler=None, use_apex_amp=False):
    """Calculate R1 gradient penalty for WGAN-GP.

    R1 regularizer comes from:
    "Which Training Methods for GANs do actually Converge?" ICML'2018

    Different from original gradient penalty, this regularizer only penalized
    gradient w.r.t. real data.

    Args:
        discriminator (nn.Module): Network for the discriminator.
        real_data (Tensor): Real input data.
        mask (Tensor): Masks for inpainting. Default: None.
        norm_mode (str): This argument decides along which dimension the norm
            of the gradients will be calculated. Currently, we support ["pixel"
            , "HWC"]. Defaults to "pixel".

    Returns:
        Tensor: A tensor for gradient penalty.
    """
    batch_size = real_data.shape[0]
    real_data = real_data.clone().requires_grad_()
    disc_pred = discriminator(real_data)
    if loss_scaler:
        disc_pred = loss_scaler.scale(disc_pred)
    elif use_apex_amp:
        _loss_scaler = _amp_state.loss_scalers[0]
        disc_pred = _loss_scaler.loss_scale() * disc_pred.float()
    gradients = autograd.grad(outputs=disc_pred, inputs=real_data, grad_outputs=torch.ones_like(disc_pred), create_graph=True, retain_graph=True, only_inputs=True)[0]
    if loss_scaler:
        inv_scale = 1.0 / loss_scaler.get_scale()
        gradients = gradients * inv_scale
    elif use_apex_amp:
        inv_scale = 1.0 / _loss_scaler.loss_scale()
        gradients = gradients * inv_scale
    if mask is not None:
        gradients = gradients * mask
    if norm_mode == 'pixel':
        gradients_penalty = (gradients.norm(2, dim=1) ** 2).mean()
    elif norm_mode == 'HWC':
        gradients_penalty = gradients.pow(2).reshape(batch_size, -1).sum(1).mean()
    else:
        raise NotImplementedError(f'Currently, we only support ["pixel", "HWC"] norm mode but got {norm_mode}.')
    if mask is not None:
        gradients_penalty /= torch.mean(mask)
    return gradients_penalty


class R1GradientPenalty(nn.Module):
    """R1 gradient penalty for WGAN-GP.

    R1 regularizer comes from:
    "Which Training Methods for GANs do actually Converge?" ICML'2018

    Different from original gradient penalty, this regularizer only penalized
    gradient w.r.t. real data.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            disc_pred_fake=disc_pred_fake,
            disc_pred_real=disc_pred_real,
            fake_imgs=fake_imgs,
            real_imgs=real_imgs,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we will need to provide ``discriminator`` and
    ``real_data`` as input. Thus, an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            discriminator='disc',
            real_data='real_imgs')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        norm_mode (str): This argument decides along which dimension the norm
            of the gradients will be calculated. Currently, we support ["pixel"
            , "HWC"]. Defaults to "pixel".
        interval (int, optional): The interval of calculating this loss.
            Defaults to 1.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_r1_gp'.
    """

    def __init__(self, loss_weight=1.0, norm_mode='pixel', interval=1, data_info=None, use_apex_amp=False, loss_name='loss_r1_gp'):
        super().__init__()
        self.loss_weight = loss_weight
        self.norm_mode = norm_mode
        self.interval = interval
        self.data_info = data_info
        self.use_apex_amp = use_apex_amp
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``r1_gradient_penalty_loss``.
        """
        if self.interval > 1:
            assert self.data_info is not None
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            if self.interval > 1 and outputs_dict['iteration'] % self.interval != 0:
                return None
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight, norm_mode=self.norm_mode, use_apex_amp=self.use_apex_amp))
            return r1_gradient_penalty_loss(**kwargs)
        else:
            return r1_gradient_penalty_loss(*args, weight=self.loss_weight, norm_mode=self.norm_mode, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


class GANLoss(nn.Module):
    """Define GAN loss.

    Args:
        gan_type (str): Support 'vanilla', 'lsgan', 'wgan', 'hinge',
            'wgan-logistic-ns'.
        real_label_val (float): The value for real label. Default: 1.0.
        fake_label_val (float): The value for fake label. Default: 0.0.
        loss_weight (float): Loss weight. Default: 1.0.
            Note that loss_weight is only for generators; and it is always 1.0
            for discriminators.
    """

    def __init__(self, gan_type, real_label_val=1.0, fake_label_val=0.0, loss_weight=1.0):
        super().__init__()
        self.gan_type = gan_type
        self.loss_weight = loss_weight
        self.real_label_val = real_label_val
        self.fake_label_val = fake_label_val
        if self.gan_type == 'vanilla':
            self.loss = nn.BCEWithLogitsLoss()
        elif self.gan_type == 'lsgan':
            self.loss = nn.MSELoss()
        elif self.gan_type == 'wgan':
            self.loss = self._wgan_loss
        elif self.gan_type == 'wgan-logistic-ns':
            self.loss = self._wgan_logistic_ns_loss
        elif self.gan_type == 'hinge':
            self.loss = nn.ReLU()
        else:
            raise NotImplementedError(f'GAN type {self.gan_type} is not implemented.')

    def _wgan_loss(self, input, target):
        """wgan loss.

        Args:
            input (Tensor): Input tensor.
            target (bool): Target label.

        Returns:
            Tensor: wgan loss.
        """
        return -input.mean() if target else input.mean()

    def _wgan_logistic_ns_loss(self, input, target):
        """WGAN loss in logistically non-saturating mode.

        This loss is widely used in StyleGANv2.

        Args:
            input (Tensor): Input tensor.
            target (bool): Target label.

        Returns:
            Tensor: wgan loss.
        """
        return F.softplus(-input).mean() if target else F.softplus(input).mean()

    def get_target_label(self, input, target_is_real):
        """Get target label.

        Args:
            input (Tensor): Input tensor.
            target_is_real (bool): Whether the target is real or fake.

        Returns:
            (bool | Tensor): Target tensor. Return bool for wgan, otherwise,                 return Tensor.
        """
        if self.gan_type in ['wgan', 'wgan-logistic-ns']:
            return target_is_real
        target_val = self.real_label_val if target_is_real else self.fake_label_val
        return input.new_ones(input.size()) * target_val

    def forward(self, input, target_is_real, is_disc=False):
        """
        Args:
            input (Tensor): The input for the loss module, i.e., the network
                prediction.
            target_is_real (bool): Whether the targe is real or fake.
            is_disc (bool): Whether the loss for discriminators or not.
                Default: False.

        Returns:
            Tensor: GAN loss value.
        """
        target_label = self.get_target_label(input, target_is_real)
        if self.gan_type == 'hinge':
            if is_disc:
                input = -input if target_is_real else input
                loss = self.loss(1 + input).mean()
            else:
                loss = -input.mean()
        else:
            loss = self.loss(input, target_label)
        return loss if is_disc else loss * self.loss_weight


def gen_path_regularizer(generator, num_batches, mean_path_length, pl_batch_shrink=1, decay=0.01, weight=1.0, pl_batch_size=None, sync_mean_buffer=False, loss_scaler=None, use_apex_amp=False):
    """Generator Path Regularization.

    Path regularization is proposed in StyelGAN2, which can help the improve
    the continuity of the latent space. More details can be found in:
    Analyzing and Improving the Image Quality of StyleGAN, CVPR2020.

    Args:
        generator (nn.Module): The generator module. Note that this loss
            requires that the generator contains ``return_latents`` interface,
            with which we can get the latent code of the current sample.
        num_batches (int): The number of samples used in calculating this loss.
        mean_path_length (Tensor): The mean path length, calculated by moving
            average.
        pl_batch_shrink (int, optional): The factor of shrinking the batch size
            for saving GPU memory. Defaults to 1.
        decay (float, optional): Decay for moving average of mean path length.
            Defaults to 0.01.
        weight (float, optional): Weight of this loss item. Defaults to ``1.``.
        pl_batch_size (int | None, optional): The batch size in calculating
            generator path. Once this argument is set, the ``num_batches`` will
            be overridden with this argument and won't be affectted by
            ``pl_batch_shrink``. Defaults to None.
        sync_mean_buffer (bool, optional): Whether to sync mean path length
            across all of GPUs. Defaults to False.

    Returns:
        tuple[Tensor]: The penalty loss, detached mean path tensor, and             current path length.
    """
    if pl_batch_shrink > 1:
        num_batches = max(1, num_batches // pl_batch_shrink)
    if pl_batch_size is not None:
        num_batches = pl_batch_size
    output_dict = generator(None, num_batches=num_batches, return_latents=True)
    fake_img, latents = output_dict['fake_img'], output_dict['latent']
    noise = torch.randn_like(fake_img) / np.sqrt(fake_img.shape[2] * fake_img.shape[3])
    if loss_scaler:
        loss = loss_scaler.scale((fake_img * noise).sum())[0]
        grad = autograd.grad(outputs=loss, inputs=latents, grad_outputs=torch.ones(()), create_graph=True, retain_graph=True, only_inputs=True)[0]
        inv_scale = 1.0 / loss_scaler.get_scale()
        grad = grad * inv_scale
    elif use_apex_amp:
        _loss_scaler = _amp_state.loss_scalers[1]
        loss = _loss_scaler.loss_scale() * (fake_img * noise).sum().float()
        grad = autograd.grad(outputs=loss, inputs=latents, grad_outputs=torch.ones(()), create_graph=True, retain_graph=True, only_inputs=True)[0]
        inv_scale = 1.0 / _loss_scaler.loss_scale()
        grad = grad * inv_scale
    else:
        grad = autograd.grad(outputs=(fake_img * noise).sum(), inputs=latents, grad_outputs=torch.ones(()), create_graph=True, retain_graph=True, only_inputs=True)[0]
    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))
    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)
    if sync_mean_buffer and dist.is_initialized():
        dist.all_reduce(path_mean)
        path_mean = path_mean / float(dist.get_world_size())
    path_penalty = (path_lengths - path_mean).pow(2).mean() * weight
    return path_penalty, path_mean.detach(), path_lengths


class GeneratorPathRegularizer(nn.Module):
    """Generator Path Regularizer.

    Path regularization is proposed in StyelGAN2, which can help the improve
    the continuity of the latent space. More details can be found in:
    Analyzing and Improving the Image Quality of StyleGAN, CVPR2020.

    Users can achieve lazy regularization by setting ``interval`` arguments
    here.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            fake_imgs=fake_imgs,
            disc_pred_fake_g=disc_pred_fake_g,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we will need to provide ``generator`` and ``num_batches``
    as input. Thus an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            generator='gen',
            num_batches='batch_size')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        pl_batch_shrink (int, optional): The factor of shrinking the batch size
            for saving GPU memory. Defaults to 1.
        decay (float, optional): Decay for moving average of mean path length.
            Defaults to 0.01.
        pl_batch_size (int | None, optional): The batch size in calculating
            generator path. Once this argument is set, the ``num_batches`` will
            be overridden with this argument and won't be affectted by
            ``pl_batch_shrink``. Defaults to None.
        sync_mean_buffer (bool, optional): Whether to sync mean path length
            across all of GPUs. Defaults to False.
        interval (int, optional): The interval of calculating this loss. This
            argument is used to support lazy regularization. Defaults to 1.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_path_regular'.
    """

    def __init__(self, loss_weight=1.0, pl_batch_shrink=1, decay=0.01, pl_batch_size=None, sync_mean_buffer=False, interval=1, data_info=None, use_apex_amp=False, loss_name='loss_path_regular'):
        super().__init__()
        self.loss_weight = loss_weight
        self.pl_batch_shrink = pl_batch_shrink
        self.decay = decay
        self.pl_batch_size = pl_batch_size
        self.sync_mean_buffer = sync_mean_buffer
        self.interval = interval
        self.data_info = data_info
        self.use_apex_amp = use_apex_amp
        self._loss_name = loss_name
        self.register_buffer('mean_path_length', torch.tensor(0.0))

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``gen_path_regularizer``.
        """
        if self.interval > 1:
            assert self.data_info is not None
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            if self.interval > 1 and outputs_dict['iteration'] % self.interval != 0:
                return None
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight, mean_path_length=self.mean_path_length, pl_batch_shrink=self.pl_batch_shrink, decay=self.decay, use_apex_amp=self.use_apex_amp, pl_batch_size=self.pl_batch_size, sync_mean_buffer=self.sync_mean_buffer))
            path_penalty, self.mean_path_length, _ = gen_path_regularizer(**kwargs)
            return path_penalty
        else:
            return gen_path_regularizer(*args, weight=self.loss_weight, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


def third_party_net_loss(net, weight=1.0, **kwargs):
    return net(**kwargs) * weight


class FaceIdLoss(nn.Module):
    """Face similarity loss. Generally this loss is used to keep the id
    consistency of the input face image and output face image.

    In this loss, we may need to provide ``gt``, ``pred`` and ``x``. Thus,
    an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:
        data_info = dict(
            gt='real_imgs',
            pred='fake_imgs')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        facenet (dict, optional): Config dict for facenet. Defaults to
            dict(type='ArcFace', ir_se50_weights=None, device='cuda').
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_id'.
    """

    def __init__(self, loss_weight=1.0, data_info=None, facenet=dict(type='ArcFace', ir_se50_weights=None, device='cuda'), loss_name='loss_id'):
        super(FaceIdLoss, self).__init__()
        self.loss_weight = loss_weight
        self.data_info = data_info
        self.net = build_module(facenet)
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``third_party_net_loss``.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight))
            return third_party_net_loss(self.net, *args, **kwargs)
        else:
            return third_party_net_loss(self.net, *args, weight=self.loss_weight, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


class CLIPLoss(nn.Module):
    """Clip loss. In styleclip, this loss is used to optimize the latent code
    to generate image that match the text.

    In this loss, we may need to provide ``image``, ``text``. Thus,
    an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:
        data_info = dict(
            image='fake_imgs',
            text='descriptions')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        clip_model (dict, optional): Kwargs for clip loss model. Defaults to
            dict().
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_clip'.
    """

    def __init__(self, loss_weight=1.0, data_info=None, clip_model=dict(), loss_name='loss_clip'):
        super(CLIPLoss, self).__init__()
        self.loss_weight = loss_weight
        self.data_info = data_info
        self.net = CLIPLossModel(**clip_model)
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function,
        ``third_party_net_loss``.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight))
            return third_party_net_loss(self.net, *args, **kwargs)
        else:
            return third_party_net_loss(self.net, *args, weight=self.loss_weight, **kwargs)

    @staticmethod
    def loss_name():
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return 'clip_loss'


class MSELoss(nn.Module):
    """MSE loss.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            disc_pred_fake=disc_pred_fake,
            disc_pred_real=disc_pred_real,
            fake_imgs=fake_imgs,
            real_imgs=real_imgs,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we may need to provide ``pred`` and ``target`` as input.
    Thus, an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            pred='fake_imgs',
            target='real_imgs')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_mse'.
    """

    def __init__(self, loss_weight=1.0, data_info=None, loss_name='loss_mse'):
        super().__init__()
        self.loss_weight = loss_weight
        self.data_info = data_info
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function, ``mse_loss``.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight))
            return mse_loss(**kwargs)
        else:
            return mse_loss(*args, weight=self.loss_weight, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


class L1Loss(nn.Module):
    """L1 loss.

    **Note for the design of ``data_info``:**
    In ``MMGeneration``, almost all of loss modules contain the argument
    ``data_info``, which can be used for constructing the link between the
    input items (needed in loss calculation) and the data from the generative
    model. For example, in the training of GAN model, we will collect all of
    important data/modules into a dictionary:

    .. code-block:: python
        :caption: Code from StaticUnconditionalGAN, train_step
        :linenos:

        data_dict_ = dict(
            gen=self.generator,
            disc=self.discriminator,
            disc_pred_fake=disc_pred_fake,
            disc_pred_real=disc_pred_real,
            fake_imgs=fake_imgs,
            real_imgs=real_imgs,
            iteration=curr_iter,
            batch_size=batch_size)

    But in this loss, we may need to provide ``pred`` and ``target`` as input.
    Thus, an example of the ``data_info`` is:

    .. code-block:: python
        :linenos:

        data_info = dict(
            pred='fake_imgs',
            target='real_imgs')

    Then, the module will automatically construct this mapping from the input
    data dictionary.

    Args:
        loss_weight (float, optional): Weight of this loss item.
            Defaults to ``1.``.
        reduction (str, optional): Same as built-in losses of PyTorch.
            Defaults to 'mean'.
        avg_factor (float | None, optional): Average factor when computing the
            mean of losses. Defaults to ``None``.
        data_info (dict, optional): Dictionary contains the mapping between
            loss input args and data dictionary. If ``None``, this module will
            directly pass the input data to the loss function.
            Defaults to None.
        loss_name (str, optional): Name of the loss item. If you want this loss
            item to be included into the backward graph, `loss_` must be the
            prefix of the name. Defaults to 'loss_l1'.
    """

    def __init__(self, loss_weight=1.0, reduction='mean', avg_factor=None, data_info=None, loss_name='loss_l1'):
        super().__init__()
        if reduction not in _reduction_modes:
            raise ValueError(f'Unsupported reduction mode: {reduction}. Supported ones are: {_reduction_modes}')
        self.loss_weight = loss_weight
        self.reduction = reduction
        self.avg_factor = avg_factor
        self.data_info = data_info
        self._loss_name = loss_name

    def forward(self, *args, **kwargs):
        """Forward function.

        If ``self.data_info`` is not ``None``, a dictionary containing all of
        the data and necessary modules should be passed into this function.
        If this dictionary is given as a non-keyword argument, it should be
        offered as the first argument. If you are using keyword argument,
        please name it as `outputs_dict`.

        If ``self.data_info`` is ``None``, the input argument or key-word
        argument will be directly passed to loss function, ``l1_loss``.
        """
        if self.data_info is not None:
            if len(args) == 1:
                assert isinstance(args[0], dict), 'You should offer a dictionary containing network outputs for building up computational graph of this loss module.'
                outputs_dict = args[0]
            elif 'outputs_dict' in kwargs:
                assert len(args) == 0, 'If the outputs dict is given in keyworded arguments, no further non-keyworded arguments should be offered.'
                outputs_dict = kwargs.pop('outputs_dict')
            else:
                raise NotImplementedError('Cannot parsing your arguments passed to this loss module. Please check the usage of this module')
            loss_input_dict = {k: outputs_dict[v] for k, v in self.data_info.items()}
            kwargs.update(loss_input_dict)
            kwargs.update(dict(weight=self.loss_weight, reduction=self.reduction))
            return l1_loss(**kwargs)
        else:
            return l1_loss(*args, weight=self.loss_weight, reduction=self.reduction, avg_factor=self.avg_factor, **kwargs)

    def loss_name(self):
        """Loss Name.

        This function must be implemented and will return the name of this
        loss function. This name will be used to combine different loss items
        by simple sum operation. In addition, if you want this loss item to be
        included into the backward graph, `loss_` must be the prefix of the
        name.

        Returns:
            str: The name of this loss item.
        """
        return self._loss_name


class BaseTranslationModel(nn.Module, metaclass=ABCMeta):
    """Base Translation Model.

    Translation models can transfer images from one domain to
    another. Domain information like `default_domain`,
    `reachable_domains` are needed to initialize the class.
    And we also provide query functions like `is_domain_reachable`,
    `get_other_domains`.

    You can get a specific generator based on the domain,
    and by specifying `target_domain` in the forward function,
    you can decide the domain of generated images.
    Considering the difference among different image translation models,
    we only provide the external interfaces mentioned above.
    When you implement image translation with a specific method,
    you can inherit both `BaseTranslationModel`
    and the method (e.g BaseGAN) and implement abstract methods.

    Args:
        default_domain (str): Default output domain.
        reachable_domains (list[str]): Domains that can be generated by
            the model.
        related_domains (list[str]): Domains involved in training and
            testing. `reachable_domains` must be contained in
            `related_domains`. However, related_domains may contain
            source domains that are used to retrieve source images from
            data_batch but not in reachable_domains.
        train_cfg (dict): Config for training. Default: None.
        test_cfg (dict): Config for testing. Default: None.
    """

    def __init__(self, default_domain, reachable_domains, related_domains, train_cfg=None, test_cfg=None):
        self._default_domain = default_domain
        self._reachable_domains = reachable_domains
        self._related_domains = related_domains
        assert self._default_domain in self._reachable_domains
        assert set(self._reachable_domains) <= set(self._related_domains)
        self.train_cfg = deepcopy(train_cfg) if train_cfg else None
        self.test_cfg = deepcopy(test_cfg) if test_cfg else None
        self._parse_train_cfg()
        if test_cfg is not None:
            self._parse_test_cfg()

    @abstractmethod
    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""

    @abstractmethod
    def _parse_test_cfg(self):
        """Parsing test config and set some attributes for testing."""

    def forward(self, img, test_mode=False, **kwargs):
        """Forward function.

        Args:
            img (tensor): Input image tensor.
            test_mode (bool): Whether in test mode or not. Default: False.
            kwargs (dict): Other arguments.
        """
        if not test_mode:
            return self.forward_train(img, **kwargs)
        return self.forward_test(img, **kwargs)

    def forward_train(self, img, target_domain, **kwargs):
        """Forward function for training.

        Args:
            img (tensor): Input image tensor.
            target_domain (str): Target domain of output image.
            kwargs (dict): Other arguments.

        Returns:
            dict: Forward results.
        """
        target = self.translation(img, target_domain=target_domain, **kwargs)
        results = dict(source=img, target=target)
        return results

    def forward_test(self, img, target_domain, **kwargs):
        """Forward function for testing.

        Args:
            img (tensor): Input image tensor.
            target_domain (str): Target domain of output image.
            kwargs (dict): Other arguments.

        Returns:
            dict: Forward results.
        """
        target = self.translation(img, target_domain=target_domain, **kwargs)
        results = dict(source=img.cpu(), target=target.cpu())
        return results

    def is_domain_reachable(self, domain):
        """Whether image of this domain can be generated."""
        return domain in self._reachable_domains

    def get_other_domains(self, domain):
        """get other domains."""
        return list(set(self._related_domains) - set([domain]))

    @abstractmethod
    def _get_target_generator(self, domain):
        """get target generator."""

    def translation(self, image, target_domain=None, **kwargs):
        """Translation Image to target style.

        Args:
            image (tensor): Image tensor with a shape of (N, C, H, W).
            target_domain (str, optional): Target domain of output image.
                Default to None.

        Returns:
            dict: Image tensor of target style.
        """
        if target_domain is None:
            target_domain = self._default_domain
        _model = self._get_target_generator(target_domain)
        outputs = _model(image, **kwargs)
        return outputs


class StaticTranslationGAN(BaseTranslationModel, BaseGAN):
    """Basic translation model based on static unconditional GAN.

    Args:
        generator (dict): Config for the generator.
        discriminator (dict): Config for the discriminator.
        gan_loss (dict): Config for the gan loss.
        pretrained (str | optional): Path for pretrained model.
            Defaults to None.
        disc_auxiliary_loss (dict | optional): Config for auxiliary loss to
            discriminator. Defaults to None.
        gen_auxiliary_loss (dict | optional): Config for auxiliary loss
            to generator. Defaults to None.
    """

    def __init__(self, generator, discriminator, gan_loss, *args, pretrained=None, disc_auxiliary_loss=None, gen_auxiliary_loss=None, **kwargs):
        BaseGAN.__init__(self)
        BaseTranslationModel.__init__(self, *args, **kwargs)
        self._gen_cfg = deepcopy(generator)
        self.generators = nn.ModuleDict()
        for domain in self._reachable_domains:
            self.generators[domain] = build_module(generator)
        self._disc_cfg = deepcopy(discriminator)
        if discriminator is not None:
            self.discriminators = nn.ModuleDict()
            for domain in self._reachable_domains:
                self.discriminators[domain] = build_module(discriminator)
        else:
            self.discriminators = None
        if gan_loss is not None:
            self.gan_loss = build_module(gan_loss)
        else:
            self.gan_loss = None
        if disc_auxiliary_loss:
            self.disc_auxiliary_losses = build_module(disc_auxiliary_loss)
            if not isinstance(self.disc_auxiliary_losses, nn.ModuleList):
                self.disc_auxiliary_losses = nn.ModuleList([self.disc_auxiliary_losses])
        else:
            self.disc_auxiliary_loss = None
        if gen_auxiliary_loss:
            self.gen_auxiliary_losses = build_module(gen_auxiliary_loss)
            if not isinstance(self.gen_auxiliary_losses, nn.ModuleList):
                self.gen_auxiliary_losses = nn.ModuleList([self.gen_auxiliary_losses])
        else:
            self.gen_auxiliary_losses = None
        self.init_weights(pretrained)

    def init_weights(self, pretrained=None):
        """Initialize weights for the model.

        Args:
            pretrained (str, optional): Path for pretrained weights. If given
                None, pretrained weights will not be loaded. Default: None.
        """
        for domain in self._reachable_domains:
            self.generators[domain].init_weights(pretrained=pretrained)
            self.discriminators[domain].init_weights(pretrained=pretrained)

    def _parse_train_cfg(self):
        """Parsing train config and set some attributes for training."""
        if self.train_cfg is None:
            self.train_cfg = dict()
        self.disc_steps = self.train_cfg.get('disc_steps', 1)
        self.disc_init_steps = 0 if self.train_cfg is None else self.train_cfg.get('disc_init_steps', 0)
        self.real_img_key = self.train_cfg.get('real_img_key', 'real_img')

    def _parse_test_cfg(self):
        """Parsing test config and set some attributes for testing."""
        if self.test_cfg is None:
            self.test_cfg = dict()
        self.batch_size = self.test_cfg.get('batch_size', 1)

    def get_module(self, module):
        """Get `nn.ModuleDict` to fit the `MMDistributedDataParallel`
        interface.

        Args:
            module (MMDistributedDataParallel | nn.ModuleDict): The input
                module that needs processing.

        Returns:
            nn.ModuleDict: The ModuleDict of multiple networks.
        """
        if isinstance(module, MMDistributedDataParallel):
            return module.module
        return module

    def _get_target_generator(self, domain):
        """get target generator."""
        assert self.is_domain_reachable(domain), f'{domain} domain is not reachable, available domain list is            {self._reachable_domains}'
        return self.get_module(self.generators)[domain]

    def _get_target_discriminator(self, domain):
        """get target discriminator."""
        assert self.is_domain_reachable(domain), f'{domain} domain is not reachable, available domain list is            {self._reachable_domains}'
        return self.get_module(self.discriminators)[domain]


class SimpleModule(nn.Module):

    def __init__(self):
        super().__init__()
        self.a = nn.Parameter(torch.tensor([1.0, 2.0]))
        if torch.__version__ >= '1.7.0':
            self.register_buffer('b', torch.tensor([2.0, 3.0]), persistent=True)
            self.register_buffer('c', torch.tensor([0.0, 1.0]), persistent=False)
        else:
            self.register_buffer('b', torch.tensor([2.0, 3.0]))
            self.c = torch.tensor([0.0, 1.0])


class SimpleModel(nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.module_a = SimpleModule()
        self.module_b = SimpleModule()
        self.module_a_ema = SimpleModule()
        self.module_b_ema = SimpleModule()


class SimpleModelNoEMA(nn.Module):

    def __init__(self) ->None:
        super().__init__()
        self.module_a = SimpleModule()
        self.module_b = SimpleModule()


class ExampleModel(nn.Module):

    def __init__(self):
        super(ExampleModel, self).__init__()
        self.test_cfg = None

    def train_step(self, data_batch, optimizer):
        output = dict(results=dict(img=data_batch['imgs']))
        return output


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AdaptiveInstanceNorm,
     lambda: ([], {'in_channel': 4, 'style_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4])], {}),
     False),
    (BigGANConditionBN,
     lambda: ([], {'num_features': 4, 'linear_input_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4])], {}),
     False),
    (CatersianGrid,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ConstantInput,
     lambda: ([], {'channel': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DenoisingDownsample,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DenoisingUpsample,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Dist2LogitLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 4, 4]), torch.rand([4, 1, 4, 4])], {}),
     False),
    (EmbedSequential,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (EqualLinearActModule,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (EqualizedLRLinearModule,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FIDInceptionA,
     lambda: ([], {'in_channels': 4, 'pool_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FIDInceptionC,
     lambda: ([], {'in_channels': 4, 'channels_7x7': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FIDInceptionE_1,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FIDInceptionE_2,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FullyConnectedLayer,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (InceptionV3,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 4, 4])], {}),
     False),
    (L1Loss,
     lambda: ([], {}),
     lambda: ([], {'pred': torch.rand([4, 4]), 'target': torch.rand([4, 4])}),
     False),
    (MSELoss,
     lambda: ([], {}),
     lambda: ([], {'pred': torch.rand([4, 4]), 'target': torch.rand([4, 4])}),
     False),
    (MiniBatchStddevLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ModMBStddevLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ModulatedConv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 1, 'style_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4])], {}),
     False),
    (ModulatedPEConv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 1, 'style_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4])], {}),
     False),
    (ModulatedToRGB,
     lambda: ([], {'in_channels': 4, 'style_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4])], {}),
     False),
    (NoiseInjection,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (PNetLin,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 512, 512]), torch.rand([4, 3, 512, 512])], {}),
     False),
    (PixelNorm,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SEModule,
     lambda: ([], {'channels': 4, 'reduction': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SNConv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SNLinear,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ScalingLayer,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 4, 4])], {}),
     True),
    (SinusoidalPositionalEmbedding,
     lambda: ([], {'embedding_dim': 4, 'padding_idx': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SynthesisInput,
     lambda: ([], {'style_channels': 4, 'channels': 4, 'size': 4, 'sampling_rate': 4, 'bandwidth': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (bottleneck_IR,
     lambda: ([], {'in_channel': 4, 'depth': 1, 'stride': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (vgg16,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     False),
]

class Test_open_mmlab_mmgeneration(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

