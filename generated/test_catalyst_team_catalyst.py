import sys
_module = sys.modules[__name__]
del sys
catalyst = _module
__version__ = _module
contrib = _module
data = _module
cv = _module
mixins = _module
blur = _module
flare = _module
rotate = _module
tests = _module
test_mixin = _module
reader = _module
transforms = _module
tensor = _module
nlp = _module
dataset = _module
language_modeling = _module
test_language_modeling_dataset = _module
test_text_classification_dataset = _module
text_classification = _module
transforms = _module
datasets = _module
mnist = _module
utils = _module
dl = _module
callbacks = _module
alchemy_logger = _module
cutmix_callback = _module
gradnorm_logger = _module
knn_metric = _module
mask_inference = _module
neptune_logger = _module
periodic_loader_callback = _module
perplexity_metric = _module
telegram_logger = _module
test_gradnorm_logger = _module
test_perplexity_callback = _module
test_tracer_callback = _module
tracer_callback = _module
visdom_logger = _module
wandb_logger = _module
experiment = _module
runner = _module
models = _module
classification = _module
encoders = _module
resnet = _module
segmentation = _module
abn = _module
blocks = _module
core = _module
fpn = _module
psp = _module
unet = _module
bridge = _module
core = _module
unet = _module
core = _module
decoder = _module
core = _module
fpn = _module
psp = _module
unet = _module
encoder = _module
core = _module
resnet = _module
unet = _module
head = _module
core = _module
fpn = _module
unet = _module
linknet = _module
functional = _module
hydra = _module
bert = _module
sequential = _module
test_hydra = _module
nn = _module
criterion = _module
ce = _module
circle = _module
contrastive = _module
dice = _module
focal = _module
functional = _module
gan = _module
huber = _module
iou = _module
lovasz = _module
margin = _module
triplet = _module
wing = _module
modules = _module
common = _module
lama = _module
pooling = _module
rms_norm = _module
se = _module
optimizers = _module
lamb = _module
lookahead = _module
qhadamw = _module
radam = _module
ralamb = _module
schedulers = _module
base = _module
onecycle = _module
test_criterion = _module
test_optimizer = _module
registry = _module
scripts = _module
check_index_model = _module
collect_env = _module
create_index_model = _module
find_thresholds = _module
tools = _module
tensorboard = _module
test_tensorboard = _module
argparse = _module
compression = _module
confusion_matrix = _module
image = _module
tensor = _module
test_image = _module
misc = _module
text = _module
pandas = _module
parallel = _module
plotly = _module
serialization = _module
test_argparse = _module
test_dataset = _module
test_misc = _module
test_pandas = _module
visualization = _module
callback = _module
checkpoint = _module
early_stop = _module
exception = _module
formatters = _module
logging = _module
metrics = _module
optimizer = _module
scheduler = _module
timer = _module
validation = _module
experiment = _module
legacy = _module
runner = _module
state = _module
data = _module
augmentor = _module
collate_fn = _module
dataset = _module
sampler = _module
image2embedding = _module
process_images = _module
project_embeddings = _module
split_dataframe = _module
tag2label = _module
test_tag2label = _module
text2embedding = _module
test_sampler = _module
confusion_matrix = _module
inference = _module
meter = _module
accuracy = _module
auc = _module
f1_score = _module
iou = _module
ppv_tpr_f1 = _module
mixup = _module
config = _module
experiment = _module
supervised = _module
test_config = _module
test_core = _module
test_supervised = _module
runner = _module
supervised = _module
init = _module
run = _module
trace = _module
test_main = _module
utils = _module
trace = _module
wizard = _module
frozen_class = _module
meters = _module
apmeter = _module
aucmeter = _module
averagevaluemeter = _module
classerrormeter = _module
confusionmeter = _module
mapmeter = _module
movingaveragevaluemeter = _module
msemeter = _module
ppv_tpr_f1_meter = _module
test_averagevaluemeter = _module
test_ppv_tpr_f1 = _module
settings = _module
time_manager = _module
typing = _module
utils = _module
checkpoint = _module
components = _module
dict = _module
distributed = _module
hash = _module
initialization = _module
loader = _module
accuracy = _module
dice = _module
f1_score = _module
focal = _module
iou = _module
test_dice = _module
test_iou = _module
numpy = _module
parser = _module
pipelines = _module
scripts = _module
seed = _module
sys = _module
registery_foo = _module
test_hash = _module
test_registry = _module
test_torch = _module
conf = _module
src = _module
dataset = _module
model = _module
cifar_simple = _module
experiments = _module
simple_experiment = _module
model = _module
cifar_stages = _module
experiment = _module
model = _module
distilbert_text_classification = _module
setup = _module
_tests_contrib_dl_callbacks = _module
experiment = _module
model = _module
_tests_cv_classification = _module
model = _module
test1 = _module
model = _module
test2 = _module
SimpleExperiment1 = _module
SimpleExperiment2 = _module
model = _module
_tests_cv_classification_transforms = _module
model = _module
_tests_cv_segmentation = _module
dataset = _module
_tests_dl_callbacks = _module
experiment = _module
model = _module
_tests_scripts = _module
core_contrib_data = _module
core_contrib_dl = _module
core_contrib_models = _module
core_contrib_nn = _module
core_data = _module
cv_contrib_models = _module
cv_z_segmentation = _module
cv_z_unets = _module
dl_callbacks = _module
dl_experiment = _module
dl_runner = _module
dl_utils = _module
dl_z_classification = _module
dl_z_contirb_functional = _module
dl_z_distributed_01 = _module
dl_z_distributed_02 = _module
dl_z_distributed_03 = _module
dl_z_distributed_04 = _module
dl_z_distributed_05 = _module
dl_z_distributed_06 = _module
dl_z_distributed_07 = _module
dl_z_distributed_08 = _module
dl_z_distributed_09 = _module
dl_z_distributed_10 = _module
dl_z_distributed_11 = _module
dl_z_distributed_12 = _module
dl_z_distributed_13 = _module
dl_z_distributed_14 = _module
dl_z_distributed_15 = _module
dl_z_distributed_16 = _module
dl_z_distributed_17 = _module
dl_z_docs_distributed_0 = _module
dl_z_docs_distributed_1 = _module
dl_z_docs_distributed_2 = _module
dl_z_docs_distributed_3 = _module
dl_z_mvp_distributed_mnist_ae = _module
dl_z_mvp_mnist = _module
dl_z_mvp_mnist_ae = _module
dl_z_mvp_mnist_gan = _module
dl_z_mvp_mnist_unet = _module
dl_z_mvp_mnist_vae = _module
dl_z_mvp_projector = _module
nlp_contrib_models = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, numbers, numpy, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import numpy as np


import torch


from typing import Iterable


from typing import Union


from torch.utils.data import Dataset


from typing import List


from typing import Mapping


import logging


from torch.utils.model_zoo import tqdm


from typing import Dict


from torch.nn import DataParallel


from torch.nn.parallel import DistributedDataParallel


from math import ceil


from scipy import stats


from sklearn.metrics import accuracy_score


from sklearn.metrics import f1_score


from sklearn.metrics import precision_score


from sklearn.metrics import recall_score


from sklearn.neighbors import NearestNeighbors


import torch.nn.functional as F


from collections import OrderedDict


import copy


from torch.utils.data import DataLoader


from torch import nn


from typing import Tuple


import collections


from numbers import Number


import torch.nn as nn


from torch.optim import Adam


import torchvision


from abc import ABC


from abc import abstractmethod


from torch.nn import functional as F


from functools import partial


from copy import deepcopy


from typing import Optional


from torch.nn.modules.loss import *


from torch import Tensor


from torch.nn.modules.loss import _Loss


from itertools import filterfalse as ifilterfalse


import math


from torch.nn.modules import *


from torch.optim import *


from typing import Callable


from torch.optim.optimizer import Optimizer


from collections import defaultdict


from torch.optim import Optimizer


from torch.optim.lr_scheduler import *


from torch.optim.lr_scheduler import _LRScheduler


from torch import optim


from collections import namedtuple


import re


from typing import BinaryIO


from collections.abc import Iterable


from torch import __version__ as torch_version


import string


from typing import Any


from torch.utils.data import DistributedSampler


from copy import copy


import warnings


from torch.utils.data.dataloader import default_collate


from torch.utils.data import Sampler


from typing import Iterator


from collections import Counter


from random import choices


from random import sample


from torch.utils.data.sampler import BatchSampler


from torch.utils.data.sampler import Sampler


from typing import Sequence


from sklearn.metrics import confusion_matrix as confusion_matrix_fn


import torch.distributed


from torch.optim.lr_scheduler import ReduceLROnPlateau


from typing import Generator


from torch.jit import ScriptModule


from torch.utils.data.dataloader import default_collate as default_collate_fn


import inspect


from torch.jit import load


from torch.jit import save


from torch.jit import trace


import numbers


from torch.optim import lr_scheduler


from torch.utils import data


import random


import torch.backends


from torch.backends import cudnn


from torch.utils.data import Subset


import torch.optim as optim


from torch.utils.data import TensorDataset


class _SimpleNet(nn.Module):

    def __init__(self, input_shape: Tuple[int]):
        super().__init__()
        assert len(input_shape) == 3
        c, h, w = input_shape
        self.conv1 = nn.Conv2d(in_channels=c, out_channels=64, kernel_size=3)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2)
        self.flatten = nn.Flatten()
        for conv in [self.conv1, self.conv2]:
            h_kernel, w_kernel = conv.kernel_size
            h_stride, w_stride = conv.stride
            c = conv.out_channels
            h, w = self.conv2d_size_out(size=(h, w), kernel_size=(h_kernel, w_kernel), stride=(h_stride, w_stride))
        self.fc1 = nn.Linear(in_features=c * h * w, out_features=10)

    def forward(self, x: torch.Tensor):
        for conv in [self.conv1, self.conv2]:
            x = conv(x)
            x = self.relu(x)
        x = self.flatten(x)
        x = self.fc1(x)
        return x

    @staticmethod
    def conv2d_size_out(*, size: Tuple[int], kernel_size: Tuple[int], stride: Tuple[int]):
        """Computes output size for 2D convolution layer.
        cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)
        cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)
        to understand the shape for dense layer's input.

        Args:
            size (Tuple[int]): size of input.
            kernel_size (Tuple[int]): size of convolution kernel.
            stride (Tuple[int]): size of convolution stride.

        Returns:
            int: output size
        """
        size, kernel_size, stride = map(lambda x: torch.tensor(x, dtype=torch.int32), (size, kernel_size, stride))
        output_size = (size - (kernel_size - 1) - 1) / stride + 1
        h, w = map(lambda x: x.item(), output_size)
        return h, w


class _TracedNet(nn.Module):
    """
    Simple model for the testing.
    """

    def __init__(self, input_shape: Tuple[int]):
        """
        Args:
            input_shape (Tuple[int]): Shape of input tensor.
        """
        super().__init__()
        assert len(input_shape) == 3
        c, h, w = input_shape
        self.conv1 = nn.Conv2d(in_channels=c, out_channels=64, kernel_size=3)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2)
        self.flatten = nn.Flatten()
        for conv in [self.conv1, self.conv2]:
            h_kernel, w_kernel = conv.kernel_size
            h_stride, w_stride = conv.stride
            c = conv.out_channels
            h, w = self.conv2d_size_out(size=(h, w), kernel_size=(h_kernel, w_kernel), stride=(h_stride, w_stride))
        self.fc1 = nn.Linear(in_features=c * h * w, out_features=10)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """
        Args:
            x (torch.Tensor): Input tensor

        Returns:
            (torch.Tensor): Output tensor
        """
        for conv in [self.conv1, self.conv2]:
            x = conv(x)
            x = self.relu(x)
        x = self.flatten(x)
        x = self.fc1(x)
        return x

    @staticmethod
    def conv2d_size_out(*, size: Tuple[int], kernel_size: Tuple[int], stride: Tuple[int]) ->Tuple[int, int]:
        """
        Computes output size for 2D convolution layer.
        cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)
        cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)
        to understand the shape for dense layer's input.

        Args:
            size (Tuple[int]): size of input.
            kernel_size (Tuple[int]): size of convolution kernel.
            stride (Tuple[int]): size of convolution stride.

        Returns:
            (Tuple[int, int]): output size
        """
        size, kernel_size, stride = map(lambda x: torch.tensor(x, dtype=torch.int32), (size, kernel_size, stride))
        output_size = (size - (kernel_size - 1) - 1) / stride + 1
        h, w = map(lambda x: x.item(), output_size)
        return h, w


class EncoderSpec(ABC, nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    @property
    @abstractmethod
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        pass

    @property
    @abstractmethod
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        pass


RESNET_PARAMS = {'resnet18': {'channels': [64, 64, 128, 256, 512], 'strides': [2, 4, 8, 16, 32]}, 'resnet34': {'channels': [64, 64, 128, 256, 512], 'strides': [2, 4, 8, 16, 32]}, 'resnet50': {'channels': [64, 256, 512, 1024, 2048], 'strides': [2, 4, 8, 16, 32]}, 'resnet101': {'channels': [64, 256, 512, 1024, 2048], 'strides': [2, 4, 8, 16, 32]}, 'resnet152': {'channels': [64, 256, 512, 1024, 2048], 'strides': [2, 4, 8, 16, 32]}}


def _take(elements, indexes):
    return [elements[i] for i in indexes]


class ABN(nn.Module):
    """Activated Batch Normalization.

    This gathers a `BatchNorm2d` and an activation function in a single module.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self, num_features: int, activation: str='leaky_relu', batchnorm_params: Dict=None, activation_params: Dict=None, use_batchnorm: bool=True):
        """
        Args:
            num_features (int): number of feature channels
                in the input and output
            activation (str): name of the activation functions, one of:
                ``'leaky_relu'``, ``'elu'`` or ``'none'``.
            batchnorm_params (dict): additional ``nn.BatchNorm2d`` params
            activation_params (dict): additional params for activation fucntion
            use_batchnorm (bool): @TODO: Docs. Contribution is welcome
        """
        super().__init__()
        batchnorm_params = batchnorm_params or {}
        activation_params = activation_params or {}
        layers = []
        if use_batchnorm:
            layers.append(nn.BatchNorm2d(num_features=num_features, **batchnorm_params))
        if activation is not None and activation.lower() != 'none':
            layers.append(nn.__dict__[activation](inplace=True, **activation_params))
        self.net = nn.Sequential(*layers)

    def forward(self, x):
        """Forward call."""
        x = self.net(x)
        return x


class EncoderBlock(ABC, nn.Module):
    """@TODO: Docs (add description, `Example`). Contribution is welcome."""

    def __init__(self, in_channels: int, out_channels: int, in_strides: int=None):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.in_strides = in_strides

    @property
    @abstractmethod
    def out_strides(self) ->int:
        """@TODO: Docs. Contribution is welcome."""
        pass

    @property
    @abstractmethod
    def block(self) ->nn.Module:
        """@TODO: Docs. Contribution is welcome."""
        pass

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        return self.block(x)


class DecoderBlock(ABC, nn.Module):
    """@TODO: Docs (add description, `Example`). Contribution is welcome."""

    def __init__(self, in_channels: int, enc_channels: int, out_channels: int, in_strides: int=None, *args, **kwargs):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        super().__init__()
        self.in_channels = in_channels
        self.enc_channels = enc_channels
        self.out_channels = out_channels
        self.in_strides = in_strides
        self.block = self._get_block(*args, **kwargs)

    @abstractmethod
    def _get_block(self, *args, **kwargs) ->nn.Module:
        pass

    @property
    def out_strides(self) ->int:
        """@TODO: Docs. Contribution is welcome."""
        return self.in_strides // 2 if self.in_strides is not None else None

    @abstractmethod
    def forward(self, bottom: torch.Tensor, left: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        pass


class DecoderFPNBlock(DecoderBlock):
    """@TODO: Docs (add description, `Example`). Contribution is welcome."""

    def __init__(self, in_channels: int, enc_channels: int, out_channels: int, in_strides: int=None, upsample_scale: int=2, interpolation_mode: str='nearest', align_corners: bool=None, aggregate_first: bool=False, **kwargs):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        self.upsample_scale = upsample_scale
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        super().__init__(in_channels, enc_channels, out_channels, in_strides, **kwargs)

    def _get_block(self):
        block = nn.Conv2d(self.enc_channels, self.out_channels, kernel_size=1)
        return block

    def forward(self, bottom: torch.Tensor, left: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        x = F.interpolate(bottom, scale_factor=self.upsample_scale, mode=self.interpolation_mode, align_corners=self.align_corners)
        left = self.block(left)
        x = x + left
        return x


class Conv3x3GNReLU(nn.Module):
    """@TODO: Docs (add description, `Example`). Contribution is welcome."""

    def __init__(self, in_channels, out_channels, upsample=False, upsample_scale: int=2, interpolation_mode: str='bilinear', align_corners: bool=True):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        super().__init__()
        self.upsample = upsample
        self.upsample_scale = upsample_scale
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        self.block = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False), nn.GroupNorm(32, out_channels), nn.ReLU(inplace=True))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        x = self.block(x)
        if self.upsample:
            x = F.interpolate(x, scale_factor=self.upsample_scale, mode=self.interpolation_mode, align_corners=self.align_corners)
        return x


class SegmentationBlock(nn.Module):
    """@TODO: Docs (add description, `Example`). Contribution is welcome."""

    def __init__(self, in_channels: int, out_channels: int, num_upsamples: int=0):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        blocks = [Conv3x3GNReLU(in_channels, out_channels, upsample=bool(num_upsamples))]
        if num_upsamples > 1:
            for _ in range(1, num_upsamples):
                blocks.append(Conv3x3GNReLU(out_channels, out_channels, upsample=True))
        self.block = nn.Sequential(*blocks)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        return self.block(x)


def _get_block(in_channels: int, out_channels: int, abn_block: nn.Module=ABN, activation: str='ReLU', kernel_size: int=3, padding: int=1, first_stride: int=1, second_stride: int=1, complexity: int=1, **kwargs):
    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=first_stride, bias=False, **kwargs), abn_block(out_channels, activation=activation)]
    if complexity > 0:
        layers_ = [nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=second_stride, bias=False, **kwargs), abn_block(out_channels, activation=activation)] * complexity
        layers = layers + layers_
    block = nn.Sequential(*layers)
    return block


class PyramidBlock(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: int, out_channels: int, pool_size: int, use_batchnorm: bool=True, interpolation_mode: str='bilinear', align_corners: bool=True, complexity: int=0):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        if pool_size == 1:
            use_batchnorm = False
        self._block = nn.Sequential(nn.AdaptiveAvgPool2d(output_size=(pool_size, pool_size)), _get_block(in_channels, out_channels, abn_block=partial(ABN, use_batchnorm=use_batchnorm), complexity=complexity))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        h, w = x.shape[-2:]
        x = self._block(x)
        x = F.interpolate(x, size=(h, w), mode=self.interpolation_mode, align_corners=self.align_corners)
        return x


class PSPBlock(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: int, pool_sizes: Tuple[int]=(1, 2, 3, 6), use_batchnorm: bool=True):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.stages = nn.ModuleList([PyramidBlock(in_channels, in_channels // len(pool_sizes), pool_size, use_batchnorm=use_batchnorm) for pool_size in pool_sizes])

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        xs = [stage(x) for stage in self.stages] + [x]
        x = torch.cat(xs, dim=1)
        return x


class EncoderDownsampleBlock(EncoderBlock):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: int, out_channels: int, in_strides: int=None, abn_block: nn.Module=ABN, activation: str='ReLU', first_stride: int=2, second_stride: int=1, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, out_channels, in_strides)
        self._out_strides = in_strides * first_stride * second_stride if in_strides is not None else None
        self._block = _get_block(in_channels=in_channels, out_channels=out_channels, abn_block=abn_block, activation=activation, first_stride=first_stride, second_stride=second_stride, **kwargs)

    @property
    def out_strides(self) ->int:
        """@TODO: Docs. Contribution is welcome."""
        return self._out_strides

    @property
    def block(self):
        """@TODO: Docs. Contribution is welcome."""
        return self._block


class EncoderUpsampleBlock(EncoderBlock):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: int, out_channels: int, in_strides: int=None, abn_block: nn.Module=ABN, activation: str='ReLU', first_stride: int=1, second_stride: int=1, pool_first: bool=False, upsample_scale: int=2, interpolation_mode: str='nearest', align_corners: bool=None, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, out_channels, in_strides)
        if in_strides is None:
            self._out_strides = None
        elif pool_first:
            self._out_strides = in_strides * first_stride * second_stride * 2 // upsample_scale
        else:
            self._out_strides = in_strides * first_stride * second_stride // upsample_scale
        self.pool_first = pool_first
        self.upsample_scale = upsample_scale
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        self._block = _get_block(in_channels=in_channels, out_channels=out_channels, abn_block=abn_block, activation=activation, first_stride=first_stride, second_stride=second_stride, **kwargs)

    @property
    def out_strides(self) ->int:
        """@TODO: Docs. Contribution is welcome."""
        return self._out_strides

    @property
    def block(self):
        """@TODO: Docs. Contribution is welcome."""
        return self._block

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        if self.pool_first:
            x = F.max_pool2d(x, kernel_size=self.upsample_scale, stride=self.upsample_scale)
        x = F.interpolate(x, scale_factor=self.upsample_scale, mode=self.interpolation_mode, align_corners=self.align_corners)
        return self.block(x)


def _upsample(x: torch.Tensor, scale: int=None, size: int=None, interpolation_mode: str='bilinear', align_corners: bool=True) ->torch.Tensor:
    if scale is None:
        x = F.interpolate(x, size=size, mode=interpolation_mode, align_corners=align_corners)
    else:
        x = F.interpolate(x, scale_factor=scale, mode=interpolation_mode, align_corners=align_corners)
    return x


class DecoderConcatBlock(DecoderBlock):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: int, enc_channels: int, out_channels: int, in_strides: int=None, abn_block: nn.Module=ABN, activation: str='ReLU', pre_dropout_rate: float=0.0, post_dropout_rate: float=0.0, upsample_scale: int=None, interpolation_mode: str='bilinear', align_corners: bool=True, aggregate_first: bool=False, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        self.upsample_scale = upsample_scale
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        self.aggregate_first = aggregate_first
        super().__init__(in_channels, enc_channels, out_channels, in_strides, abn_block=abn_block, activation=activation, pre_dropout_rate=pre_dropout_rate, post_dropout_rate=post_dropout_rate, **kwargs)

    def _get_block(self, abn_block: nn.Module=ABN, activation: str='ReLU', pre_dropout_rate: float=0.0, post_dropout_rate: float=0.0, **kwargs):
        layers = []
        if pre_dropout_rate > 0:
            layers.append(nn.Dropout2d(pre_dropout_rate, inplace=True))
        layers.append(_get_block(in_channels=self.in_channels + self.enc_channels, out_channels=self.out_channels, abn_block=abn_block, activation=activation, first_stride=1, second_stride=1, **kwargs))
        if post_dropout_rate > 0:
            layers.append(nn.Dropout2d(pre_dropout_rate, inplace=True))
        block = nn.Sequential(*layers)
        return block

    def forward(self, bottom: torch.Tensor, left: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        if self.aggregate_first:
            x = torch.cat([bottom, left], 1)
            x = _upsample(x, scale=self.upsample_scale, interpolation_mode=self.interpolation_mode, align_corners=self.align_corners)
        else:
            x = _upsample(bottom, scale=self.upsample_scale, size=left.shape[2:], interpolation_mode=self.interpolation_mode, align_corners=self.align_corners)
            x = torch.cat([x, left], 1)
        return self.block(x)


class DecoderSumBlock(DecoderConcatBlock):
    """@TODO: Docs (add description, `Example`). Contribution is welcome"""

    def __init__(self, enc_channels: int, **kwargs):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        super().__init__(enc_channels=0, **kwargs)

    def forward(self, bottom: torch.Tensor, left: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        if self.aggregate_first:
            x = bottom + left
            x = _upsample(x, scale=self.upsample_scale, interpolation_mode=self.interpolation_mode, align_corners=self.align_corners)
            x = self.block(x)
        else:
            x = _upsample(bottom, scale=self.upsample_scale, size=left.shape[2:], interpolation_mode=self.interpolation_mode, align_corners=self.align_corners)
            x = self.block(x)
            x = x + left
        return x


class BridgeSpec(ABC, nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], in_strides: List[int]):
        """
        Args:
            in_channels (List[int]): number of channels in the input sample
            in_strides (List[int]): the stride of the block
        """
        super().__init__()
        self._in_channels = in_channels
        self._in_strides = in_strides

    @property
    def in_channels(self) ->List[int]:
        """Number of channels in the input sample."""
        return self._in_channels

    @property
    def in_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        return self._in_strides

    @property
    @abstractmethod
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        pass

    @property
    @abstractmethod
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        pass

    @abstractmethod
    def forward(self, x: List[torch.Tensor]) ->List[torch.Tensor]:
        """Forward call."""
        pass


class UnetBridge(BridgeSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], in_strides: List[int], out_channels: int, block_fn: EncoderBlock=EncoderDownsampleBlock, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, in_strides)
        self.block = block_fn(in_channels=in_channels[-1], in_strides=in_strides[-1], out_channels=out_channels, **kwargs)
        self._out_channels = in_channels + [self.block.out_channels]
        self._out_strides = in_strides + [self.block.out_strides]

    @property
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        return self._out_channels

    @property
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        return self._out_strides

    def forward(self, x: List[torch.Tensor]) ->List[torch.Tensor]:
        """Forward call."""
        x_: torch.Tensor = x[-1]
        x_: torch.Tensor = self.block(x_)
        output = x + [x_]
        return output


class DecoderSpec(ABC, nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], in_strides: List[int]):
        """
        Args:
            in_channels (List[int]): number of channels in the input sample
            in_strides (List[int]): the stride of the block
        """
        super().__init__()
        self.in_channels = in_channels
        self.in_strides = in_strides

    @property
    @abstractmethod
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        pass

    @property
    @abstractmethod
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        pass

    @abstractmethod
    def forward(self, x: List[torch.Tensor]) ->torch.Tensor:
        """Forward call."""
        pass


class HeadSpec(ABC, nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], out_channles: int, in_strides: List[int]=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_channels = in_channels
        self.in_strides = in_strides
        self.out_channles = out_channles

    @abstractmethod
    def forward(self, x: List[torch.Tensor]) ->torch.Tensor:
        """Forward call."""
        pass


class UnetEncoder(EncoderSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: int, num_channels: int, num_blocks: int, layers_indices: List[int]=None, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.num_filters = num_channels
        self.num_blocks = num_blocks
        self._layers_indices = layers_indices or list(range(num_blocks))
        self._channels = [(self.num_filters * 2 ** i) for i in range(self.num_blocks)]
        self._strides = [(2 ** i) for i in range(self.num_blocks)]
        self._channels = _take(self._channels, self._layers_indices)
        self._strides = _take(self._strides, self._layers_indices)
        for i in range(num_blocks):
            in_channels = in_channels if not i else num_channels * 2 ** (i - 1)
            out_channels = num_channels * 2 ** i
            self.add_module(f'block{i + 1}', EncoderDownsampleBlock(in_channels, out_channels, first_stride=1, **kwargs))
            if i != self.num_blocks - 1:
                self.add_module(f'pool{i + 1}', nn.MaxPool2d(2, 2))

    @property
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        return self._channels

    @property
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        return self._strides

    def forward(self, x: torch.Tensor) ->List[torch.Tensor]:
        """Forward call."""
        output = []
        for i in range(self.num_blocks):
            x = self.__getattr__(f'block{i + 1}')(x)
            output.append(x)
            if i != self.num_blocks - 1:
                x = self.__getattr__(f'pool{i + 1}')(x)
        output = _take(output, self._layers_indices)
        return output


class FPNDecoder(DecoderSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], in_strides: List[int], pyramid_channels: int=256, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, in_strides)
        out_strides_ = [in_strides[-1]]
        self.center_conv = nn.Conv2d(in_channels[-1], pyramid_channels, kernel_size=1)
        reversed_features = list(reversed(in_channels[:-1]))
        blocks = []
        for encoder_features in reversed_features:
            blocks.append(DecoderFPNBlock(in_channels=pyramid_channels, enc_channels=encoder_features, out_channels=pyramid_channels, in_strides=out_strides_[-1], **kwargs))
            out_strides_.append(blocks[-1].out_strides)
        self.blocks = nn.ModuleList(blocks)
        self._out_channels = [pyramid_channels] * len(in_channels)
        self._out_strides = out_strides_

    @property
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        return self._out_channels

    @property
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        return self._out_strides

    def forward(self, x: List[torch.Tensor]) ->List[torch.Tensor]:
        """Forward call."""
        fpn_features = [self.center_conv(x[-1])]
        reversed_features = list(reversed(x[:-1]))
        for _i, (fpn_block, encoder_output) in enumerate(zip(self.blocks, reversed_features)):
            fpn_features.append(fpn_block(fpn_features[-1], encoder_output))
        return fpn_features


class PSPDecoder(DecoderSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], in_strides: List[int], downsample_factor: int=8, use_batchnorm: bool=True, out_channels: int=512):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, in_strides)
        self.block_offset = self._get_block_offset(downsample_factor)
        psp_out_channels: int = self._get(in_channels)
        self.psp = PSPBlock(psp_out_channels, pool_sizes=(1, 2, 3, 6), use_batchnorm=use_batchnorm)
        self.conv = _get_block(psp_out_channels * 2, out_channels, kernel_size=1, padding=0, abn_block=partial(ABN, use_batchnorm=use_batchnorm), complexity=0)
        self._out_channels = out_channels
        self.downsample_factor = downsample_factor

    @property
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        return [self._out_channels]

    @property
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        return [self.downsample_factor]

    def _get_block_offset(self, downsample_factor: int):
        offset = self.in_strides.index(downsample_factor)
        return offset

    def _get(self, xs: List):
        return xs[self.block_offset]

    def forward(self, x: List[torch.Tensor]) ->List[torch.Tensor]:
        """Forward call."""
        features = self._get(x)
        x = self.psp(features)
        x = self.conv(x)
        return [x]


class UNetDecoder(DecoderSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], in_strides: List[int], block_fn: DecoderBlock=DecoderConcatBlock, **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, in_strides)
        out_channels_ = [in_channels[-1]]
        out_strides_ = [in_strides[-1]]
        reversed_channels = list(reversed(in_channels[:-1]))
        blocks: List[DecoderBlock] = []
        for encoder_channels in reversed_channels:
            out_channels_.append(encoder_channels)
            blocks.append(block_fn(in_channels=out_channels_[-2], enc_channels=encoder_channels, out_channels=out_channels_[-1], in_strides=out_strides_[-1], **kwargs))
            out_strides_.append(blocks[-1].out_strides)
        self.blocks = nn.ModuleList(blocks)
        self._out_channels = out_channels_
        self._out_strides = out_strides_

    @property
    def out_channels(self) ->List[int]:
        """Number of channels produced by the block."""
        return self._out_channels

    @property
    def out_strides(self) ->List[int]:
        """@TODO: Docs. Contribution is welcome."""
        return self._out_strides

    def forward(self, x: List[torch.Tensor]) ->List[torch.Tensor]:
        """Forward call."""
        decoder_outputs = [x[-1]]
        reversed_features = list(reversed(x[:-1]))
        for _i, (decoder_block, encoder_output) in enumerate(zip(self.blocks, reversed_features)):
            decoder_outputs.append(decoder_block(decoder_outputs[-1], encoder_output))
        return decoder_outputs


class FPNHead(HeadSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], out_channels: int, hid_channel: int=256, in_strides: List[int]=None, dropout: float=0.0, num_upsample_blocks: int=0, upsample_scale: int=1, interpolation_mode: str='bilinear', align_corners: bool=True):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, out_channels, in_strides)
        self.upsample_scale = upsample_scale
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        segmentation_blocks = []
        for i, in_channels_ in enumerate(in_channels):
            if in_strides is not None:
                i = np.log2(in_strides[i]) - num_upsample_blocks - np.log2(upsample_scale)
            segmentation_blocks.append(SegmentationBlock(in_channels=in_channels_, out_channels=hid_channel, num_upsamples=int(i)))
        self.segmentation_blocks = nn.ModuleList(segmentation_blocks)
        additional_layers = [EncoderUpsampleBlock(hid_channel, hid_channel)] * num_upsample_blocks
        if dropout > 0:
            additional_layers.append(nn.Dropout2d(p=dropout, inplace=True))
        self.head = nn.Sequential(*additional_layers, nn.Conv2d(hid_channel, out_channels, 1))

    def forward(self, x: List[torch.Tensor]) ->torch.Tensor:
        """Forward call."""
        x = list(map(lambda block, features: block(features), self.segmentation_blocks, x))
        x = sum(x)
        x = self.head(x)
        if self.upsample_scale > 1:
            x = F.interpolate(x, scale_factor=self.upsample_scale, mode=self.interpolation_mode, align_corners=self.align_corners)
        return x


class UnetHead(HeadSpec):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_channels: List[int], out_channels: int, in_strides: List[int]=None, dropout: float=0.0, num_upsample_blocks: int=0, upsample_scale: int=1, interpolation_mode: str='bilinear', align_corners: bool=True):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(in_channels, out_channels, in_strides)
        self.upsample_scale = upsample_scale
        self.interpolation_mode = interpolation_mode
        self.align_corners = align_corners
        in_channels_ = in_channels[-1]
        additional_layers = [EncoderUpsampleBlock(in_channels_, in_channels_)] * num_upsample_blocks
        if dropout > 0:
            additional_layers.append(nn.Dropout2d(p=dropout, inplace=True))
        self.head = nn.Sequential(*additional_layers, nn.Conv2d(in_channels_, out_channels, 1))

    def forward(self, x: List[torch.Tensor]) ->torch.Tensor:
        """Forward call."""
        x_ = x[-1]
        x = self.head(x_)
        if self.upsample_scale > 1:
            x = F.interpolate(x, scale_factor=self.upsample_scale, mode=self.interpolation_mode, align_corners=self.align_corners)
        return x


class Normalize(nn.Module):
    """Performs :math:`L_p` normalization of inputs over specified dimension.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self, **normalize_kwargs):
        """
        Args:
            **normalize_kwargs: see ``torch.nn.functional.normalize`` params
        """
        super().__init__()
        self.normalize_kwargs = normalize_kwargs

    def forward(self, x):
        """Forward call."""
        return F.normalize(x, **self.normalize_kwargs)


LateAddCallbak = Callable[['Registry'], None]


class RegistryException(Exception):
    """Exception class for all registry errors."""

    def __init__(self, message):
        """
        Init.

        Args:
            message: exception message
        """
        super().__init__(message)


class ResidualWrapper(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, net):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.net = net

    def forward(self, x):
        """Forward call."""
        return x + self.net(x)


def _process_additional_params(params, layers):
    if isinstance(params, List):
        assert len(params) == len(layers)
    else:
        params = [params] * len(layers)
    return params


class SequentialNet(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, hiddens, layer_fn: Union[str, Dict, List], norm_fn: Union[str, Dict, List]=None, dropout_fn: Union[str, Dict, List]=None, activation_fn: Union[str, Dict, List]=None, residual: Union[bool, str]=False, layer_order: List=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        assert len(hiddens) > 1, 'No sequence found'
        layer_fn = _process_additional_params(layer_fn, hiddens[1:])
        norm_fn = _process_additional_params(norm_fn, hiddens[1:])
        dropout_fn = _process_additional_params(dropout_fn, hiddens[1:])
        activation_fn = _process_additional_params(activation_fn, hiddens[1:])
        if isinstance(residual, bool) and residual:
            residual = 'hard'
            residual = _process_additional_params(residual, hiddens[1:])
        layer_order = layer_order or ['layer', 'norm', 'drop', 'act']

        def _layer_fn(layer_fn, f_in, f_out, **kwargs):
            layer_fn = MODULES.get_if_str(layer_fn)
            layer_fn = layer_fn(f_in, f_out, **kwargs)
            return layer_fn

        def _normalization_fn(normalization_fn, f_in, f_out, **kwargs):
            normalization_fn = MODULES.get_if_str(normalization_fn)
            normalization_fn = normalization_fn(f_out, **kwargs) if normalization_fn is not None else None
            return normalization_fn

        def _dropout_fn(dropout_fn, f_in, f_out, **kwargs):
            dropout_fn = MODULES.get_if_str(dropout_fn)
            dropout_fn = dropout_fn(**kwargs) if dropout_fn is not None else None
            return dropout_fn

        def _activation_fn(activation_fn, f_in, f_out, **kwargs):
            activation_fn = MODULES.get_if_str(activation_fn)
            activation_fn = activation_fn(**kwargs) if activation_fn is not None else None
            return activation_fn
        name2fn = {'layer': _layer_fn, 'norm': _normalization_fn, 'drop': _dropout_fn, 'act': _activation_fn}
        name2params = {'layer': layer_fn, 'norm': norm_fn, 'drop': dropout_fn, 'act': activation_fn}
        net = []
        for i, (f_in, f_out) in enumerate(utils.pairwise(hiddens)):
            block = []
            for key in layer_order:
                sub_fn = name2fn[key]
                sub_params = deepcopy(name2params[key][i])
                if isinstance(sub_params, Dict):
                    sub_module = sub_params.pop('module')
                else:
                    sub_module = sub_params
                    sub_params = {}
                sub_block = sub_fn(sub_module, f_in, f_out, **sub_params)
                if sub_block is not None:
                    block.append((f'{key}', sub_block))
            block_ = OrderedDict(block)
            block = torch.nn.Sequential(block_)
            if block_.get('act', None) is not None:
                activation = block_['act']
                activation_init = utils.get_optimal_inner_init(nonlinearity=activation)
                block.apply(activation_init)
            if residual == 'hard' or residual == 'soft' and f_in == f_out:
                block = ResidualWrapper(net=block)
            net.append((f'block_{i}', block))
        self.net = torch.nn.Sequential(OrderedDict(net))

    def forward(self, x):
        """Forward call."""
        x = self.net.forward(x)
        return x


class Hydra(nn.Module):
    """Hydra - one model to predict them all.

    @TODO: Docs. Contribution is welcome.
    """
    _parent_keyword = '_'
    _hidden_keyword = '_hidden'
    _normalize_keyword = 'normalize_output'

    def __init__(self, heads: nn.ModuleDict, encoder: nn.Module=None, embedders: nn.ModuleDict=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.encoder = encoder or nn.Sequential()
        self.heads = heads
        self.embedders = embedders or {}

    @staticmethod
    def parse_head_params(head_params: Dict, in_features: int, is_leaf: bool=False) ->Union[nn.Module, nn.ModuleDict]:
        """@TODO: Docs. Contribution is welcome."""
        if is_leaf:
            if isinstance(head_params, int):
                head_params = {'hiddens': [head_params]}
            normalize = head_params.pop(Hydra._normalize_keyword, False)
            head_params['hiddens'].insert(0, in_features)
            output = [('net', SequentialNet(**head_params))]
            if normalize:
                output.append(('normalize', Normalize()))
            output = OrderedDict(output)
            output = nn.Sequential(output)
        else:
            output = {}
            hidden_params = head_params.pop(Hydra._hidden_keyword, None)
            if hidden_params is not None:
                in_features = hidden_params if isinstance(hidden_params, int) else hidden_params['hiddens'][-1]
                output[Hydra._hidden_keyword] = Hydra.parse_head_params(head_params=hidden_params, in_features=in_features, is_leaf=True)
            for head_name_, head_params_ in head_params.items():
                output[head_name_] = Hydra.parse_head_params(head_params=head_params_, in_features=in_features, is_leaf=not head_name_.startswith(Hydra._parent_keyword))
            output = nn.ModuleDict(output)
        return output

    @staticmethod
    def forward_head(input_, head):
        """@TODO: Docs. Contribution is welcome."""
        if isinstance(head, nn.ModuleDict):
            output = {}
            net_ = getattr(head, Hydra._hidden_keyword, None)
            if net_ is not None:
                input_ = net_(input_)
                output[''] = input_
            for head_name, head_layer in head.items():
                if head_name == Hydra._hidden_keyword:
                    continue
                output[head_name] = Hydra.forward_head(input_, head_layer)
        elif isinstance(head, nn.Module):
            output = head(input_)
        else:
            raise NotImplementedError()
        return output

    def forward(self, features: torch.Tensor, **targets_kwargs):
        """Forward call."""
        embeddings = self.encoder(features)
        heads_output = self.forward_head(embeddings, self.heads)
        output = {'features': features, 'embeddings': embeddings, **utils.flatten_dict(heads_output)}
        for key, value in targets_kwargs.items():
            output[f'{key}_embeddings'] = self.embedders[key](value)
        return output

    def forward_tuple(self, features: torch.Tensor):
        """@TODO: Docs. Contribution is welcome."""
        output_kv = self.forward(features)
        output = [output_kv['features'], output_kv['embeddings']]
        output.extend([value for key, value in output_kv.items() if not key.endswith('/') and key not in ['features', 'embeddings']])
        output = tuple(output)
        return output

    @classmethod
    def get_from_params(cls, heads_params: Dict, encoder_params: Dict=None, embedders_params: Dict=None, in_features: int=None) ->'Hydra':
        """@TODO: Docs. Contribution is welcome."""
        heads_params_ = deepcopy(heads_params)
        encoder_params_ = deepcopy(encoder_params)
        embedders_params_ = deepcopy(embedders_params)

        def _get_normalization_keyword(dct: Dict):
            return dct.pop(Hydra._normalize_keyword, False) if dct is not None else False
        if encoder_params_ is not None:
            normalize_embeddings: bool = _get_normalization_keyword(encoder_params_)
            encoder = SequentialNet(**encoder_params_)
            in_features = encoder_params_['hiddens'][-1]
            if normalize_embeddings:
                encoder = nn.Sequential(encoder, Normalize())
        else:
            assert in_features is not None
            encoder = None
        heads = Hydra.parse_head_params(head_params=heads_params_, in_features=in_features)
        assert isinstance(heads, nn.ModuleDict)
        embedders = {}
        if embedders_params_ is not None:
            for key, head_params in embedders_params_.items():
                if isinstance(head_params, int):
                    head_params = {'num_embeddings': head_params}
                normalize_ = head_params.pop(Hydra._normalize_keyword, False)
                block = [('embedding', nn.Embedding(embedding_dim=in_features, **head_params))]
                if normalize_:
                    block.append(('normalize', Normalize()))
                block = OrderedDict(block)
                block = nn.Sequential(block)
                embedders[key] = block
            embedders = nn.ModuleDict(embedders)
        net = cls(heads=heads, encoder=encoder, embedders=embedders)
        return net


class BertClassifier(nn.Module):
    """Simplified version of the same class by HuggingFace.

    See ``transformers/modeling_distilbert.py`` in the transformers repository.
    """

    def __init__(self, pretrained_model_name: str, num_classes: Optional[int]=None):
        """
        Args:
            pretrained_model_name (str): HuggingFace model name.
                See transformers/modeling_auto.py
            num_classes (int, optional): the number of class labels
                in the classification task
        """
        super().__init__()
        config = AutoConfig.from_pretrained(pretrained_model_name, num_labels=num_classes)
        self.distilbert = AutoModel.from_pretrained(pretrained_model_name, config=config)
        self.pre_classifier = nn.Linear(config.dim, config.dim)
        self.classifier = nn.Sequential(nn.ReLU(), nn.Dropout(config.seq_classif_dropout), nn.Linear(config.dim, num_classes))

    def forward(self, features: torch.Tensor, attention_mask: Optional[torch.Tensor]=None, head_mask: Optional[torch.Tensor]=None) ->torch.Tensor:
        """Compute class probabilities for the input sequence.

        Args:
            features (torch.Tensor): ids of each token,
                size ([bs, seq_length]
            attention_mask (torch.Tensor, optional): binary tensor,
                used to select tokens which are used to compute attention
                scores in the self-attention heads, size [bs, seq_length]
            head_mask (torch.Tensor, optional): 1.0 in head_mask indicates that
                we keep the head, size: [num_heads]
                or [num_hidden_layers x num_heads]

        Returns:
            PyTorch Tensor with predicted class probabilities
        """
        assert attention_mask is not None, 'attention mask is none'
        distilbert_output = self.distilbert(input_ids=features, attention_mask=attention_mask, head_mask=head_mask)
        hidden_state = distilbert_output[0]
        pooled_output = hidden_state[:, (0)]
        pooled_output = self.pre_classifier(pooled_output)
        logits = self.classifier(pooled_output)
        return logits


class NaiveCrossEntropyLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, size_average=True):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.size_average = size_average

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """Calculates loss between ``input`` and ``target`` tensors.

        Args:
            input (torch.Tensor): input tensor of shape ...
            target (torch.Tensor): target tensor of shape ...

        @TODO: Docs (add shapes). Contribution is welcome.
        """
        assert input.size() == target.size()
        input = F.log_softmax(input)
        loss = -torch.sum(input * target)
        loss = loss / input.size()[0] if self.size_average else loss
        return loss


class SymmetricCrossEntropyLoss(nn.Module):
    """The Symmetric Cross Entropy loss.

    It has been proposed in `Symmetric Cross Entropy for Robust Learning
    with Noisy Labels`_.

    .. _Symmetric Cross Entropy for Robust Learning with Noisy Labels:
        https://arxiv.org/abs/1908.06112
    """

    def __init__(self, alpha: float=1.0, beta: float=1.0):
        """
        Args:
            alpha(float):
                corresponds to overfitting issue of CE
            beta(float):
                corresponds to flexible exploration on the robustness of RCE
        """
        super(SymmetricCrossEntropyLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta

    def forward(self, input: torch.Tensor, target: torch.Tensor) ->torch.Tensor:
        """Calculates loss between ``input`` and ``target`` tensors.

        Args:
            input (torch.Tensor): input tensor of size
                (batch_size, num_classes)
            target (torch.Tensor): target tensor of size (batch_size), where
                values of a vector correspond to class index
        """
        num_classes = input.shape[1]
        target_one_hot = F.one_hot(target, num_classes).float()
        assert target_one_hot.shape == input.shape
        input = torch.clamp(input, min=1e-07, max=1.0)
        target_one_hot = torch.clamp(target_one_hot, min=0.0001, max=1.0)
        cross_entropy = (-torch.sum(target_one_hot * torch.log(input), dim=1)).mean()
        reverse_cross_entropy = (-torch.sum(input * torch.log(target_one_hot), dim=1)).mean()
        loss = self.alpha * cross_entropy + self.beta * reverse_cross_entropy
        return loss


class MaskCrossEntropyLoss(torch.nn.CrossEntropyLoss):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, *args, target_name: str='targets', mask_name: str='mask', **kwargs):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__(*args, **kwargs)
        self.target_name = target_name
        self.mask_name = mask_name
        self.reduction = 'none'

    def forward(self, input: torch.Tensor, target_mask: torch.Tensor) ->torch.Tensor:
        """Calculates loss between ``input`` and ``target_mask`` tensors.

        @TODO: Docs. Contribution is welcome.
        """
        target = target_mask[self.target_name]
        mask = target_mask[self.mask_name]
        loss = super().forward(input, target)
        loss = torch.mean(loss[mask == 1])
        return loss


def _convert_label_to_similarity(normed_features: Tensor, labels: Tensor) ->Tuple[Tensor, Tensor]:
    similarity_matrix = normed_features @ normed_features.transpose(1, 0)
    label_matrix = labels.unsqueeze(1) == labels.unsqueeze(0)
    positive_matrix = label_matrix.triu(diagonal=1)
    negative_matrix = label_matrix.logical_not().triu(diagonal=1)
    similarity_matrix = similarity_matrix.view(-1)
    positive_matrix = positive_matrix.view(-1)
    negative_matrix = negative_matrix.view(-1)
    sp, sn = similarity_matrix[positive_matrix], similarity_matrix[negative_matrix]
    return sp, sn


class CircleLoss(nn.Module):
    """
    CircleLoss from
    "Circle Loss: A Unified Perspective of Pair Similarity Optimization"
    https://arxiv.org/abs/2002.10857

    Adapter from:
    https://github.com/TinyZeaMays/CircleLoss

    Example:
        >>> import torch
        >>> from torch.nn import functional as F
        >>> from catalyst.contrib.nn import CircleLoss
        >>>
        >>> features = F.normalize(torch.rand(256, 64, requires_grad=True))
        >>> labels = torch.randint(high=10, size=(256,))
        >>> criterion = CircleLoss(margin=0.25, gamma=256)
        >>> criterion(features, labels)
    """

    def __init__(self, margin: float, gamma: float) ->None:
        """

        Args:
            margin: margin to use
            gamma: gamma to use
        """
        super().__init__()
        self.margin = margin
        self.gamma = gamma
        self.soft_plus = nn.Softplus()

    def forward(self, normed_features: Tensor, labels: Tensor) ->Tensor:
        """

        Args:
            normed_features: batch with samples features of shape
                [bs; feature_len]
            labels: batch with samples correct labels of shape [bs; ]

        Returns:
            (Tensor): circle loss
        """
        sp, sn = _convert_label_to_similarity(normed_features, labels)
        ap = torch.clamp_min(-sp.detach() + 1 + self.margin, min=0.0)
        an = torch.clamp_min(sn.detach() + self.margin, min=0.0)
        delta_p = 1 - self.margin
        delta_n = self.margin
        logit_p = -ap * (sp - delta_p) * self.gamma
        logit_n = an * (sn - delta_n) * self.gamma
        loss = self.soft_plus(torch.logsumexp(logit_n, dim=0) + torch.logsumexp(logit_p, dim=0))
        return loss


class ContrastiveEmbeddingLoss(nn.Module):
    """The Contrastive embedding loss.

    It has been proposed in `Dimensionality Reduction
    by Learning an Invariant Mapping`_.

    .. _Dimensionality Reduction by Learning an Invariant Mapping:
        http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    """

    def __init__(self, margin=1.0, reduction='mean'):
        """
        Args:
            margin: margin parameter
            reduction: criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, embeddings_left: torch.Tensor, embeddings_right: torch.Tensor, distance_true) ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Args:
            embeddings_left (torch.Tensor): left objects embeddings
            embeddings_right (torch.Tensor): right objects embeddings
            distance_true: true distances

        Returns:
            torch.Tensor: loss
        """
        diff = embeddings_left - embeddings_right
        distance_pred = torch.sqrt(torch.sum(torch.pow(diff, 2), 1))
        bs = len(distance_true)
        margin_distance = self.margin - distance_pred
        margin_distance_ = torch.clamp(margin_distance, min=0.0)
        loss = (1 - distance_true) * torch.pow(distance_pred, 2) + distance_true * torch.pow(margin_distance_, 2)
        if self.reduction == 'mean':
            loss = torch.sum(loss) / 2.0 / bs
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


class ContrastiveDistanceLoss(nn.Module):
    """The Contrastive distance loss.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, margin=1.0, reduction='mean'):
        """
        Args:
            margin: margin parameter
            reduction (str): criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, distance_pred, distance_true) ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Args:
            distance_pred: predicted distances
            distance_true: true distances

        Returns:
            torch.Tensor: loss
        """
        bs = len(distance_true)
        margin_distance = self.margin - distance_pred
        margin_distance_ = torch.clamp(margin_distance, min=0.0)
        loss = (1 - distance_true) * torch.pow(distance_pred, 2) + distance_true * torch.pow(margin_distance_, 2)
        if self.reduction == 'mean':
            loss = torch.sum(loss) / 2.0 / bs
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


class ContrastivePairwiseEmbeddingLoss(nn.Module):
    """ContrastivePairwiseEmbeddingLoss  proof of concept criterion.

    Still work in progress.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, margin=1.0, reduction='mean'):
        """
        Args:
            margin: margin parameter
            reduction: criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, embeddings_pred, embeddings_true) ->torch.Tensor:
        """Forward propagation method for the contrastive loss.

        Work in progress.

        Args:
            embeddings_pred: predicted embeddings
            embeddings_true: true embeddings

        Returns:
            torch.Tensor: loss
        """
        device = embeddings_pred.device
        pairwise_similarity = torch.einsum('se,ae->sa', embeddings_pred, embeddings_true)
        bs = embeddings_pred.shape[0]
        batch_idx = torch.arange(bs, device=device)
        loss = F.cross_entropy(pairwise_similarity, batch_idx, reduction=self.reduction)
        return loss


class DiceLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, eps: float=1e-07, threshold: float=None, activation: str='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.loss_fn = partial(metrics.dice, eps=eps, threshold=threshold, activation=activation)

    def forward(self, logits: torch.Tensor, targets: torch.Tensor):
        """Calculates loss between ``logits`` and ``target`` tensors.

        @TODO: Docs. Contribution is welcome
        """
        dice = self.loss_fn(logits, targets)
        return 1 - dice


class BCEDiceLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, eps: float=1e-07, threshold: float=None, activation: str='Sigmoid', bce_weight: float=0.5, dice_weight: float=0.5):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        if bce_weight == 0 and dice_weight == 0:
            raise ValueError('Both bce_wight and dice_weight cannot be equal to 0 at the same time.')
        self.bce_weight = bce_weight
        self.dice_weight = dice_weight
        if self.bce_weight != 0:
            self.bce_loss = nn.BCEWithLogitsLoss()
        if self.dice_weight != 0:
            self.dice_loss = DiceLoss(eps=eps, threshold=threshold, activation=activation)

    def forward(self, outputs, targets):
        """@TODO: Docs. Contribution is welcome."""
        if self.bce_weight == 0:
            return self.dice_weight * self.dice_loss(outputs, targets)
        if self.dice_weight == 0:
            return self.bce_weight * self.bce_loss(outputs, targets)
        dice = self.dice_weight * self.dice_loss(outputs, targets)
        bce = self.bce_weight * self.bce_loss(outputs, targets)
        return dice + bce


class FocalLossBinary(_Loss):
    """Compute focal loss for binary classification problem.

    It has been proposed in `Focal Loss for Dense Object Detection`_ paper.

    @TODO: Docs (add `Example`). Contribution is welcome.

    .. _Focal Loss for Dense Object Detection:
        https://arxiv.org/abs/1708.02002
    """

    def __init__(self, ignore: int=None, reduced: bool=False, gamma: float=2.0, alpha: float=0.25, threshold: float=0.5, reduction: str='mean'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        if reduced:
            self.loss_fn = partial(metrics.reduced_focal_loss, gamma=gamma, threshold=threshold, reduction=reduction)
        else:
            self.loss_fn = partial(metrics.sigmoid_focal_loss, gamma=gamma, alpha=alpha, reduction=reduction)

    def forward(self, logits, targets):
        """
        Args:
            logits: [bs; ...]
            targets: [bs; ...]

        @TODO: Docs. Contribution is welcome.
        """
        targets = targets.view(-1)
        logits = logits.view(-1)
        if self.ignore is not None:
            not_ignored = targets != self.ignore
            logits = logits[not_ignored]
            targets = targets[not_ignored]
        loss = self.loss_fn(logits, targets)
        return loss


class FocalLossMultiClass(FocalLossBinary):
    """Compute focal loss for multi-class problem.
    Ignores targets having -1 label.

    It has been proposed in `Focal Loss for Dense Object Detection`_ paper.

    @TODO: Docs (add `Example`). Contribution is welcome.

    .. _Focal Loss for Dense Object Detection:
        https://arxiv.org/abs/1708.02002
    """

    def forward(self, logits, targets):
        """
        Args:
            logits: [bs; num_classes; ...]
            targets: [bs; ...]

        @TODO: Docs. Contribution is welcome.
        """
        num_classes = logits.size(1)
        loss = 0
        targets = targets.view(-1)
        logits = logits.view(-1, num_classes)
        if self.ignore is not None:
            not_ignored = targets != self.ignore
        for cls in range(num_classes):
            cls_label_target = (targets == cls + 0).long()
            cls_label_input = logits[..., cls]
            if self.ignore is not None:
                cls_label_target = cls_label_target[not_ignored]
                cls_label_input = cls_label_input[not_ignored]
            loss += self.loss_fn(cls_label_input, cls_label_target)
        return loss


class MeanOutputLoss(nn.Module):
    """
    Criterion to compute simple mean of the output, completely ignoring target
    (maybe useful e.g. for WGAN real/fake validity averaging.
    """

    def forward(self, output, target):
        """Compute criterion.

        @TODO: Docs (add typing). Contribution is welcome.
        """
        return output.mean()


class GradientPenaltyLoss(nn.Module):
    """Criterion to compute gradient penalty.

    WARN: SHOULD NOT BE RUN WITH CriterionCallback,
        use special GradientPenaltyCallback instead
    """

    def forward(self, fake_data, real_data, critic, critic_condition_args):
        """Compute gradient penalty.

        Args:
            @TODO: Docs. Contribution is welcome.
        """
        device = real_data.device
        alpha = torch.rand((real_data.size(0), 1, 1, 1), device=device)
        interpolates = (alpha * real_data + (1 - alpha) * fake_data).detach()
        interpolates.requires_grad_(True)
        with torch.set_grad_enabled(True):
            d_interpolates = critic(interpolates, *critic_condition_args)
        fake = torch.ones((real_data.size(0), 1), device=device, requires_grad=False)
        gradients = torch.autograd.grad(outputs=d_interpolates, inputs=interpolates, grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True)[0]
        gradients = gradients.view(gradients.size(0), -1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        return gradient_penalty


class HuberLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, clip_delta=1.0, reduction='mean'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.clip_delta = clip_delta
        self.reduction = reduction or 'none'

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor, weights=None) ->torch.Tensor:
        """@TODO: Docs. Contribution is welcome."""
        td_error = y_true - y_pred
        td_error_abs = torch.abs(td_error)
        quadratic_part = torch.clamp(td_error_abs, max=self.clip_delta)
        linear_part = td_error_abs - quadratic_part
        loss = 0.5 * quadratic_part ** 2 + self.clip_delta * linear_part
        if weights is not None:
            loss = torch.mean(loss * weights, dim=1)
        else:
            loss = torch.mean(loss, dim=1)
        if self.reduction == 'mean':
            loss = torch.mean(loss)
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


class IoULoss(nn.Module):
    """The intersection over union (Jaccard) loss.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, eps: float=1e-07, threshold: float=None, activation: str='Sigmoid'):
        """
        Args:
            eps (float): epsilon to avoid zero division
            threshold (float): threshold for outputs binarization
            activation (str): An torch.nn activation applied to the outputs.
                Must be one of ``'none'``, ``'Sigmoid'``, ``'Softmax2d'``
        """
        super().__init__()
        self.metric_fn = partial(metrics.iou, eps=eps, threshold=threshold, activation=activation)

    def forward(self, outputs, targets):
        """@TODO: Docs. Contribution is welcome."""
        iou = self.metric_fn(outputs, targets)
        return 1 - iou


class BCEIoULoss(nn.Module):
    """The Intersection over union (Jaccard) with BCE loss.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, eps: float=1e-07, threshold: float=None, activation: str='Sigmoid', reduction: str='mean'):
        """
        Args:
            eps (float): epsilon to avoid zero division
            threshold (float): threshold for outputs binarization
            activation (str): An torch.nn activation applied to the outputs.
                Must be one of ``'none'``, ``'Sigmoid'``, ``'Softmax2d'``
            reduction (str): Specifies the reduction to apply
                to the output of BCE
        """
        super().__init__()
        self.bce_loss = nn.BCEWithLogitsLoss(reduction=reduction)
        self.iou_loss = IoULoss(eps, threshold, activation)

    def forward(self, outputs, targets):
        """@TODO: Docs. Contribution is welcome."""
        iou = self.iou_loss.forward(outputs, targets)
        bce = self.bce_loss(outputs, targets)
        loss = iou + bce
        return loss


def _flatten_binary_scores(logits, targets, ignore=None):
    """
    Flattens predictions in the batch (binary case).
    Remove targets equal to "ignore"
    """
    logits = logits.reshape(-1)
    targets = targets.reshape(-1)
    if ignore is None:
        return logits, targets
    valid = targets != ignore
    logits_ = logits[valid]
    targets_ = targets[valid]
    return logits_, targets_


def _lovasz_grad(gt_sorted):
    """
    Compute gradient of the Lovasz extension w.r.t sorted errors,
    see Alg. 1 in paper
    """
    p = len(gt_sorted)
    gts = gt_sorted.sum()
    intersection = gts - gt_sorted.float().cumsum(0)
    union = gts + (1 - gt_sorted).float().cumsum(0)
    jaccard = 1.0 - intersection / union
    if p > 1:
        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]
    return jaccard


def _lovasz_hinge_flat(logits, targets):
    """The binary Lovasz hinge loss.

    Args:
        logits: [P] Variable, logits at each prediction
            (between -iinfinity and +iinfinity)
        targets: [P] Tensor, binary ground truth targets (0 or 1)
    """
    if len(targets) == 0:
        return logits.sum() * 0.0
    signs = 2.0 * targets.float() - 1.0
    errors = 1.0 - logits * signs
    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)
    perm = perm.data
    gt_sorted = targets[perm]
    grad = _lovasz_grad(gt_sorted)
    loss = torch.dot(F.relu(errors_sorted), grad)
    return loss


def isnan(x):
    return x != x


def mean(values, ignore_nan=False, empty=0):
    """
    Nanmean compatible with generators.
    """
    values = iter(values)
    if ignore_nan:
        values = ifilterfalse(isnan, values)
    try:
        n = 1
        acc = next(values)
    except StopIteration:
        if empty == 'raise':
            raise ValueError('Empty mean')
        return empty
    for n, v in enumerate(values, 2):
        acc += v
    if n == 1:
        return acc
    return acc / n


def _lovasz_hinge(logits, targets, per_image=True, ignore=None):
    """The binary Lovasz hinge loss.

    Args:
        logits: [B, H, W] Variable, logits at each pixel
            (between -infinity and +infinity)
        targets: [B, H, W] Tensor, binary ground truth masks (0 or 1)
        per_image: compute the loss per image instead of per batch
        ignore: void class id
    """
    if per_image:
        loss = mean(_lovasz_hinge_flat(*_flatten_binary_scores(logit.unsqueeze(0), target.unsqueeze(0), ignore)) for logit, target in zip(logits, targets))
    else:
        loss = _lovasz_hinge_flat(*_flatten_binary_scores(logits, targets, ignore))
    return loss


class LovaszLossBinary(_Loss):
    """Creates a criterion that optimizes a binary Lovasz loss.

    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate
    for the optimization of the intersection-over-union measure
    in neural networks`_.

    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization
        of the intersection-over-union measure in neural networks:
        https://arxiv.org/abs/1705.08790
    """

    def __init__(self, per_image=False, ignore=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        self.per_image = per_image

    def forward(self, logits, targets):
        """Forward propagation method for the Lovasz loss.

        Args:
            logits: [bs; ...]
            targets: [bs; ...]

        @TODO: Docs. Contribution is welcome.
        """
        loss = _lovasz_hinge(logits, targets, per_image=self.per_image, ignore=self.ignore)
        return loss


def _flatten_probabilities(probabilities, targets, ignore=None):
    """
    Flattens predictions in the batch
    """
    if probabilities.dim() == 3:
        B, H, W = probabilities.size()
        probabilities = probabilities.view(B, 1, H, W)
    B, C, H, W = probabilities.size()
    probabilities = probabilities.permute(0, 2, 3, 1).contiguous().view(-1, C)
    targets = targets.view(-1)
    if ignore is None:
        return probabilities, targets
    valid = targets != ignore
    probabilities_ = probabilities[valid.nonzero().squeeze()]
    targets_ = targets[valid]
    return probabilities_, targets_


def _lovasz_softmax_flat(probabilities, targets, classes='present'):
    """The multi-class Lovasz-Softmax loss.

    Args:
        probabilities: [P, C]
            class probabilities at each prediction (between 0 and 1)
        targets: [P] ground truth targets (between 0 and C - 1)
        classes: "all" for all,
            "present" for classes present in targets,
             or a list of classes to average.
    """
    if probabilities.numel() == 0:
        return probabilities * 0.0
    C = probabilities.size(1)
    losses = []
    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes
    for c in class_to_sum:
        fg = (targets == c).float()
        if classes == 'present' and fg.sum() == 0:
            continue
        if C == 1:
            if len(class_to_sum) > 1:
                raise ValueError('Sigmoid output possible only with 1 class')
            class_pred = probabilities[:, (0)]
        else:
            class_pred = probabilities[:, (c)]
        errors = (fg - class_pred).abs()
        errors_sorted, perm = torch.sort(errors, 0, descending=True)
        perm = perm.data
        fg_sorted = fg[perm]
        losses.append(torch.dot(errors_sorted, _lovasz_grad(fg_sorted)))
    return mean(losses)


def _lovasz_softmax(probabilities, targets, classes='present', per_image=False, ignore=None):
    """The multi-class Lovasz-Softmax loss.

    Args:
        probabilities: [B, C, H, W]
            class probabilities at each prediction (between 0 and 1).
            Interpreted as binary (sigmoid) output
            with outputs of size [B, H, W].
        targets: [B, H, W] ground truth targets (between 0 and C - 1)
        classes: "all" for all,
            "present" for classes present in targets,
            or a list of classes to average.
        per_image: compute the loss per image instead of per batch
        ignore: void class targets
    """
    if per_image:
        loss = mean(_lovasz_softmax_flat(*_flatten_probabilities(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes) for prob, lab in zip(probabilities, targets))
    else:
        loss = _lovasz_softmax_flat(*_flatten_probabilities(probabilities, targets, ignore), classes=classes)
    return loss


class LovaszLossMultiClass(_Loss):
    """Creates a criterion that optimizes a multi-class Lovasz loss.

    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate
    for the optimization of the intersection-over-union measure
    in neural networks`_.

    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization
        of the intersection-over-union measure in neural networks:
        https://arxiv.org/abs/1705.08790
    """

    def __init__(self, per_image=False, ignore=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        self.per_image = per_image

    def forward(self, logits, targets):
        """Forward propagation method for the Lovasz loss.

        Args:
            logits: [bs; num_classes; ...]
            targets: [bs; ...]

        @TODO: Docs. Contribution is welcome.
        """
        loss = _lovasz_softmax(logits, targets, per_image=self.per_image, ignore=self.ignore)
        return loss


class LovaszLossMultiLabel(_Loss):
    """Creates a criterion that optimizes a multi-label Lovasz loss.

    It has been proposed in `The Lovasz-Softmax loss: A tractable surrogate
    for the optimization of the intersection-over-union measure
    in neural networks`_.

    .. _The Lovasz-Softmax loss\\: A tractable surrogate for the optimization
        of the intersection-over-union measure in neural networks:
        https://arxiv.org/abs/1705.08790
    """

    def __init__(self, per_image=False, ignore=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.ignore = ignore
        self.per_image = per_image

    def forward(self, logits, targets):
        """Forward propagation method for the Lovasz loss.

        Args:
            logits: [bs; num_classes; ...]
            targets: [bs; num_classes; ...]

        @TODO: Docs. Contribution is welcome.
        """
        losses = [_lovasz_hinge(logits[:, (i), (...)], targets[:, (i), (...)], per_image=self.per_image, ignore=self.ignore) for i in range(logits.shape[1])]
        loss = torch.mean(torch.stack(losses))
        return loss


_EPS = 1e-08


def _create_margin_mask(labels: torch.Tensor) ->torch.Tensor:
    equal_labels_mask = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))
    marign_mask = 2 * equal_labels_mask.float() - 1
    return marign_mask


def _skip_labels_mask(labels: torch.Tensor, skip_labels: Union[int, List[int]]) ->torch.Tensor:
    skip_labels = torch.tensor(skip_labels, dtype=labels.dtype, device=labels.device).reshape(-1)
    skip_condition = (labels.unsqueeze(-1) == skip_labels).any(-1)
    skip_mask = ~(skip_condition.unsqueeze(-1) & skip_condition.unsqueeze(0))
    return skip_mask


def euclidean_distance(x: torch.Tensor, y: torch.Tensor=None) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    x_norm = (x ** 2).sum(1).unsqueeze(1)
    if y is not None:
        y_norm = (y ** 2).sum(1).unsqueeze(0)
    else:
        y = x
        y_norm = x_norm.t()
    dist = x_norm + y_norm - 2.0 * torch.mm(x, torch.transpose(y, 0, 1))
    dist.clamp_min_(0.0)
    return dist


def margin_loss(embeddings: torch.Tensor, labels: torch.Tensor, alpha: float=0.2, beta: float=1.0, skip_labels: Union[int, List[int]]=-1) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    embeddings = F.normalize(embeddings, p=2.0, dim=1)
    distances = euclidean_distance(embeddings, embeddings)
    margin_mask = _create_margin_mask(labels)
    skip_mask = _skip_labels_mask(labels, skip_labels).float()
    loss = torch.mul(skip_mask, F.relu(alpha + torch.mul(margin_mask, torch.sub(distances, beta))))
    return loss.sum() / (skip_mask.sum() + _EPS)


class MarginLoss(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, alpha: float=0.2, beta: float=1.0, skip_labels: Union[int, List[int]]=-1):
        """
        Args:
            alpha (float):
            beta (float):
            skip_labels (int or List[int]):

        @TODO: Docs. Contribution is welcome.
        """
        super().__init__()
        self.alpha = alpha
        self.beta = beta
        self.skip_labels = skip_labels

    def forward(self, embeddings: torch.Tensor, targets: torch.Tensor) ->torch.Tensor:
        """Forward propagation method for the margin loss.

        @TODO: Docs. Contribution is welcome.
        """
        return margin_loss(embeddings, targets, alpha=self.alpha, beta=self.beta, skip_labels=self.skip_labels)


class TripletLoss(nn.Module):
    """Triplet loss with hard positive/negative mining.

    Reference:
        Code imported from https://github.com/NegatioN/OnlineMiningTripletLoss
    """

    def __init__(self, margin: float=0.3):
        """
        Args:
            margin (float): margin for triplet
        """
        super().__init__()
        self.margin = margin
        self.ranking_loss = nn.MarginRankingLoss(margin=margin)

    def _pairwise_distances(self, embeddings, squared=False):
        """Compute the 2D matrix of distances between all the embeddings.

        Args:
            embeddings: tensor of shape (batch_size, embed_dim)
            squared (bool): if true, output is the pairwise squared euclidean
                distance matrix. If false, output is the pairwise euclidean
                distance matrix

        Returns:
            torch.Tensor: pairwise matrix of size (batch_size, batch_size)
        """
        square = torch.mm(embeddings, embeddings.t())
        diag = torch.diag(square)
        distances = diag.view(-1, 1) - 2.0 * square + diag.view(1, -1)
        distances[distances < 0] = 0
        if not squared:
            mask = distances.eq(0).float()
            distances = distances + mask * 1e-16
            distances = (1.0 - mask) * torch.sqrt(distances)
        return distances

    def _get_anchor_positive_triplet_mask(self, labels):
        """
        Return a 2D mask where mask[a, p] is True
        if a and p are distinct and have same label.

        Args:
            labels: tf.int32 `Tensor` with shape [batch_size]

        Returns:
            mask: tf.bool `Tensor` with shape [batch_size, batch_size]
        """
        indices_equal = torch.eye(labels.size(0)).bool()
        indices_equal = indices_equal
        indices_not_equal = ~indices_equal
        labels_equal = labels.unsqueeze(0) == labels.unsqueeze(1)
        return labels_equal & indices_not_equal

    def _get_anchor_negative_triplet_mask(self, labels):
        """Return 2D mask where mask[a, n] is True if a and n have same label.

        Args:
            labels: tf.int32 `Tensor` with shape [batch_size]

        Returns:
            mask: tf.bool `Tensor` with shape [batch_size, batch_size]
        """
        return ~(labels.unsqueeze(0) == labels.unsqueeze(1))

    def _batch_hard_triplet_loss(self, embeddings, labels, margin, squared=True):
        """
        Build the triplet loss over a batch of embeddings.
        For each anchor, we get the hardest positive and
        hardest negative to form a triplet.

        Args:
            labels: labels of the batch, of size (batch_size,)
            embeddings: tensor of shape (batch_size, embed_dim)
            margin: margin for triplet loss
            squared: Boolean. If true, output is the pairwise squared
                     euclidean distance matrix. If false, output is the
                     pairwise euclidean distance matrix.

        Returns:
            triplet_loss: scalar tensor containing the triplet loss
        """
        pairwise_dist = self._pairwise_distances(embeddings, squared=squared)
        mask_anchor_positive = self._get_anchor_positive_triplet_mask(labels).float()
        anchor_positive_dist = mask_anchor_positive * pairwise_dist
        hardest_positive_dist, _ = anchor_positive_dist.max(1, keepdim=True)
        mask_anchor_negative = self._get_anchor_negative_triplet_mask(labels).float()
        max_anchor_negative_dist, _ = pairwise_dist.max(1, keepdim=True)
        anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)
        hardest_negative_dist, _ = anchor_negative_dist.min(1, keepdim=True)
        tl = hardest_positive_dist - hardest_negative_dist + margin
        tl[tl < 0] = 0
        triplet_loss = tl.mean()
        return triplet_loss

    def forward(self, embeddings, targets):
        """Forward propagation method for the triplet loss.

        Args:
            embeddings: tensor of shape (batch_size, embed_dim)
            targets: labels of the batch, of size (batch_size,)

        Returns:
            triplet_loss: scalar tensor containing the triplet loss
        """
        return self._batch_hard_triplet_loss(embeddings, targets, self.margin)


def create_negative_mask(labels: torch.Tensor, neg_label: int=-1) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    neg_labels = torch.ge(labels, neg_label)
    pos_labels = ~neg_labels
    i_less_neg = pos_labels.unsqueeze(1).unsqueeze(2)
    j_less_neg = pos_labels.unsqueeze(1).unsqueeze(0)
    k_less_neg = pos_labels.unsqueeze(0).unsqueeze(0)
    anchors = labels.unsqueeze(1).unsqueeze(2)
    negatives = labels.unsqueeze(0).unsqueeze(0)
    k_equal = torch.eq(anchors + neg_label, negatives)
    k_less_or_equal = k_equal | k_less_neg
    mask = i_less_neg & j_less_neg & k_less_or_equal
    return mask


def batch_all(labels: torch.Tensor, exclude_negatives: bool=True) ->torch.Tensor:
    """Create a 3D mask of all possible triplets.

    Args:
        @TODO: Docs. Contribution is welcome.
    """
    batch_size = labels.size(0)
    indices_equal = torch.eye(batch_size, device=labels.device).type(torch.bool)
    indices_not_equal = ~indices_equal
    i_not_equal_j = indices_not_equal.unsqueeze(2)
    i_not_equal_k = indices_not_equal.unsqueeze(1)
    j_not_equal_k = indices_not_equal.unsqueeze(0)
    distinct_indices = i_not_equal_j & i_not_equal_k & j_not_equal_k
    label_equal = torch.eq(labels.unsqueeze(0), labels.unsqueeze(1))
    yi_equal_yj = label_equal.unsqueeze(2)
    yi_equal_yk = label_equal.unsqueeze(1)
    yi_not_equal_yk = ~yi_equal_yk
    valid_labels = yi_equal_yj & yi_not_equal_yk
    mask = distinct_indices & valid_labels
    if exclude_negatives:
        mask = mask & create_negative_mask(labels)
    return mask.float()


def cosine_distance(x: torch.Tensor, z: Optional[torch.Tensor]=None) ->torch.Tensor:
    """Calculate cosine distance between x and z.

    Args:
        @TODO: Docs. Contribution is welcome.
    """
    x = F.normalize(x)
    if z is not None:
        z = F.normalize(z)
    else:
        z = x.clone()
    return torch.sub(1, torch.mm(x, z.transpose(0, 1)))


def triplet_loss(embeddings: torch.Tensor, labels: torch.Tensor, margin: float=0.3) ->torch.Tensor:
    """@TODO: Docs. Contribution is welcome."""
    cosine_dists = cosine_distance(embeddings)
    mask = batch_all(labels)
    anchor_positive_dist = cosine_dists.unsqueeze(2)
    anchor_negative_dist = cosine_dists.unsqueeze(1)
    triplet_loss_value = F.relu(anchor_positive_dist - anchor_negative_dist + margin)
    triplet_loss_value = torch.mul(triplet_loss_value, mask)
    num_positive_triplets = torch.gt(triplet_loss_value, _EPS).sum().float()
    triplet_loss_value = triplet_loss_value.sum() / (num_positive_triplets + _EPS)
    return triplet_loss_value


class TripletLossV2(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, margin=0.3):
        """
        Args:
            margin (float): margin for triplet.
        """
        super().__init__()
        self.margin = margin

    def forward(self, embeddings, targets):
        """@TODO: Docs. Contribution is welcome."""
        return triplet_loss(embeddings, targets, margin=self.margin)


class TripletPairwiseEmbeddingLoss(nn.Module):
    """TripletPairwiseEmbeddingLoss  proof of concept criterion.

    Still work in progress.

    @TODO: Docs. Contribution is welcome.
    """

    def __init__(self, margin: float=0.3, reduction: str='mean'):
        """
        Args:
            margin (float): margin parameter
            reduction (str): criterion reduction type
        """
        super().__init__()
        self.margin = margin
        self.reduction = reduction or 'none'

    def forward(self, embeddings_pred, embeddings_true):
        """
        Work in progress.

        Args:
            embeddings_pred: predicted embeddings
                with shape [batch_size, embedding_size]
            embeddings_true: true embeddings
                with shape [batch_size, embedding_size]

        Returns:
            torch.Tensor: loss
        """
        device = embeddings_pred.device
        pairwise_similarity = torch.einsum('se,ae->sa', embeddings_pred, embeddings_true)
        bs = embeddings_pred.shape[0]
        batch_idx = torch.arange(bs, device=device)
        negative_similarity = pairwise_similarity + torch.diag(torch.full([bs], -10 ** 9, device=device))
        hard_negative_ids = negative_similarity.argmax(dim=-1)
        negative_similarities = pairwise_similarity[batch_idx, hard_negative_ids]
        positive_similarities = pairwise_similarity[batch_idx, batch_idx]
        loss = torch.relu(self.margin - positive_similarities + negative_similarities)
        if self.reduction == 'mean':
            loss = torch.sum(loss) / bs
        elif self.reduction == 'sum':
            loss = torch.sum(loss)
        return loss


def wing_loss(outputs: torch.Tensor, targets: torch.Tensor, width: int=5, curvature: float=0.5, reduction: str='mean') ->torch.Tensor:
    """The Wing loss.

    It has been proposed in `Wing Loss for Robust Facial Landmark Localisation
    with Convolutional Neural Networks`_.

    Args:
        @TODO: Docs. Contribution is welcome.

    Adapted from:
    https://github.com/BloodAxe/pytorch-toolbelt (MIT License)

    .. _Wing Loss for Robust Facial Landmark Localisation with Convolutional
        Neural Networks: https://arxiv.org/abs/1711.06753
    """
    diff_abs = (targets - outputs).abs()
    loss = diff_abs.clone()
    idx_smaller = diff_abs < width
    idx_bigger = diff_abs >= width
    loss[idx_smaller] = width * torch.log(1 + diff_abs[idx_smaller] / curvature)
    c = width - width * math.log(1 + width / curvature)
    loss[idx_bigger] = loss[idx_bigger] - c
    if reduction == 'sum':
        loss = loss.sum()
    if reduction == 'mean':
        loss = loss.mean()
    return loss


class WingLoss(nn.Module):
    """Creates a criterion that optimizes a Wing loss.

    It has been proposed in `Wing Loss for Robust Facial Landmark Localisation
    with Convolutional Neural Networks`_.

    Examples:
        @TODO: Docs. Contribution is welcome.

    Adapted from:
    https://github.com/BloodAxe/pytorch-toolbelt

    .. _Wing Loss for Robust Facial Landmark Localisation with Convolutional
        Neural Networks: https://arxiv.org/abs/1711.06753
    """

    def __init__(self, width: int=5, curvature: float=0.5, reduction: str='mean'):
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        super().__init__()
        self.loss_fn = partial(wing_loss, width=width, curvature=curvature, reduction=reduction)

    def forward(self, outputs: torch.Tensor, targets: torch.Tensor) ->torch.Tensor:
        """
        Args:
            @TODO: Docs. Contribution is welcome.
        """
        loss = self.loss_fn(outputs, targets)
        return loss


class Flatten(nn.Module):
    """Flattens the input. Does not affect the batch size.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()

    def forward(self, x):
        """Forward call."""
        return x.view(x.shape[0], -1)


class Lambda(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, lambda_fn):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.lambda_fn = lambda_fn

    def forward(self, x):
        """Forward call."""
        return self.lambda_fn(x)


class GaussianNoise(nn.Module):
    """
    A gaussian noise module.

    Shape:

    - Input: (batch, \\*)
    - Output: (batch, \\*) (same shape as input)
    """

    def __init__(self, stddev: float=0.1):
        """
        Args:
            stddev (float): The standard deviation of the normal distribution.
                Default: 0.1.
        """
        super().__init__()
        self.stddev = stddev

    def forward(self, x: torch.Tensor):
        """Forward call."""
        noise = torch.empty_like(x)
        noise.normal_(0, self.stddev)


class TemporalLastPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) ->torch.Tensor:
        """Forward call."""
        x_out = x[:, -1:, :]
        return x_out


class TemporalAvgPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) ->torch.Tensor:
        """Forward call."""
        if mask is None:
            x_out = x.mean(1, keepdim=True)
        else:
            x_ = torch.sum(x * mask.float(), dim=1, keepdim=True)
            mask_ = torch.sum(mask.float(), dim=1, keepdim=True)
            x_out = x_ / mask_
        return x_out


class TemporalMaxPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) ->torch.Tensor:
        """Forward call."""
        if mask is not None:
            mask_ = (~mask.bool()).float() * (-x.max()).float()
            x = torch.sum(x + mask_, dim=1, keepdim=True)
        x_out = x.max(1, keepdim=True)[0]
        return x_out


def outer_init(layer: nn.Module) ->None:
    """
    Initialization for output layers of policy and value networks typically
    used in deep reinforcement learning literature.
    """
    if isinstance(layer, (nn.Linear, nn.Conv1d, nn.Conv2d)):
        v = 0.003
        nn.init.uniform_(layer.weight.data, -v, v)
        if layer.bias is not None:
            nn.init.uniform_(layer.bias.data, -v, v)


class TemporalAttentionPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""
    name2activation = {'softmax': nn.Softmax(dim=1), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid()}

    def __init__(self, in_features, activation=None, kernel_size=1, **params):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_features = in_features
        activation = activation or 'softmax'
        self.attention_pooling = nn.Sequential(nn.Conv1d(in_channels=in_features, out_channels=1, kernel_size=kernel_size, **params), TemporalAttentionPooling.name2activation[activation])
        self.attention_pooling.apply(outer_init)

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) ->torch.Tensor:
        """
        Args:
            x (torch.Tensor): tensor of size
                (batch_size, history_len, feature_size)

        @TODO: Docs. Contribution is welcome.
        """
        batch_size, history_len, feature_size = x.shape
        x = x.view(batch_size, history_len, -1)
        x_a = x.transpose(1, 2)
        x_attn = (self.attention_pooling(x_a) * x_a).transpose(1, 2)
        x_attn = x_attn.sum(1, keepdim=True)
        return x_attn


class TemporalConcatPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_features, history_len=1):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_features = in_features
        self.out_features = in_features * history_len

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) ->torch.Tensor:
        """
        Args:
            x (torch.Tensor): tensor of size
                (batch_size, history_len, feature_size)

        @TODO: Docs. Contribution is welcome.
        """
        x = x.view(x.shape[0], -1)
        return x


class TemporalDropLastWrapper(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, net):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.net = net

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None):
        """@TODO: Docs. Contribution is welcome."""
        x = x[:, :-1, :]
        x_out = self.net(x)
        return x_out


def _get_pooling(key, in_features, **params):
    """@TODO: Docs. Contribution is welcome."""
    key_ = key.split('_', 1)[0]
    if key_ == 'last':
        return TemporalLastPooling()
    elif key_ == 'avg':
        layer = TemporalAvgPooling()
    elif key_ == 'max':
        layer = TemporalMaxPooling()
    elif key_ in ['softmax', 'tanh', 'sigmoid']:
        layer = TemporalAttentionPooling(in_features=in_features, activation=key_, **params)
    else:
        raise NotImplementedError()
    if 'droplast' in key:
        layer = TemporalDropLastWrapper(layer)
    return layer


class LamaPooling(nn.Module):
    """@TODO: Docs. Contribution is welcome."""
    available_groups = ['last', 'avg', 'avg_droplast', 'max', 'max_droplast', 'sigmoid', 'sigmoid_droplast', 'softmax', 'softmax_droplast', 'tanh', 'tanh_droplast']

    def __init__(self, in_features, groups=None):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.in_features = in_features
        self.groups = groups or ['last', 'avg_droplast', 'max_droplast', 'softmax_droplast']
        self.out_features = in_features * len(self.groups)
        groups = {}
        for key in self.groups:
            if isinstance(key, str):
                groups[key] = _get_pooling(key, self.in_features)
            elif isinstance(key, dict):
                key_ = key.pop('key')
                groups[key_] = _get_pooling(key_, in_features, **key)
            else:
                raise NotImplementedError()
        self.groups = nn.ModuleDict(groups)

    def forward(self, x: torch.Tensor, mask: torch.Tensor=None) ->torch.Tensor:
        """
        Args:
            x (torch.Tensor): tensor of size
                (batch_size, history_len, feature_size)

        @TODO: Docs. Contribution is welcome.
        """
        batch_size, history_len, feature_size = x.shape
        x_ = []
        for pooling_fn in self.groups.values():
            features_ = pooling_fn(x, mask)
            x_.append(features_)
        x = torch.cat(x_, dim=1)
        x = x.view(batch_size, -1)
        return x


class GlobalAvgPool2d(nn.Module):
    """Applies a 2D global average pooling operation over an input signal
    composed of several input planes.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self):
        """Constructor method for the ``GlobalAvgPool2d`` class."""
        super().__init__()

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        h, w = x.shape[2:]
        return F.avg_pool2d(input=x, kernel_size=(h, w))

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features


class GlobalMaxPool2d(nn.Module):
    """Applies a 2D global max pooling operation over an input signal
    composed of several input planes.

    @TODO: Docs (add `Example`). Contribution is welcome.
    """

    def __init__(self):
        """Constructor method for the ``GlobalMaxPool2d`` class."""
        super().__init__()

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        h, w = x.shape[2:]
        return F.max_pool2d(input=x, kernel_size=(h, w))

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features


class GlobalConcatPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self):
        """Constructor method for the ``GlobalConcatPool2d`` class."""
        super().__init__()
        self.avg = GlobalAvgPool2d()
        self.max = GlobalMaxPool2d()

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.avg(x), self.max(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features * 2


class GlobalAttnPool2d(nn.Module):
    """@TODO: Docs. Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        activation_fn = MODULES.get_if_str(activation_fn)
        self.attn = nn.Sequential(nn.Conv2d(in_features, 1, kernel_size=1, stride=1, padding=0, bias=False), activation_fn())

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        x_a = self.attn(x)
        x = x * x_a
        x = torch.sum(x, dim=[-2, -1], keepdim=True)
        return x

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features


class GlobalAvgAttnPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.avg = GlobalAvgPool2d()
        self.attn = GlobalAttnPool2d(in_features, activation_fn)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.avg(x), self.attn(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features * 2


class GlobalMaxAttnPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.max = GlobalMaxPool2d()
        self.attn = GlobalAttnPool2d(in_features, activation_fn)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.max(x), self.attn(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features * 2


class GlobalConcatAttnPool2d(nn.Module):
    """@TODO: Docs (add `Example`). Contribution is welcome."""

    def __init__(self, in_features, activation_fn='Sigmoid'):
        """@TODO: Docs. Contribution is welcome."""
        super().__init__()
        self.avg = GlobalAvgPool2d()
        self.max = GlobalMaxPool2d()
        self.attn = GlobalAttnPool2d(in_features, activation_fn)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """Forward call."""
        return torch.cat([self.avg(x), self.max(x), self.attn(x)], 1)

    @staticmethod
    def out_features(in_features):
        """Returns number of channels produced by the pooling.

        Args:
            in_features: number of channels in the input sample
        """
        return in_features * 3


class RMSNorm(nn.Module):
    """An implementation of RMS Normalization.

    @TODO: Docs (link to paper). Contribution is welcome.
    """

    def __init__(self, dimension: int, epsilon: float=1e-08, is_bias: bool=False):
        """
        Args:
            dimension (int): the dimension of the layer output to normalize
            epsilon (float): an epsilon to prevent dividing by zero
                in case the layer has zero variance. (default = 1e-8)
            is_bias (bool): a boolean value whether to include bias term
                while normalization
        """
        super().__init__()
        self.dimension = dimension
        self.epsilon = epsilon
        self.is_bias = is_bias
        self.scale = nn.Parameter(torch.ones(self.dimension))
        if self.is_bias:
            self.bias = nn.Parameter(torch.zeros(self.dimension))

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        """@TODO: Docs. Contribution is welcome."""
        x_std = torch.sqrt(torch.mean(x ** 2, -1, keepdim=True))
        x_norm = x / (x_std + self.epsilon)
        if self.is_bias:
            return self.scale * x_norm + self.bias
        return self.scale * x_norm


class SqueezeAndExcitation(nn.Module):
    """
    The channel-wise SE (Squeeze and Excitation) block from the
    [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) paper.

    Adapted from
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65939
    and
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178

    Shape:

    - Input: (batch, channels, height, width)
    - Output: (batch, channels, height, width) (same shape as input)
    """

    def __init__(self, in_channels: int, r: int=16):
        """
        Args:
            in_channels (int): The number of channels
                in the feature map of the input.
            r (int): The reduction ratio of the intermediate channels.
                Default: 16.
        """
        super().__init__()
        self.linear_1 = nn.Linear(in_channels, in_channels // r)
        self.linear_2 = nn.Linear(in_channels // r, in_channels)

    def forward(self, x: torch.Tensor):
        """Forward call."""
        input_x = x
        x = x.view(*x.shape[:-2], -1).mean(-1)
        x = F.relu(self.linear_1(x), inplace=True)
        x = self.linear_2(x)
        x = x.unsqueeze(-1).unsqueeze(-1)
        x = torch.sigmoid(x)
        x = torch.mul(input_x, x)
        return x


class ChannelSqueezeAndSpatialExcitation(nn.Module):
    """
    The sSE (Channel Squeeze and Spatial Excitation) block from the
    [Concurrent Spatial and Channel Squeeze & Excitation
    in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579) paper.

    Adapted from
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178

    Shape:

    - Input: (batch, channels, height, width)
    - Output: (batch, channels, height, width) (same shape as input)
    """

    def __init__(self, in_channels: int):
        """
        Args:
            in_channels (int): The number of channels
                in the feature map of the input.
        """
        super().__init__()
        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1, stride=1)

    def forward(self, x: torch.Tensor):
        """Forward call."""
        input_x = x
        x = self.conv(x)
        x = torch.sigmoid(x)
        x = torch.mul(input_x, x)
        return x


class ConcurrentSpatialAndChannelSqueezeAndChannelExcitation(nn.Module):
    """
    The scSE (Concurrent Spatial and Channel Squeeze and Channel Excitation)
    block from the [Concurrent Spatial and Channel Squeeze & Excitation
    in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579) paper.

    Adapted from
    https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/66178

    Shape:

    - Input: (batch, channels, height, width)
    - Output: (batch, channels, height, width) (same shape as input)
    """

    def __init__(self, in_channels: int, r: int=16):
        """
        Args:
            in_channels (int): The number of channels
                in the feature map of the input.
            r (int): The reduction ratio of the intermediate channels.
                Default: 16.
        """
        super().__init__()
        self.cse_block = SqueezeAndExcitation(in_channels, r)
        self.sse_block = ChannelSqueezeAndSpatialExcitation(in_channels)

    def forward(self, x: torch.Tensor):
        """Forward call."""
        cse = self.cse_block(x)
        sse = self.sse_block(x)
        x = torch.add(cse, sse)
        return x


class SomeModel(torch.nn.Module):
    """Dummy test torch model."""
    pass


class SomeOptimizer(torch.nn.Module):
    """Dummy test torch optimizer."""

    def __init__(self, **kwargs):
        """Dummy optimizer"""
        super().__init__()


class SomeScheduler(torch.nn.Module):
    """Dummy test torch scheduler."""

    def __init__(self, **kwargs):
        """Dummy scheduler"""
        super().__init__()


class _ForwardOverrideModel(nn.Module):
    """Model that calls specified method instead of forward.

    (Workaround, single method tracing is not supported)
    """

    def __init__(self, model, method_name):
        super().__init__()
        self.model = model
        self.method_name = method_name

    def forward(self, *args, **kwargs):
        return getattr(self.model, self.method_name)(*args, **kwargs)


def get_fn_argsnames(fn: Callable[..., Any], exclude: List[str]=None):
    """Return parameter names of Callable.

    Args:
        fn (Callable[..., Any]): target Callable
        exclude (List[str]): exclude list of parameters

    Returns:
        list: contains parameter names of `fn`
    """
    argspec = inspect.getfullargspec(fn)
    params = argspec.args + argspec.kwonlyargs
    if exclude is not None:
        params = list(filter(lambda x: x not in exclude, params))
    return params


def _get_input_argnames(fn: Callable[..., Any], exclude: List[str]=None) ->List[str]:
    """
    Function to get input argument names of function.

    Args:
        fn (Callable[..., Any]): Function to get argument names from
        exclude (List[str]): List of string of names to exclude

    Returns:
        (List[str]): List of input argument names
    """
    argspec = inspect.getfullargspec(fn)
    assert argspec.varargs is None and argspec.varkw is None, 'not supported by PyTorch'
    return get_fn_argsnames(fn, exclude=exclude)


class _TracingModelWrapper(nn.Module):
    """Wrapper that traces model with batch instead of calling it.

    (Workaround, to use native model batch handler)
    """

    def __init__(self, model, method_name):
        super().__init__()
        self.model = model
        self.method_name = method_name
        self.tracing_result: ScriptModule

    def __call__(self, *args, **kwargs):
        method_model = _ForwardOverrideModel(self.model, self.method_name)
        try:
            assert len(args) == 0, 'only KV support implemented'
            fn = getattr(self.model, self.method_name)
            method_argnames = _get_input_argnames(fn=fn, exclude=['self'])
            method_input = tuple(kwargs[name] for name in method_argnames)
            self.tracing_result = trace(method_model, method_input)
        except Exception:
            self.tracing_result = trace(method_model, *args, **kwargs)
        output = self.model.forward(*args, **kwargs)
        return output


class Model(torch.nn.Module):
    """
    @TODO: Docs. Contribution is welcome
    """

    def __init__(self, **kwargs):
        """
        @TODO: Docs. Contribution is welcome
        """
        super().__init__()

    def forward(self, x):
        """
        @TODO: Docs. Contribution is welcome
        """
        return x

    @classmethod
    def get_from_params(cls, **model_params) ->'Model':
        """
        @TODO: Docs. Contribution is welcome
        """
        model = cls(**model_params)
        return model


class SimpleNet(nn.Module):
    """
    @TODO: Docs. Contribution is welcome
    """

    def __init__(self):
        """
        @TODO: Docs. Contribution is welcome
        """
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        """
        @TODO: Docs. Contribution is welcome
        """
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


class Net(nn.Module):

    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5, 1)
        self.conv2 = nn.Conv2d(20, 50, 5, 1)
        self.fc1 = nn.Linear(4 * 4 * 50, 500)
        self.fc2 = nn.Linear(500, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2, 2)
        x = x.view(-1, 4 * 4 * 50)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x


class Projector(nn.Module):

    def __init__(self, input_size: int):
        super().__init__()
        self.linear = nn.Linear(input_size, 1)

    def forward(self, X: torch.Tensor) ->torch.Tensor:
        return self.linear(X).squeeze(-1)


class ClassifyAE(torch.nn.Module):

    def __init__(self, in_features, hid_features, out_features):
        super().__init__()
        self.encoder = nn.Sequential(nn.Linear(in_features, hid_features), nn.Tanh())
        self.decoder = nn.Sequential(nn.Linear(hid_features, in_features), nn.Sigmoid())
        self.clf = nn.Linear(hid_features, out_features)

    def forward(self, x):
        z = self.encoder(x)
        y_hat = self.clf(z)
        x_ = self.decoder(z)
        return y_hat, x_


class ClassifyUnet(nn.Module):

    def __init__(self, in_channels, in_hw, out_features):
        super().__init__()
        self.encoder = nn.Sequential(nn.Conv2d(in_channels, in_channels, 3, 1, 1), nn.Tanh())
        self.decoder = nn.Conv2d(in_channels, in_channels, 3, 1, 1)
        self.clf = nn.Linear(in_channels * in_hw * in_hw, out_features)

    def forward(self, x):
        z = self.encoder(x)
        z_ = z.view(z.size(0), -1)
        y_hat = self.clf(z_)
        x_ = self.decoder(z)
        return y_hat, x_


LOG_SCALE_MAX = 2


LOG_SCALE_MIN = -10


def normal_logprob(mu, sigma, z):
    normalization_constant = -sigma.log() - 0.5 * np.log(2 * np.pi)
    square_term = -0.5 * ((z - mu) / sigma) ** 2
    logprob_vec = normalization_constant + square_term
    logprob = logprob_vec.sum(1)
    return logprob


def normal_sample(mu, sigma):
    return mu + sigma * torch.randn_like(sigma)


class ClassifyVAE(torch.nn.Module):

    def __init__(self, in_features, hid_features, out_features):
        super().__init__()
        self.encoder = torch.nn.Linear(in_features, hid_features * 2)
        self.decoder = nn.Sequential(nn.Linear(hid_features, in_features), nn.Sigmoid())
        self.clf = torch.nn.Linear(hid_features, out_features)

    def forward(self, x, deterministic=False):
        z = self.encoder(x)
        bs, z_dim = z.shape
        loc, log_scale = z[:, :z_dim // 2], z[:, z_dim // 2:]
        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)
        scale = torch.exp(log_scale)
        z_ = loc if deterministic else normal_sample(loc, scale)
        z_logprob = normal_logprob(loc, scale, z_)
        z_ = z_.view(bs, -1)
        x_ = self.decoder(z_)
        y_hat = self.clf(z_)
        return y_hat, x_, z_logprob, loc, log_scale


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (ChannelSqueezeAndSpatialExcitation,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ClassifyAE,
     lambda: ([], {'in_features': 4, 'hid_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ClassifyUnet,
     lambda: ([], {'in_channels': 4, 'in_hw': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ClassifyVAE,
     lambda: ([], {'in_features': 4, 'hid_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (ConcurrentSpatialAndChannelSqueezeAndChannelExcitation,
     lambda: ([], {'in_channels': 16}),
     lambda: ([torch.rand([4, 16, 4, 16])], {}),
     False),
    (ContrastiveDistanceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (ContrastiveEmbeddingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (ContrastivePairwiseEmbeddingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (DecoderConcatBlock,
     lambda: ([], {'in_channels': 4, 'enc_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (DecoderFPNBlock,
     lambda: ([], {'in_channels': 4, 'enc_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 8, 8])], {}),
     False),
    (DecoderSumBlock,
     lambda: ([], {'enc_channels': 4, 'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (EncoderDownsampleBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (EncoderUpsampleBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GaussianNoise,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalAvgPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalConcatPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalMaxPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (HuberLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (LamaPooling,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (Lambda,
     lambda: ([], {'lambda_fn': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LovaszLossBinary,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (LovaszLossMultiClass,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (LovaszLossMultiLabel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (MarginLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (MeanOutputLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Model,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (NaiveCrossEntropyLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Normalize,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (PSPBlock,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Projector,
     lambda: ([], {'input_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PyramidBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'pool_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (RMSNorm,
     lambda: ([], {'dimension': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ResidualWrapper,
     lambda: ([], {'net': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SqueezeAndExcitation,
     lambda: ([], {'in_channels': 16}),
     lambda: ([torch.rand([4, 16, 4, 16])], {}),
     False),
    (TemporalAttentionPooling,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (TemporalAvgPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TemporalConcatPooling,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TemporalDropLastWrapper,
     lambda: ([], {'net': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TemporalLastPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TemporalMaxPooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TripletLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (TripletLossV2,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (TripletPairwiseEmbeddingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (UnetEncoder,
     lambda: ([], {'in_channels': 4, 'num_channels': 4, 'num_blocks': 4}),
     lambda: ([torch.rand([4, 4, 64, 64])], {}),
     False),
    (UnetHead,
     lambda: ([], {'in_channels': [4, 4], 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 64, 64])], {}),
     False),
    (WingLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
]

class Test_catalyst_team_catalyst(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

    def test_041(self):
        self._check(*TESTCASES[41])

    def test_042(self):
        self._check(*TESTCASES[42])

    def test_043(self):
        self._check(*TESTCASES[43])

    def test_044(self):
        self._check(*TESTCASES[44])

    def test_045(self):
        self._check(*TESTCASES[45])

    def test_046(self):
        self._check(*TESTCASES[46])

