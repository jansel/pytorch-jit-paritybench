import sys
_module = sys.modules[__name__]
del sys
conf = _module
atari_dqn_async_cpu = _module
atari_dqn_async_gpu = _module
atari_dqn_async_serial = _module
example_1 = _module
example_2 = _module
example_3 = _module
example_4 = _module
example_5 = _module
example_6 = _module
example_6a = _module
example_7 = _module
rlpyt = _module
agents = _module
base = _module
dqn = _module
atari = _module
atari_catdqn_agent = _module
atari_dqn_agent = _module
atari_r2d1_agent = _module
mixin = _module
catdqn_agent = _module
dqn_agent = _module
epsilon_greedy = _module
r2d1_agent = _module
pg = _module
categorical = _module
gaussian = _module
mujoco = _module
qpg = _module
ddpg_agent = _module
sac_agent = _module
sac_v_agent = _module
td3_agent = _module
algos = _module
cat_dqn = _module
dqn = _module
r2d1 = _module
a2c = _module
base = _module
ppo = _module
ddpg = _module
sac = _module
sac_v = _module
td3 = _module
utils = _module
distributions = _module
base = _module
categorical = _module
discrete = _module
epsilon_greedy = _module
gaussian = _module
envs = _module
atari_env = _module
gym = _module
gym_schema = _module
experiments = _module
configs = _module
atari_dqn = _module
atari_dqn_debug = _module
atari_r2d1 = _module
atari_ff_a2c = _module
atari_ff_ppo = _module
atari_lstm_a2c = _module
atari_lstm_ppo = _module
mujoco_a2c = _module
mujoco_ppo = _module
mujoco_ddpg = _module
mujoco_sac = _module
mujoco_sac_v = _module
mujoco_td3 = _module
launch_atari_r2d1_async_alt = _module
launch_atari_r2d1_async_alt_seaquest = _module
launch_atari_r2d1_async_gpu = _module
launch_atari_dqn_cpu_basic_1of2 = _module
launch_atari_dqn_cpu_basic_2of2 = _module
launch_atari_dqn_gpu_pong = _module
launch_atari_r2d1_async_alt_amidar = _module
launch_atari_r2d1_async_alt_gravitar = _module
launch_atari_r2d1_async_alt_pong = _module
launch_atari_r2d1_async_gpu_amidar = _module
launch_atari_r2d1_async_gpu_test = _module
launch_atari_r2d1_long_4tr_gravitar = _module
launch_atari_catdqn_gpu_basic = _module
launch_atari_dpd_dqn_gpu_basic = _module
launch_atari_dqn_cpu_basic = _module
launch_atari_dqn_gpu = _module
launch_atari_dqn_gpu_basic = _module
launch_atari_ernbw_gpu_basic = _module
launch_atari_r2d1_gpu_basic = _module
launch_atari_dqn_async_cpu = _module
launch_atari_dqn_async_gpu = _module
launch_atari_dqn_async_gpu_scale_pri = _module
launch_atari_dqn_async_serial = _module
launch_atari_dqn_gpu_noeval = _module
launch_atari_dqn_serial = _module
launch_atari_r2d1_async_alt_chopper_command = _module
launch_atari_r2d1_async_alt_qbert = _module
launch_atari_r2d1_async_gpu_qbert = _module
launch_atari_r2d1_long_4tr_asteroids = _module
launch_atari_r2d1_long_4tr_chopper_command = _module
launch_atari_r2d1_long_4tr_seaquest = _module
launch_atari_r2d1_long_gt_ad = _module
launch_atari_r2d1_long_sq_cc = _module
atari_catdqn_gpu = _module
atari_dqn_cpu = _module
atari_dqn_gpu = _module
atari_dqn_gpu_noeval = _module
atari_dqn_serial = _module
atari_r2d1_async_alt = _module
atari_r2d1_async_gpu = _module
atari_r2d1_gpu = _module
launch_atari_ff_a2c_gpu_basic = _module
launch_atari_ff_a2c_gpu_low_lr = _module
launch_atari_ff_a2c_gpu_multi = _module
launch_atari_ff_lstm_a2c_ppo_gpu = _module
launch_atari_ff_ppo_gpu = _module
launch_atari_ff_ppo_gpu_basic = _module
launch_atari_lstm_a2c_gpu_basic = _module
launch_atari_lstm_ppo_gpu_basic = _module
launch_atari_ff_a2c_cpu = _module
launch_atari_lstm_a2c_cpu = _module
launch_atari_lstm_a2c_gpu = _module
atari_ff_a2c_cpu = _module
atari_ff_a2c_gpu = _module
atari_ff_a2c_gpu_multi = _module
atari_ff_ppo_gpu = _module
atari_lstm_a2c_cpu = _module
atari_lstm_a2c_gpu = _module
atari_lstm_ppo_gpu = _module
launch_mujoco_ppo_ddpg_td3_sac_serial = _module
launch_mujoco_sac_serial = _module
launch_mujoco_ppo_serial = _module
launch_mujoco_ppo_serial_hc = _module
launch_mujoco_a2c_cpu = _module
launch_mujoco_ppo_cpu = _module
launch_mujoco_ppo_gpu = _module
mujoco_ff_a2c_cpu = _module
mujoco_ff_ppo_cpu = _module
mujoco_ff_ppo_gpu = _module
mujoco_ff_ppo_serial = _module
launch_mujoco_ddpg_async_serial = _module
launch_mujoco_ddpg_serial = _module
launch_mujoco_ddpg_serial_bstl = _module
launch_mujoco_sac_async_gpu = _module
launch_mujoco_sac_serial_bstl = _module
launch_mujoco_sac_serial_hc = _module
launch_mujoco_td3_async_cpu = _module
launch_mujoco_td3_serial = _module
launch_mujoco_td3_serial_bstl = _module
launch_mujoco_td3_serial_hc = _module
launch_mujoco_ddpg_cpu = _module
launch_mujoco_ddpg_td3_sac_serial = _module
mujoco_ddpg_async_serial = _module
mujoco_ddpg_cpu = _module
mujoco_ddpg_serial = _module
mujoco_sac_async_gpu = _module
mujoco_sac_serial = _module
mujoco_td3_async_cpu = _module
mujoco_td3_serial = _module
models = _module
conv2d = _module
atari_catdqn_model = _module
atari_dqn_model = _module
atari_r2d1_model = _module
dueling = _module
mlp = _module
atari_ff_model = _module
atari_lstm_model = _module
mujoco_ff_model = _module
mujoco_lstm_model = _module
mlp = _module
running_mean_std = _module
utils = _module
replays = _module
async_ = _module
frame = _module
n_step = _module
non_sequence = _module
n_step = _module
prioritized = _module
time_limit = _module
uniform = _module
sequence = _module
sum_tree = _module
runners = _module
async_rl = _module
minibatch_rl = _module
sync_rl = _module
samplers = _module
action_server = _module
alternating_sampler = _module
collectors = _module
cpu_sampler = _module
gpu_sampler = _module
serial_sampler = _module
buffer = _module
collections = _module
parallel = _module
cpu = _module
collectors = _module
sampler = _module
gpu = _module
sampler = _module
worker = _module
serial = _module
sampler = _module
spaces = _module
composite = _module
float_box = _module
gym_wrapper = _module
gym_wrapper_schema = _module
int_box = _module
array = _module
buffer = _module
collections = _module
launching = _module
affinity = _module
exp_launcher = _module
variant = _module
logging = _module
autoargs = _module
console = _module
context = _module
logger = _module
tabulate = _module
misc = _module
prog_bar = _module
quick_args = _module
seed = _module
shmemarray = _module
synchronize = _module
tensor = _module
setup = _module
tests = _module
test_gym_wrapper = _module
test_rlpyt = _module
test_serial_sampler = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, numbers, numpy, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'


import torch


from torch.nn.parallel import DistributedDataParallel as DDP


from torch.nn.parallel import DistributedDataParallelCPU as DDPC


import numpy as np


from collections import namedtuple


import math


import torch.nn.functional as F


import torch.distributed as dist


import time


from collections import deque


import torch.distributed


from collections import OrderedDict


from inspect import Signature as Sig


from inspect import Parameter as Param


import string


from enum import Enum


import queue


def conv2d_output_shape(h, w, kernel_size=1, stride=1, padding=0, dilation=1):
    """
    Returns output H, W after convolution/pooling on input H, W.
    """
    kh, kw = kernel_size if isinstance(kernel_size, tuple) else (kernel_size,) * 2
    sh, sw = stride if isinstance(stride, tuple) else (stride,) * 2
    ph, pw = padding if isinstance(padding, tuple) else (padding,) * 2
    d = dilation
    h = (h + 2 * ph - d * (kh - 1) - 1) // sh + 1
    w = (w + 2 * pw - d * (kw - 1) - 1) // sw + 1
    return h, w


class Conv2dModel(torch.nn.Module):
    """2-D Convolutional model component, with option for max-pooling vs
    downsampling for strides > 1.  Requires number of input channels, but
    not input shape.  Uses ``torch.nn.Conv2d``.
    """

    def __init__(self, in_channels, channels, kernel_sizes, strides, paddings=None, nonlinearity=torch.nn.ReLU, use_maxpool=False, head_sizes=None):
        super().__init__()
        if paddings is None:
            paddings = [(0) for _ in range(len(channels))]
        assert len(channels) == len(kernel_sizes) == len(strides) == len(paddings)
        in_channels = [in_channels] + channels[:-1]
        ones = [(1) for _ in range(len(strides))]
        if use_maxpool:
            maxp_strides = strides
            strides = ones
        else:
            maxp_strides = ones
        conv_layers = [torch.nn.Conv2d(in_channels=ic, out_channels=oc, kernel_size=k, stride=s, padding=p) for ic, oc, k, s, p in zip(in_channels, channels, kernel_sizes, strides, paddings)]
        sequence = list()
        for conv_layer, maxp_stride in zip(conv_layers, maxp_strides):
            sequence.extend([conv_layer, nonlinearity()])
            if maxp_stride > 1:
                sequence.append(torch.nn.MaxPool2d(maxp_stride))
        self.conv = torch.nn.Sequential(*sequence)

    def forward(self, input):
        """Computes the convolution stack on the input; assumes correct shape
        already: [B,C,H,W]."""
        return self.conv(input)

    def conv_out_size(self, h, w, c=None):
        """Helper function ot return the output size for a given input shape,
        without actually performing a forward pass through the model."""
        for child in self.conv.children():
            try:
                h, w = conv2d_output_shape(h, w, child.kernel_size, child.stride, child.padding)
            except AttributeError:
                pass
            try:
                c = child.out_channels
            except AttributeError:
                pass
        return h * w * c


class MlpModel(torch.nn.Module):
    """Multilayer Perceptron with last layer linear.

    Args:
        input_size (int): number of inputs
        hidden_sizes (list): can be empty list for none (linear model).
        output_size: linear layer at output, or if ``None``, the last hidden size will be the output size and will have nonlinearity applied
        nonlinearity: torch nonlinearity Module (not Functional).
    """

    def __init__(self, input_size, hidden_sizes, output_size=None, nonlinearity=torch.nn.ReLU):
        super().__init__()
        if isinstance(hidden_sizes, int):
            hidden_sizes = [hidden_sizes]
        hidden_layers = [torch.nn.Linear(n_in, n_out) for n_in, n_out in zip([input_size] + hidden_sizes[:-1], hidden_sizes)]
        sequence = list()
        for layer in hidden_layers:
            sequence.extend([layer, nonlinearity()])
        if output_size is not None:
            last_size = hidden_sizes[-1] if hidden_sizes else input_size
            sequence.append(torch.nn.Linear(last_size, output_size))
        self.model = torch.nn.Sequential(*sequence)
        self._output_size = hidden_sizes[-1] if output_size is None else output_size

    def forward(self, input):
        """Compute the model on the input, assuming input shape [B,input_size]."""
        return self.model(input)

    @property
    def output_size(self):
        """Retuns the output size of the model."""
        return self._output_size


class Conv2dHeadModel(torch.nn.Module):
    """Model component composed of a ``Conv2dModel`` component followed by 
    a fully-connected ``MlpModel`` head.  Requires full input image shape to
    instantiate the MLP head.
    """

    def __init__(self, image_shape, channels, kernel_sizes, strides, hidden_sizes, output_size=None, paddings=None, nonlinearity=torch.nn.ReLU, use_maxpool=False):
        super().__init__()
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels, kernel_sizes=kernel_sizes, strides=strides, paddings=paddings, nonlinearity=nonlinearity, use_maxpool=use_maxpool)
        conv_out_size = self.conv.conv_out_size(h, w)
        if hidden_sizes or output_size:
            self.head = MlpModel(conv_out_size, hidden_sizes, output_size=output_size, nonlinearity=nonlinearity)
            if output_size is not None:
                self._output_size = output_size
            else:
                self._output_size = hidden_sizes if isinstance(hidden_sizes, int) else hidden_sizes[-1]
        else:
            self.head = lambda x: x
            self._output_size = conv_out_size

    def forward(self, input):
        """Compute the convolution and fully connected head on the input;
        assumes correct input shape: [B,C,H,W]."""
        return self.head(self.conv(input).view(input.shape[0], -1))

    @property
    def output_size(self):
        """Returns the final output size after MLP head."""
        return self._output_size


class DistributionalHeadModel(torch.nn.Module):
    """An MLP head which reshapes output to [B, output_size, n_atoms]."""

    def __init__(self, input_size, layer_sizes, output_size, n_atoms):
        super().__init__()
        self.mlp = MlpModel(input_size, layer_sizes, output_size * n_atoms)
        self._output_size = output_size
        self._n_atoms = n_atoms

    def forward(self, input):
        return self.mlp(input).view(-1, self._output_size, self._n_atoms)


class ScaleGrad(torch.autograd.Function):
    """Model component to scale gradients back from layer, without affecting
    the forward pass.  Used e.g. in dueling heads DQN models."""

    @staticmethod
    def forward(ctx, tensor, scale):
        """Stores the ``scale`` input to ``ctx`` for application in
        ``backward()``; simply returns the input ``tensor``."""
        ctx.scale = scale
        return tensor

    @staticmethod
    def backward(ctx, grad_output):
        """Return the ``grad_output`` multiplied by ``ctx.scale``.  Also returns
        a ``None`` as placeholder corresponding to (non-existent) gradient of 
        the input ``scale`` of ``forward()``."""
        return grad_output * ctx.scale, None


scale_grad = getattr(ScaleGrad, 'apply', None)


class DistributionalDuelingHeadModel(torch.nn.Module):
    """Model component for Dueling Distributional (Categorical) DQN, like
    ``DuelingHeadModel``, but handles `n_atoms` outputs for each state-action
    Q-value distribution.
    """

    def __init__(self, input_size, hidden_sizes, output_size, n_atoms, grad_scale=2 ** (-1 / 2)):
        super().__init__()
        if isinstance(hidden_sizes, int):
            hidden_sizes = [hidden_sizes]
        self.advantage_hidden = MlpModel(input_size, hidden_sizes)
        self.advantage_out = torch.nn.Linear(hidden_sizes[-1], output_size * n_atoms, bias=False)
        self.advantage_bias = torch.nn.Parameter(torch.zeros(n_atoms))
        self.value = MlpModel(input_size, hidden_sizes, output_size=n_atoms)
        self._grad_scale = grad_scale
        self._output_size = output_size
        self._n_atoms = n_atoms

    def forward(self, input):
        x = scale_grad(input, self._grad_scale)
        advantage = self.advantage(x)
        value = self.value(x).view(-1, 1, self._n_atoms)
        return value + (advantage - advantage.mean(dim=1, keepdim=True))

    def advantage(self, input):
        x = self.advantage_hidden(input)
        x = self.advantage_out(x)
        x = x.view(-1, self._output_size, self._n_atoms)
        return x + self.advantage_bias


def infer_leading_dims(tensor, dim):
    """Looks for up to two leading dimensions in ``tensor``, before
    the data dimensions, of which there are assumed to be ``dim`` number.
    For use at beginning of model's ``forward()`` method, which should 
    finish with ``restore_leading_dims()`` (see that function for help.)
    Returns:
    lead_dim: int --number of leading dims found.
    T: int --size of first leading dim, if two leading dims, o/w 1.
    B: int --size of first leading dim if one, second leading dim if two, o/w 1.
    shape: tensor shape after leading dims.
    """
    lead_dim = tensor.dim() - dim
    assert lead_dim in (0, 1, 2)
    if lead_dim == 2:
        T, B = tensor.shape[:2]
    else:
        T = 1
        B = 1 if lead_dim == 0 else tensor.shape[0]
    shape = tensor.shape[lead_dim:]
    return lead_dim, T, B, shape


def restore_leading_dims(tensors, lead_dim, T=1, B=1):
    """Reshapes ``tensors`` (one or `tuple`, `list`) to to have ``lead_dim``
    leading dimensions, which will become [], [B], or [T,B].  Assumes input
    tensors already have a leading Batch dimension, which might need to be
    removed. (Typically the last layer of model will compute with leading
    batch dimension.)  For use in model ``forward()`` method, so that output
    dimensions match input dimensions, and the same model can be used for any
    such case.  Use with outputs from ``infer_leading_dims()``."""
    is_seq = isinstance(tensors, (tuple, list))
    tensors = tensors if is_seq else (tensors,)
    if lead_dim == 2:
        tensors = tuple(t.view((T, B) + t.shape[1:]) for t in tensors)
    if lead_dim == 0:
        assert B == 1
        tensors = tuple(t.squeeze(0) for t in tensors)
    return tensors if is_seq else tensors[0]


class AtariCatDqnModel(torch.nn.Module):
    """2D conlutional network feeding into MLP with ``n_atoms`` outputs
    per action, representing a discrete probability distribution of Q-values."""

    def __init__(self, image_shape, output_size, n_atoms=51, fc_sizes=512, dueling=False, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiates the neural network according to arguments; network defaults
        stored within this method."""
        super().__init__()
        self.dueling = dueling
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings or [0, 1, 1], use_maxpool=use_maxpool)
        conv_out_size = self.conv.conv_out_size(h, w)
        if dueling:
            self.head = DistributionalDuelingHeadModel(conv_out_size, fc_sizes, output_size=output_size, n_atoms=n_atoms)
        else:
            self.head = DistributionalHeadModel(conv_out_size, fc_sizes, output_size=output_size, n_atoms=n_atoms)

    def forward(self, observation, prev_action, prev_reward):
        """Returns the probability masses ``num_atoms x num_actions`` for the Q-values
        for each state/observation, using softmax output nonlinearity."""
        img = observation.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv_out = self.conv(img.view(T * B, *img_shape))
        p = self.head(conv_out.view(T * B, -1))
        p = F.softmax(p, dim=-1)
        p = restore_leading_dims(p, lead_dim, T, B)
        return p


class DuelingHeadModel(torch.nn.Module):
    """Model component for dueling DQN.  For each state Q-value, uses a scalar
    output for mean (bias), and vector output for relative advantages
    associated with each action, so the Q-values are computed as: Mean +
    (Advantages - mean(Advantages)).  Uses a shared bias for all Advantage
    outputs.Gradient scaling can be applied, affecting preceding layers in the
    backward pass.
    """

    def __init__(self, input_size, hidden_sizes, output_size, grad_scale=2 ** (-1 / 2)):
        super().__init__()
        if isinstance(hidden_sizes, int):
            hidden_sizes = [hidden_sizes]
        self.advantage_hidden = MlpModel(input_size, hidden_sizes)
        self.advantage_out = torch.nn.Linear(hidden_sizes[-1], output_size, bias=False)
        self.advantage_bias = torch.nn.Parameter(torch.zeros(1))
        self.value = MlpModel(input_size, hidden_sizes, output_size=1)
        self._grad_scale = grad_scale

    def forward(self, input):
        """Computes Q-values through value and advantage heads; applies gradient
        scaling."""
        x = scale_grad(input, self._grad_scale)
        advantage = self.advantage(x)
        value = self.value(x)
        return value + (advantage - advantage.mean(dim=-1, keepdim=True))

    def advantage(self, input):
        """Computes shared-bias advantages."""
        x = self.advantage_hidden(input)
        return self.advantage_out(x) + self.advantage_bias


class AtariDqnModel(torch.nn.Module):
    """Standard convolutional network for DQN.  2-D convolution for multiple
    video frames per observation, feeding an MLP for Q-value outputs for
    the action set.
    """

    def __init__(self, image_shape, output_size, fc_sizes=512, dueling=False, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiates the neural network according to arguments; network defaults
        stored within this method."""
        super().__init__()
        self.dueling = dueling
        c, h, w = image_shape
        self.conv = Conv2dModel(in_channels=c, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings or [0, 1, 1], use_maxpool=use_maxpool)
        conv_out_size = self.conv.conv_out_size(h, w)
        if dueling:
            self.head = DuelingHeadModel(conv_out_size, fc_sizes, output_size)
        else:
            self.head = MlpModel(conv_out_size, fc_sizes, output_size)

    def forward(self, observation, prev_action, prev_reward):
        """
        Compute action Q-value estimates from input state.
        Infers leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Convolution layers process as [T*B,
        image_shape[0], image_shape[1],...,image_shape[-1]], with T=1,B=1 when not given.  Expects uint8 images in
        [0,255] and converts them to float32 in [0,1] (to minimize image data
        storage and transfer).  Used in both sampler and in algorithm (both
        via the agent).
        """
        img = observation.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv_out = self.conv(img.view(T * B, *img_shape))
        q = self.head(conv_out.view(T * B, -1))
        q = restore_leading_dims(q, lead_dim, T, B)
        return q


RESERVED_NAMES = 'get', 'items'


def tuple_itemgetter(i):

    def _tuple_itemgetter(obj):
        return tuple.__getitem__(obj, i)
    return _tuple_itemgetter


def namedarraytuple(typename, field_names, return_namedtuple_cls=False, classname_suffix=False):
    """
    Returns a new subclass of a namedtuple which exposes indexing / slicing
    reads and writes applied to all contained objects, which must share
    indexing (__getitem__) behavior (e.g. numpy arrays or torch tensors).

    (Code follows pattern of collections.namedtuple.)

    >>> PointsCls = namedarraytuple('Points', ['x', 'y'])
    >>> p = PointsCls(np.array([0, 1]), y=np.array([10, 11]))
    >>> p
    Points(x=array([0, 1]), y=array([10, 11]))
    >>> p.x                         # fields accessible by name
    array([0, 1])
    >>> p[0]                        # get location across all fields
    Points(x=0, y=10)               # (location can be index or slice)
    >>> p.get(0)                    # regular tuple-indexing into field
    array([0, 1])
    >>> x, y = p                    # unpack like a regular tuple
    >>> x
    array([0, 1])
    >>> p[1] = 2                    # assign value to location of all fields
    >>> p
    Points(x=array([0, 2]), y=array([10, 2]))
    >>> p[1] = PointsCls(3, 30)     # assign to location field-by-field
    >>> p
    Points(x=array([0, 3]), y=array([10, 30]))
    >>> 'x' in p                    # check field name instead of object
    True
    """
    nt_typename = typename
    if classname_suffix:
        nt_typename += '_nt'
        typename += '_nat'
    try:
        module = sys._getframe(1).f_globals.get('__name__', '__main__')
    except (AttributeError, ValueError):
        module = None
    NtCls = namedtuple(nt_typename, field_names, module=module)

    def __getitem__(self, loc):
        try:
            return type(self)(*(None if s is None else s[loc] for s in self))
        except IndexError as e:
            for j, s in enumerate(self):
                if s is None:
                    continue
                try:
                    _ = s[loc]
                except IndexError:
                    raise Exception(f"Occured in {self.__class__} at field '{self._fields[j]}'.") from e
    __getitem__.__doc__ = f'Return a new {typename} instance containing the selected index or slice from each field.'

    def __setitem__(self, loc, value):
        """
        If input value is the same named[array]tuple type, iterate through its
        fields and assign values into selected index or slice of corresponding
        field.  Else, assign whole of value to selected index or slice of
        all fields.  Ignore fields that are both None.
        """
        if not (isinstance(value, tuple) and getattr(value, '_fields', None) == self._fields):
            value = tuple(None if s is None else value for s in self)
        try:
            for j, (s, v) in enumerate(zip(self, value)):
                if s is not None or v is not None:
                    s[loc] = v
        except (ValueError, IndexError, TypeError) as e:
            raise Exception(f"Occured in {self.__class__} at field '{self._fields[j]}'.") from e

    def __contains__(self, key):
        """Checks presence of field name (unlike tuple; like dict)."""
        return key in self._fields

    def get(self, index):
        """Retrieve value as if indexing into regular tuple."""
        return tuple.__getitem__(self, index)

    def items(self):
        """Iterate ordered (field_name, value) pairs (like OrderedDict)."""
        for k, v in zip(self._fields, self):
            yield k, v
    for method in (__getitem__, __setitem__, get, items):
        method.__qualname__ = f'{typename}.{method.__name__}'
    arg_list = repr(NtCls._fields).replace("'", '')[1:-1]
    class_namespace = {'__doc__': f'{typename}({arg_list})', '__slots__': (), '__getitem__': __getitem__, '__setitem__': __setitem__, '__contains__': __contains__, 'get': get, 'items': items}
    for index, name in enumerate(NtCls._fields):
        if name in RESERVED_NAMES:
            raise ValueError(f'Disallowed field name: {name}.')
        itemgetter_object = tuple_itemgetter(index)
        doc = f'Alias for field number {index}'
        class_namespace[name] = property(itemgetter_object, doc=doc)
    result = type(typename, (NtCls,), class_namespace)
    result.__module__ = NtCls.__module__
    if return_namedtuple_cls:
        return result, NtCls
    return result


RnnState = namedarraytuple('RnnState', ['h', 'c'])


class AtariR2d1Model(torch.nn.Module):
    """2D convolutional neural network (for multiple video frames per
    observation) feeding into an LSTM and MLP output for Q-value outputs for
    the action set."""

    def __init__(self, image_shape, output_size, fc_size=512, lstm_size=512, head_size=512, dueling=False, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiates the neural network according to arguments; network defaults
        stored within this method."""
        super().__init__()
        self.dueling = dueling
        self.conv = Conv2dHeadModel(image_shape=image_shape, channels=channels or [32, 64, 64], kernel_sizes=kernel_sizes or [8, 4, 3], strides=strides or [4, 2, 1], paddings=paddings or [0, 1, 1], use_maxpool=use_maxpool, hidden_sizes=fc_size)
        self.lstm = torch.nn.LSTM(self.conv.output_size + output_size + 1, lstm_size)
        if dueling:
            self.head = DuelingHeadModel(lstm_size, head_size, output_size)
        else:
            self.head = MlpModel(lstm_size, head_size, output_size=output_size)

    def forward(self, observation, prev_action, prev_reward, init_rnn_state):
        """Feedforward layers process as [T*B,H]. Return same leading dims as
        input, can be [T,B], [B], or []."""
        img = observation.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        conv_out = self.conv(img.view(T * B, *img_shape))
        lstm_input = torch.cat([conv_out.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        q = self.head(lstm_out.view(T * B, -1))
        q = restore_leading_dims(q, lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return q, next_rnn_state


class AtariFfModel(torch.nn.Module):
    """
    Feedforward model for Atari agents: a convolutional network feeding an
    MLP with outputs for action probabilities and state-value estimate.
    """

    def __init__(self, image_shape, output_size, fc_sizes=512, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiate neural net module according to inputs."""
        super().__init__()
        self.conv = Conv2dHeadModel(image_shape=image_shape, channels=channels or [16, 32], kernel_sizes=kernel_sizes or [8, 4], strides=strides or [4, 2], paddings=paddings or [0, 1], use_maxpool=use_maxpool, hidden_sizes=fc_sizes)
        self.pi = torch.nn.Linear(self.conv.output_size, output_size)
        self.value = torch.nn.Linear(self.conv.output_size, 1)

    def forward(self, image, prev_action, prev_reward):
        """
        Compute action probabilities and value estimate from input state.
        Infers leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Convolution layers process as [T*B,
        *image_shape], with T=1,B=1 when not given.  Expects uint8 images in
        [0,255] and converts them to float32 in [0,1] (to minimize image data
        storage and transfer).  Used in both sampler and in algorithm (both
        via the agent).
        """
        img = image.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        fc_out = self.conv(img.view(T * B, *img_shape))
        pi = F.softmax(self.pi(fc_out), dim=-1)
        v = self.value(fc_out).squeeze(-1)
        pi, v = restore_leading_dims((pi, v), lead_dim, T, B)
        return pi, v


class AtariLstmModel(torch.nn.Module):
    """Recurrent model for Atari agents: a convolutional network into an FC layer
    into an LSTM which outputs action probabilities and state-value estimate.
    """

    def __init__(self, image_shape, output_size, fc_sizes=512, lstm_size=512, use_maxpool=False, channels=None, kernel_sizes=None, strides=None, paddings=None):
        """Instantiate neural net module according to inputs."""
        super().__init__()
        self.conv = Conv2dHeadModel(image_shape=image_shape, channels=channels or [16, 32], kernel_sizes=kernel_sizes or [8, 4], strides=strides or [4, 2], paddings=paddings or [0, 1], use_maxpool=use_maxpool, hidden_sizes=fc_sizes)
        self.lstm = torch.nn.LSTM(self.conv.output_size + output_size + 1, lstm_size)
        self.pi = torch.nn.Linear(lstm_size, output_size)
        self.value = torch.nn.Linear(lstm_size, 1)

    def forward(self, image, prev_action, prev_reward, init_rnn_state):
        """
        Compute action probabilities and value estimate from input state.
        Infers leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Convolution layers process as [T*B,
        *image_shape], with T=1,B=1 when not given.  Expects uint8 images in
        [0,255] and converts them to float32 in [0,1] (to minimize image data
        storage and transfer).  Recurrent layers processed as [T,B,H]. Used in
        both sampler and in algorithm (both via the agent).  Also returns the
        next RNN state.
        """
        img = image.type(torch.float)
        img = img.mul_(1.0 / 255)
        lead_dim, T, B, img_shape = infer_leading_dims(img, 3)
        fc_out = self.conv(img.view(T * B, *img_shape))
        lstm_input = torch.cat([fc_out.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        pi = F.softmax(self.pi(lstm_out.view(T * B, -1)), dim=-1)
        v = self.value(lstm_out.view(T * B, -1)).squeeze(-1)
        pi, v = restore_leading_dims((pi, v), lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return pi, v, next_rnn_state


class RunningMeanStdModel(torch.nn.Module):
    """Adapted from OpenAI baselines.  Maintains a running estimate of mean
    and variance of data along each dimension, accessible in the `mean` and
    `var` attributes.  Supports multi-GPU training by all-reducing statistics
    across GPUs."""

    def __init__(self, shape):
        super().__init__()
        self.register_buffer('mean', torch.zeros(shape))
        self.register_buffer('var', torch.ones(shape))
        self.register_buffer('count', torch.zeros(()))
        self.shape = shape

    def update(self, x):
        _, T, B, _ = infer_leading_dims(x, len(self.shape))
        x = x.view(T * B, *self.shape)
        batch_mean = x.mean(dim=0)
        batch_var = x.var(dim=0, unbiased=False)
        batch_count = T * B
        if dist.is_initialized():
            mean_var = torch.stack([batch_mean, batch_var])
            dist.all_reduce(mean_var)
            world_size = dist.get_world_size()
            mean_var /= world_size
            batch_count *= world_size
            batch_mean, batch_var = mean_var[0], mean_var[1]
        if self.count == 0:
            self.mean[:] = batch_mean
            self.var[:] = batch_var
        else:
            delta = batch_mean - self.mean
            total = self.count + batch_count
            self.mean[:] = self.mean + delta * batch_count / total
            m_a = self.var * self.count
            m_b = batch_var * batch_count
            M2 = m_a + m_b + delta ** 2 * self.count * batch_count / total
            self.var[:] = M2 / total
        self.count += batch_count


class MujocoFfModel(torch.nn.Module):
    """
    Model commonly used in Mujoco locomotion agents: an MLP which outputs
    distribution means, separate parameter for learned log_std, and separate
    MLP for state-value estimate.
    """

    def __init__(self, observation_shape, action_size, hidden_sizes=None, hidden_nonlinearity=torch.nn.Tanh, mu_nonlinearity=torch.nn.Tanh, init_log_std=0.0, normalize_observation=False, norm_obs_clip=10, norm_obs_var_clip=1e-06):
        """Instantiate neural net modules according to inputs."""
        super().__init__()
        self._obs_ndim = len(observation_shape)
        input_size = int(np.prod(observation_shape))
        hidden_sizes = hidden_sizes or [64, 64]
        mu_mlp = MlpModel(input_size=input_size, hidden_sizes=hidden_sizes, output_size=action_size, nonlinearity=hidden_nonlinearity)
        if mu_nonlinearity is not None:
            self.mu = torch.nn.Sequential(mu_mlp, mu_nonlinearity())
        else:
            self.mu = mu_mlp
        self.v = MlpModel(input_size=input_size, hidden_sizes=hidden_sizes, output_size=1, nonlinearity=hidden_nonlinearity)
        self.log_std = torch.nn.Parameter(init_log_std * torch.ones(action_size))
        if normalize_observation:
            self.obs_rms = RunningMeanStdModel(observation_shape)
            self.norm_obs_clip = norm_obs_clip
            self.norm_obs_var_clip = norm_obs_var_clip
        self.normalize_observation = normalize_observation

    def forward(self, observation, prev_action, prev_reward):
        """
        Compute mean, log_std, and value estimate from input state. Infers
        leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Intermediate feedforward layers
        process as [T*B,H], with T=1,B=1 when not given. Used both in sampler
        and in algorithm (both via the agent).
        """
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        if self.normalize_observation:
            obs_var = self.obs_rms.var
            if self.norm_obs_var_clip is not None:
                obs_var = torch.clamp(obs_var, min=self.norm_obs_var_clip)
            observation = torch.clamp((observation - self.obs_rms.mean) / obs_var.sqrt(), -self.norm_obs_clip, self.norm_obs_clip)
        obs_flat = observation.view(T * B, -1)
        mu = self.mu(obs_flat)
        v = self.v(obs_flat).squeeze(-1)
        log_std = self.log_std.repeat(T * B, 1)
        mu, log_std, v = restore_leading_dims((mu, log_std, v), lead_dim, T, B)
        return mu, log_std, v

    def update_obs_rms(self, observation):
        if self.normalize_observation:
            self.obs_rms.update(observation)


class MujocoLstmModel(torch.nn.Module):
    """
    Recurrent model for Mujoco locomotion agents: an MLP into an LSTM which
    outputs distribution means, log_std, and state-value estimate.
    """

    def __init__(self, observation_shape, action_size, hidden_sizes=None, lstm_size=256, nonlinearity=torch.nn.ReLU, normalize_observation=False, norm_obs_clip=10, norm_obs_var_clip=1e-06):
        super().__init__()
        self._obs_n_dim = len(observation_shape)
        self._action_size = action_size
        hidden_sizes = hidden_sizes or [256, 256]
        mlp_input_size = int(np.prod(observation_shape))
        self.mlp = MlpModel(input_size=mlp_input_size, hidden_sizes=hidden_sizes, output_size=None, nonlinearity=nonlinearity)
        mlp_output_size = hidden_sizes[-1] if hidden_sizes else mlp_input_size
        self.lstm = torch.nn.LSTM(mlp_output_size + action_size + 1, lstm_size)
        self.head = torch.nn.Linear(lstm_size, action_size * 2 + 1)
        if normalize_observation:
            self.obs_rms = RunningMeanStdModel(observation_shape)
            self.norm_obs_clip = norm_obs_clip
            self.norm_obs_var_clip = norm_obs_var_clip
        self.normalize_observation = normalize_observation

    def forward(self, observation, prev_action, prev_reward, init_rnn_state):
        """
        Compute mean, log_std, and value estimate from input state. Infer
        leading dimensions of input: can be [T,B], [B], or []; provides
        returns with same leading dims.  Intermediate feedforward layers
        process as [T*B,H], and recurrent layers as [T,B,H], with T=1,B=1 when
        not given. Used both in sampler and in algorithm (both via the agent).
        Also returns the next RNN state.
        """
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_n_dim)
        if self.normalize_observation:
            obs_var = self.obs_rms.var
            if self.norm_obs_var_clip is not None:
                obs_var = torch.clamp(obs_var, min=self.norm_obs_var_clip)
            observation = torch.clamp((observation - self.obs_rms.mean) / obs_var.sqrt(), -self.norm_obs_clip, self.norm_obs_clip)
        mlp_out = self.mlp(observation.view(T * B, -1))
        lstm_input = torch.cat([mlp_out.view(T, B, -1), prev_action.view(T, B, -1), prev_reward.view(T, B, 1)], dim=2)
        init_rnn_state = None if init_rnn_state is None else tuple(init_rnn_state)
        lstm_out, (hn, cn) = self.lstm(lstm_input, init_rnn_state)
        outputs = self.head(lstm_out.view(T * B, -1))
        mu = outputs[:, :self._action_size]
        log_std = outputs[:, self._action_size:-1]
        v = outputs[:, (-1)]
        mu, log_std, v = restore_leading_dims((mu, log_std, v), lead_dim, T, B)
        next_rnn_state = RnnState(h=hn, c=cn)
        return mu, log_std, v, next_rnn_state

    def update_obs_rms(self, observation):
        if self.normalize_observation:
            self.obs_rms.update(observation)


class MuMlpModel(torch.nn.Module):
    """MLP neural net for action mean (mu) output for DDPG agent."""

    def __init__(self, observation_shape, hidden_sizes, action_size, output_max=1):
        """Instantiate neural net according to inputs."""
        super().__init__()
        self._output_max = output_max
        self._obs_ndim = len(observation_shape)
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)), hidden_sizes=hidden_sizes, output_size=action_size)

    def forward(self, observation, prev_action, prev_reward):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        mu = self._output_max * torch.tanh(self.mlp(observation.view(T * B, -1)))
        mu = restore_leading_dims(mu, lead_dim, T, B)
        return mu


class PiMlpModel(torch.nn.Module):
    """Action distrubition MLP model for SAC agent."""

    def __init__(self, observation_shape, hidden_sizes, action_size):
        super().__init__()
        self._obs_ndim = len(observation_shape)
        self._action_size = action_size
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)), hidden_sizes=hidden_sizes, output_size=action_size * 2)

    def forward(self, observation, prev_action, prev_reward):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        output = self.mlp(observation.view(T * B, -1))
        mu, log_std = output[:, :self._action_size], output[:, self._action_size:]
        mu, log_std = restore_leading_dims((mu, log_std), lead_dim, T, B)
        return mu, log_std


class QofMuMlpModel(torch.nn.Module):
    """Q portion of the model for DDPG, an MLP."""

    def __init__(self, observation_shape, hidden_sizes, action_size):
        """Instantiate neural net according to inputs."""
        super().__init__()
        self._obs_ndim = len(observation_shape)
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)) + action_size, hidden_sizes=hidden_sizes, output_size=1)

    def forward(self, observation, prev_action, prev_reward, action):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        q_input = torch.cat([observation.view(T * B, -1), action.view(T * B, -1)], dim=1)
        q = self.mlp(q_input).squeeze(-1)
        q = restore_leading_dims(q, lead_dim, T, B)
        return q


class VMlpModel(torch.nn.Module):

    def __init__(self, observation_shape, hidden_sizes, action_size=None):
        super().__init__()
        self._obs_ndim = len(observation_shape)
        self.mlp = MlpModel(input_size=int(np.prod(observation_shape)), hidden_sizes=hidden_sizes, output_size=1)

    def forward(self, observation, prev_action, prev_reward):
        lead_dim, T, B, _ = infer_leading_dims(observation, self._obs_ndim)
        v = self.mlp(observation.view(T * B, -1)).squeeze(-1)
        v = restore_leading_dims(v, lead_dim, T, B)
        return v


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (Conv2dModel,
     lambda: ([], {'in_channels': 4, 'channels': [4, 4], 'kernel_sizes': [4, 4], 'strides': [4, 4]}),
     lambda: ([torch.rand([4, 4, 64, 64])], {}),
     True),
    (DistributionalDuelingHeadModel,
     lambda: ([], {'input_size': 4, 'hidden_sizes': 4, 'output_size': 4, 'n_atoms': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DistributionalHeadModel,
     lambda: ([], {'input_size': 4, 'layer_sizes': 1, 'output_size': 4, 'n_atoms': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DuelingHeadModel,
     lambda: ([], {'input_size': 4, 'hidden_sizes': 4, 'output_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MlpModel,
     lambda: ([], {'input_size': 4, 'hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MuMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4, 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (MujocoFfModel,
     lambda: ([], {'observation_shape': [4, 4], 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (PiMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4, 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (QofMuMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4, 'action_size': 4}),
     lambda: ([torch.rand([4, 4, 2, 2]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (VMlpModel,
     lambda: ([], {'observation_shape': [4, 4], 'hidden_sizes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
]

class Test_astooke_rlpyt(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

