import sys
_module = sys.modules[__name__]
del sys
conf = _module
locallaunch = _module
mmpt = _module
datasets = _module
fairseqmmdataset = _module
mmdataset = _module
evaluators = _module
evaluator = _module
metric = _module
predictor = _module
losses = _module
fairseqmmloss = _module
loss = _module
nce = _module
models = _module
fairseqmmmodel = _module
mmfusion = _module
mmfusionnlg = _module
transformermodel = _module
modules = _module
mm = _module
retri = _module
vectorpool = _module
processors = _module
dedupprocessor = _module
dsprocessor = _module
how2processor = _module
how2retriprocessor = _module
s3dg = _module
processor = _module
tasks = _module
fairseqmmtask = _module
milncetask = _module
retritask = _module
task = _module
vlmtask = _module
utils = _module
load_config = _module
shardedtensor = _module
localjob = _module
predict = _module
pretokenization = _module
extract = _module
model = _module
pathbuilder = _module
preprocessing = _module
random_sequence_shuffler = _module
shard_feature = _module
videoreader = _module
setup = _module
examples = _module
adaptive_span = _module
adagrad_with_grad_clip = _module
adaptive_span_attention = _module
adaptive_span_loss = _module
adaptive_span_model = _module
adaptive_span_model_wrapper = _module
src = _module
data = _module
speech_to_text_dataset_with_domain = _module
attention_head_selection = _module
head_selection_s2t_transformer = _module
head_selection_transformer = _module
attn_head_selector = _module
head_selection_transformer_layer = _module
multihead_attention_selection = _module
multihead_functional = _module
speech_to_text_head_selection = _module
generate_manifests = _module
deduplicate_lines = _module
extract_bt_data = _module
summarize = _module
get_bitext = _module
gru_transformer = _module
normalize = _module
tok = _module
mine = _module
save_encoder = _module
encoder_analysis = _module
data2vec = _module
add_class_target_dataset = _module
image_dataset = _module
mae_finetuning_image_dataset = _module
mae_image_dataset = _module
modality = _module
path_dataset = _module
fb_convert_beit_cp = _module
audio_classification = _module
data2vec2 = _module
data2vec_audio = _module
data2vec_image_classification = _module
data2vec_text = _module
data2vec_text_classification = _module
data2vec_vision = _module
mae = _module
mae_image_classification = _module
modalities = _module
audio = _module
base = _module
images = _module
modules = _module
text = _module
utils = _module
convert_audioset_labels = _module
glue = _module
glue_lr = _module
unprocess_data = _module
valids = _module
audio_classification = _module
image_classification = _module
image_pretraining = _module
mae_image_classification = _module
mae_image_pretraining = _module
multimodal = _module
discriminative_reranking_nmt = _module
criterions = _module
discriminative_reranking_criterion = _module
drnmt_rerank = _module
discriminative_reranking_model = _module
prep_data = _module
discriminative_reranking_task = _module
emotion_models = _module
duration_predictor = _module
pitch_predictor = _module
utils = _module
fairseq_models = _module
preprocess = _module
build_hifigan_manifest = _module
build_translation_manifests = _module
create_core_manifest = _module
extract_f0 = _module
process_km = _module
split_emov_km_tsv_by_uttid = _module
split_km = _module
split_km_tsv = _module
synthesize = _module
fast_noisy_channel = _module
noisy_channel_beam_search = _module
noisy_channel_sequence_generator = _module
noisy_channel_translation = _module
measure_teacher_quality = _module
dump_hubert_feature = _module
dump_hubert_feature_s2t = _module
dump_km_label = _module
dump_mfcc_feature = _module
dump_w2v2_feature = _module
feature_utils = _module
learn_kmeans = _module
update_ckpt = _module
laser_src = _module
laser_lstm = _module
laser_task = _module
laser_transformer = _module
multitask_data_utils = _module
latent_depth_src = _module
latent_depth = _module
latent_multilingual_transformer = _module
latent_transformer = _module
latent_layers = _module
multilingual_translation_latent_depth = _module
linformer_src = _module
linformer_roberta = _module
linformer_sentence_encoder = _module
linformer_sentence_encoder_layer = _module
multihead_linear_attention = _module
clean_histogram = _module
dedup_data = _module
remove_too_much_punc = _module
tokenize_indic = _module
tokenize_thai = _module
tokenize_zh = _module
detok = _module
binarize = _module
check_iswlt_test_data = _module
check_self_overlaps = _module
check_valid_test_overlaps = _module
dedup_all = _module
download_ted_and_extract = _module
download_wmt19_and_before = _module
remove_valid_test_in_train = _module
dedup = _module
fasttext_multi_filter = _module
noisychannel = _module
rerank = _module
rerank_generate = _module
rerank_options = _module
rerank_score_bw = _module
rerank_score_lm = _module
rerank_tune = _module
rerank_utils = _module
paraphrase = _module
pointer_generator_src = _module
transformer_pg = _module
postprocess = _module
commonsense_qa = _module
commonsense_qa_task = _module
multiprocessing_bpe_encoder = _module
preprocess_RACE = _module
wsc = _module
wsc_criterion = _module
wsc_task = _module
wsc_utils = _module
rxf = _module
rxf_src = _module
label_smoothed_cross_entropy_r3f = _module
sentence_prediction_r3f = _module
simultaneous_translation = _module
simul_t2t_enja = _module
convtransformer_simul_trans = _module
transformer_monotonic_attention = _module
fixed_pre_decision = _module
monotonic_multihead_attention = _module
monotonic_transformer_layer = _module
test_alignment_train = _module
test_text_models = _module
functions = _module
monotonic_attention = _module
p_choose_strategy = _module
speech_recognition = _module
ASG_loss = _module
cross_entropy_acc = _module
asr_dataset = _module
collaters = _module
data_utils = _module
replabels = _module
asr_prep_json = _module
infer = _module
kaldi = _module
kaldi_decoder = _module
kaldi_initializer = _module
vggtransformer = _module
w2l_conv_glu_enc = _module
new = _module
decoders = _module
base_decoder = _module
decoder = _module
decoder_config = _module
flashlight_decoder = _module
viterbi_decoder = _module
infer = _module
speech_recognition = _module
wer_utils = _module
w2l_decoder = _module
speech_synthesis = _module
data_utils = _module
evaluation = _module
eval_asr = _module
eval_f0 = _module
eval_sp = _module
get_eval_manifest = _module
generate_waveform = _module
denoise_and_vad_audio = _module
denoiser = _module
demucs = _module
pretrained = _module
resample = _module
utils = _module
get_common_voice_audio_manifest = _module
get_feature_manifest = _module
get_ljspeech_audio_manifest = _module
get_speaker_embedding = _module
get_vctk_audio_manifest = _module
speaker_embedder = _module
vad = _module
utils = _module
speech_text_joint_to_text = _module
multi_modality_compound = _module
multi_modality_cross_entropy = _module
text_guide_cross_entropy_acc = _module
pair_denoising_dataset = _module
joint_speech_text_pretrain_transformer = _module
s2t_dualinputtransformer = _module
s2t_dualinputwavtransformer = _module
s2t_dualinputxmtransformer = _module
convert_model = _module
g2p_encode = _module
pair_denoising = _module
speech_text_denoise_pretrain = _module
speech_text_joint = _module
speech_to_speech = _module
core = _module
data_utils = _module
get_metrics = _module
generate_waveform_from_code = _module
prep_s2spect_data = _module
prep_s2ut_data = _module
prep_sn_data = _module
prep_sn_output_data = _module
unity = _module
sequence_generator = _module
sequence_generator_multi_decoder = _module
data_utils = _module
prep_covost_data = _module
prep_librispeech_data = _module
prep_mtedx_data = _module
prep_mustc_data = _module
seg_mustc_data = _module
fairseq_simul_st_agent = _module
dump_abx_feats = _module
continuation_eval = _module
bleu_utils = _module
cut_as = _module
ppx = _module
self_auto_bleu = _module
speech2unit = _module
clustering = _module
cluster_kmeans = _module
dump_feats = _module
quantize_with_kmeans = _module
cpc_feature_reader = _module
hubert_feature_reader = _module
logmel_feature_reader = _module
utils = _module
w2v2_feature_reader = _module
resynthesize_speech = _module
sample = _module
convert_to_16k = _module
glow = _module
multiproc = _module
synthesize_audio_from_units = _module
tacotron2 = _module
audio_processing = _module
cleaners = _module
cmudict = _module
layers = _module
model = _module
numbers = _module
stft = _module
symbols = _module
utils = _module
waveglow_denoiser = _module
tts_data = _module
utils = _module
data_utils = _module
eval = _module
cont_metrics = _module
generate_waveform = _module
inference_dataset = _module
naive_decoder = _module
prepare_dataset = _module
preprocess_f0 = _module
quantize_f0 = _module
sample = _module
join_units_manifest = _module
truncated_laplace = _module
score = _module
translation_moe_src = _module
logsumexp_moe = _module
mean_pool_gating_network = _module
translation_moe = _module
truncated_bptt = _module
transformer_xl_model = _module
truncated_bptt_lm_task = _module
aggregate_scores = _module
meteor = _module
repeat_lines = _module
wav2vec = _module
libri_labels = _module
unsupervised = _module
extracted_features_dataset = _module
random_input_dataset = _module
copy_aligned_text = _module
prepare_data_from_w2v = _module
unsup_select = _module
wav2vec_u = _module
apply_pca = _module
copy_labels = _module
filter_lexicon = _module
filter_tsv = _module
g2p_wrd_to_phn = _module
ltr_to_wrd = _module
mean_pool = _module
merge_clusters = _module
normalize_and_filter_text = _module
normalize_text = _module
pca = _module
phonemize_with_sil = _module
remove_silence = _module
vads = _module
wav2vec_apply_cluster_faiss = _module
wav2vec_cluster_faiss = _module
wav2vec_extract_features = _module
wer = _module
wrd_to_ltr = _module
unpaired_audio_text = _module
w2vu_generate = _module
wav2vec_featurize = _module
wav2vec_manifest = _module
eval_speaker_clf_task = _module
gen_audio_embedding = _module
query_occupations_from_wikidata = _module
preprocess_nli = _module
fairseq = _module
benchmark = _module
benchmark_multihead_attention = _module
dummy_dataset = _module
dummy_lm = _module
dummy_masked_lm = _module
dummy_model = _module
dummy_mt = _module
binarizer = _module
checkpoint_utils = _module
config = _module
adaptive_loss = _module
composite_loss = _module
cross_entropy = _module
ctc = _module
fairseq_criterion = _module
fastspeech2_loss = _module
hubert_criterion = _module
label_smoothed_cross_entropy = _module
label_smoothed_cross_entropy_latency_augmented = _module
label_smoothed_cross_entropy_with_alignment = _module
label_smoothed_cross_entropy_with_ctc = _module
label_smoothed_cross_entropy_with_rdrop = _module
legacy_masked_lm = _module
masked_lm = _module
model_criterion = _module
nat_loss = _module
sentence_prediction = _module
sentence_prediction_adapters = _module
sentence_ranking = _module
speech_to_speech_criterion = _module
speech_ulm_criterion = _module
tacotron2_loss = _module
wav2vec_criterion = _module
add_class_target_dataset = _module
add_target_dataset = _module
append_token_dataset = _module
audio_utils = _module
data_cfg = _module
dataset_transforms = _module
concataugment = _module
noisyoverlapaugment = _module
feature_transforms = _module
delta_deltas = _module
global_cmvn = _module
specaugment = _module
utterance_cmvn = _module
frm_text_to_speech_dataset = _module
hubert_dataset = _module
multi_modality_dataset = _module
raw_audio_dataset = _module
speech_to_speech_dataset = _module
speech_to_text_dataset = _module
speech_to_text_joint_dataset = _module
text_to_speech_dataset = _module
waveform_transforms = _module
noiseaugment = _module
backtranslation_dataset = _module
base_wrapper_dataset = _module
bucket_pad_length_dataset = _module
codedataset = _module
colorize_dataset = _module
concat_dataset = _module
concat_sentences_dataset = _module
data_utils = _module
denoising_dataset = _module
dictionary = _module
encoders = _module
byte_bpe = _module
byte_utils = _module
bytes = _module
characters = _module
fastbpe = _module
gpt2_bpe = _module
gpt2_bpe_utils = _module
hf_bert_bpe = _module
hf_byte_bpe = _module
moses_tokenizer = _module
nltk_tokenizer = _module
sentencepiece_bpe = _module
space_tokenizer = _module
subword_nmt_bpe = _module
utils = _module
fairseq_dataset = _module
fasta_dataset = _module
huffman = _module
huffman_coder = _module
huffman_mmap_indexed_dataset = _module
id_dataset = _module
indexed_dataset = _module
iterators = _module
language_pair_dataset = _module
legacy = _module
block_pair_dataset = _module
masked_lm_dataset = _module
masked_lm_dictionary = _module
list_dataset = _module
lm_context_window_dataset = _module
lru_cache_dataset = _module
mask_tokens_dataset = _module
monolingual_dataset = _module
multi_corpus_dataset = _module
multi_corpus_sampled_dataset = _module
multilingual = _module
multilingual_data_manager = _module
multilingual_utils = _module
sampled_multi_dataset = _module
sampled_multi_epoch_dataset = _module
sampling_method = _module
nested_dictionary_dataset = _module
noising = _module
num_samples_dataset = _module
numel_dataset = _module
offset_tokens_dataset = _module
pad_dataset = _module
padding_mask_dataset = _module
plasma_utils = _module
prepend_dataset = _module
prepend_token_dataset = _module
raw_label_dataset = _module
replace_dataset = _module
resampling_dataset = _module
roll_dataset = _module
round_robin_zip_datasets = _module
shorten_dataset = _module
sort_dataset = _module
span_mask_tokens_dataset = _module
strip_token_dataset = _module
subsample_dataset = _module
text_compressor = _module
token_block_dataset = _module
transform_eos_concat_langpair_dataset = _module
transform_eos_dataset = _module
transform_eos_lang_pair_dataset = _module
dataclass = _module
configs = _module
constants = _module
initialize = _module
distributed = _module
distributed_timeout_wrapper = _module
fully_sharded_data_parallel = _module
legacy_distributed_data_parallel = _module
module_proxy_wrapper = _module
tpu_distributed_data_parallel = _module
utils = _module
file_chunker_utils = _module
file_io = _module
file_utils = _module
hub_utils = _module
incremental_decoding_utils = _module
iterative_refinement_generator = _module
logging = _module
meters = _module
metrics = _module
progress_bar = _module
model_parallel = _module
vocab_parallel_cross_entropy = _module
megatron_trainer = _module
pipeline_parallel_transformer = _module
layers = _module
model = _module
roberta = _module
model = _module
transformer = _module
transformer_lm = _module
multihead_attention = _module
transformer_layer = _module
bart = _module
hub_interface = _module
model = _module
composite_encoder = _module
distributed_fairseq_model = _module
ema = _module
ema = _module
fairseq_decoder = _module
fairseq_encoder = _module
fairseq_incremental_decoder = _module
fairseq_model = _module
fconv = _module
fconv_lm = _module
fconv_self_att = _module
hubert = _module
hubert = _module
hubert_asr = _module
huggingface = _module
hf_gpt2 = _module
lightconv = _module
lightconv_lm = _module
lstm = _module
lstm_lm = _module
masked_lm = _module
model_utils = _module
multilingual_transformer = _module
nat = _module
cmlm_transformer = _module
fairseq_nat_model = _module
insertion_transformer = _module
iterative_nonautoregressive_transformer = _module
levenshtein_transformer = _module
levenshtein_utils = _module
nat_crf_transformer = _module
nonautoregressive_ensembles = _module
nonautoregressive_transformer = _module
alignment_utils = _module
enc_dec = _module
hub_interface = _module
model = _module
model_camembert = _module
model_gottbert = _module
model_xlmr = _module
ctc_decoder = _module
stacked_embedding = _module
transformer_decoder_aug = _module
transformer_encoder = _module
s2s_conformer = _module
s2s_conformer_translatotron2 = _module
s2s_conformer_unity = _module
s2s_transformer = _module
speech_to_text = _module
berard = _module
convtransformer = _module
hub_interface = _module
augmented_memory_attention = _module
convolution = _module
emformer = _module
multi_modality_model = _module
s2t_conformer = _module
s2t_transformer = _module
s2t_wav_transformer = _module
utils = _module
xm_transformer = _module
xm_transformer_unity = _module
text_to_speech = _module
codehifigan = _module
fastspeech2 = _module
hifigan = _module
hub_interface = _module
tacotron2 = _module
tts_transformer = _module
vocoder = _module
transformer_base = _module
transformer_config = _module
transformer_decoder = _module
transformer_decoder_aug = _module
transformer_encoder = _module
transformer_legacy = _module
transformer_align = _module
transformer_from_pretrained_xlm = _module
transformer_lm = _module
transformer_ulm = _module
utils = _module
wav2vec = _module
wav2vec2 = _module
wav2vec2_asr = _module
xmod = _module
hub_interface = _module
model = _module
transformer_layer_xmod = _module
adaptive_input = _module
adaptive_softmax = _module
base_layer = _module
beamable_mm = _module
character_token_embedder = _module
checkpoint_activations = _module
conformer_layer = _module
conv_tbc = _module
cross_entropy = _module
downsampled_multihead_attention = _module
dynamic_convolution = _module
dynamic_crf_layer = _module
dynamicconv_layer = _module
cuda_function_gen = _module
dynamicconv_layer = _module
setup = _module
ema_module = _module
espnet_multihead_attention = _module
fairseq_dropout = _module
fp32_batch_norm = _module
fp32_group_norm = _module
fp32_instance_norm = _module
gelu = _module
grad_multiply = _module
gumbel_vector_quantizer = _module
kmeans_attention = _module
kmeans_vector_quantizer = _module
layer_drop = _module
layer_norm = _module
learned_positional_embedding = _module
lightconv_layer = _module
lightconv_layer = _module
setup = _module
lightweight_convolution = _module
linearized_convolution = _module
location_attention = _module
lstm_cell_with_zoneout = _module
multihead_attention = _module
positional_embedding = _module
positional_encoding = _module
quant_noise = _module
quantization = _module
pq = _module
em = _module
qconv = _module
qemb = _module
qlinear = _module
utils = _module
quantization_options = _module
scalar = _module
qact = _module
qconv = _module
qemb = _module
qlinear = _module
ops = _module
utils = _module
rotary_positional_embedding = _module
same_pad = _module
scalar_bias = _module
sinusoidal_positional_embedding = _module
sparse_multihead_attention = _module
sparse_transformer_sentence_encoder = _module
sparse_transformer_sentence_encoder_layer = _module
transformer_layer = _module
transformer_layer_aug = _module
transformer_sentence_encoder = _module
transformer_sentence_encoder_layer = _module
transpose_last = _module
unfold = _module
vggblock = _module
nan_detector = _module
ngram_repeat_block = _module
optim = _module
adadelta = _module
adafactor = _module
adagrad = _module
adam = _module
adamax = _module
amp_optimizer = _module
bmuf = _module
composite = _module
cpu_adam = _module
dynamic_loss_scaler = _module
fairseq_optimizer = _module
fp16_optimizer = _module
fused_adam = _module
fused_lamb = _module
lr_scheduler = _module
cosine_lr_scheduler = _module
fairseq_lr_scheduler = _module
fixed_schedule = _module
inverse_square_root_schedule = _module
manual_lr_scheduler = _module
pass_through = _module
polynomial_decay_schedule = _module
reduce_lr_on_plateau = _module
step_lr_scheduler = _module
tri_stage_lr_scheduler = _module
triangular_lr_scheduler = _module
nag = _module
sgd = _module
shard = _module
options = _module
pdb = _module
quantization_utils = _module
registry = _module
scoring = _module
bertscore = _module
bleu = _module
chrf = _module
tokenizer = _module
search = _module
sequence_generator = _module
sequence_scorer = _module
speech_generator = _module
audio_finetuning = _module
audio_pretraining = _module
cross_lingual_lm = _module
denoising = _module
fairseq_task = _module
frm_text_to_speech = _module
hubert_pretraining = _module
language_modeling = _module
multilingual_denoising = _module
multilingual_language_modeling = _module
multilingual_masked_lm = _module
multilingual_translation = _module
nlu_finetuning = _module
online_backtranslation = _module
semisupervised_translation = _module
span_masked_lm = _module
speech_to_speech = _module
speech_ulm_task = _module
text_to_speech = _module
translation = _module
translation_from_pretrained_bart = _module
translation_from_pretrained_xlm = _module
translation_lev = _module
translation_multi_simple_epoch = _module
token_generation_constraints = _module
trainer = _module
utils = _module
fairseq_cli = _module
eval_lm = _module
generate = _module
hydra_train = _module
hydra_validate = _module
interactive = _module
train = _module
validate = _module
hubconf = _module
dependency_submitit_launcher = _module
launcher = _module
release_utils = _module
scripts = _module
average_checkpoints = _module
build_sym_alignment = _module
check_installation = _module
compare_namespaces = _module
count_docs = _module
read_binarized = _module
rm_pt = _module
shard_docs = _module
split_train_valid_docs = _module
spm_decode = _module
spm_encode = _module
spm_train = _module
setup = _module
tests = _module
test_bmuf = _module
test_distributed_timeout_wrapper = _module
test_module_proxy_wrapper = _module
test_utils = _module
utils = _module
gpu = _module
test_binaries_gpu = _module
test_ema_gpu = _module
speech = _module
test_convtransformer_simul_trans = _module
test_dual_input_wav_transformer = _module
test_dualinput_s2t_transformer = _module
test_fastspeech2 = _module
test_s2s_transformer = _module
test_s2t_conformer = _module
test_s2t_transformer = _module
test_tts_transformer = _module
test_wav2vec2 = _module
test_xm_transformer = _module
asr_test_base = _module
test_collaters = _module
test_cross_entropy = _module
test_data_utils = _module
test_vggtransformer = _module
test_denoising = _module
test_masked_lm = _module
test_multilingual_denoising = _module
test_span_masked_lm = _module
test_activation_checkpointing = _module
test_amp_optimizer = _module
test_average_checkpoints = _module
test_backtranslation_dataset = _module
test_binaries = _module
test_binarizer = _module
test_character_token_embedder = _module
test_checkpoint_utils = _module
test_concat_dataset = _module
test_constraints = _module
test_convtbc = _module
test_dataclass_utils = _module
test_dataset = _module
test_dictionary = _module
test_ema = _module
test_espnet_multihead_attention = _module
test_export = _module
test_file_chunker_utils = _module
test_file_io = _module
test_fp16_optimizer = _module
test_hf_hub = _module
test_huffman = _module
test_inference_dropout = _module
test_iopath = _module
test_iterators = _module
test_label_smoothing = _module
test_lm_context_window = _module
test_lstm_jitable = _module
test_memory_efficient_fp16 = _module
test_metrics = _module
test_multi_corpus_dataset = _module
test_multi_corpus_sampled_dataset = _module
test_multihead_attention = _module
test_noising = _module
test_online_backtranslation = _module
test_plasma_utils = _module
test_positional_encoding = _module
test_reproducibility = _module
test_resampling_dataset = _module
test_roberta = _module
test_rotary_positional_embedding = _module
test_sequence_generator = _module
test_sequence_scorer = _module
test_sparse_multihead_attention = _module
test_token_block_dataset = _module
test_train = _module
test_transformer = _module
test_utils = _module
test_valid_subset_checks = _module
utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from collections import OrderedDict


from torch.utils.data import Dataset


from torch.utils.data.dataloader import default_collate


import torch


import random


import numpy as np


import math


from torch import nn


from torch.nn import functional as F


from typing import Optional


from typing import Iterable


from collections import defaultdict


from collections import deque


from typing import Tuple


from typing import List


import torch as th


import torch.nn.functional as F


import torch.nn as nn


import re


from torch.utils.data import DataLoader


from torch.utils.data.distributed import DistributedSampler


from torch.utils.data.sampler import Sampler


import pandas as pd


from torch.optim import Adagrad


import logging


from typing import Dict


from torch.nn.modules.loss import _Loss


from torch import Tensor


from typing import Any


from torch.nn import Parameter


from torch.nn.functional import linear


from torch.nn.functional import softmax


from torch.nn.functional import dropout


from torch.nn.functional import pad


from torch.nn.functional import has_torch_function


from torch.nn.functional import handle_torch_function


from torch.nn.functional import _in_projection_packed


import warnings


from typing import Callable


from typing import Set


from torchvision.datasets.vision import VisionDataset


from torchvision.transforms import ToTensor


from torchvision import datasets


from torchvision import transforms


from functools import partial


import time


import torch.distributed as dist


from enum import Enum


from enum import auto


from collections import namedtuple


from sklearn import metrics as sklearn_metrics


import itertools


from scipy.io.wavfile import read


from scipy.ndimage import gaussian_filter1d


import torchaudio


from typing import NamedTuple


from torch.testing._internal.common_utils import TestCase


from collections.abc import Iterable


import itertools as it


from typing import Union


from itertools import groupby


import matplotlib.pyplot as plt


import torch.hub


import functools


import inspect


import torch.utils.data


from scipy.interpolate import interp1d


import copy


from functools import reduce


import torchaudio.compliance.kaldi as kaldi


from torch.autograd import Variable


from scipy.signal import get_window


from math import sqrt


import torch.distributions as distr


import collections


import scipy


import torch.multiprocessing as mp


from torch.utils.data import DistributedSampler


from types import SimpleNamespace


from itertools import starmap


from torch.distributions.categorical import Categorical


from torch import autograd


import sklearn


from torch.utils import benchmark


import typing as tp


from abc import ABC


from abc import abstractmethod


from collections import Counter


from itertools import chain


from sklearn.metrics import f1_score


from sklearn.metrics import matthews_corrcoef as _matthews_corrcoef


from scipy.stats import pearsonr


from scipy.stats import spearmanr


from functools import lru_cache


from typing import BinaryIO


import queue


from typing import Iterator


from typing import Sequence


from typing import Mapping


from functools import wraps


import uuid


from numbers import Number


from torch.nn.parallel import DistributedDataParallel


from torch import device as Device


from itertools import repeat


from torch.nn import Conv1d


from torch.nn import ConvTranspose1d


from torch.nn.utils import remove_weight_norm


from torch.nn.utils import weight_norm


import torch.utils.checkpoint as checkpoint


from torch.nn.modules.utils import _single


from torch.autograd import Function


from torch.utils.cpp_extension import BuildExtension


from torch.utils.cpp_extension import CUDAExtension


from inspect import isfunction


from torch.nn.modules.utils import _pair


from torch.nn.modules.conv import _ConvNd


import torch.onnx.operators


from numpy.random import uniform


import torch.optim


from collections.abc import Collection


import types


import torch.optim.lr_scheduler


from torch.optim.optimizer import Optimizer


from torch.optim.optimizer import required


from itertools import accumulate


from typing import TYPE_CHECKING


import numpy


from torch.utils import cpp_extension


from copy import deepcopy


from inspect import currentframe


from inspect import getframeinfo


from torch.utils.checkpoint import checkpoint


from torch.cuda.amp import GradScaler


from torch.cuda.amp import autocast


import string


class MMPTModel(nn.Module):
    """An e2e wrapper of inference model.
    """

    @classmethod
    def from_pretrained(cls, config, checkpoint='checkpoint_best.pt'):
        config = recursive_config(config)
        mmtask = Task.config_task(config)
        checkpoint_path = os.path.join(config.eval.save_path, checkpoint)
        mmtask.build_model(checkpoint=checkpoint_path)
        video_encoder = S3D('pretrained_models/s3d_dict.npy', 512)
        video_encoder.load_state_dict(torch.load('pretrained_models/s3d_howto100m.pth'))
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name, use_fast=config.dataset.use_fast)
        aligner = Aligner(config.dataset)
        return MMPTModel(config, mmtask.model, video_encoder), tokenizer, aligner

    def __init__(self, config, model, video_encoder, **kwargs):
        super().__init__()
        self.max_video_len = config.dataset.max_video_len
        self.video_encoder = video_encoder
        self.model = model

    def forward(self, video_frames, caps, cmasks, return_score=False):
        bsz = video_frames.size(0)
        assert bsz == 1, 'only bsz=1 is supported now.'
        seq_len = video_frames.size(1)
        video_frames = video_frames.view(-1, *video_frames.size()[2:])
        vfeats = self.video_encoder(video_frames.permute(0, 4, 1, 2, 3))
        vfeats = vfeats['video_embedding']
        vfeats = vfeats.view(bsz, seq_len, vfeats.size(-1))
        padding = torch.zeros(bsz, self.max_video_len - seq_len, vfeats.size(-1))
        vfeats = torch.cat([vfeats, padding], dim=1)
        vmasks = torch.cat([torch.ones((bsz, seq_len), dtype=torch.bool), torch.zeros((bsz, self.max_video_len - seq_len), dtype=torch.bool)], dim=1)
        output = self.model(caps, cmasks, vfeats, vmasks)
        if return_score:
            output = {'score': torch.bmm(output['pooled_video'][:, None, :], output['pooled_text'][:, :, None]).squeeze(-1).squeeze(-1)}
        return output


class MMFusion(nn.Module):
    """a MMPT wrapper class for MMBert style models.
    TODO: move isolated mask to a subclass.
    """

    def __init__(self, config, **kwargs):
        super().__init__()
        transformer_config = AutoConfig.from_pretrained(config.dataset.bert_name)
        self.hidden_size = transformer_config.hidden_size
        self.is_train = False
        if config.dataset.train_path is not None:
            self.is_train = True
        self.num_hidden_layers = transformer_config.num_hidden_layers
        self.last_iso_layer = 0
        if config.dataset.num_iso_layer is not None:
            self.last_iso_layer = config.dataset.num_iso_layer - 1 + 1
        if config.model.mm_encoder_cls is not None:
            mm_encoder_cls = getattr(transformermodel, config.model.mm_encoder_cls)
            model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
            model_config.max_video_len = config.dataset.max_video_len
            model_config.use_seg_emb = config.model.use_seg_emb
            self.mm_encoder = mm_encoder_cls.from_pretrained(config.dataset.bert_name, config=model_config)
        elif config.model.video_encoder_cls is not None and config.model.text_encoder_cls is not None:
            video_encoder_cls = getattr(transformermodel, config.model.video_encoder_cls)
            model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
            model_config.max_video_len = config.dataset.max_video_len
            if hasattr(model_config, 'num_layers'):
                model_config.num_layers = config.model.num_hidden_video_layers
            else:
                model_config.num_hidden_layers = config.model.num_hidden_video_layers
            self.video_encoder = video_encoder_cls.from_pretrained(config.dataset.bert_name, config=model_config)
            text_encoder_cls = getattr(transformermodel, config.model.text_encoder_cls)
            self.text_encoder = text_encoder_cls.from_pretrained(config.dataset.bert_name)
        else:
            raise ValueError('the encoder must be either MM or two backbones.')

    def forward(self, caps, cmasks, vfeats, vmasks, **kwargs):
        raise NotImplementedError('Please derive MMFusion module.')

    def _mm_on_the_fly(self, cmasks, vmasks, attention_mask):
        """helper function for mask, seg_ids and token_type_ids."""
        if attention_mask is None:
            attention_mask = self._mm_attention_mask(cmasks, vmasks)
        """
        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
        | first sequence    | second sequence |
        """
        token_type_ids = torch.cat([torch.zeros((vmasks.size(0), vmasks.size(1) + 2), dtype=torch.long, device=vmasks.device), torch.ones((cmasks.size(0), cmasks.size(1) - 2), dtype=torch.long, device=cmasks.device)], dim=1)
        return attention_mask, token_type_ids

    def _mm_attention_mask(self, cmasks, vmasks):
        assert cmasks.size(0) == vmasks.size(0), '{}, {}, {}, {}'.format(str(cmasks.size()), str(vmasks.size()), str(cmasks.size(0)), str(vmasks.size(0)))
        mm_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:]], dim=1)
        if self.last_iso_layer == 0:
            return mm_mask
        else:
            batch_size = cmasks.size(0)
            iso_mask = self._make_iso_mask(batch_size, cmasks, vmasks)
            mm_mask = mm_mask[:, None, :].repeat(1, mm_mask.size(-1), 1)
            iso_mm_masks = []
            iso_mask = iso_mask[:, None, :, :].repeat(1, self.last_iso_layer, 1, 1)
            iso_mm_masks.append(iso_mask)
            if self.last_iso_layer < self.num_hidden_layers:
                mm_mask = mm_mask[:, None, :, :].repeat(1, self.num_hidden_layers - self.last_iso_layer, 1, 1)
                iso_mm_masks.append(mm_mask)
            iso_mm_masks = torch.cat(iso_mm_masks, dim=1)
            return iso_mm_masks

    def _make_iso_mask(self, batch_size, cmasks, vmasks):
        cls_self_mask = torch.cat([torch.ones((batch_size, 1), dtype=torch.bool, device=cmasks.device), torch.zeros((batch_size, cmasks.size(1) + vmasks.size(1) - 1), dtype=torch.bool, device=cmasks.device)], dim=1)
        iso_video_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=cmasks.device), vmasks, cmasks[:, 1:2], torch.zeros((batch_size, cmasks.size(1) - 2), dtype=torch.bool, device=cmasks.device)], dim=1)
        iso_text_mask = torch.cat([torch.zeros((batch_size, 2 + vmasks.size(1)), dtype=torch.bool, device=cmasks.device), cmasks[:, 2:]], dim=1)
        cls_self_mask = cls_self_mask[:, None, :]
        iso_video_mask = iso_video_mask[:, None, :].repeat(1, vmasks.size(1) + 1, 1)
        iso_text_mask = iso_text_mask[:, None, :].repeat(1, cmasks.size(1) - 2, 1)
        return torch.cat([cls_self_mask, iso_video_mask, iso_text_mask], dim=1)

    def _pooling_vt_layer(self, layered_sequence_output, cmasks, vmasks):
        layer_idx = self.last_iso_layer if self.last_iso_layer > 0 else self.num_hidden_layers
        hidden_state = layered_sequence_output[layer_idx]
        batch_size = cmasks.size(0)
        text_offset = vmasks.size(1) + 2
        video_outputs = hidden_state[:, 1:text_offset]
        video_attention_mask = torch.cat([vmasks, torch.ones((batch_size, 1), dtype=torch.bool, device=vmasks.device)], dim=1)
        assert video_outputs.size(1) == video_attention_mask.size(1)
        pooled_video = torch.sum(video_outputs * video_attention_mask.unsqueeze(-1), dim=1) / video_attention_mask.sum(1, keepdim=True)
        text_attention_mask = cmasks[:, 2:]
        text_outputs = hidden_state[:, text_offset:]
        assert text_outputs.size(1) == text_attention_mask.size(1)
        pooled_text = torch.sum(text_outputs * text_attention_mask.unsqueeze(-1), dim=1) / text_attention_mask.sum(1, keepdim=True)
        return pooled_video, pooled_text


class MMFusionMFMMLM(MMFusion):
    """forward function for MFM and MLM."""

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, video_label=None, text_label=None, **kwargs):
        output_hidden_states = False if self.is_train else True
        target_vfeats, non_masked_frame_mask = None, None
        if video_label is not None:
            target_vfeats = vfeats.masked_select(video_label.unsqueeze(-1)).view(-1, vfeats.size(-1))
            vfeats[video_label] = 0.0
            non_masked_frame_mask = vmasks.clone()
            non_masked_frame_mask[video_label] = False
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, masked_frame_labels=video_label, target_video_hidden_states=target_vfeats, non_masked_frame_mask=non_masked_frame_mask, masked_lm_labels=text_label, output_hidden_states=output_hidden_states)
        video_logits, text_logits = outputs[0], outputs[1]
        if self.is_train:
            return {'video_logits': video_logits, 'text_logits': text_logits}
        pooled_video, pooled_text = self._pooling_vt_layer(outputs[2], cmasks, vmasks)
        return {'pooled_video': pooled_video, 'pooled_text': pooled_text}


class MMFusionMTM(MMFusionMFMMLM):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        """
        For reproducibility:
        self.mm_encoder will be initialized then discarded.
        """
        model_config = AutoConfig.from_pretrained(config.dataset.bert_name)
        model_config.max_video_len = config.dataset.max_video_len
        model_config.use_seg_emb = config.model.use_seg_emb
        self.mm_encoder = MMBertForMTM.from_pretrained(config.dataset.bert_name, config=model_config)


class MMFusionShare(MMFusion):
    """A retrival wrapper using mm_encoder as both video/text backbone.
    TODO: move formally.
    """

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, video_label=None, text_label=None, output_hidden_states=False, **kwargs):
        pooled_video = self.forward_video(vfeats, vmasks, caps, cmasks, output_hidden_states)
        pooled_text = self.forward_text(caps, cmasks, output_hidden_states)
        return {'pooled_video': pooled_video, 'pooled_text': pooled_text}

    def forward_video(self, vfeats, vmasks, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = caps[:, :2]
        attention_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:2]], dim=1)
        token_type_ids = torch.zeros((vmasks.size(0), vmasks.size(1) + 2), dtype=torch.long, device=vmasks.device)
        outputs = self.mm_encoder(input_ids=input_ids, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        video_outputs = outputs[0]
        if output_hidden_states:
            return video_outputs
        batch_size = cmasks.size(0)
        video_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=vmasks.device), vmasks, torch.ones((batch_size, 1), dtype=torch.bool, device=vmasks.device)], dim=1)
        assert video_outputs.size(1) == video_attention_mask.size(1)
        video_attention_mask = video_attention_mask.type(video_outputs.dtype) / video_attention_mask.sum(1, keepdim=True)
        pooled_video = torch.bmm(video_outputs.transpose(2, 1), video_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_video

    def forward_text(self, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = torch.cat([caps[:, :1], caps[:, 2:]], dim=1)
        attention_mask = torch.cat([cmasks[:, :1], cmasks[:, 2:]], dim=1)
        token_type_ids = torch.cat([torch.zeros((cmasks.size(0), 1), dtype=torch.long, device=cmasks.device), torch.ones((cmasks.size(0), cmasks.size(1) - 2), dtype=torch.long, device=cmasks.device)], dim=1)
        outputs = self.mm_encoder(input_ids=input_ids, input_video_embeds=None, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        text_outputs = outputs[0]
        if output_hidden_states:
            return text_outputs
        batch_size = caps.size(0)
        text_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=cmasks.device), cmasks[:, 2:]], dim=1)
        assert text_outputs.size(1) == text_attention_mask.size(1)
        text_attention_mask = text_attention_mask.type(text_outputs.dtype) / text_attention_mask.sum(1, keepdim=True)
        pooled_text = torch.bmm(text_outputs.transpose(2, 1), text_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_text


class MMFusionSeparate(MMFusionShare):

    def forward_video(self, vfeats, vmasks, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = caps[:, :2]
        attention_mask = torch.cat([cmasks[:, :1], vmasks, cmasks[:, 1:2]], dim=1)
        token_type_ids = torch.zeros((vmasks.size(0), vmasks.size(1) + 2), dtype=torch.long, device=vmasks.device)
        outputs = self.video_encoder(input_ids=input_ids, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        video_outputs = outputs[0]
        if output_hidden_states:
            return video_outputs
        batch_size = cmasks.size(0)
        video_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=vmasks.device), vmasks, torch.ones((batch_size, 1), dtype=torch.bool, device=vmasks.device)], dim=1)
        assert video_outputs.size(1) == video_attention_mask.size(1)
        video_attention_mask = video_attention_mask.type(video_outputs.dtype) / video_attention_mask.sum(1, keepdim=True)
        pooled_video = torch.bmm(video_outputs.transpose(2, 1), video_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_video

    def forward_text(self, caps, cmasks, output_hidden_states=False, **kwargs):
        input_ids = torch.cat([caps[:, :1], caps[:, 2:]], dim=1)
        attention_mask = torch.cat([cmasks[:, :1], cmasks[:, 2:]], dim=1)
        token_type_ids = torch.zeros((cmasks.size(0), cmasks.size(1) - 1), dtype=torch.long, device=cmasks.device)
        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)
        text_outputs = outputs[0]
        if output_hidden_states:
            return text_outputs
        batch_size = caps.size(0)
        text_attention_mask = torch.cat([torch.zeros((batch_size, 1), dtype=torch.bool, device=cmasks.device), cmasks[:, 2:]], dim=1)
        assert text_outputs.size(1) == text_attention_mask.size(1)
        text_attention_mask = text_attention_mask.type(text_outputs.dtype) / text_attention_mask.sum(1, keepdim=True)
        pooled_text = torch.bmm(text_outputs.transpose(2, 1), text_attention_mask.unsqueeze(2)).squeeze(-1)
        return pooled_text


class MMFusionJoint(MMFusion):
    """fine-tuning wrapper for retrival task."""

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, video_label=None, text_label=None, **kwargs):
        output_hidden_states = True
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        separate_forward_split = None if self.is_train else vmasks.size(1) + 2
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states, separate_forward_split=separate_forward_split)
        pooled_video, pooled_text = self._pooling_vt_layer(outputs[2], cmasks, vmasks)
        return {'pooled_video': pooled_video, 'pooled_text': pooled_text}


class MMFusionActionSegmentation(MMFusion):
    """Fine-tuning wrapper for action segmentation.
    TODO: rename this for VLM.
    """

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, **kwargs):
        caps = caps.view(-1, caps.size(-1))
        cmasks = cmasks.view(-1, cmasks.size(-1))
        vfeats = vfeats.view(-1, vfeats.size(2), vfeats.size(3))
        vmasks = vmasks.view(-1, vmasks.size(-1))
        attention_mask = attention_mask.view(-1, attention_mask.size(2), attention_mask.size(3)) if attention_mask is not None else None
        output_hidden_states = True
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        logits = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states)
        return {'logits': logits[0][:, 1:vmasks.size(1) + 1]}


class MMFusionActionLocalization(MMFusion):
    """fine-tuning model for retrival task."""

    def __init__(self, config, **kwargs):
        super().__init__(config)
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, **kwargs):
        caps = caps.squeeze(0)
        cmasks = cmasks.squeeze(0)
        vfeats = vfeats.squeeze(0)
        vmasks = vmasks.squeeze(0)
        attention_mask = attention_mask.squeeze(0) if attention_mask is not None else None
        output_hidden_states = True
        dummy_vfeats = torch.zeros((caps.size(0), 1, vfeats.size(-1)), device=vfeats.device, dtype=vfeats.dtype)
        dummy_vmasks = torch.ones((caps.size(0), 1), dtype=torch.bool, device=vfeats.device)
        dummy_caps = torch.LongTensor([[self.cls_token_id, self.sep_token_id, self.pad_token_id, self.sep_token_id]]).repeat(vfeats.size(0), 1)
        dummy_cmasks = torch.BoolTensor([[0, 1, 0, 1]]).repeat(vfeats.size(0), 1)
        attention_mask, token_type_ids = self._mm_on_the_fly(dummy_cmasks, vmasks, None)
        outputs = self.mm_encoder(input_ids=dummy_caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states)
        layer_idx = self.last_iso_layer if self.last_iso_layer > 0 else self.num_hidden_layers
        video_seq = outputs[2][layer_idx][:, 1:vmasks.size(1) + 1].masked_select(vmasks.unsqueeze(-1)).view(-1, self.hidden_size)
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, dummy_vmasks, None)
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=dummy_vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=output_hidden_states)
        _, pooled_text = self._pooling_vt_layer(outputs[2], cmasks, dummy_vmasks)
        logits = torch.mm(video_seq, pooled_text.transpose(1, 0))
        return {'logits': logits}


class MMFusionSeparateActionSegmentation(MMFusionSeparate):
    """Fine-tuning wrapper for action segmentation."""

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask=None, **kwargs):
        caps = caps.view(-1, caps.size(-1))
        cmasks = cmasks.view(-1, cmasks.size(-1))
        vfeats = vfeats.view(-1, vfeats.size(2), vfeats.size(3))
        vmasks = vmasks.view(-1, vmasks.size(-1))
        logits = self.forward_video(vfeats, vmasks, caps, cmasks, output_hidden_states=True)
        return {'logits': logits[:, 1:vmasks.size(1) + 1]}


class MMFusionSeparateActionLocalization(MMFusionSeparate):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id

    def forward(self, caps, cmasks, vfeats, vmasks, **kwargs):
        caps = caps.squeeze(0)
        cmasks = cmasks.squeeze(0)
        vfeats = vfeats.squeeze(0)
        vmasks = vmasks.squeeze(0)
        dummy_caps = torch.LongTensor([[self.cls_token_id, self.sep_token_id, self.pad_token_id, self.sep_token_id]]).repeat(vfeats.size(0), 1)
        dummy_cmasks = torch.BoolTensor([[0, 1, 0, 1]]).repeat(vfeats.size(0), 1)
        outputs = self.forward_video(vfeats, vmasks, dummy_caps, dummy_cmasks, output_hidden_states=True)
        video_seq = outputs[:, 1:vmasks.size(1) + 1].masked_select(vmasks.unsqueeze(-1)).view(-1, self.hidden_size)
        pooled_text = self.forward_text(caps, cmasks, output_hidden_states=False)
        logits = torch.mm(video_seq, pooled_text.transpose(1, 0))
        return {'logits': logits}


class MMFusionShareActionLocalization(MMFusionShare):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        tokenizer = AutoTokenizer.from_pretrained(config.dataset.bert_name)
        self.cls_token_id = tokenizer.cls_token_id
        self.sep_token_id = tokenizer.sep_token_id
        self.pad_token_id = tokenizer.pad_token_id

    def forward(self, caps, cmasks, vfeats, vmasks, **kwargs):
        caps = caps.squeeze(0)
        cmasks = cmasks.squeeze(0)
        vfeats = vfeats.squeeze(0)
        vmasks = vmasks.squeeze(0)
        dummy_caps = torch.LongTensor([[self.cls_token_id, self.sep_token_id, self.pad_token_id, self.sep_token_id]]).repeat(vfeats.size(0), 1)
        dummy_cmasks = torch.BoolTensor([[0, 1, 0, 1]]).repeat(vfeats.size(0), 1)
        outputs = self.forward_video(vfeats, vmasks, dummy_caps, dummy_cmasks, output_hidden_states=True)
        video_seq = outputs[:, 1:vmasks.size(1) + 1].masked_select(vmasks.unsqueeze(-1)).view(-1, self.hidden_size)
        pooled_text = self.forward_text(caps, cmasks, output_hidden_states=False)
        logits = torch.mm(video_seq, pooled_text.transpose(1, 0))
        return {'logits': logits}


class MMFusionNLG(MMFusion):

    def __init__(self, config, **kwargs):
        super().__init__(config)
        if config.model.max_decode_length is not None:
            self.max_length = min(config.model.max_decode_length, config.dataset.max_len - config.dataset.max_video_len - 3)
        else:
            self.max_length = config.dataset.max_len - config.dataset.max_video_len - 3
        self.gen_param = config.gen_param if config.gen_param is not None else {}

    def forward(self, caps, cmasks, vfeats, vmasks, attention_mask, video_label=None, text_label=None, **kwargs):
        """use pre-trained LM header for generation."""
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        outputs = self.mm_encoder(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, masked_lm_labels=text_label)
        return {'logits': outputs[0]}

    @torch.no_grad()
    def generate(self, caps, cmasks, vfeats, vmasks, attention_mask=None, bos_token_id=None, eos_token_id=None, **kwargs):
        assert caps.size(1) == 3
        attention_mask, token_type_ids = self._mm_on_the_fly(cmasks, vmasks, attention_mask)
        output = self.mm_encoder.generate(input_ids=caps, input_video_embeds=vfeats, attention_mask=attention_mask, token_type_ids=token_type_ids, bos_token_id=bos_token_id, eos_token_id=eos_token_id, max_length=self.max_length, **self.gen_param)
        return output


class BertMFMMLMPredictionHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.bias = nn.Parameter(torch.zeros(config.vocab_size))
        self.decoder.bias = self.bias

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        video_logits, text_logits = None, None
        if video_hidden_states is not None:
            video_hidden_states = self.transform(video_hidden_states)
            non_masked_frame_logits = torch.mm(video_hidden_states, non_masked_frame_hidden_states.transpose(1, 0))
            masked_frame_logits = torch.bmm(video_hidden_states.unsqueeze(1), target_video_hidden_states.unsqueeze(-1)).squeeze(-1)
            video_logits = torch.cat([masked_frame_logits, non_masked_frame_logits], dim=1)
        if text_hidden_states is not None:
            text_hidden_states = self.transform(text_hidden_states)
            text_logits = self.decoder(text_hidden_states)
        return video_logits, text_logits


class MFMMLMHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.predictions = BertMFMMLMPredictionHead(config)

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        video_logits, text_logits = self.predictions(video_hidden_states, target_video_hidden_states, non_masked_frame_hidden_states, text_hidden_states)
        return video_logits, text_logits


class BertMTMPredictionHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        non_masked_frame_hidden_states = non_masked_frame_hidden_states.transpose(1, 0)
        video_logits, text_logits = None, None
        if video_hidden_states is not None:
            video_hidden_states = self.transform(video_hidden_states)
            masked_frame_logits = torch.bmm(video_hidden_states.unsqueeze(1), target_video_hidden_states.unsqueeze(-1)).squeeze(-1)
            non_masked_frame_logits = torch.mm(video_hidden_states, non_masked_frame_hidden_states)
            video_on_vocab_logits = self.decoder(video_hidden_states)
            video_logits = torch.cat([masked_frame_logits, non_masked_frame_logits, video_on_vocab_logits], dim=1)
        if text_hidden_states is not None:
            text_hidden_states = self.transform(text_hidden_states)
            text_on_vocab_logits = self.decoder(text_hidden_states)
            text_on_video_logits = torch.mm(text_hidden_states, non_masked_frame_hidden_states)
            text_logits = torch.cat([text_on_vocab_logits, text_on_video_logits], dim=1)
        return video_logits, text_logits


class MTMHead(nn.Module):

    def __init__(self, config):
        super().__init__()
        self.predictions = BertMTMPredictionHead(config)

    def forward(self, video_hidden_states=None, target_video_hidden_states=None, non_masked_frame_hidden_states=None, text_hidden_states=None):
        video_logits, text_logits = self.predictions(video_hidden_states, target_video_hidden_states, non_masked_frame_hidden_states, text_hidden_states)
        return video_logits, text_logits


class VideoTokenMLP(nn.Module):

    def __init__(self, config):
        super().__init__()
        input_dim = config.input_dim if hasattr(config, 'input_dim') else 512
        self.linear1 = nn.Linear(input_dim, config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size)
        self.activation = ACT2FN[config.hidden_act]
        self.linear2 = nn.Linear(config.hidden_size, config.hidden_size)

    def forward(self, hidden_states):
        hidden_states = self.linear1(hidden_states)
        hidden_states = self.activation(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        hidden_states = self.linear2(hidden_states)
        return hidden_states


class AlignHead(nn.Module):
    """this will load pre-trained weights for NSP, which is desirable."""

    def __init__(self, config):
        super().__init__()
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, dropout_pooled_output):
        logits = self.seq_relationship(dropout_pooled_output)
        return logits


class STConv3D(nn.Module):

    def __init__(self, input_dim, output_dim, kernel_size, stride=1, padding=0, separable=False):
        super(STConv3D, self).__init__()
        self.separable = separable
        self.relu = nn.ReLU(inplace=True)
        assert len(kernel_size) == 3
        if separable and kernel_size[0] != 1:
            spatial_kernel_size = [1, kernel_size[1], kernel_size[2]]
            temporal_kernel_size = [kernel_size[0], 1, 1]
            if isinstance(stride, list) and len(stride) == 3:
                spatial_stride = [1, stride[1], stride[2]]
                temporal_stride = [stride[0], 1, 1]
            else:
                spatial_stride = [1, stride, stride]
                temporal_stride = [stride, 1, 1]
            if isinstance(padding, list) and len(padding) == 3:
                spatial_padding = [0, padding[1], padding[2]]
                temporal_padding = [padding[0], 0, 0]
            else:
                spatial_padding = [0, padding, padding]
                temporal_padding = [padding, 0, 0]
        if separable:
            self.conv1 = nn.Conv3d(input_dim, output_dim, kernel_size=spatial_kernel_size, stride=spatial_stride, padding=spatial_padding, bias=False)
            self.bn1 = nn.BatchNorm3d(output_dim)
            self.conv2 = nn.Conv3d(output_dim, output_dim, kernel_size=temporal_kernel_size, stride=temporal_stride, padding=temporal_padding, bias=False)
            self.bn2 = nn.BatchNorm3d(output_dim)
        else:
            self.conv1 = nn.Conv3d(input_dim, output_dim, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)
            self.bn1 = nn.BatchNorm3d(output_dim)

    def forward(self, input):
        out = self.relu(self.bn1(self.conv1(input)))
        if self.separable:
            out = self.relu(self.bn2(self.conv2(out)))
        return out


class SelfGating(nn.Module):

    def __init__(self, input_dim):
        super(SelfGating, self).__init__()
        self.fc = nn.Linear(input_dim, input_dim)

    def forward(self, input_tensor):
        """Feature gating as used in S3D-G.
      """
        spatiotemporal_average = th.mean(input_tensor, dim=[2, 3, 4])
        weights = self.fc(spatiotemporal_average)
        weights = th.sigmoid(weights)
        return weights[:, :, None, None, None] * input_tensor


class InceptionBlock(nn.Module):

    def __init__(self, input_dim, num_outputs_0_0a, num_outputs_1_0a, num_outputs_1_0b, num_outputs_2_0a, num_outputs_2_0b, num_outputs_3_0b, gating=True):
        super(InceptionBlock, self).__init__()
        self.conv_b0 = STConv3D(input_dim, num_outputs_0_0a, [1, 1, 1])
        self.conv_b1_a = STConv3D(input_dim, num_outputs_1_0a, [1, 1, 1])
        self.conv_b1_b = STConv3D(num_outputs_1_0a, num_outputs_1_0b, [3, 3, 3], padding=1, separable=True)
        self.conv_b2_a = STConv3D(input_dim, num_outputs_2_0a, [1, 1, 1])
        self.conv_b2_b = STConv3D(num_outputs_2_0a, num_outputs_2_0b, [3, 3, 3], padding=1, separable=True)
        self.maxpool_b3 = th.nn.MaxPool3d((3, 3, 3), stride=1, padding=1)
        self.conv_b3_b = STConv3D(input_dim, num_outputs_3_0b, [1, 1, 1])
        self.gating = gating
        self.output_dim = num_outputs_0_0a + num_outputs_1_0b + num_outputs_2_0b + num_outputs_3_0b
        if gating:
            self.gating_b0 = SelfGating(num_outputs_0_0a)
            self.gating_b1 = SelfGating(num_outputs_1_0b)
            self.gating_b2 = SelfGating(num_outputs_2_0b)
            self.gating_b3 = SelfGating(num_outputs_3_0b)

    def forward(self, input):
        """Inception block
      """
        b0 = self.conv_b0(input)
        b1 = self.conv_b1_a(input)
        b1 = self.conv_b1_b(b1)
        b2 = self.conv_b2_a(input)
        b2 = self.conv_b2_b(b2)
        b3 = self.maxpool_b3(input)
        b3 = self.conv_b3_b(b3)
        if self.gating:
            b0 = self.gating_b0(b0)
            b1 = self.gating_b1(b1)
            b2 = self.gating_b2(b2)
            b3 = self.gating_b3(b3)
        return th.cat((b0, b1, b2, b3), dim=1)


class MaxPool3dTFPadding(th.nn.Module):

    def __init__(self, kernel_size, stride=None, padding='SAME'):
        super(MaxPool3dTFPadding, self).__init__()
        if padding == 'SAME':
            padding_shape = self._get_padding_shape(kernel_size, stride)
            self.padding_shape = padding_shape
            self.pad = th.nn.ConstantPad3d(padding_shape, 0)
        self.pool = th.nn.MaxPool3d(kernel_size, stride, ceil_mode=True)

    def _get_padding_shape(self, filter_shape, stride):

        def _pad_top_bottom(filter_dim, stride_val):
            pad_along = max(filter_dim - stride_val, 0)
            pad_top = pad_along // 2
            pad_bottom = pad_along - pad_top
            return pad_top, pad_bottom
        padding_shape = []
        for filter_dim, stride_val in zip(filter_shape, stride):
            pad_top, pad_bottom = _pad_top_bottom(filter_dim, stride_val)
            padding_shape.append(pad_top)
            padding_shape.append(pad_bottom)
        depth_top = padding_shape.pop(0)
        depth_bottom = padding_shape.pop(0)
        padding_shape.append(depth_top)
        padding_shape.append(depth_bottom)
        return tuple(padding_shape)

    def forward(self, inp):
        inp = self.pad(inp)
        out = self.pool(inp)
        return out


class Sentence_Embedding(nn.Module):

    def __init__(self, embd_dim, num_embeddings=66250, word_embedding_dim=300, token_to_word_path='dict.npy', max_words=16, output_dim=2048):
        super(Sentence_Embedding, self).__init__()
        self.word_embd = nn.Embedding(num_embeddings, word_embedding_dim)
        self.fc1 = nn.Linear(word_embedding_dim, output_dim)
        self.fc2 = nn.Linear(output_dim, embd_dim)
        self.word_to_token = {}
        self.max_words = max_words
        token_to_word = np.load(token_to_word_path)
        for i, t in enumerate(token_to_word):
            self.word_to_token[t] = i + 1

    def _zero_pad_tensor_token(self, tensor, size):
        if len(tensor) >= size:
            return tensor[:size]
        else:
            zero = th.zeros(size - len(tensor)).long()
            return th.cat((tensor, zero), dim=0)

    def _split_text(self, sentence):
        w = re.findall("[\\w']+", str(sentence))
        return w

    def _words_to_token(self, words):
        words = [self.word_to_token[word] for word in words if word in self.word_to_token]
        if words:
            we = self._zero_pad_tensor_token(th.LongTensor(words), self.max_words)
            return we
        else:
            return th.zeros(self.max_words).long()

    def _words_to_ids(self, x):
        split_x = [self._words_to_token(self._split_text(sent.lower())) for sent in x]
        return th.stack(split_x, dim=0)

    def forward(self, x):
        x = self._words_to_ids(x)
        x = self.word_embd(x)
        x = F.relu(self.fc1(x))
        x = th.max(x, dim=1)[0]
        x = self.fc2(x)
        return {'text_embedding': x}


class S3D(nn.Module):

    def __init__(self, dict_path, num_classes=512, gating=True, space_to_depth=True):
        super(S3D, self).__init__()
        self.num_classes = num_classes
        self.gating = gating
        self.space_to_depth = space_to_depth
        if space_to_depth:
            self.conv1 = STConv3D(24, 64, [2, 4, 4], stride=1, padding=(1, 2, 2), separable=False)
        else:
            self.conv1 = STConv3D(3, 64, [3, 7, 7], stride=2, padding=(1, 3, 3), separable=False)
        self.conv_2b = STConv3D(64, 64, [1, 1, 1], separable=False)
        self.conv_2c = STConv3D(64, 192, [3, 3, 3], padding=1, separable=True)
        self.gating = SelfGating(192)
        self.maxpool_2a = MaxPool3dTFPadding(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding='SAME')
        self.maxpool_3a = MaxPool3dTFPadding(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding='SAME')
        self.mixed_3b = InceptionBlock(192, 64, 96, 128, 16, 32, 32)
        self.mixed_3c = InceptionBlock(self.mixed_3b.output_dim, 128, 128, 192, 32, 96, 64)
        self.maxpool_4a = MaxPool3dTFPadding(kernel_size=(3, 3, 3), stride=(2, 2, 2), padding='SAME')
        self.mixed_4b = InceptionBlock(self.mixed_3c.output_dim, 192, 96, 208, 16, 48, 64)
        self.mixed_4c = InceptionBlock(self.mixed_4b.output_dim, 160, 112, 224, 24, 64, 64)
        self.mixed_4d = InceptionBlock(self.mixed_4c.output_dim, 128, 128, 256, 24, 64, 64)
        self.mixed_4e = InceptionBlock(self.mixed_4d.output_dim, 112, 144, 288, 32, 64, 64)
        self.mixed_4f = InceptionBlock(self.mixed_4e.output_dim, 256, 160, 320, 32, 128, 128)
        self.maxpool_5a = self.maxPool3d_5a_2x2 = MaxPool3dTFPadding(kernel_size=(2, 2, 2), stride=(2, 2, 2), padding='SAME')
        self.mixed_5b = InceptionBlock(self.mixed_4f.output_dim, 256, 160, 320, 32, 128, 128)
        self.mixed_5c = InceptionBlock(self.mixed_5b.output_dim, 384, 192, 384, 48, 128, 128)
        self.fc = nn.Linear(self.mixed_5c.output_dim, num_classes)
        self.text_module = Sentence_Embedding(num_classes, token_to_word_path=dict_path)

    def _space_to_depth(self, input):
        """3D space to depth trick for TPU optimization.
      """
        B, C, T, H, W = input.shape
        input = input.view(B, C, T // 2, 2, H // 2, 2, W // 2, 2)
        input = input.permute(0, 3, 5, 7, 1, 2, 4, 6)
        input = input.contiguous().view(B, 8 * C, T // 2, H // 2, W // 2)
        return input

    def forward(self, inputs):
        """Defines the S3DG base architecture."""
        if self.space_to_depth:
            inputs = self._space_to_depth(inputs)
        net = self.conv1(inputs)
        if self.space_to_depth:
            net = net[:, :, 1:, 1:, 1:]
        net = self.maxpool_2a(net)
        net = self.conv_2b(net)
        net = self.conv_2c(net)
        if self.gating:
            net = self.gating(net)
        net = self.maxpool_3a(net)
        net = self.mixed_3b(net)
        net = self.mixed_3c(net)
        net = self.maxpool_4a(net)
        net = self.mixed_4b(net)
        net = self.mixed_4c(net)
        net = self.mixed_4d(net)
        net = self.mixed_4e(net)
        net = self.mixed_4f(net)
        net = self.maxpool_5a(net)
        net = self.mixed_5b(net)
        net = self.mixed_5c(net)
        net = th.mean(net, dim=[2, 3, 4])
        return {'video_embedding': self.fc(net), 'mixed_5c': net}


def get_mask_from_lengths(lengths):
    max_len = torch.max(lengths).item()
    ids = torch.arange(0, max_len, out=torch.LongTensor(max_len))
    mask = ids < lengths.unsqueeze(1)
    return mask


class GlobalAvgPool(torch.nn.Module):

    def __init__(self):
        super(GlobalAvgPool, self).__init__()

    def forward(self, x, lengths=None):
        """Average pooling across time steps (dim=1) with optionally lengths.
        Args:
            x: torch.Tensor of shape (N, T, ...)
            lengths: None or torch.Tensor of shape (N,)
            dim: dimension to pool
        """
        if lengths is None:
            return x.mean(dim=1, keepdim=False)
        else:
            mask = get_mask_from_lengths(lengths).type(x.type())
            mask_shape = list(mask.size()) + [(1) for _ in range(x.ndimension() - 2)]
            mask = mask.reshape(*mask_shape)
            numer = (x * mask).sum(dim=1, keepdim=False)
            denom = mask.sum(dim=1, keepdim=False)
            return numer / denom


class AdaptiveMask(nn.Module):
    """Soft masking function for adaptive size.
    It masks out the last K values of an input. The masking value
    goes from 1 to 0 gradually, so K can be learned with
    back-propagation.
    Args:
        max_size: maximum size (i.e. input dimension)
        ramp_size: size of the ramp going from 0 to 1
        init_val: initial size proportion not to be masked out
        shape: learn multiple sizes independent of each other
    """

    def __init__(self, max_size, ramp_size, init_val=0, shape=(1,)):
        nn.Module.__init__(self)
        self._max_size = max_size
        self._ramp_size = ramp_size
        self.current_val = nn.Parameter(torch.zeros(*shape) + init_val)
        mask_template = torch.linspace(1 - max_size, 0, steps=max_size)
        self.register_buffer('mask_template', mask_template)

    def forward(self, x):
        mask = self.mask_template.float() + self.current_val.float() * self._max_size
        mask = mask / self._ramp_size + 1
        mask = mask.clamp(0, 1)
        if x.size(-1) < self._max_size:
            mask = mask.narrow(-1, self._max_size - x.size(-1), x.size(-1))
        x = (x * mask).type_as(x)
        return x

    def get_current_max_size(self, include_ramp=True):
        current_size = math.ceil(self.current_val.max().item() * self._max_size)
        if include_ramp:
            current_size += self._ramp_size
        current_size = max(0, min(self._max_size, current_size))
        return current_size

    def get_current_avg_size(self, include_ramp=True):
        current_size = math.ceil(self.current_val.float().mean().item() * self._max_size)
        if include_ramp:
            current_size += self._ramp_size
        current_size = max(0, min(self._max_size, current_size))
        return current_size

    def clamp_param(self):
        """this need to be called after each update"""
        self.current_val.data.clamp_(0, 1)


class AdaptiveSpan(nn.Module):
    """Adaptive attention span for Transformerself.
    This module learns an attention span length from data for each
    self-attention head.
    Args:
        attn_span: maximum attention span
        adapt_span_loss: loss coefficient for the span length
        adapt_span_ramp: length of the masking ramp
        adapt_span_init: initial size ratio
        adapt_span_cache: adapt cache size to reduce memory usage
    """

    def __init__(self, attn_span, adapt_span_ramp, adapt_span_init, n_head, adapt_span_layer, **kargs):
        nn.Module.__init__(self)
        self._max_span = attn_span
        self._n_head = n_head
        self._adapt_span_layer = adapt_span_layer
        if self._adapt_span_layer:
            self._mask = AdaptiveMask(max_size=self._max_span, ramp_size=adapt_span_ramp, init_val=adapt_span_init)
        else:
            self._mask = AdaptiveMask(max_size=self._max_span, ramp_size=adapt_span_ramp, init_val=adapt_span_init, shape=(n_head, 1, 1))

    def forward(self, attn, normalize=True):
        """mask attention with the right span"""
        self.clamp_param()
        if self._adapt_span_layer:
            attn = self._mask(attn)
        else:
            B = attn.size(0)
            M = attn.size(1)
            attn = attn.reshape(B // self._n_head, self._n_head, M, -1)
            attn = self._mask(attn)
            attn = attn.view(B, M, -1)
        return attn

    def get_trim_len(self):
        """how much of memory can be trimmed to reduce computation"""
        L = self._max_span
        trim_len = min(L - 1, L - self._mask.get_current_max_size())
        trim_len = math.floor(trim_len / 64) * 64
        return trim_len

    def trim_memory(self, query, key, value, key_pe):
        """trim out unnecessary memory beforehand to reduce computation"""
        trim_len = self.get_trim_len()
        cache_size = key.size(1) - query.size(1)
        trim_len_cache = trim_len - (self._max_span - cache_size)
        if trim_len_cache > 0:
            key = key[:, trim_len_cache:, :]
            value = value[:, trim_len_cache:, :]
        elif trim_len_cache < 0:
            key = F.pad(key, [0, 0, -trim_len_cache, 0])
            value = F.pad(value, [0, 0, -trim_len_cache, 0])
        if trim_len > 0:
            if key_pe is not None:
                key_pe = key_pe[:, :, trim_len:]
        return key, value, key_pe

    def get_cache_size(self):
        """determine how long the cache should be"""
        trim_len = self.get_trim_len()
        return min(self._max_span, self._max_span - trim_len + 64)

    def get_loss(self):
        """a loss term for regularizing the span length"""
        return self._max_span * self._mask.current_val.float().mean()

    def get_current_max_span(self):
        return self._mask.get_current_max_size()

    def get_current_avg_span(self):
        return self._mask.get_current_avg_size()

    def clamp_param(self):
        self._mask.clamp_param()


def _skew(X, pad_value):
    """shift every row 1 step to right"""
    B, M, L = X.size()
    X = F.pad(X, (0, M + 1), value=pad_value)
    X = X.view(B, -1)
    X = X[:, :-M]
    X = X.view(B, M, M + L)
    return X


def _unskew(X):
    """reverse _skew operation"""
    B, M, L = X.size()
    L -= M
    X = X.view(B, -1)
    X = F.pad(X, (0, M))
    X = X.view(B, M, M + L + 1)
    X = X[:, :, :L]
    return X


class SeqAttention(nn.Module):
    """Sequential self-attention layer.
    Each token will attend to its previous fixed number of steps.
    Note that attention doesn't include the current step itself.
    """

    def __init__(self, d_model, n_head, attn_span, dropout, adapt_span_layer, **kargs):
        nn.Module.__init__(self)
        self.dropout = nn.Dropout(dropout)
        self.d_model = d_model
        self.attn_span = attn_span
        self.adaptive_span = AdaptiveSpan(attn_span=attn_span, n_head=n_head, adapt_span_layer=adapt_span_layer, **kargs)

    def forward(self, query, key, value, key_pe):
        key, value, key_pe = self.adaptive_span.trim_memory(query, key, value, key_pe)
        attn_cont = torch.matmul(query, key.transpose(-1, -2))
        attn_cont = _unskew(attn_cont)
        attn_pos = torch.matmul(query, key_pe)
        attn = attn_cont + attn_pos
        attn = attn / math.sqrt(self.d_model)
        attn = F.softmax(attn.float(), dim=-1).type_as(attn)
        attn = self.adaptive_span(attn)
        attn = self.dropout(attn)
        attn_cont = _skew(attn, 0)
        out = torch.matmul(attn_cont, value)
        return out

    def get_cache_size(self):
        return self.adaptive_span.get_cache_size()


class MultiHeadSeqAttention(nn.Module):

    def __init__(self, d_model, n_head, **kargs):
        nn.Module.__init__(self)
        assert d_model % n_head == 0
        self.n_head = n_head
        self.head_dim = d_model // n_head
        self.attn = SeqAttention(d_model=self.head_dim, n_head=n_head, **kargs)
        self.proj_query = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_query.weight)
        self.proj_out = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_out.weight)
        self.proj_val = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_val.weight)
        self.proj_key = nn.Linear(d_model, d_model, bias=False)
        nn.init.xavier_normal_(self.proj_key.weight)

    def head_reshape(self, x):
        K = self.n_head
        D = self.head_dim
        x = x.view(x.size()[:-1] + (K, D))
        x = x.transpose(1, 2).contiguous()
        x = x.view(-1, x.size(-2), x.size(-1))
        return x

    def forward(self, query, key, value, key_pe):
        B = query.size(0)
        K = self.n_head
        D = self.head_dim
        M = query.size(1)
        query = self.proj_query(query)
        query = self.head_reshape(query)
        value = self.proj_val(value)
        value = self.head_reshape(value)
        key = self.proj_key(key)
        key = self.head_reshape(key)
        out = self.attn(query, key, value, key_pe)
        out = out.view(B, K, M, D)
        out = out.transpose(1, 2).contiguous()
        out = out.view(B, M, -1)
        out = self.proj_out(out)
        return out


class FeedForwardLayer(nn.Module):

    def __init__(self, d_model, d_inner, dropout, **kargs):
        nn.Module.__init__(self)
        self.fc1 = nn.Linear(d_model, d_inner)
        self.fc2 = nn.Linear(d_inner, d_model)
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)
        self.dropout = nn.Dropout(dropout)

    def forward(self, h):
        h1 = F.relu(self.fc1(h))
        h1 = self.dropout(h1)
        h2 = self.fc2(h1)
        return h2


def LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, export=False):
    if torch.jit.is_scripting() or torch.jit.is_tracing():
        export = True
    if not export and torch.cuda.is_available() and has_fused_layernorm:
        return FusedLayerNorm(normalized_shape, eps, elementwise_affine)
    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)


class TransformerSeqLayer(nn.Module):

    def __init__(self, d_model, **kargs):
        nn.Module.__init__(self)
        self.attn = MultiHeadSeqAttention(d_model=d_model, **kargs)
        self.norm1 = LayerNorm(d_model)
        self.ff = FeedForwardLayer(d_model=d_model, **kargs)
        self.norm2 = LayerNorm(d_model)

    def forward(self, h, h_cache, key_pe):
        h_all = torch.cat([h_cache, h], dim=1)
        attn_out = self.attn(h, h_all, h_all, key_pe)
        h = self.norm1(h + attn_out)
        if self.ff is not None:
            ff_out = self.ff(h)
            out = self.norm2(h + ff_out)
        else:
            out = h
        return out

    def get_cache_size(self):
        return self.attn.attn.get_cache_size()


class TransformerSeq(nn.Module):

    def __init__(self, vocab_size, d_model, n_head, n_layer, attn_span, emb_dropout, aux_loss_scaler, adapt_span_layer, **kargs):
        nn.Module.__init__(self)
        self.in_emb = nn.Embedding(vocab_size, d_model)
        nn.init.normal_(self.in_emb.weight, mean=0, std=d_model ** -0.5)
        self.out_emb = nn.Linear(d_model, vocab_size)
        self.aux_loss_scaler = aux_loss_scaler
        if emb_dropout > 0:
            self.emb_dropout = nn.Dropout(emb_dropout)
        else:
            self.emb_dropout = None
        self.key_pe = nn.Parameter(torch.randn(1, d_model // n_head, attn_span))
        self.layers = nn.ModuleList()
        self.layers.extend(TransformerSeqLayer(d_model=d_model, n_head=n_head, attn_span=attn_span, adapt_span_layer=adapt_span_layer, **kargs) for _ in range(n_layer))

    def forward(self, x, h_cache, target=None):
        block_size = x.size(1)
        h = self.in_emb(x)
        if self.emb_dropout is not None:
            h = self.emb_dropout(h)
        h_cache_next = []
        for l, layer in enumerate(self.layers):
            cache_size = layer.attn.attn.get_cache_size()
            if cache_size > block_size:
                h_cache_next_l = torch.cat([h_cache[l][:, -cache_size + block_size:, :], h], dim=1).detach()
            else:
                h_cache_next_l = h[:, -cache_size:, :].detach()
            h_cache_next.append(h_cache_next_l)
            h = layer(h, h_cache[l], self.key_pe)
        if self.emb_dropout is not None:
            h = self.emb_dropout(h)
        out = F.log_softmax(self.out_emb(h).float(), dim=-1).type_as(h)
        dummy_loss = None
        return out, h_cache_next, dummy_loss

    def get_aux_loss(self):
        loss = 0.0
        for layer in self.layers:
            loss += layer.attn.attn.adaptive_span.get_loss()
        return self.aux_loss_scaler * loss

    def get_current_max_span(self):
        max_span = 0.0
        for layer in self.layers:
            max_span = max(max_span, layer.attn.attn.adaptive_span.get_current_max_span())
        return max_span

    def get_current_avg_span(self):
        avg_span = 0.0
        for layer in self.layers:
            avg_span += layer.attn.attn.adaptive_span.get_current_avg_span()
        return avg_span / len(self.layers)


class HeadSelectionLoss(_Loss):

    def __init__(self, args):
        super().__init__()
        self.args = args
        self.kl_weight = getattr(args, 'kl_weight', 0.0)

    def forward(self, head_samples, sample_sizes, prior=0.5, eps=1e-07):
        """
        head_scores: (num_tasks, num_layers, num_heads)
        sample_sizes: (num_tasks, )
        """
        kl_loss = (head_samples * (torch.log(head_samples + eps) - math.log(prior))).sum(-1).sum(-1)
        kl_loss /= torch.numel(head_samples) / head_samples.size(0)
        kl_loss = self.kl_weight * torch.matmul(kl_loss, sample_sizes)
        return kl_loss


class AttnHeadSelector(nn.Module):
    """
    Latent variable modeling of attention head selection
    """

    def __init__(self, num_tasks, num_layers, total_num_heads, num_heads, select_strategy='group', head_select_temp=5.0):
        super(AttnHeadSelector, self).__init__()
        self.num_tasks = num_tasks
        self.num_layers = num_layers
        self.total_num_heads = total_num_heads
        self.num_heads = num_heads
        self.select_strategy = select_strategy
        self.temp = head_select_temp
        self.head_logits = torch.nn.Parameter(torch.Tensor(self.num_tasks, self.num_layers, total_num_heads), requires_grad=True)
        nn.init.uniform_(self.head_logits, a=math.log(0.01), b=math.log(1.0))

    def gumbel_sample(self, logits, tau=1.0):
        gumbels1 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels2 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels1 = (logits + gumbels1 - gumbels2) / tau
        y_soft = gumbels1.sigmoid()
        return y_soft

    def subset_select(self, y_soft, topk, dim=-1):
        top_values, top_inds = torch.topk(y_soft, k=topk, dim=dim)
        top_ret = 1.0 - top_values.detach() + top_values
        return top_inds.detach(), top_ret

    def group_selet(self, y_soft, topk, dim=-1):
        top_values, top_inds = torch.max(y_soft.view(self.num_tasks, self.num_layers, -1, topk), dim=2)
        top_inds = top_inds * topk + torch.arange(topk, device=top_inds.device).unsqueeze(0).unsqueeze(1)
        top_ret = 1.0 - top_values.detach() + top_values
        return top_inds.detach(), top_ret

    def head_select(self, task_ids=None):
        self.head_samples = self.gumbel_sample(self.head_logits, tau=self.temp)
        if self.select_strategy == 'subset':
            self.subset_heads, self.subset_weights = self.subset_select(self.head_samples, topk=self.num_heads)
        elif self.select_strategy == 'group':
            self.subset_heads, self.subset_weights = self.group_selet(self.head_samples, topk=self.num_heads)
        else:
            raise ValueError('{} is not supported'.format(self.select_strategy))
        self.batch_subset = self.subset_heads[task_ids, :, :]
        self.batch_weights = self.subset_weights[task_ids, :, :]

    def forward(self, layer_idx):
        assert layer_idx is not None
        batch_subset = self.batch_subset[:, layer_idx, :]
        batch_weights = self.batch_weights[:, layer_idx, :]
        return batch_subset, batch_weights


class PatchEmbed(nn.Module):
    """Image to Patch Embedding"""

    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        if isinstance(img_size, int):
            img_size = img_size, img_size
        if isinstance(patch_size, int):
            patch_size = patch_size, patch_size
        num_patches = img_size[1] // patch_size[1] * (img_size[0] // patch_size[0])
        self.patch_shape = img_size[0] // patch_size[0], img_size[1] // patch_size[1]
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches
        self.conv = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        x = self.conv(x).flatten(2).transpose(1, 2)
        return x


class LinearNorm(torch.nn.Module):

    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):
        super(LinearNorm, self).__init__()
        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)
        torch.nn.init.xavier_uniform_(self.linear_layer.weight, gain=torch.nn.init.calculate_gain(w_init_gain))

    def forward(self, x):
        return self.linear_layer(x)


class ConvNorm(torch.nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=None, dilation=1, bias=True, w_init_gain='linear'):
        super(ConvNorm, self).__init__()
        if padding is None:
            assert kernel_size % 2 == 1
            padding = int(dilation * (kernel_size - 1) / 2)
        self.conv = torch.nn.Conv1d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, bias=bias)
        torch.nn.init.xavier_uniform_(self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))

    def forward(self, signal):
        conv_signal = self.conv(signal)
        return conv_signal


class LocationLayer(nn.Module):

    def __init__(self, attention_n_filters, attention_kernel_size, attention_dim):
        super(LocationLayer, self).__init__()
        padding = int((attention_kernel_size - 1) / 2)
        self.location_conv = ConvNorm(2, attention_n_filters, kernel_size=attention_kernel_size, padding=padding, bias=False, stride=1, dilation=1)
        self.location_dense = LinearNorm(attention_n_filters, attention_dim, bias=False, w_init_gain='tanh')

    def forward(self, attention_weights_cat):
        processed_attention = self.location_conv(attention_weights_cat)
        processed_attention = processed_attention.transpose(1, 2)
        processed_attention = self.location_dense(processed_attention)
        return processed_attention


class Attention(nn.Module):

    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim, attention_location_n_filters, attention_location_kernel_size):
        super(Attention, self).__init__()
        self.query_layer = LinearNorm(attention_rnn_dim, attention_dim, bias=False, w_init_gain='tanh')
        self.memory_layer = LinearNorm(embedding_dim, attention_dim, bias=False, w_init_gain='tanh')
        self.v = LinearNorm(attention_dim, 1, bias=False)
        self.location_layer = LocationLayer(attention_location_n_filters, attention_location_kernel_size, attention_dim)
        self.score_mask_value = -float('inf')

    def get_alignment_energies(self, query, processed_memory, attention_weights_cat):
        """
        PARAMS
        ------
        query: decoder output (batch, n_mel_channels * n_frames_per_step)
        processed_memory: processed encoder outputs (B, T_in, attention_dim)
        attention_weights_cat: cumulative and prev. att weights (B, 2, max_time)

        RETURNS
        -------
        alignment (batch, max_time)
        """
        processed_query = self.query_layer(query.unsqueeze(1))
        processed_attention_weights = self.location_layer(attention_weights_cat)
        energies = self.v(torch.tanh(processed_query + processed_attention_weights + processed_memory))
        energies = energies.squeeze(-1)
        return energies

    def forward(self, attention_hidden_state, memory, processed_memory, attention_weights_cat, mask):
        """
        PARAMS
        ------
        attention_hidden_state: attention rnn last output
        memory: encoder outputs
        processed_memory: processed encoder outputs
        attention_weights_cat: previous and cummulative attention weights
        mask: binary mask for padded data
        """
        alignment = self.get_alignment_energies(attention_hidden_state, processed_memory, attention_weights_cat)
        if mask is not None:
            alignment.data.masked_fill_(mask, self.score_mask_value)
        attention_weights = F.softmax(alignment, dim=1)
        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)
        attention_context = attention_context.squeeze(1)
        return attention_context, attention_weights


class RelativePositionBias(nn.Module):

    def __init__(self, window_size, num_heads):
        super().__init__()
        self.window_size = window_size
        self.num_relative_distance = (2 * window_size[0] - 1) * (2 * window_size[1] - 1) + 3
        self.relative_position_bias_table = nn.Parameter(torch.zeros(self.num_relative_distance, num_heads))
        coords_h = torch.arange(window_size[0])
        coords_w = torch.arange(window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += window_size[0] - 1
        relative_coords[:, :, 1] += window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * window_size[1] - 1
        relative_position_index = torch.zeros(size=(window_size[0] * window_size[1] + 1,) * 2, dtype=relative_coords.dtype)
        relative_position_index[1:, 1:] = relative_coords.sum(-1)
        relative_position_index[0, 0:] = self.num_relative_distance - 3
        relative_position_index[0:, 0] = self.num_relative_distance - 2
        relative_position_index[0, 0] = self.num_relative_distance - 1
        self.register_buffer('relative_position_index', relative_position_index)

    def forward(self):
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(self.window_size[0] * self.window_size[1] + 1, self.window_size[0] * self.window_size[1] + 1, -1)
        return relative_position_bias.permute(2, 0, 1).contiguous()


class DropPath(nn.Module):
    """Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks)."""

    def __init__(self, drop_prob=None):
        super(DropPath, self).__init__()
        self.drop_prob = drop_prob

    def forward(self, x):
        if self.drop_prob == 0.0 or not self.training:
            return x
        keep_prob = 1 - self.drop_prob
        shape = (x.shape[0],) + (1,) * (x.ndim - 1)
        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)
        random_tensor.floor_()
        output = x.div(keep_prob) * random_tensor
        return output

    def extra_repr(self) ->str:
        return 'p={}'.format(self.drop_prob)


class Block(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4.0, drop=0.0, attn_drop=0.0, drop_path=0.0, init_values=None, window_size=None):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.attn = Attention(dim, num_heads=num_heads, attn_drop=attn_drop, proj_drop=drop, window_size=window_size)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = nn.LayerNorm(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = nn.Sequential(nn.Linear(dim, mlp_hidden_dim), nn.GELU(), nn.Linear(mlp_hidden_dim, dim), nn.Dropout(drop))
        if init_values > 0:
            self.gamma_1 = nn.Parameter(init_values * torch.ones(dim), requires_grad=True)
            self.gamma_2 = nn.Parameter(init_values * torch.ones(dim), requires_grad=True)
        else:
            self.gamma_1, self.gamma_2 = None, None

    def forward(self, x, rel_pos_bias=None):
        None
        if self.gamma_1 is None:
            x = x + self.drop_path(self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))
            fc_feature = self.drop_path(self.mlp(self.norm2(x)))
            x = x + fc_feature
        else:
            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x), rel_pos_bias=rel_pos_bias))
            fc_feature = self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))
            x = x + fc_feature
        return x, fc_feature


def deprecation_warning(message, stacklevel=3):
    warnings.warn(message, stacklevel=stacklevel)


def relu_squared(x: torch.Tensor):
    return F.relu(x).pow(2)


def get_activation_fn(activation: str) ->Callable:
    """Returns the activation function corresponding to `activation`"""
    if activation == 'relu':
        return F.relu
    elif activation == 'relu_squared':
        return relu_squared
    elif activation == 'gelu':
        return gelu
    elif activation == 'gelu_fast':
        deprecation_warning('--activation-fn=gelu_fast has been renamed to gelu_accurate')
        return gelu_accurate
    elif activation == 'gelu_accurate':
        return gelu_accurate
    elif activation == 'tanh':
        return torch.tanh
    elif activation == 'linear':
        return lambda x: x
    elif activation == 'swish':
        return torch.nn.SiLU
    else:
        raise RuntimeError('--activation-fn {} not supported'.format(activation))


class ConvolutionModule(torch.nn.Module):
    """Convolution block used in the conformer block"""

    def __init__(self, embed_dim, channels, depthwise_kernel_size, dropout, activation_fn='swish', bias=False, export=False):
        """
        Args:
            embed_dim: Embedding dimension
            channels: Number of channels in depthwise conv layers
            depthwise_kernel_size: Depthwise conv layer kernel size
            dropout: dropout value
            activation_fn: Activation function to use after depthwise convolution kernel
            bias: If bias should be added to conv layers
            export: If layernorm should be exported to jit
        """
        super(ConvolutionModule, self).__init__()
        assert (depthwise_kernel_size - 1) % 2 == 0, "kernel_size should be a odd number for 'SAME' padding"
        self.layer_norm = LayerNorm(embed_dim, export=export)
        self.pointwise_conv1 = torch.nn.Conv1d(embed_dim, 2 * channels, kernel_size=1, stride=1, padding=0, bias=bias)
        self.glu = torch.nn.GLU(dim=1)
        self.depthwise_conv = torch.nn.Conv1d(channels, channels, depthwise_kernel_size, stride=1, padding=(depthwise_kernel_size - 1) // 2, groups=channels, bias=bias)
        self.batch_norm = torch.nn.BatchNorm1d(channels)
        self.activation = get_activation_fn(activation_fn)(channels)
        self.pointwise_conv2 = torch.nn.Conv1d(channels, embed_dim, kernel_size=1, stride=1, padding=0, bias=bias)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: Input of shape B X T X C
        Returns:
          Tensor of shape B X T X C
        """
        x = self.layer_norm(x)
        x = x.transpose(1, 2)
        x = self.pointwise_conv1(x)
        x = self.glu(x)
        x = self.depthwise_conv(x)
        x = self.batch_norm(x)
        x = self.activation(x)
        x = self.pointwise_conv2(x)
        x = self.dropout(x)
        return x.transpose(1, 2)


class ESPNETMultiHeadedAttention(nn.Module):
    """Multi-Head Attention layer.
    Args:
        n_head: The number of heads.
        n_feat: The number of features.
        dropout: Dropout rate.
    """

    def __init__(self, n_feat, n_head, dropout):
        """Construct an MultiHeadedAttention object."""
        super(ESPNETMultiHeadedAttention, self).__init__()
        assert n_feat % n_head == 0
        self.d_k = n_feat // n_head
        self.h = n_head
        self.linear_q = nn.Linear(n_feat, n_feat)
        self.linear_k = nn.Linear(n_feat, n_feat)
        self.linear_v = nn.Linear(n_feat, n_feat)
        self.linear_out = nn.Linear(n_feat, n_feat)
        self.attn = None
        self.dropout = nn.Dropout(p=dropout)

    def forward_qkv(self, query, key, value, **kwargs):
        """Transform query, key and value.
        Args:
            query: Query tensor  B X T1 X C
            key: Key tensor B X T2 X C
            value: Value tensor  B X T2 X C
        Returns:
            torch.Tensor: Transformed query tensor  B X n_head X T1 X d_k
            torch.Tensor: Transformed key tensor B X n_head X T2 X d_k
            torch.Tensor: Transformed value tensor  B X n_head X T2 X d_k
        """
        n_batch = query.size(0)
        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)
        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)
        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        return q, k, v

    def forward_attention(self, value, scores, mask):
        """Compute attention context vector.
        Args:
            value: Transformed value B X n_head X T2 X d_k.
            scores: Attention score  B X n_head X T1 X T2
            mask: Mask  T2 X B
        Returns:
            torch.Tensor: Transformed value  B X T1 X d_model
                weighted by the attention score  B X T1 X T2
        """
        n_batch = value.size(0)
        if mask is not None:
            scores = scores.masked_fill(mask.unsqueeze(1).unsqueeze(2), float('-inf'))
            self.attn = torch.softmax(scores, dim=-1)
        else:
            self.attn = torch.softmax(scores, dim=-1)
        p_attn = self.dropout(self.attn)
        x = torch.matmul(p_attn, value)
        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k)
        return self.linear_out(x)

    def forward(self, query, key, value, key_padding_mask=None, **kwargs):
        """Compute scaled dot product attention.
        Args:
            query (torch.Tensor): Query tensor T X B X C
            key (torch.Tensor): Key tensor T X B X C
            value (torch.Tensor): Value tensor T X B X C
            mask (torch.Tensor): Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X D.
        """
        query = query.transpose(0, 1)
        key = key.transpose(0, 1)
        value = value.transpose(0, 1)
        q, k, v = self.forward_qkv(query, key, value)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        scores = self.forward_attention(v, scores, key_padding_mask)
        scores = scores.transpose(0, 1)
        return scores, None


class FeedForwardModule(torch.nn.Module):
    """Positionwise feed forward layer used in conformer"""

    def __init__(self, input_feat, hidden_units, dropout1, dropout2, activation_fn='swish', bias=True):
        """
        Args:
            input_feat: Input feature dimension
            hidden_units: Hidden unit dimension
            dropout1: dropout value for layer1
            dropout2: dropout value for layer2
            activation_fn: Name of activation function
            bias: If linear layers should have bias
        """
        super(FeedForwardModule, self).__init__()
        self.layer_norm = LayerNorm(input_feat)
        self.w_1 = torch.nn.Linear(input_feat, hidden_units, bias=bias)
        self.w_2 = torch.nn.Linear(hidden_units, input_feat, bias=bias)
        self.dropout1 = torch.nn.Dropout(dropout1)
        self.dropout2 = torch.nn.Dropout(dropout2)
        self.activation = get_activation_fn(activation_fn)(hidden_units)

    def forward(self, x):
        """
        Args:
            x: Input Tensor of shape  T X B X C
        Returns:
            Tensor of shape T X B X C
        """
        x = self.layer_norm(x)
        x = self.w_1(x)
        x = self.activation(x)
        x = self.dropout1(x)
        x = self.w_2(x)
        return self.dropout2(x)


logger = logging.getLogger('fairseq_cli.validate')


class FairseqDropout(nn.Module):

    def __init__(self, p, module_name=None):
        super().__init__()
        self.p = p
        self.module_name = module_name
        self.apply_during_inference = False

    def forward(self, x, inplace: bool=False):
        if self.p > 0 and (self.training or self.apply_during_inference):
            return F.dropout(x, p=self.p, training=True, inplace=inplace)
        else:
            return x

    def make_generation_fast_(self, name: str, retain_dropout: bool=False, retain_dropout_modules: Optional[List[str]]=None, **kwargs):
        if retain_dropout:
            if retain_dropout_modules is not None and self.module_name is None:
                logger.warning('Cannot enable dropout during inference for module {} because module_name was not set'.format(name))
            elif retain_dropout_modules is None or self.module_name in retain_dropout_modules:
                logger.info('Enabling dropout during inference for module: {}'.format(name))
                self.apply_during_inference = True
            else:
                logger.info('Disabling dropout for module: {}'.format(name))


class FairseqDecoder(nn.Module):
    """Base class for decoders."""

    def __init__(self, dictionary):
        super().__init__()
        self.dictionary = dictionary
        self.onnx_trace = False
        self.adaptive_softmax = None

    def forward(self, prev_output_tokens, encoder_out=None, **kwargs):
        """
        Args:
            prev_output_tokens (LongTensor): shifted output tokens of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (dict, optional): output from the encoder, used for
                encoder-side attention

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, **kwargs)
        x = self.output_layer(x)
        return x, extra

    def extract_features(self, prev_output_tokens, encoder_out=None, **kwargs):
        """
        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        raise NotImplementedError

    def output_layer(self, features, **kwargs):
        """
        Project features to the default output size, e.g., vocabulary size.

        Args:
            features (Tensor): features returned by *extract_features*.
        """
        raise NotImplementedError

    def get_normalized_probs(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], log_probs: bool, sample: Optional[Dict[str, Tensor]]=None):
        """Get normalized probabilities (or log probs) from a net's output."""
        return self.get_normalized_probs_scriptable(net_output, log_probs, sample)

    def get_normalized_probs_scriptable(self, net_output: Tuple[Tensor, Optional[Dict[str, List[Optional[Tensor]]]]], log_probs: bool, sample: Optional[Dict[str, Tensor]]=None):
        """Get normalized probabilities (or log probs) from a net's output."""
        if hasattr(self, 'adaptive_softmax') and self.adaptive_softmax is not None:
            if sample is not None:
                assert 'target' in sample
                target = sample['target']
            else:
                target = None
            out = self.adaptive_softmax.get_log_prob(net_output[0], target=target)
            return out.exp_() if not log_probs else out
        logits = net_output[0]
        if log_probs:
            return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)
        else:
            return utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace)

    def max_positions(self):
        """Maximum input length supported by the decoder."""
        return 1000000.0

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade old state dicts to work with newer code."""
        return state_dict

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True


class FairseqIncrementalState(object):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.init_incremental_state()

    def init_incremental_state(self):
        self._incremental_state_id = str(uuid.uuid4())

    def _get_full_incremental_state_key(self, key: str) ->str:
        return '{}.{}'.format(self._incremental_state_id, key)

    def get_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], key: str) ->Optional[Dict[str, Optional[Tensor]]]:
        """Helper for getting incremental state for an nn.Module."""
        full_key = self._get_full_incremental_state_key(key)
        if incremental_state is None or full_key not in incremental_state:
            return None
        return incremental_state[full_key]

    def set_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], key: str, value: Dict[str, Optional[Tensor]]) ->Optional[Dict[str, Dict[str, Optional[Tensor]]]]:
        """Helper for setting incremental state for an nn.Module."""
        if incremental_state is not None:
            full_key = self._get_full_incremental_state_key(key)
            incremental_state[full_key] = value
        return incremental_state


def with_incremental_state(cls):
    cls.__bases__ = (FairseqIncrementalState,) + tuple(b for b in cls.__bases__ if b != FairseqIncrementalState)
    return cls


@with_incremental_state
class FairseqIncrementalDecoder(FairseqDecoder):
    """Base class for incremental decoders.

    Incremental decoding is a special mode at inference time where the Model
    only receives a single timestep of input corresponding to the previous
    output token (for teacher forcing) and must produce the next output
    *incrementally*. Thus the model must cache any long-term state that is
    needed about the sequence, e.g., hidden states, convolutional states, etc.

    Compared to the standard :class:`FairseqDecoder` interface, the incremental
    decoder interface allows :func:`forward` functions to take an extra keyword
    argument (*incremental_state*) that can be used to cache state across
    time-steps.

    The :class:`FairseqIncrementalDecoder` interface also defines the
    :func:`reorder_incremental_state` method, which is used during beam search
    to select and reorder the incremental state based on the selection of beams.

    To learn more about how incremental decoding works, refer to `this blog
    <http://www.telesens.co/2019/04/21/understanding-incremental-decoding-in-fairseq/>`_.
    """

    def __init__(self, dictionary):
        super().__init__(dictionary)

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):
        """
        Args:
            prev_output_tokens (LongTensor): shifted output tokens of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (dict, optional): output from the encoder, used for
                encoder-side attention
            incremental_state (dict, optional): dictionary used for storing
                state during :ref:`Incremental decoding`

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        raise NotImplementedError

    def extract_features(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):
        """
        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        raise NotImplementedError

    def reorder_incremental_state(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order: Tensor):
        """Reorder incremental state.

        This will be called when the order of the input has changed from the
        previous time step. A typical use case is beam search, where the input
        order changes between time steps based on the selection of beams.
        """
        pass

    def reorder_incremental_state_scripting(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order: Tensor):
        """Main entry point for reordering the incremental state.

        Due to limitations in TorchScript, we call this function in
        :class:`fairseq.sequence_generator.SequenceGenerator` instead of
        calling :func:`reorder_incremental_state` directly.
        """
        for module in self.modules():
            if hasattr(module, 'reorder_incremental_state'):
                result = module.reorder_incremental_state(incremental_state, new_order)
                if result is not None:
                    incremental_state = result

    def set_beam_size(self, beam_size):
        """Sets the beam size in the decoder and all children."""
        if getattr(self, '_beam_size', -1) != beam_size:
            seen = set()

            def apply_set_beam_size(module):
                if module != self and hasattr(module, 'set_beam_size') and module not in seen:
                    seen.add(module)
                    module.set_beam_size(beam_size)
            self.apply(apply_set_beam_size)
            self._beam_size = beam_size


def _mask_for_xformers(mask: Tensor, to_dtype: Optional[torch.dtype]=None):
    """
    call to pytorch multihead accepts three mask types:
        - ByteTensor where non-zero means to mask
        - FloatTensor which is an additive mask
        - BoolTensor where True means to mask
    xFormers currently accepts boolean and additive maks. For boolean masks
    the values have opposite meaning. For a BoolTensor True mean to keep the value.
    """
    float_types = [torch.float, torch.float16]
    additive = mask.dtype in float_types
    to_dtype = mask.dtype if to_dtype is None else to_dtype
    to_additive = to_dtype in float_types
    if additive:
        if to_additive:
            return mask
        mask = mask < 0
    if to_additive:
        new_mask = torch.zeros_like(mask, dtype=to_dtype)
        new_mask = new_mask.masked_fill_(mask, -float('inf'))
        return new_mask
    mask = ~mask
    mask = mask
    return mask


def quant_noise(module, p, block_size):
    """
    Wraps modules and applies quantization noise to the weights for
    subsequent quantization with Iterative Product Quantization as
    described in "Training with Quantization Noise for Extreme Model Compression"

    Args:
        - module: nn.Module
        - p: amount of Quantization Noise
        - block_size: size of the blocks for subsequent quantization with iPQ

    Remarks:
        - Module weights must have the right sizes wrt the block size
        - Only Linear, Embedding and Conv2d modules are supported for the moment
        - For more detail on how to quantize by blocks with convolutional weights,
          see "And the Bit Goes Down: Revisiting the Quantization of Neural Networks"
        - We implement the simplest form of noise here as stated in the paper
          which consists in randomly dropping blocks
    """
    if p <= 0:
        return module
    assert isinstance(module, (nn.Linear, nn.Embedding, nn.Conv2d))
    is_conv = module.weight.ndim == 4
    if not is_conv:
        assert module.weight.size(1) % block_size == 0, 'Input features must be a multiple of block sizes'
    elif module.kernel_size == (1, 1):
        assert module.in_channels % block_size == 0, 'Input channels must be a multiple of block sizes'
    else:
        k = module.kernel_size[0] * module.kernel_size[1]
        assert k % block_size == 0, 'Kernel size must be a multiple of block size'

    def _forward_pre_hook(mod, input):
        if mod.training:
            if not is_conv:
                weight = mod.weight
                in_features = weight.size(1)
                out_features = weight.size(0)
                mask = torch.zeros(in_features // block_size * out_features, device=weight.device)
                mask.bernoulli_(p)
                mask = mask.repeat_interleave(block_size, -1).view(-1, in_features)
            else:
                weight = mod.weight
                in_channels = mod.in_channels
                out_channels = mod.out_channels
                if mod.kernel_size == (1, 1):
                    mask = torch.zeros(int(in_channels // block_size * out_channels), device=weight.device)
                    mask.bernoulli_(p)
                    mask = mask.repeat_interleave(block_size, -1).view(-1, in_channels)
                else:
                    mask = torch.zeros(weight.size(0), weight.size(1), device=weight.device)
                    mask.bernoulli_(p)
                    mask = mask.unsqueeze(2).unsqueeze(3).repeat(1, 1, mod.kernel_size[0], mod.kernel_size[1])
            mask = mask
            s = 1 / (1 - p)
            mod.weight.data = s * weight.masked_fill(mask, 0)
    module.register_forward_pre_hook(_forward_pre_hook)
    return module


class MultiheadAttention(FairseqIncrementalDecoder):
    """Multi-headed attention.

    See "Attention Is All You Need" for more details.
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, dictionary=None, q_noise=0.0, qn_block_size=8, xformers_att_config: Optional[str]=None, xformers_blocksparse_layout: Optional[torch.Tensor]=None, xformers_blocksparse_blocksize: Optional[int]=16):
        super().__init__(dictionary)
        xformers_att_config = utils.eval_str_dict(xformers_att_config)
        self.use_xformers = xformers_att_config is not None
        if self.use_xformers and not _xformers_available:
            raise ImportError('\n\n  Please install xFormers.')
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim
        self.num_heads = num_heads
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, 'embed_dim must be divisible by num_heads'
        self.scaling = self.head_dim ** -0.5
        self.self_attention = self_attention
        self.encoder_decoder_attention = encoder_decoder_attention
        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and value to be of the same size'
        self.k_proj = quant_noise(nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.v_proj = quant_noise(nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.q_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        if add_bias_kv:
            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))
            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None
        self.add_zero_attn = add_zero_attn
        self.beam_size = 1
        self.reset_parameters()
        if self.use_xformers:
            xformers_att_config['dropout'] = xformers_att_config.get('dropout', dropout)
            xformers_att_config['num_heads'] = xformers_att_config.get('num_heads', num_heads)
            if xformers_blocksparse_layout is not None:
                xformers_att_config['block_size'] = xformers_blocksparse_blocksize
                xformers_att_config['layout'] = xformers_blocksparse_layout
                xformers_att_config['name'] = 'blocksparse'
            self.attention = build_attention(xformers_att_config)
        self.onnx_trace = False
        self.skip_embed_dim_check = False
        self.init_incremental_state()

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def reset_parameters(self):
        if self.qkv_same_dim:
            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))
        else:
            nn.init.xavier_uniform_(self.k_proj.weight)
            nn.init.xavier_uniform_(self.v_proj.weight)
            nn.init.xavier_uniform_(self.q_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.out_proj.bias is not None:
            nn.init.constant_(self.out_proj.bias, 0.0)
        if self.bias_k is not None:
            nn.init.xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            nn.init.xavier_normal_(self.bias_v)

    def _get_reserve_head_index(self, num_heads_to_keep: int):
        k_proj_heads_norm = []
        q_proj_heads_norm = []
        v_proj_heads_norm = []
        for i in range(self.num_heads):
            start_idx = i * self.head_dim
            end_idx = (i + 1) * self.head_dim
            k_proj_heads_norm.append(torch.sum(torch.abs(self.k_proj.weight[start_idx:end_idx,])).tolist() + torch.sum(torch.abs(self.k_proj.bias[start_idx:end_idx])).tolist())
            q_proj_heads_norm.append(torch.sum(torch.abs(self.q_proj.weight[start_idx:end_idx,])).tolist() + torch.sum(torch.abs(self.q_proj.bias[start_idx:end_idx])).tolist())
            v_proj_heads_norm.append(torch.sum(torch.abs(self.v_proj.weight[start_idx:end_idx,])).tolist() + torch.sum(torch.abs(self.v_proj.bias[start_idx:end_idx])).tolist())
        heads_norm = []
        for i in range(self.num_heads):
            heads_norm.append(k_proj_heads_norm[i] + q_proj_heads_norm[i] + v_proj_heads_norm[i])
        sorted_head_index = sorted(range(self.num_heads), key=lambda k: heads_norm[k], reverse=True)
        reserve_head_index = []
        for i in range(num_heads_to_keep):
            start = sorted_head_index[i] * self.head_dim
            end = (sorted_head_index[i] + 1) * self.head_dim
            reserve_head_index.append((start, end))
        return reserve_head_index

    def _adaptive_prune_heads(self, reserve_head_index: List[Tuple[int, int]]):
        new_q_weight = []
        new_q_bias = []
        new_k_weight = []
        new_k_bias = []
        new_v_weight = []
        new_v_bias = []
        new_out_proj_weight = []
        for ele in reserve_head_index:
            start_idx, end_idx = ele
            new_q_weight.append(self.q_proj.weight[start_idx:end_idx,])
            new_q_bias.append(self.q_proj.bias[start_idx:end_idx])
            new_k_weight.append(self.k_proj.weight[start_idx:end_idx,])
            new_k_bias.append(self.k_proj.bias[start_idx:end_idx])
            new_v_weight.append(self.v_proj.weight[start_idx:end_idx,])
            new_v_bias.append(self.v_proj.bias[start_idx:end_idx])
            new_out_proj_weight.append(self.out_proj.weight[:, start_idx:end_idx])
        new_q_weight = torch.cat(new_q_weight).detach()
        new_k_weight = torch.cat(new_k_weight).detach()
        new_v_weight = torch.cat(new_v_weight).detach()
        new_out_proj_weight = torch.cat(new_out_proj_weight, dim=-1).detach()
        new_q_weight.requires_grad = True
        new_k_weight.requires_grad = True
        new_v_weight.requires_grad = True
        new_out_proj_weight.requires_grad = True
        new_q_bias = torch.cat(new_q_bias).detach()
        new_q_bias.requires_grad = True
        new_k_bias = torch.cat(new_k_bias).detach()
        new_k_bias.requires_grad = True
        new_v_bias = torch.cat(new_v_bias).detach()
        new_v_bias.requires_grad = True
        self.q_proj.weight = torch.nn.Parameter(new_q_weight)
        self.q_proj.bias = torch.nn.Parameter(new_q_bias)
        self.k_proj.weight = torch.nn.Parameter(new_k_weight)
        self.k_proj.bias = torch.nn.Parameter(new_k_bias)
        self.v_proj.weight = torch.nn.Parameter(new_v_weight)
        self.v_proj.bias = torch.nn.Parameter(new_v_bias)
        self.out_proj.weight = torch.nn.Parameter(new_out_proj_weight)
        self.num_heads = len(reserve_head_index)
        self.embed_dim = self.head_dim * self.num_heads
        self.q_proj.out_features = self.embed_dim
        self.k_proj.out_features = self.embed_dim
        self.v_proj.out_features = self.embed_dim

    def _set_skip_embed_dim_check(self):
        self.skip_embed_dim_check = True

    def _pad_masks(self, key_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor]) ->Tuple[Optional[Tensor], Optional[Tensor]]:
        if attn_mask is not None:
            shape = attn_mask.size()[:-1] + torch.Size([1])
            attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(shape)], dim=-1)
        if key_padding_mask is not None:
            shape = key_padding_mask.size()[:-1] + torch.Size([1])
            key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(shape)], dim=-1)
        return key_padding_mask, attn_mask

    def _add_bias(self, k: Tensor, v: Tensor, key_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor], bsz: int) ->Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:
        assert self.bias_k is not None
        assert self.bias_v is not None
        k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])
        v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])
        key_padding_mask, attn_mask = self._pad_masks(key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        return k, v, key_padding_mask, attn_mask

    def _append_zero_attn(self, k: Tensor, v: Tensor, key_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor]) ->Tuple[Tensor, Tensor, Optional[Tensor], Optional[Tensor]]:
        zero_attn_shape = k.size()[:-2] + torch.Size([1]) + k.size()[-1:]
        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=-2)
        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=-2)
        key_padding_mask, attn_mask = self._pad_masks(key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        return k, v, key_padding_mask, attn_mask

    def _xformers_attn_forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, need_weights: bool=True, attn_mask: Optional[Tensor]=None) ->Tuple[Tensor, Optional[Tensor]]:
        tgt_len, bsz, embed_dim = query.size()
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == tgt_len
        if self.self_attention:
            key = query
            value = query
        elif self.encoder_decoder_attention:
            value = key
        q = self.q_proj(query)
        k = self.k_proj(key)
        v = self.v_proj(value)
        if self.bias_k is not None:
            assert self.bias_v is not None
            k, v, attn_mask, key_padding_mask = self._add_bias(k, v, attn_mask, key_padding_mask, bsz)

        def fold_heads(x):
            return x.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)

        def split_heads(x):
            return x.contiguous().view(-1, bsz, self.num_heads, self.head_dim).transpose(0, 1).transpose(1, 2)
        massage = split_heads if self.attention.requires_head_dimension else fold_heads
        q = massage(q)
        if k is not None:
            k = massage(k)
        if v is not None:
            v = massage(v)
        if self.add_zero_attn:
            k, v, key_padding_mask, attn_mask = self._append_zero_attn(k=k, v=v, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        kwargs = {}
        if attn_mask is not None and self.attention.supports_attention_mask:
            attn_mask = _mask_for_xformers(attn_mask, to_dtype=q.dtype)
            kwargs['att_mask'] = attn_mask
        if key_padding_mask is not None:
            key_padding_mask = _mask_for_xformers(key_padding_mask, to_dtype=torch.bool)
            if not self.attention.requires_separate_masks:
                attn_mask = maybe_merge_masks(attn_mask, key_padding_mask, batch_size=bsz, src_len=k.size(-2), tgt_len=q.size(-2), num_heads=self.num_heads)
                key_padding_mask = None
                kwargs['att_mask'] = attn_mask
            if self.attention.supports_key_padding_mask:
                kwargs['key_padding_mask'] = key_padding_mask
        y = self.attention(q, k, v, **kwargs)
        y = y.view(bsz, self.num_heads, tgt_len, self.head_dim).transpose(1, 2).flatten(start_dim=2, end_dim=3).transpose(0, 1)
        assert list(y.size()) == [tgt_len, bsz, embed_dim]
        y = self.out_proj(y)
        return y, None

    def forward(self, query: Tensor, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) ->Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time x Batch x Channel

        Args:
            key_padding_mask (ByteTensor, optional): mask to exclude
                keys that are pads, of shape `(batch, src_len)`, where
                padding elements are indicated by 1s.
            need_weights (bool, optional): return the attention weights,
                averaged over heads (default: False).
            attn_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
            before_softmax (bool, optional): return the raw attention
                weights and values before the attention softmax.
            need_head_weights (bool, optional): return the attention
                weights for each head. Implies *need_weights*. Default:
                return the average attention weights over all heads.
        """
        if need_head_weights:
            need_weights = True
        is_tpu = query.device.type == 'xla'
        tgt_len, bsz, embed_dim = query.size()
        src_len = tgt_len
        if not self.skip_embed_dim_check:
            assert embed_dim == self.embed_dim, f'query dim {embed_dim} != {self.embed_dim}'
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        if key is not None:
            src_len, key_bsz, _ = key.size()
            if not torch.jit.is_scripting():
                assert value is not None
                assert src_len, key_bsz == value.shape[:2]
        if not self.onnx_trace and not is_tpu and incremental_state is None and not static_kv and not torch.jit.is_scripting() and not self.skip_embed_dim_check:
            assert key is not None and value is not None
            if self.use_xformers:
                return self._xformers_attn_forward(query, key, value, key_padding_mask, need_weights, attn_mask)
            else:
                return F.multi_head_attention_forward(query, key, value, self.embed_dim, self.num_heads, torch.empty([0]), torch.cat((self.q_proj.bias, self.k_proj.bias, self.v_proj.bias)), self.bias_k, self.bias_v, self.add_zero_attn, self.dropout_module.p, self.out_proj.weight, self.out_proj.bias, self.training or self.dropout_module.apply_during_inference, key_padding_mask.bool() if key_padding_mask is not None else None, need_weights, attn_mask, use_separate_proj_weight=True, q_proj_weight=self.q_proj.weight, k_proj_weight=self.k_proj.weight, v_proj_weight=self.v_proj.weight)
        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if saved_state is not None and 'prev_key' in saved_state:
                if static_kv:
                    assert self.encoder_decoder_attention and not self.self_attention
                    key = value = None
        else:
            saved_state = None
        if self.self_attention:
            q = self.q_proj(query)
            k = self.k_proj(query)
            v = self.v_proj(query)
        elif self.encoder_decoder_attention:
            q = self.q_proj(query)
            if key is None:
                assert value is None
                k = v = None
            else:
                if self.beam_size > 1 and bsz == key.size(1):
                    key = key.view(key.size(0), -1, self.beam_size, key.size(2))[:, :, 0, :]
                    if key_padding_mask is not None:
                        key_padding_mask = key_padding_mask.view(-1, self.beam_size, key_padding_mask.size(1))[:, 0, :]
                k = self.k_proj(key)
                v = self.v_proj(key)
        else:
            assert key is not None and value is not None
            q = self.q_proj(query)
            k = self.k_proj(key)
            v = self.v_proj(value)
        q *= self.scaling
        if self.bias_k is not None:
            assert self.bias_v is not None
            k, v, attn_mask, key_padding_mask = self._add_bias(k, v, attn_mask, key_padding_mask, bsz)
        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        kv_bsz = bsz
        if k is not None:
            kv_bsz = k.size(1)
            k = k.contiguous().view(-1, kv_bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if v is not None:
            v = v.contiguous().view(-1, kv_bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if saved_state is not None:
            if 'prev_key' in saved_state:
                _prev_key = saved_state['prev_key']
                assert _prev_key is not None
                kv_bsz = _prev_key.size(0)
                prev_key = _prev_key.view(kv_bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    k = prev_key
                else:
                    assert k is not None
                    k = torch.cat([prev_key, k], dim=1)
                src_len = k.size(1)
            if 'prev_value' in saved_state:
                _prev_value = saved_state['prev_value']
                assert _prev_value is not None
                assert kv_bsz == _prev_value.size(0)
                prev_value = _prev_value.view(kv_bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    v = prev_value
                else:
                    assert v is not None
                    v = torch.cat([prev_value, v], dim=1)
            prev_key_padding_mask: Optional[Tensor] = None
            if 'prev_key_padding_mask' in saved_state:
                prev_key_padding_mask = saved_state['prev_key_padding_mask']
            assert k is not None and v is not None
            key_padding_mask = MultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=kv_bsz, src_len=k.size(1), static_kv=static_kv)
            saved_state['prev_key'] = k.view(kv_bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_value'] = v.view(kv_bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_key_padding_mask'] = key_padding_mask
            assert incremental_state is not None
            incremental_state = self._set_input_buffer(incremental_state, saved_state)
        assert k is not None
        assert k.size(1) == src_len
        if key_padding_mask is not None and key_padding_mask.dim() == 0:
            key_padding_mask = None
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == kv_bsz
            assert key_padding_mask.size(1) == src_len
        if self.add_zero_attn:
            assert v is not None
            src_len += 1
            k, v, key_padding_mask, attn_mask = self._append_zero_attn(k=k, v=v, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        if self.encoder_decoder_attention and bsz != kv_bsz:
            attn_weights = torch.einsum('bxhtd,bhsd->bxhts', q.view((kv_bsz, -1, self.num_heads) + q.size()[1:]), k.view((kv_bsz, self.num_heads) + k.size()[1:]))
            attn_weights = attn_weights.reshape((-1,) + attn_weights.size()[-2:])
        else:
            attn_weights = torch.bmm(q, k.transpose(1, 2))
        attn_weights = self.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(0)
            if self.onnx_trace:
                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)
            attn_weights += attn_mask
        if key_padding_mask is not None:
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            if not is_tpu:
                attn_weights = attn_weights.view(kv_bsz, -1, self.num_heads, tgt_len, src_len)
                attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).unsqueeze(3), float('-inf'))
            else:
                attn_weights = attn_weights.transpose(0, 2)
                attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))
                attn_weights = attn_weights.transpose(0, 2)
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        if before_softmax:
            return attn_weights, v
        attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)
        attn_weights = attn_weights_float.type_as(attn_weights)
        attn_probs = self.dropout_module(attn_weights)
        assert v is not None
        attn: Optional[Tensor] = None
        if self.encoder_decoder_attention and bsz != kv_bsz:
            attn = torch.einsum('bxhts,bhsd->bxhtd', attn_probs.view((kv_bsz, -1, self.num_heads) + attn_probs.size()[1:]), v.view((kv_bsz, self.num_heads) + v.size()[1:]))
            attn = attn.reshape((-1,) + attn.size()[-2:])
        else:
            attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
        if self.onnx_trace and attn.size(1) == 1:
            attn = attn.contiguous().view(tgt_len, bsz, self.embed_dim)
        else:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)
        attn = self.out_proj(attn)
        attn_weights: Optional[Tensor] = None
        if need_weights:
            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)
            if not need_head_weights:
                attn_weights = attn_weights.mean(dim=0)
        return attn, attn_weights

    @staticmethod
    def _append_prev_key_padding_mask(key_padding_mask: Optional[Tensor], prev_key_padding_mask: Optional[Tensor], batch_size: int, src_len: int, static_kv: bool) ->Optional[Tensor]:
        if prev_key_padding_mask is not None and static_kv:
            new_key_padding_mask = prev_key_padding_mask
        elif prev_key_padding_mask is not None and key_padding_mask is not None:
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), key_padding_mask.float()], dim=1)
        elif prev_key_padding_mask is not None:
            if src_len > prev_key_padding_mask.size(1):
                filler = torch.zeros((batch_size, src_len - prev_key_padding_mask.size(1)), device=prev_key_padding_mask.device)
                new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), filler.float()], dim=1)
            else:
                new_key_padding_mask = prev_key_padding_mask.float()
        elif key_padding_mask is not None:
            if src_len > key_padding_mask.size(1):
                filler = torch.zeros((batch_size, src_len - key_padding_mask.size(1)), device=key_padding_mask.device)
                new_key_padding_mask = torch.cat([filler.float(), key_padding_mask.float()], dim=1)
            else:
                new_key_padding_mask = key_padding_mask.float()
        else:
            new_key_padding_mask = prev_key_padding_mask
        return new_key_padding_mask

    @torch.jit.export
    def reorder_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_order: Tensor):
        """Reorder buffered internal state (for incremental generation)."""
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            for k in input_buffer.keys():
                input_buffer_k = input_buffer[k]
                if input_buffer_k is not None:
                    if self.encoder_decoder_attention:
                        if input_buffer_k.size(0) * self.beam_size == new_order.size(0):
                            return incremental_state
                        elif self.beam_size > 1:
                            input_buffer[k] = input_buffer_k.index_select(0, new_order.reshape(-1, self.beam_size)[:, 0] // self.beam_size)
                        else:
                            input_buffer[k] = input_buffer_k.index_select(0, new_order)
                    else:
                        input_buffer[k] = input_buffer_k.index_select(0, new_order)
            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
        return incremental_state

    def set_beam_size(self, beam_size):
        """Used for effiecient beamable enc-dec attention"""
        self.beam_size = beam_size

    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]) ->Dict[str, Optional[Tensor]]:
        result = self.get_incremental_state(incremental_state, 'attn_state')
        if result is not None:
            return result
        else:
            empty_result: Dict[str, Optional[Tensor]] = {}
            return empty_result

    def _set_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], buffer: Dict[str, Optional[Tensor]]):
        return self.set_incremental_state(incremental_state, 'attn_state', buffer)

    def apply_sparse_mask(self, attn_weights, tgt_len: int, src_len: int, bsz: int):
        return attn_weights

    def upgrade_state_dict_named(self, state_dict, name):
        prefix = name + '.' if name != '' else ''
        items_to_add = {}
        keys_to_remove = []
        for k in state_dict.keys():
            if k.endswith(prefix + 'in_proj_weight'):
                dim = int(state_dict[k].shape[0] / 3)
                items_to_add[prefix + 'q_proj.weight'] = state_dict[k][:dim]
                items_to_add[prefix + 'k_proj.weight'] = state_dict[k][dim:2 * dim]
                items_to_add[prefix + 'v_proj.weight'] = state_dict[k][2 * dim:]
                keys_to_remove.append(k)
                k_bias = prefix + 'in_proj_bias'
                if k_bias in state_dict.keys():
                    dim = int(state_dict[k].shape[0] / 3)
                    items_to_add[prefix + 'q_proj.bias'] = state_dict[k_bias][:dim]
                    items_to_add[prefix + 'k_proj.bias'] = state_dict[k_bias][dim:2 * dim]
                    items_to_add[prefix + 'v_proj.bias'] = state_dict[k_bias][2 * dim:]
                    keys_to_remove.append(prefix + 'in_proj_bias')
        for k in keys_to_remove:
            del state_dict[k]
        for key, value in items_to_add.items():
            state_dict[key] = value


class RelPositionMultiHeadedAttention(ESPNETMultiHeadedAttention):
    """Multi-Head Attention layer with relative position encoding.
    Paper: https://arxiv.org/abs/1901.02860
    Args:
        n_head: The number of heads.
        n_feat: The number of features.
        dropout: Dropout rate.
        zero_triu: Whether to zero the upper triangular part of attention matrix.
    """

    def __init__(self, n_feat, n_head, dropout, zero_triu=False):
        """Construct an RelPositionMultiHeadedAttention object."""
        super().__init__(n_feat, n_head, dropout)
        self.zero_triu = zero_triu
        self.linear_pos = nn.Linear(n_feat, n_feat, bias=False)
        self.pos_bias_u = nn.Parameter(torch.zeros(self.h, self.d_k))
        self.pos_bias_v = nn.Parameter(torch.zeros(self.h, self.d_k))
        torch.nn.init.xavier_uniform_(self.pos_bias_u)
        torch.nn.init.xavier_uniform_(self.pos_bias_v)

    def rel_shift(self, x):
        """Compute relative positional encoding.
        Args:
            x: Input tensor B X n_head X T X 2T-1
        Returns:
            torch.Tensor: Output tensor.
        """
        zero_pad = torch.zeros((*x.size()[:3], 1), device=x.device, dtype=x.dtype)
        x_padded = torch.cat([zero_pad, x], dim=-1)
        x_padded = x_padded.view(*x.size()[:2], x.size(3) + 1, x.size(2))
        x = x_padded[:, :, 1:].view_as(x)[:, :, :, :x.size(-1) // 2 + 1]
        if self.zero_triu:
            ones = torch.ones((x.size(2), x.size(3)), device=x.device)
            x = x * torch.tril(ones, x.size(3) - x.size(2))[None, None, :, :]
        return x

    def forward(self, query, key, value, pos_emb, key_padding_mask=None, **kwargs):
        """Compute scaled dot product attention.
        Args:
            query: Query tensor T X B X C
            key: Key tensor T X B X C
            value: Value tensor T X B X C
            pos_emb: Positional embedding tensor B X 2T-1 X C
            key_padding_mask: Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X C.
        """
        query = query.transpose(0, 1)
        key = key.transpose(0, 1)
        value = value.transpose(0, 1)
        pos_emb = pos_emb.transpose(0, 1)
        q, k, v = self.forward_qkv(query, key, value)
        q = q.transpose(1, 2)
        n_batch_pos = pos_emb.size(0)
        p = self.linear_pos(pos_emb).view(n_batch_pos, -1, self.h, self.d_k)
        p = p.transpose(1, 2)
        q_with_bias_u = (q + self.pos_bias_u).transpose(1, 2)
        q_with_bias_v = (q + self.pos_bias_v).transpose(1, 2)
        matrix_ac = torch.matmul(q_with_bias_u, k.transpose(-2, -1))
        matrix_bd = torch.matmul(q_with_bias_v, p.transpose(-2, -1))
        matrix_bd = self.rel_shift(matrix_bd)
        scores = (matrix_ac + matrix_bd) / math.sqrt(self.d_k)
        scores = self.forward_attention(v, scores, key_padding_mask)
        scores = scores.transpose(0, 1)
        return scores, None


class RotaryPositionalEmbedding(torch.nn.Module):

    def __init__(self, dim, base=10000, precision=torch.half):
        """Rotary positional embedding
        Reference : https://blog.eleuther.ai/rotary-embeddings/
        Paper: https://arxiv.org/pdf/2104.09864.pdf
        Args:
            dim: Dimension of embedding
            base: Base value for exponential
            precision: precision to use for numerical values
        """
        super().__init__()
        inv_freq = 1.0 / base ** (torch.arange(0, dim, 2).float() / dim)
        self.register_buffer('inv_freq', inv_freq)
        self.seq_len_cached = None
        self.cos_cached = None
        self.sin_cached = None
        self.precision = precision

    def forward(self, x, seq_len=None):
        """
        Args:
            x: Input x with T X B X C
            seq_len: Sequence length of input x
        """
        if seq_len != self.seq_len_cached:
            self.seq_len_cached = seq_len
            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)
            freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            self.cos_cached = emb.cos()[:, None, None, :]
            self.sin_cached = emb.sin()[:, None, None, :]
        return self.cos_cached, self.sin_cached


def rotate_half(x):
    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=x1.ndim - 1)


def apply_rotary_pos_emb(q, k, cos, sin, offset: int=0):
    cos, sin = cos[offset:q.shape[0] + offset, ...], sin[offset:q.shape[0] + offset, ...]
    return q * cos + rotate_half(q) * sin, k * cos + rotate_half(k) * sin


class RotaryPositionMultiHeadedAttention(ESPNETMultiHeadedAttention):

    def __init__(self, n_feat, n_head, dropout, precision, rotary_emd_base=10000):
        """Construct an RotaryPositionMultiHeadedAttention object."""
        super().__init__(n_feat, n_head, dropout)
        precision = torch.float
        self.rotary_ndims = self.d_k
        if precision == 'fp16':
            precision = torch.half
        self.rotary_emb = RotaryPositionalEmbedding(self.rotary_ndims, base=rotary_emd_base, precision=precision)

    def forward(self, query, key, value, key_padding_mask=None, **kwargs):
        """Compute rotary position attention.
        Args:
            query: Query tensor T X B X C
            key: Key tensor T X B X C
            value: Value tensor T X B X C
            key_padding_mask: Mask tensor T X B
        Returns:
            torch.Tensor: Output tensor T X B X D.
        Notes:
            Assumes self attn
        """
        T, B, C = value.size()
        query = query.view(T, B, self.h, self.d_k)
        key = key.view(T, B, self.h, self.d_k)
        value = value.view(T, B, self.h, self.d_k)
        cos, sin = self.rotary_emb(value, seq_len=T)
        query, key = apply_rotary_pos_emb(query, key, cos, sin, offset=0)
        query = query.view(T, B, self.h * self.d_k)
        key = key.view(T, B, self.h * self.d_k)
        value = value.view(T, B, self.h * self.d_k)
        query = query.transpose(0, 1)
        key = key.transpose(0, 1)
        value = value.transpose(0, 1)
        q, k, v = self.forward_qkv(query, key, value)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        scores = self.forward_attention(v, scores, key_padding_mask)
        scores = scores.transpose(0, 1)
        return scores, None


class ConformerEncoderLayer(torch.nn.Module):
    """Conformer block based on https://arxiv.org/abs/2005.08100. We currently don't support relative positional encoding in MHA"""

    def __init__(self, embed_dim, ffn_embed_dim, attention_heads, dropout, use_fp16, depthwise_conv_kernel_size=31, activation_fn='swish', attn_type=None, pos_enc_type='abs'):
        """
        Args:
            embed_dim: Input embedding dimension
            ffn_embed_dim: FFN layer dimension
            attention_heads: Number of attention heads in MHA
            dropout: dropout value
            depthwise_conv_kernel_size: Size of kernel in depthwise conv layer in convolution module
            activation_fn: Activation function name to use in convulation block and feed forward block
            attn_type: MHA implementation from ESPNET vs fairseq
            pos_enc_type: Positional encoding type - abs, rope, rel_pos
        """
        self.pos_enc_type = pos_enc_type
        super(ConformerEncoderLayer, self).__init__()
        self.ffn1 = FeedForwardModule(embed_dim, ffn_embed_dim, dropout, dropout)
        self.self_attn_layer_norm = LayerNorm(embed_dim, export=False)
        self.self_attn_dropout = torch.nn.Dropout(dropout)
        if attn_type == 'espnet':
            if self.pos_enc_type == 'rel_pos':
                self.self_attn = RelPositionMultiHeadedAttention(embed_dim, attention_heads, dropout=dropout)
            elif self.pos_enc_type == 'rope':
                self.self_attn = RotaryPositionMultiHeadedAttention(embed_dim, attention_heads, dropout=dropout, precision=use_fp16)
            elif self.pos_enc_type == 'abs':
                self.self_attn = ESPNETMultiHeadedAttention(embed_dim, attention_heads, dropout=dropout)
            else:
                raise Exception(f'Unsupported attention type {self.pos_enc_type}')
        else:
            self.self_attn = MultiheadAttention(embed_dim, attention_heads, dropout=dropout)
        self.conv_module = ConvolutionModule(embed_dim=embed_dim, channels=embed_dim, depthwise_kernel_size=depthwise_conv_kernel_size, dropout=dropout, activation_fn=activation_fn)
        self.ffn2 = FeedForwardModule(embed_dim, ffn_embed_dim, dropout, dropout, activation_fn=activation_fn)
        self.final_layer_norm = LayerNorm(embed_dim, export=False)

    def forward(self, x, encoder_padding_mask: Optional[torch.Tensor], position_emb: Optional[torch.Tensor]=None):
        """
        Args:
            x: Tensor of shape T X B X C
            encoder_padding_mask: Optional mask tensor
            positions:
        Returns:
            Tensor of shape T X B X C
        """
        residual = x
        x = self.ffn1(x)
        x = x * 0.5 + residual
        residual = x
        x = self.self_attn_layer_norm(x)
        if self.pos_enc_type == 'rel_pos':
            x, attn = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, pos_emb=position_emb, need_weights=False)
        else:
            x, attn = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, need_weights=False)
        x = self.self_attn_dropout(x)
        x = x + residual
        residual = x
        x = x.transpose(0, 1)
        x = self.conv_module(x)
        x = x.transpose(0, 1)
        x = residual + x
        residual = x
        x = self.ffn2(x)
        layer_result = x
        x = x * 0.5 + residual
        x = self.final_layer_norm(x)
        return x, (attn, layer_result)


class ConformerWav2Vec2EncoderLayer(ConformerEncoderLayer):
    """Encoder layer for Wav2vec2 encoder"""

    def forward(self, x: torch.Tensor, self_attn_mask: torch.Tensor=None, self_attn_padding_mask: torch.Tensor=None, need_weights: bool=False, att_args=None, position_emb=None):
        return super().forward(x, self_attn_padding_mask, position_emb)


class SamePad(nn.Module):

    def __init__(self, kernel_size, causal=False):
        super().__init__()
        if causal:
            self.remove = kernel_size - 1
        else:
            self.remove = 1 if kernel_size % 2 == 0 else 0

    def forward(self, x):
        if self.remove > 0:
            x = x[:, :, :-self.remove]
        return x


class TransformerSentenceEncoderLayer(nn.Module):
    """
    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained
    models.
    """

    def __init__(self, embedding_dim: int=768, ffn_embedding_dim: int=3072, num_attention_heads: int=8, dropout: float=0.1, attention_dropout: float=0.1, activation_dropout: float=0.1, activation_fn: str='relu', export: bool=False, q_noise: float=0.0, qn_block_size: int=8, init_fn: Callable=None) ->None:
        super().__init__()
        if init_fn is not None:
            init_fn()
        self.embedding_dim = embedding_dim
        self.num_attention_heads = num_attention_heads
        self.attention_dropout = attention_dropout
        self.q_noise = q_noise
        self.qn_block_size = qn_block_size
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.activation_dropout_module = FairseqDropout(activation_dropout, module_name=self.__class__.__name__)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.self_attn = self.build_self_attention(self.embedding_dim, num_attention_heads, dropout=attention_dropout, self_attention=True, q_noise=q_noise, qn_block_size=qn_block_size)
        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)
        self.fc1 = self.build_fc1(self.embedding_dim, ffn_embedding_dim, q_noise=q_noise, qn_block_size=qn_block_size)
        self.fc2 = self.build_fc2(ffn_embedding_dim, self.embedding_dim, q_noise=q_noise, qn_block_size=qn_block_size)
        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_self_attention(self, embed_dim, num_attention_heads, dropout, self_attention, q_noise, qn_block_size):
        return MultiheadAttention(embed_dim, num_attention_heads, dropout=dropout, self_attention=True, q_noise=q_noise, qn_block_size=qn_block_size)

    def forward(self, x: torch.Tensor, self_attn_mask: Optional[torch.Tensor]=None, self_attn_padding_mask: Optional[torch.Tensor]=None):
        """
        LayerNorm is applied either before or after the self-attention/ffn
        modules similar to the original Transformer implementation.
        """
        residual = x
        x, attn = self.self_attn(query=x, key=x, value=x, key_padding_mask=self_attn_padding_mask, need_weights=False, attn_mask=self_attn_mask)
        x = self.dropout_module(x)
        x = residual + x
        x = self.self_attn_layer_norm(x)
        residual = x
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = residual + x
        x = self.final_layer_norm(x)
        return x, attn


class TransposeLast(nn.Module):

    def __init__(self, deconstruct_idx=None, tranpose_dim=-2):
        super().__init__()
        self.deconstruct_idx = deconstruct_idx
        self.tranpose_dim = tranpose_dim

    def forward(self, x):
        if self.deconstruct_idx is not None:
            x = x[self.deconstruct_idx]
        return x.transpose(self.tranpose_dim, -1)


def ChoiceEnum(choices: List[str]):
    """return the Enum class used to enforce list of choices"""
    return StrEnum('Choices', {k: k for k in choices})


def split_non_tensors(mixed: Union[torch.Tensor, Tuple[Any]]) ->Tuple[Tuple[torch.Tensor], Dict[str, List[Any]]]:
    """
    Usage::

        x = torch.Tensor([1])
        y = torch.Tensor([2])
        tensors, packed_non_tensors = split_non_tensors((x, y, None, 3))
        recon = unpack_non_tensors(tensors, packed_non_tensors)
        assert recon == (x, y, None, 3)
    """
    if isinstance(mixed, torch.Tensor):
        return (mixed,), None
    tensors = []
    packed_non_tensors = {'is_tensor': [], 'objects': []}
    for o in mixed:
        if isinstance(o, torch.Tensor):
            packed_non_tensors['is_tensor'].append(True)
            tensors.append(o)
        else:
            packed_non_tensors['is_tensor'].append(False)
            packed_non_tensors['objects'].append(o)
    return tuple(tensors), packed_non_tensors


def unpack_kwargs(kwarg_keys: List[str], flat_args: List[Any]) ->Tuple[List[Any], Dict[str, Any]]:
    if len(kwarg_keys) == 0:
        return flat_args, {}
    args = flat_args[:-len(kwarg_keys)]
    kwargs = {k: v for k, v in zip(kwarg_keys, flat_args[-len(kwarg_keys):])}
    return args, kwargs


def unpack_non_tensors(tensors: Tuple[torch.Tensor], packed_non_tensors: Dict[str, List[Any]]) ->Tuple[Any]:
    if packed_non_tensors is None:
        return tensors
    assert isinstance(packed_non_tensors, dict)
    mixed = []
    is_tensor_list = packed_non_tensors['is_tensor']
    objects = packed_non_tensors['objects']
    assert len(tensors) + len(objects) == len(is_tensor_list)
    obj_i = tnsr_i = 0
    for is_tensor in is_tensor_list:
        if is_tensor:
            mixed.append(tensors[tnsr_i])
            tnsr_i += 1
        else:
            mixed.append(objects[obj_i])
            obj_i += 1
    return tuple(mixed)


class CheckpointFunction(torch.autograd.Function):
    """Similar to the torch version, but support non-Tensor outputs.

    The caller is expected to provide a dict (*parent_ctx_dict*) that will hold
    the non-Tensor outputs. These should be combined with the Tensor *outputs*
    by calling ``unpack_non_tensors``.
    """

    @staticmethod
    def forward(ctx, run_function, parent_ctx_dict, kwarg_keys, *args):
        if torch.is_grad_enabled():
            checkpoint.check_backward_validity(args)
        ctx.run_function = run_function
        ctx.kwarg_keys = kwarg_keys
        ctx.fwd_rng_state = utils.get_rng_state()
        tensor_inputs, packed_non_tensor_inputs = split_non_tensors(args)
        if parent_ctx_dict['offload']:
            ctx.fwd_device = tuple(x.device for x in tensor_inputs)
            ctx.grad_requirements = tuple(x.requires_grad for x in tensor_inputs)
            tensor_inputs = tuple(x for x in tensor_inputs)
        else:
            ctx.fwd_device, ctx.grad_requirements = None, None
        ctx.save_for_backward(*tensor_inputs)
        ctx.packed_non_tensor_inputs = packed_non_tensor_inputs
        with torch.no_grad():
            unpacked_args, unpacked_kwargs = unpack_kwargs(kwarg_keys, args)
            outputs = run_function(*unpacked_args, **unpacked_kwargs)
        if isinstance(outputs, torch.Tensor):
            return outputs
        else:
            outputs, packed_non_tensor_outputs = split_non_tensors(outputs)
            parent_ctx_dict['packed_non_tensor_outputs'] = packed_non_tensor_outputs
            return outputs

    @staticmethod
    def backward(ctx, *args):
        if not torch.autograd._is_checkpoint_valid():
            raise RuntimeError('Checkpointing is not compatible with .grad(), please use .backward() if possible')
        tensor_inputs: Tuple = ctx.saved_tensors
        tensor_inputs = checkpoint.detach_variable(tensor_inputs)
        if ctx.fwd_device is not None:
            tensor_inputs = [t for i, t in enumerate(tensor_inputs)]
            for i, need_grad in enumerate(ctx.grad_requirements):
                tensor_inputs[i].requires_grad = need_grad
        inputs = unpack_non_tensors(tensor_inputs, ctx.packed_non_tensor_inputs)
        bwd_rng_state = utils.get_rng_state()
        utils.set_rng_state(ctx.fwd_rng_state)
        with torch.enable_grad():
            unpacked_args, unpacked_kwargs = unpack_kwargs(ctx.kwarg_keys, inputs)
            outputs = ctx.run_function(*unpacked_args, **unpacked_kwargs)
            tensor_outputs, _ = split_non_tensors(outputs)
        utils.set_rng_state(bwd_rng_state)
        outputs_with_grad = []
        args_with_grad = []
        for i in range(len(tensor_outputs)):
            if tensor_outputs[i].requires_grad:
                outputs_with_grad.append(tensor_outputs[i])
                args_with_grad.append(args[i])
        if len(outputs_with_grad) == 0:
            raise RuntimeError('None of the outputs have requires_grad=True, this checkpoint() is not necessary')
        torch.autograd.backward(outputs_with_grad, args_with_grad)
        grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else None for inp in inputs)
        return (None, None, None) + grads


def pack_kwargs(*args, **kwargs) ->Tuple[List[str], List[Any]]:
    """
    Usage::

        kwarg_keys, flat_args = pack_kwargs(1, 2, a=3, b=4)
        args, kwargs = unpack_kwargs(kwarg_keys, flat_args)
        assert args == [1, 2]
        assert kwargs == {"a": 3, "b": 4}
    """
    kwarg_keys = []
    flat_args = list(args)
    for k, v in kwargs.items():
        kwarg_keys.append(k)
        flat_args.append(v)
    return kwarg_keys, flat_args


def _checkpointed_forward(original_forward, offload_to_cpu, *args, **kwargs):
    kwarg_keys, flat_args = pack_kwargs(*args, **kwargs)
    parent_ctx_dict = {'offload': offload_to_cpu}
    output = CheckpointFunction.apply(original_forward, parent_ctx_dict, kwarg_keys, *flat_args)
    if isinstance(output, torch.Tensor):
        return output
    else:
        packed_non_tensor_outputs = parent_ctx_dict['packed_non_tensor_outputs']
        if packed_non_tensor_outputs:
            output = unpack_non_tensors(output, packed_non_tensor_outputs)
        return output


def checkpoint_wrapper(m, offload_to_cpu=False):
    """
    A friendlier wrapper for performing activation checkpointing.

    Compared to the PyTorch version, this version:
    - wraps an nn.Module, so that all subsequent calls will use checkpointing
    - handles keyword arguments in the forward
    - handles non-Tensor outputs from the forward

    Usage::

        checkpointed_module = checkpoint_wrapper(my_module, offload_to_cpu=True)
        a, b = checkpointed_module(x, y=3, z=torch.Tensor([1]))
    """
    assert not hasattr(m, 'precheckpoint_forward'), 'checkpoint function has already been applied?'
    m.precheckpoint_forward = m.forward
    m.forward = functools.partial(_checkpointed_forward, m.precheckpoint_forward, offload_to_cpu)
    return m


def fsdp_wrap(module, min_num_params: Optional[int]=None, **kwargs):
    """
    Helper to wrap layers/modules in FSDP. This falls back to a no-op if
    fairscale is not available.

    Args:
        module (nn.Module): module to (maybe) wrap
        min_num_params (int, Optional): minimum number of layer params to wrap
    """
    try:
        if min_num_params is not None:
            num_params = sum(p.numel() for p in module.parameters())
            if num_params >= min_num_params:
                return wrap(module, **kwargs)
            else:
                return module
        else:
            return wrap(module, **kwargs)
    except ImportError:
        return module


def is_xla_tensor(tensor):
    return torch.is_tensor(tensor) and tensor.device.type == 'xla'


def index_put(tensor, indices, value):
    if is_xla_tensor(tensor):
        for _ in range(indices.dim(), tensor.dim()):
            indices = indices.unsqueeze(-1)
        if indices.size(-1) < tensor.size(-1):
            indices = indices.expand_as(tensor)
        tensor = torch.mul(tensor, ~indices) + torch.mul(value, indices)
    else:
        tensor[indices] = value
    return tensor


def init_bert_params(module):
    """
    Initialize the weights specific to the BERT Model.
    This overrides the default initializations depending on the specified arguments.
        1. If normal_init_linear_weights is set then weights of linear
           layer will be initialized using the normal distribution and
           bais will be set to the specified value.
        2. If normal_init_embed_weights is set then weights of embedding
           layer will be initialized using the normal distribution.
        3. If normal_init_proj_weights is set then weights of
           in_project_weight for MultiHeadAttention initialized using
           the normal distribution (to be validated).
    """

    def normal_(data):
        data.copy_(data.cpu().normal_(mean=0.0, std=0.02))
    if isinstance(module, nn.Linear):
        normal_(module.weight.data)
        if module.bias is not None:
            module.bias.data.zero_()
    if isinstance(module, nn.Embedding):
        normal_(module.weight.data)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
    if isinstance(module, MultiheadAttention):
        normal_(module.q_proj.weight.data)
        normal_(module.k_proj.weight.data)
        normal_(module.v_proj.weight.data)


def make_conv_pos(e, k, g):
    pos_conv = nn.Conv1d(e, e, kernel_size=k, padding=k // 2, groups=g)
    dropout = 0
    std = math.sqrt(4 * (1.0 - dropout) / (k * e))
    nn.init.normal_(pos_conv.weight, mean=0, std=std)
    nn.init.constant_(pos_conv.bias, 0)
    pos_conv = nn.utils.weight_norm(pos_conv, name='weight', dim=2)
    pos_conv = nn.Sequential(pos_conv, SamePad(k), nn.GELU())
    return pos_conv


def pad_to_multiple(x, multiple, dim=-1, value=0):
    if x is None:
        return None, 0
    tsz = x.size(dim)
    m = tsz / multiple
    remainder = math.ceil(m) * multiple - tsz
    if m.is_integer():
        return x, 0
    pad_offset = (0,) * (-1 - dim) * 2
    return F.pad(x, (*pad_offset, 0, remainder), value=value), remainder


class AltAttention(nn.Module):

    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, cosine_attention=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.cosine_attention = cosine_attention
        if cosine_attention:
            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)

    def forward(self, x, padding_mask=None, alibi_bias=None):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        dtype = q.dtype
        if self.cosine_attention:
            attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)
            logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1.0 / 0.01))).exp()
            attn = attn * logit_scale
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
        if alibi_bias is not None:
            attn = attn.type_as(alibi_bias)
            attn[:, :alibi_bias.size(1)] += alibi_bias
        if padding_mask is not None and padding_mask.any():
            attn = attn.masked_fill(padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
        attn = attn.softmax(dim=-1, dtype=torch.float32)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2)
        x = x.reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class AltBlock(nn.Module):

    def __init__(self, dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop=0.0, attn_drop=0.0, mlp_drop=0.0, post_mlp_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm, layer_norm_first=True, ffn_targets=False, cosine_attention=False):
        super().__init__()
        self.layer_norm_first = layer_norm_first
        self.ffn_targets = ffn_targets
        self.norm1 = norm_layer(dim)
        self.attn = AltAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, cosine_attention=cosine_attention)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=mlp_drop)
        self.post_mlp_dropout = nn.Dropout(post_mlp_drop, inplace=False)

    def forward(self, x, padding_mask=None, alibi_bias=None):
        if self.layer_norm_first:
            x = x + self.drop_path(self.attn(self.norm1(x), padding_mask, alibi_bias))
            r = x = self.mlp(self.norm2(x))
            t = x
            x = r + self.drop_path(self.post_mlp_dropout(x))
            if not self.ffn_targets:
                t = x
        else:
            x = x + self.drop_path(self.attn(x, padding_mask, alibi_bias))
            r = x = self.norm1(x)
            x = self.mlp(x)
            t = x
            x = self.norm2(r + self.drop_path(self.post_mlp_dropout(x)))
            if not self.ffn_targets:
                t = x
        return x, t


class Modality(Enum):
    AUDIO = auto()
    IMAGE = auto()
    TEXT = auto()


class GradMultiply(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, scale):
        ctx.scale = scale
        res = x.new(x)
        return res

    @staticmethod
    def backward(ctx, grad):
        return grad * ctx.scale, None


MaskInfo = namedtuple('MaskInfo', ['x_unmasked', 'mask', 'ids_restore', 'ids_keep'])


MaskSeed = namedtuple('MaskSeed', ['seed', 'update', 'ids'])


def _learned_alibi_bias(alibi_bias, batch_size, time_steps, heads, scale, dtype, device):
    assert alibi_bias.size(1) == heads, alibi_bias.shape
    assert alibi_bias.dtype == dtype, alibi_bias.dtype
    assert alibi_bias.device == device, alibi_bias.device
    if alibi_bias.size(-1) < time_steps:
        psz = math.ceil((time_steps - alibi_bias.size(-1)) / 2)
        alibi_bias = F.pad(alibi_bias, (psz, psz, psz, psz), mode='replicate')
    alibi_bias = alibi_bias.expand(batch_size, -1, -1, -1) * scale
    return alibi_bias[..., :time_steps, :time_steps]


def compute_mask_indices(shape: Tuple[int, int], padding_mask: Optional[torch.Tensor], mask_prob: float, mask_length: int, mask_type: str='static', mask_other: float=0.0, min_masks: int=0, no_overlap: bool=False, min_space: int=0, require_same_masks: bool=True, mask_dropout: float=0.0, add_masks: bool=False, seed: Optional[int]=None, epoch: Optional[int]=None, indices: Optional[torch.Tensor]=None, idc_select_ver: int=1, num_mask_ver: int=2) ->np.ndarray:
    """
    Computes random mask spans for a given shape

    Args:
        shape: the the shape for which to compute masks.
            should be of size 2 where first element is batch size and 2nd is timesteps
        padding_mask: optional padding mask of the same size as shape, which will prevent masking padded elements
        mask_prob: probability for each token to be chosen as start of the span to be masked. this will be multiplied by
            number of timesteps divided by length of mask span to mask approximately this percentage of all elements.
            however due to overlaps, the actual number will be smaller (unless no_overlap is True)
        mask_type: how to compute mask lengths
            static = fixed size
            uniform = sample from uniform distribution [mask_other, mask_length*2]
            normal = sample from normal distribution with mean mask_length and stdev mask_other. mask is min 1 element
            poisson = sample from possion distribution with lambda = mask length
        min_masks: minimum number of masked spans
        no_overlap: if false, will switch to an alternative recursive algorithm that prevents spans from overlapping
        min_space: only used if no_overlap is True, this is how many elements to keep unmasked between spans
        require_same_masks: if true, will randomly drop out masks until same amount of masks remains in each sample
        mask_dropout: randomly dropout this percentage of masks in each example
    """
    bsz, all_sz = shape
    mask = np.full((bsz, all_sz), False)
    if num_mask_ver == 1:
        all_num_mask = int(mask_prob * all_sz / float(mask_length) + np.random.rand())
        all_num_mask = max(min_masks, all_num_mask)
    mask_idcs = []
    for i in range(bsz):
        if seed is not None and epoch is not None and indices is not None:
            seed_i = int(hash((seed, epoch, indices[i].item())) % 1000000.0)
        else:
            seed_i = None
        rng = np.random.default_rng(seed_i)
        if padding_mask is not None:
            sz = all_sz - padding_mask[i].long().sum().item()
            assert sz >= 0, sz
        else:
            sz = all_sz
        if num_mask_ver == 1:
            if padding_mask is not None:
                num_mask = int(mask_prob * sz / float(mask_length) + np.random.rand())
                num_mask = max(min_masks, num_mask)
            else:
                num_mask = all_num_mask
        elif num_mask_ver == 2:
            num_mask = int(mask_prob * sz / float(mask_length) + rng.random())
            num_mask = max(min_masks, num_mask)
        else:
            raise ValueError()
        if mask_type == 'static':
            lengths = np.full(num_mask, mask_length)
        elif mask_type == 'uniform':
            lengths = rng.randint(mask_other, mask_length * 2 + 1, size=num_mask)
        elif mask_type == 'normal':
            lengths = rng.normal(mask_length, mask_other, size=num_mask)
            lengths = [max(1, int(round(x))) for x in lengths]
        elif mask_type == 'poisson':
            lengths = rng.poisson(mask_length, size=num_mask)
            lengths = [int(round(x)) for x in lengths]
        else:
            raise Exception('unknown mask selection ' + mask_type)
        if sum(lengths) == 0:
            if mask_type == 'static':
                raise ValueError(f'this should never happens')
            else:
                lengths = [min(mask_length, sz - 1)]
        if no_overlap:
            mask_idc = []

            def arrange(s, e, length, keep_length):
                span_start = rng.randint(s, e - length)
                mask_idc.extend(span_start + i for i in range(length))
                new_parts = []
                if span_start - s - min_space >= keep_length:
                    new_parts.append((s, span_start - min_space + 1))
                if e - span_start - length - min_space > keep_length:
                    new_parts.append((span_start + length + min_space, e))
                return new_parts
            parts = [(0, sz)]
            min_length = min(lengths)
            for length in sorted(lengths, reverse=True):
                lens = np.fromiter((e - s if e - s >= length + min_space else 0 for s, e in parts), np.int)
                l_sum = np.sum(lens)
                if l_sum == 0:
                    break
                probs = lens / np.sum(lens)
                c = rng.choice(len(parts), p=probs)
                s, e = parts.pop(c)
                parts.extend(arrange(s, e, length, min_length))
            mask_idc = np.asarray(mask_idc)
        else:
            if idc_select_ver == 1:
                min_len = min(lengths)
                if sz - min_len <= num_mask:
                    min_len = sz - num_mask - 1
                mask_idc = rng.choice(sz - min_len, num_mask, replace=False)
            elif idc_select_ver == 2:
                mask_idc = rng.choice(sz, num_mask, replace=False)
            else:
                raise ValueError()
            mask_idc = np.asarray([(mask_idc[j] + offset) for j in range(len(mask_idc)) for offset in range(lengths[j])])
        mask_idc = np.unique(mask_idc[mask_idc < sz])
        if len(mask_idc) >= sz:
            raise ValueError(f'the entire sequence is masked. sz={sz}; mask_idc[mask_idc]; index={indices[i] if indices is not None else None}')
        mask_idcs.append(mask_idc)
    target_len = None
    if require_same_masks:
        if add_masks:
            target_len = max([len(m) for m in mask_idcs])
        else:
            target_len = min([len(m) for m in mask_idcs])
    for i, mask_idc in enumerate(mask_idcs):
        if target_len is not None and len(mask_idc) > target_len:
            mask_idc = rng.choice(mask_idc, target_len, replace=False)
        mask[i, mask_idc] = True
        if target_len is not None and len(mask_idc) < target_len:
            unmasked = np.flatnonzero(~mask[i])
            to_mask = rng.choice(unmasked, target_len - len(mask_idc), replace=False)
            mask[i, to_mask] = True
        if mask_dropout > 0:
            masked = np.flatnonzero(mask[i])
            num_holes = np.rint(len(masked) * mask_dropout).astype(int)
            to_drop = rng.choice(masked, num_holes, replace=False)
            mask[i, to_drop] = False
    return mask


def gather_unmasked(x: torch.Tensor, mask_info: MaskInfo) ->torch.Tensor:
    return torch.gather(x, dim=1, index=mask_info.ids_keep)


def gather_unmasked_mask(x: torch.Tensor, mask_info: MaskInfo) ->torch.Tensor:
    return torch.gather(x, dim=1, index=mask_info.ids_keep[..., 0])


def masked_alibi(alibi_bias, mask_indices, orig_B, orig_T):
    alibi_bias = alibi_bias.view(orig_B, -1, orig_T, orig_T)
    H = alibi_bias.size(1)
    alibi_mask = mask_indices.unsqueeze(1)
    alibi_bias = alibi_bias.masked_select(alibi_mask.unsqueeze(-1))
    alibi_bias = alibi_bias.view(orig_B, H, -1, orig_T)
    M = alibi_bias.size(-2)
    alibi_bias = alibi_bias.masked_select(alibi_mask.unsqueeze(-2))
    alibi_bias = alibi_bias.view(-1, M, M)
    return alibi_bias


def random_masking(x, mask_ratio, mask_seed: Optional[MaskSeed]):
    N, L, D = x.shape
    len_keep = int(L * (1 - mask_ratio))
    generator = None
    if mask_seed is not None:
        seed = int(hash((mask_seed.seed, mask_seed.update, mask_seed.ids.sum().item())) % 1000000.0)
        generator = torch.Generator(device=x.device)
        generator.manual_seed(seed)
    noise = torch.rand(N, L, generator=generator, device=x.device)
    ids_shuffle = noise.argsort(dim=1)
    ids_restore = ids_shuffle.argsort(dim=1)
    ids_keep = ids_shuffle[:, :len_keep]
    ids_keep = ids_keep.unsqueeze(-1).expand(-1, -1, D)
    x_unmasked = torch.gather(x, dim=1, index=ids_keep)
    mask = torch.ones([N, L], dtype=x.dtype, device=x.device)
    mask[:, :len_keep] = 0
    mask = torch.gather(mask, dim=1, index=ids_restore)
    ids_restore = ids_restore.unsqueeze(-1).expand(-1, -1, D)
    return MaskInfo(x_unmasked=x_unmasked, mask=mask, ids_restore=ids_restore, ids_keep=ids_keep)


class BlockEncoder(nn.Module):

    def __init__(self, blocks, norm_layer, layer_norm_first, layerdrop, dropout):
        super().__init__()
        self.blocks = blocks
        self.norm = norm_layer
        self.layer_norm_first = layer_norm_first
        self.layerdrop = layerdrop
        self.dropout = nn.Dropout(dropout, inplace=True)

    def forward(self, x, padding_mask, alibi_bias, alibi_scale):
        if self.norm is not None and not self.layer_norm_first:
            x = self.norm(x)
        x = self.dropout(x)
        for i, blk in enumerate(self.blocks):
            if not self.training or self.layerdrop == 0 or np.random.random() > self.layerdrop:
                ab = alibi_bias
                if ab is not None and alibi_scale is not None:
                    scale = alibi_scale[i] if alibi_scale.size(0) > 1 else alibi_scale.squeeze(0)
                    ab = ab * scale.type_as(ab)
                x, _ = blk(x, padding_mask, ab)
        if self.norm is not None and self.layer_norm_first:
            x = self.norm(x)
        return x


class SamePad2d(nn.Module):

    def __init__(self, kernel_size):
        super().__init__()
        self.remove = 1 if kernel_size % 2 == 0 else 0

    def forward(self, x):
        assert len(x.size()) == 4
        if self.remove > 0:
            x = x[:, :, :-self.remove, :-self.remove]
        return x


class EncDecAttention(nn.Module):

    def __init__(self, q_dim, kv_dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.0, proj_drop=0.0, cosine_attention=False):
        super().__init__()
        self.num_heads = num_heads
        head_dim = q_dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5
        self.q_proj = nn.Linear(q_dim, q_dim, bias=qkv_bias)
        self.kv_proj = nn.Linear(kv_dim, 2 * q_dim, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(q_dim, q_dim)
        self.proj_drop = nn.Dropout(proj_drop)
        self.cosine_attention = cosine_attention
        if cosine_attention:
            self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)

    def forward(self, q, kv, padding_mask=None, alibi_bias=None):
        B, N, C = q.shape
        q = self.q_proj(q).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)
        kv = self.kv_proj(kv).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        k, v = kv[0], kv[1]
        dtype = q.dtype
        if self.cosine_attention:
            attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)
            logit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1.0 / 0.01))).exp()
            attn = attn * logit_scale
        else:
            q = q * self.scale
            attn = q @ k.transpose(-2, -1)
        if alibi_bias is not None:
            attn = attn.type_as(alibi_bias)
            attn[:, :alibi_bias.size(1)] += alibi_bias
        if padding_mask is not None and padding_mask.any():
            attn = attn.masked_fill(padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
        attn = attn.softmax(dim=-1, dtype=torch.float32)
        attn = self.attn_drop(attn)
        x = (attn @ v).transpose(1, 2)
        x = x.reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class EncDecBlock(nn.Module):

    def __init__(self, q_dim, kv_dim, num_heads, mlp_ratio=4.0, qkv_bias=False, qk_scale=None, drop=0.0, attn_drop=0.0, mlp_drop=0.0, post_mlp_drop=0.0, drop_path=0.0, act_layer=nn.GELU, norm_layer=nn.LayerNorm, layer_norm_first=True, cosine_attention=False, first_residual=True):
        super().__init__()
        self.layer_norm_first = layer_norm_first
        self.norm1 = norm_layer(q_dim)
        self.attn = EncDecAttention(q_dim, kv_dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, cosine_attention=cosine_attention)
        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()
        self.norm2 = norm_layer(q_dim)
        mlp_hidden_dim = int(q_dim * mlp_ratio)
        self.mlp = Mlp(in_features=q_dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=mlp_drop)
        self.post_mlp_dropout = nn.Dropout(post_mlp_drop, inplace=False)
        self.first_residual = first_residual

    def forward(self, q, kv, padding_mask=None, alibi_bias=None):
        r = q if self.first_residual else 0
        if self.layer_norm_first:
            x = r + self.drop_path(self.attn(self.norm1(q), kv, padding_mask, alibi_bias))
            r = x = self.mlp(self.norm2(x))
            x = r + self.drop_path(self.post_mlp_dropout(x))
        else:
            x = r + self.drop_path(self.attn(q, kv, padding_mask, alibi_bias))
            r = x = self.norm1(x)
            x = self.mlp(x)
            x = self.norm2(r + self.drop_path(self.post_mlp_dropout(x)))
        return x


def _safe_readline(fd) ->str:
    pos = fd.tell()
    while True:
        try:
            return fd.readline()
        except UnicodeDecodeError:
            pos -= 1
            fd.seek(pos)


class ChunkLineIterator:
    """
    Iterator to properly iterate over lines of a file chunck.
    """

    def __init__(self, fd, start_offset: int, end_offset: int):
        self._fd = fd
        self._start_offset = start_offset
        self._end_offset = end_offset

    def __iter__(self) ->tp.Iterable[str]:
        self._fd.seek(self._start_offset)
        line = _safe_readline(self._fd)
        while line:
            pos = self._fd.tell()
            if self._end_offset > 0 and pos > self._end_offset and pos < self._end_offset + 2 ** 32:
                break
            yield line
            line = self._fd.readline()


class Chunker:
    """
    contextmanager to read a chunck of a file line by line.
    """

    def __init__(self, path: str, start_offset: int, end_offset: int):
        self.path = path
        self.start_offset = start_offset
        self.end_offset = end_offset

    def __enter__(self) ->ChunkLineIterator:
        self.fd = open(self.path, 'r', encoding='utf-8')
        return ChunkLineIterator(self.fd, self.start_offset, self.end_offset)

    def __exit__(self, exc_type, exc_val, exc_tb) ->None:
        self.fd.close()


class PathManager:
    """
    Wrapper for insulating OSS I/O (using Python builtin operations) from
    iopath's PathManager abstraction (for transparently handling various
    internal backends).
    """

    @staticmethod
    def open(path: str, mode: str='r', buffering: int=-1, encoding: Optional[str]=None, errors: Optional[str]=None, newline: Optional[str]=None):
        if IOPathManager:
            return IOPathManager.open(path=path, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline)
        return open(path, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline)

    @staticmethod
    def copy(src_path: str, dst_path: str, overwrite: bool=False) ->bool:
        if IOPathManager:
            return IOPathManager.copy(src_path=src_path, dst_path=dst_path, overwrite=overwrite)
        return shutil.copyfile(src_path, dst_path)

    @staticmethod
    def get_local_path(path: str, **kwargs) ->str:
        if IOPathManager:
            return IOPathManager.get_local_path(path, **kwargs)
        return path

    @staticmethod
    def exists(path: str) ->bool:
        if IOPathManager:
            return IOPathManager.exists(path)
        return os.path.exists(path)

    @staticmethod
    def isfile(path: str) ->bool:
        if IOPathManager:
            return IOPathManager.isfile(path)
        return os.path.isfile(path)

    @staticmethod
    def ls(path: str) ->List[str]:
        if IOPathManager:
            return IOPathManager.ls(path)
        return os.listdir(path)

    @staticmethod
    def mkdirs(path: str) ->None:
        if IOPathManager:
            return IOPathManager.mkdirs(path)
        os.makedirs(path, exist_ok=True)

    @staticmethod
    def rm(path: str) ->None:
        if IOPathManager:
            return IOPathManager.rm(path)
        os.remove(path)

    @staticmethod
    def chmod(path: str, mode: int) ->None:
        if not PathManager.path_requires_pathmanager(path):
            os.chmod(path, mode)

    @staticmethod
    def register_handler(handler) ->None:
        if IOPathManager:
            return IOPathManager.register_handler(handler=handler)

    @staticmethod
    def copy_from_local(local_path: str, dst_path: str, overwrite: bool=False, **kwargs) ->None:
        if IOPathManager:
            return IOPathManager.copy_from_local(local_path=local_path, dst_path=dst_path, overwrite=overwrite, **kwargs)
        return shutil.copyfile(local_path, dst_path)

    @staticmethod
    def path_requires_pathmanager(path: str) ->bool:
        """Do we require PathManager to access given path?"""
        if IOPathManager:
            for p in IOPathManager._path_handlers.keys():
                if path.startswith(p):
                    return True
        return False

    @staticmethod
    def supports_rename(path: str) ->bool:
        return not PathManager.path_requires_pathmanager(path)

    @staticmethod
    def rename(src: str, dst: str):
        os.rename(src, dst)
    """
    ioPath async PathManager methods:
    """

    @staticmethod
    def opena(path: str, mode: str='r', buffering: int=-1, encoding: Optional[str]=None, errors: Optional[str]=None, newline: Optional[str]=None):
        """
        Return file descriptor with asynchronous write operations.
        """
        global IOPathManager
        if not IOPathManager:
            logging.info('ioPath is initializing PathManager.')
            try:
                IOPathManager = PathManager()
            except Exception:
                logging.exception('Failed to initialize ioPath PathManager object.')
        return IOPathManager.opena(path=path, mode=mode, buffering=buffering, encoding=encoding, errors=errors, newline=newline)

    @staticmethod
    def async_close() ->bool:
        """
        Wait for files to be written and clean up asynchronous PathManager.
        NOTE: `PathManager.async_close()` must be called at the end of any
        script that uses `PathManager.opena(...)`.
        """
        global IOPathManager
        if IOPathManager:
            return IOPathManager.async_close()
        return False


def find_offsets(filename: str, num_chunks: int) ->tp.List[int]:
    """
    given a file and a number of chuncks, find the offsets in the file
    to be able to chunk around full lines.
    """
    with open(filename, 'r', encoding='utf-8') as f:
        size = os.fstat(f.fileno()).st_size
        chunk_size = size // num_chunks
        offsets = [(0) for _ in range(num_chunks + 1)]
        for i in range(1, num_chunks):
            f.seek(chunk_size * i)
            _safe_readline(f)
            offsets[i] = f.tell()
        offsets[-1] = size
        return offsets


SPACE_NORMALIZER = re.compile('\\s+')


def tokenize_line(line):
    line = SPACE_NORMALIZER.sub(' ', line)
    line = line.strip()
    return line.split()


class Dictionary:
    """A mapping from symbols to consecutive integers"""

    def __init__(self, *, bos='<s>', pad='<pad>', eos='</s>', unk='<unk>', extra_special_symbols=None):
        self.bos_word, self.unk_word, self.pad_word, self.eos_word = bos, unk, pad, eos
        self.symbols = []
        self.count = []
        self.indices = {}
        self.bos_index = self.add_symbol(bos)
        self.pad_index = self.add_symbol(pad)
        self.eos_index = self.add_symbol(eos)
        self.unk_index = self.add_symbol(unk)
        if extra_special_symbols:
            for s in extra_special_symbols:
                self.add_symbol(s)
        self.nspecial = len(self.symbols)

    def __eq__(self, other):
        return self.indices == other.indices

    def __getitem__(self, idx):
        if idx < len(self.symbols):
            return self.symbols[idx]
        return self.unk_word

    def get_count(self, idx):
        return self.count[idx]

    def __len__(self):
        """Returns the number of symbols in the dictionary"""
        return len(self.symbols)

    def __contains__(self, sym):
        return sym in self.indices

    def index(self, sym):
        """Returns the index of the specified symbol"""
        assert isinstance(sym, str)
        if sym in self.indices:
            return self.indices[sym]
        return self.unk_index

    def string(self, tensor, bpe_symbol=None, escape_unk=False, extra_symbols_to_ignore=None, unk_string=None, include_eos=False, separator=' '):
        """Helper for converting a tensor of token indices to a string.

        Can optionally remove BPE symbols or escape <unk> words.
        """
        if torch.is_tensor(tensor) and tensor.dim() == 2:
            return '\n'.join(self.string(t, bpe_symbol, escape_unk, extra_symbols_to_ignore, include_eos=include_eos) for t in tensor)
        extra_symbols_to_ignore = set(extra_symbols_to_ignore or [])
        if not include_eos:
            extra_symbols_to_ignore.add(self.eos())

        def token_string(i):
            if i == self.unk():
                if unk_string is not None:
                    return unk_string
                else:
                    return self.unk_string(escape_unk)
            else:
                return self[i]
        if hasattr(self, 'bos_index'):
            extra_symbols_to_ignore.add(self.bos())
        sent = separator.join(token_string(i) for i in tensor if utils.item(i) not in extra_symbols_to_ignore)
        return data_utils.post_process(sent, bpe_symbol)

    def unk_string(self, escape=False):
        """Return unknown string, optionally escaped as: <<unk>>"""
        if escape:
            return '<{}>'.format(self.unk_word)
        else:
            return self.unk_word

    def add_symbol(self, word, n=1, overwrite=False):
        """Adds a word to the dictionary"""
        if word in self.indices and not overwrite:
            idx = self.indices[word]
            self.count[idx] = self.count[idx] + n
            return idx
        else:
            idx = len(self.symbols)
            self.indices[word] = idx
            self.symbols.append(word)
            self.count.append(n)
            return idx

    def update(self, new_dict):
        """Updates counts from new dictionary."""
        for word in new_dict.symbols:
            idx2 = new_dict.indices[word]
            if word in self.indices:
                idx = self.indices[word]
                self.count[idx] = self.count[idx] + new_dict.count[idx2]
            else:
                idx = len(self.symbols)
                self.indices[word] = idx
                self.symbols.append(word)
                self.count.append(new_dict.count[idx2])

    def finalize(self, threshold=-1, nwords=-1, padding_factor=8):
        """Sort symbols by frequency in descending order, ignoring special ones.

        Args:
            - threshold defines the minimum word count
            - nwords defines the total number of words in the final dictionary,
                including special symbols
            - padding_factor can be used to pad the dictionary size to be a
                multiple of 8, which is important on some hardware (e.g., Nvidia
                Tensor Cores).
        """
        if nwords <= 0:
            nwords = len(self)
        new_indices = dict(zip(self.symbols[:self.nspecial], range(self.nspecial)))
        new_symbols = self.symbols[:self.nspecial]
        new_count = self.count[:self.nspecial]
        c = Counter(dict(sorted(zip(self.symbols[self.nspecial:], self.count[self.nspecial:]))))
        for symbol, count in c.most_common(nwords - self.nspecial):
            if count >= threshold:
                new_indices[symbol] = len(new_symbols)
                new_symbols.append(symbol)
                new_count.append(count)
            else:
                break
        assert len(new_symbols) == len(new_indices)
        self.count = list(new_count)
        self.symbols = list(new_symbols)
        self.indices = new_indices
        self.pad_to_multiple_(padding_factor)

    def pad_to_multiple_(self, padding_factor):
        """Pad Dictionary size to be a multiple of *padding_factor*."""
        if padding_factor > 1:
            i = 0
            while len(self) % padding_factor != 0:
                symbol = 'madeupword{:04d}'.format(i)
                self.add_symbol(symbol, n=0)
                i += 1

    def bos(self):
        """Helper to get index of beginning-of-sentence symbol"""
        return self.bos_index

    def pad(self):
        """Helper to get index of pad symbol"""
        return self.pad_index

    def eos(self):
        """Helper to get index of end-of-sentence symbol"""
        return self.eos_index

    def unk(self):
        """Helper to get index of unk symbol"""
        return self.unk_index

    @classmethod
    def load(cls, f):
        """Loads the dictionary from a text file with the format:

        ```
        <symbol0> <count0>
        <symbol1> <count1>
        ...
        ```
        """
        d = cls()
        d.add_from_file(f)
        return d

    def add_from_file(self, f):
        """
        Loads a pre-existing dictionary from a text file and adds its symbols
        to this instance.
        """
        if isinstance(f, str):
            try:
                with open(PathManager.get_local_path(f), 'r', encoding='utf-8') as fd:
                    self.add_from_file(fd)
            except FileNotFoundError as fnfe:
                raise fnfe
            except UnicodeError:
                raise Exception('Incorrect encoding detected in {}, please rebuild the dataset'.format(f))
            return
        lines = f.readlines()
        indices_start_line = self._load_meta(lines)
        for line in lines[indices_start_line:]:
            try:
                line, field = line.rstrip().rsplit(' ', 1)
                if field == '#fairseq:overwrite':
                    overwrite = True
                    line, field = line.rsplit(' ', 1)
                else:
                    overwrite = False
                count = int(field)
                word = line
                if word in self and not overwrite:
                    raise RuntimeError("Duplicate word found when loading Dictionary: '{}'. Duplicate words can overwrite earlier ones by adding the #fairseq:overwrite flag at the end of the corresponding row in the dictionary file. If using the Camembert model, please download an updated copy of the model file.".format(word))
                self.add_symbol(word, n=count, overwrite=overwrite)
            except ValueError:
                raise ValueError(f'Incorrect dictionary format, expected \'<token> <cnt> [flags]\': "{line}"')

    def _save(self, f, kv_iterator):
        if isinstance(f, str):
            PathManager.mkdirs(os.path.dirname(f))
            with PathManager.open(f, 'w', encoding='utf-8') as fd:
                return self.save(fd)
        for k, v in kv_iterator:
            None

    def _get_meta(self):
        return [], []

    def _load_meta(self, lines):
        return 0

    def save(self, f):
        """Stores dictionary into a text file"""
        ex_keys, ex_vals = self._get_meta()
        self._save(f, zip(ex_keys + self.symbols[self.nspecial:], ex_vals + self.count[self.nspecial:]))

    def dummy_sentence(self, length):
        t = torch.Tensor(length).uniform_(self.nspecial + 1, len(self)).long()
        t[-1] = self.eos()
        return t

    def encode_line(self, line, line_tokenizer=tokenize_line, add_if_not_exist=True, consumer=None, append_eos=True, reverse_order=False) ->torch.IntTensor:
        words = line_tokenizer(line)
        if reverse_order:
            words = list(reversed(words))
        nwords = len(words)
        ids = torch.IntTensor(nwords + 1 if append_eos else nwords)
        for i, word in enumerate(words):
            if add_if_not_exist:
                idx = self.add_symbol(word)
            else:
                idx = self.index(word)
            if consumer is not None:
                consumer(word, idx)
            ids[i] = idx
        if append_eos:
            ids[nwords] = self.eos_index
        return ids

    @staticmethod
    def _add_file_to_dictionary_single_worker(filename, tokenize, eos_word, start_offset, end_offset):
        counter = Counter()
        with Chunker(filename, start_offset, end_offset) as line_iterator:
            for line in line_iterator:
                for word in tokenize(line):
                    counter.update([word])
                counter.update([eos_word])
        return counter

    @staticmethod
    def add_file_to_dictionary(filename, dict, tokenize, num_workers):

        def merge_result(counter):
            for w, c in sorted(counter.items()):
                dict.add_symbol(w, c)
        local_file = PathManager.get_local_path(filename)
        offsets = find_offsets(local_file, num_workers)
        if num_workers > 1:
            chunks = zip(offsets, offsets[1:])
            pool = Pool(processes=num_workers)
            results = []
            for start_offset, end_offset in chunks:
                results.append(pool.apply_async(Dictionary._add_file_to_dictionary_single_worker, (local_file, tokenize, dict.eos_word, start_offset, end_offset)))
            pool.close()
            pool.join()
            for r in results:
                merge_result(r.get())
        else:
            merge_result(Dictionary._add_file_to_dictionary_single_worker(local_file, tokenize, dict.eos_word, offsets[0], offsets[1]))


class EpochListening:
    """Mixin for receiving updates whenever the epoch increments."""

    @property
    def can_reuse_epoch_itr_across_epochs(self):
        """
        Whether we can reuse the :class:`fairseq.data.EpochBatchIterator` for
        this dataset across epochs.

        This needs to return ``False`` if the sample sizes can change across
        epochs, in which case we may need to regenerate batches at each epoch.
        If your dataset relies in ``set_epoch`` then you should consider setting
        this to ``False``.
        """
        return True

    def set_epoch(self, epoch):
        """Will receive the updated epoch number at the beginning of the epoch."""
        pass


class FairseqDataset(torch.utils.data.Dataset, EpochListening):
    """A dataset that provides helpers for batching."""

    def __getitem__(self, index):
        raise NotImplementedError

    def __len__(self):
        raise NotImplementedError

    def collater(self, samples):
        """Merge a list of samples to form a mini-batch.

        Args:
            samples (List[dict]): samples to collate

        Returns:
            dict: a mini-batch suitable for forwarding with a Model
        """
        raise NotImplementedError

    def num_tokens(self, index):
        """Return the number of tokens in a sample. This value is used to
        enforce ``--max-tokens`` during batching."""
        raise NotImplementedError

    def num_tokens_vec(self, indices):
        """Return the number of tokens for a set of positions defined by indices.
        This value is used to enforce ``--max-tokens`` during batching."""
        raise NotImplementedError

    def size(self, index):
        """Return an example's size as a float or tuple. This value is used when
        filtering a dataset with ``--max-positions``."""
        raise NotImplementedError

    def ordered_indices(self):
        """Return an ordered list of indices. Batches will be constructed based
        on this order."""
        return np.arange(len(self), dtype=np.int64)

    @property
    def supports_prefetch(self):
        """Whether this dataset supports prefetching."""
        return False

    def attr(self, attr: str, index: int):
        return getattr(self, attr, None)

    def prefetch(self, indices):
        """Prefetch the data required for this epoch."""
        raise NotImplementedError

    def get_batch_shapes(self):
        """
        Return a list of valid batch shapes, for example::

            [(8, 512), (16, 256), (32, 128)]

        The first dimension of each tuple is the batch size and can be ``None``
        to automatically infer the max batch size based on ``--max-tokens``.
        The second dimension of each tuple is the max supported length as given
        by :func:`fairseq.data.FairseqDataset.num_tokens`.

        This will be used by :func:`fairseq.data.FairseqDataset.batch_by_size`
        to restrict batch shapes. This is useful on TPUs to avoid too many
        dynamic shapes (and recompilations).
        """
        return None

    def batch_by_size(self, indices, max_tokens=None, max_sentences=None, required_batch_size_multiple=1):
        """
        Given an ordered set of indices, return batches according to
        *max_tokens*, *max_sentences* and *required_batch_size_multiple*.
        """
        fixed_shapes = self.get_batch_shapes()
        if fixed_shapes is not None:

            def adjust_bsz(bsz, num_tokens):
                if bsz is None:
                    assert max_tokens is not None, 'Must specify --max-tokens'
                    bsz = max_tokens // num_tokens
                if max_sentences is not None:
                    bsz = min(bsz, max_sentences)
                elif bsz >= required_batch_size_multiple and bsz % required_batch_size_multiple != 0:
                    bsz -= bsz % required_batch_size_multiple
                return bsz
            fixed_shapes = np.array([[adjust_bsz(bsz, num_tokens), num_tokens] for bsz, num_tokens in fixed_shapes])
        try:
            num_tokens_vec = self.num_tokens_vec(indices).astype('int64')
        except NotImplementedError:
            num_tokens_vec = None
        return data_utils.batch_by_size(indices, num_tokens_fn=self.num_tokens, num_tokens_vec=num_tokens_vec, max_tokens=max_tokens, max_sentences=max_sentences, required_batch_size_multiple=required_batch_size_multiple, fixed_shapes=fixed_shapes)

    def filter_indices_by_size(self, indices, max_sizes):
        """
        Filter a list of sample indices. Remove those that are longer than
        specified in *max_sizes*.

        WARNING: don't update, override method in child classes

        Args:
            indices (np.array): original array of sample indices
            max_sizes (int or list[int] or tuple[int]): max sample size,
                can be defined separately for src and tgt (then list or tuple)

        Returns:
            np.array: filtered sample array
            list: list of removed indices
        """
        if isinstance(max_sizes, float) or isinstance(max_sizes, int):
            if hasattr(self, 'sizes') and isinstance(self.sizes, np.ndarray):
                ignored = indices[self.sizes[indices] > max_sizes].tolist()
                indices = indices[self.sizes[indices] <= max_sizes]
            elif hasattr(self, 'sizes') and isinstance(self.sizes, list) and len(self.sizes) == 1:
                ignored = indices[self.sizes[0][indices] > max_sizes].tolist()
                indices = indices[self.sizes[0][indices] <= max_sizes]
            else:
                indices, ignored = data_utils._filter_by_size_dynamic(indices, self.size, max_sizes)
        else:
            indices, ignored = data_utils._filter_by_size_dynamic(indices, self.size, max_sizes)
        return indices, ignored

    @property
    def supports_fetch_outside_dataloader(self):
        """Whether this dataset supports fetching outside the workers of the dataloader."""
        return True


class StatefulContainer(object):

    def __init__(self):
        self._state = dict()
        self._factories = dict()

    def add_factory(self, name, factory: Callable[[], Any]):
        self._factories[name] = factory

    def merge_state_dict(self, state_dict: Dict[str, Any]):
        self._state.update(state_dict)

    @property
    def state_dict(self) ->Dict[str, Any]:
        return self._state

    def __getattr__(self, name):
        if name not in self._state and name in self._factories:
            self._state[name] = self._factories[name]()
        if name in self._state:
            return self._state[name]
        raise AttributeError(f'Task state has no factory for attribute {name}')


def eval_str_list(x, type=float):
    if x is None:
        return None
    if isinstance(x, str):
        x = eval(x)
    try:
        return list(map(type, x))
    except TypeError:
        return [type(x)]


def interpret_dc_type(field_type):
    if isinstance(field_type, str):
        raise RuntimeError('field should be a type')
    if field_type == Any:
        return str
    typestring = str(field_type)
    if re.match('(typing.|^)Union\\[(.*), NoneType\\]$', typestring) or typestring.startswith('typing.Optional'):
        return field_type.__args__[0]
    return field_type


class FixedPositionalEncoder(nn.Module):

    def __init__(self, pos_embed):
        super().__init__()
        self.positions = pos_embed

    def forward(self, x, padding_mask):
        return self.positions


def Linear(in_features, out_features, bias=True):
    m = nn.Linear(in_features, out_features, bias)
    nn.init.xavier_uniform_(m.weight)
    if bias:
        nn.init.constant_(m.bias, 0.0)
    return m


class LearnedPositionalEmbedding(nn.Embedding):
    """
    This module learns positional embeddings up to a fixed maximum size.
    Padding ids are ignored by either offsetting based on padding_idx
    or by setting padding_idx to None and ensuring that the appropriate
    position ids are passed to the forward function.
    """

    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        self.onnx_trace = False
        if self.padding_idx is not None:
            self.max_positions = self.num_embeddings - self.padding_idx - 1
        else:
            self.max_positions = self.num_embeddings

    def forward(self, input: Tensor, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, positions: Optional[Tensor]=None):
        """Input is expected to be of size [bsz x seqlen]."""
        assert positions is None or self.padding_idx is None, 'If positions is pre-computed then padding_idx should not be set.'
        if positions is None:
            if incremental_state is not None:
                positions = torch.zeros((1, 1), device=input.device, dtype=input.dtype).fill_(int(self.padding_idx + input.size(1)))
            else:
                positions = utils.make_positions(input, self.padding_idx, onnx_trace=self.onnx_trace)
        return F.embedding(positions, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)


class SinusoidalPositionalEmbedding(nn.Module):
    """This module produces sinusoidal positional embeddings of any length.

    Padding symbols are ignored.
    """

    def __init__(self, embedding_dim, padding_idx, init_size=1024):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.padding_idx = padding_idx if padding_idx is not None else 0
        self.weights = SinusoidalPositionalEmbedding.get_embedding(init_size, embedding_dim, padding_idx)
        self.onnx_trace = False
        self.register_buffer('_float_tensor', torch.FloatTensor(1))
        self.max_positions = int(100000.0)

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    @staticmethod
    def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int]=None):
        """Build sinusoidal embeddings.

        This matches the implementation in tensor2tensor, but differs slightly
        from the description in Section 3.5 of "Attention Is All You Need".
        """
        half_dim = embedding_dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)
        if embedding_dim % 2 == 1:
            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
        if padding_idx is not None:
            emb[padding_idx, :] = 0
        return emb

    def forward(self, input, incremental_state: Optional[Any]=None, timestep: Optional[Tensor]=None, positions: Optional[Any]=None):
        """Input is expected to be of size [bsz x seqlen]."""
        bspair = torch.onnx.operators.shape_as_tensor(input)
        bsz, seq_len = bspair[0], bspair[1]
        max_pos = self.padding_idx + 1 + seq_len
        if self.weights is None or max_pos > self.weights.size(0):
            self.weights = SinusoidalPositionalEmbedding.get_embedding(max_pos, self.embedding_dim, self.padding_idx)
        self.weights = self.weights
        if incremental_state is not None:
            pos = timestep.view(-1)[0] + 1 if timestep is not None else seq_len
            if self.onnx_trace:
                return self.weights.index_select(index=self.padding_idx + pos, dim=0).unsqueeze(1).repeat(bsz, 1, 1)
            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)
        positions = utils.make_positions(input, self.padding_idx, onnx_trace=self.onnx_trace)
        if self.onnx_trace:
            flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))
            embedding_shape = torch.cat((bsz.view(1), seq_len.view(1), torch.tensor([-1], dtype=torch.long)))
            embeddings = torch.onnx.operators.reshape_from_tensor_shape(flat_embeddings, embedding_shape)
            return embeddings
        return self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()


def PositionalEmbedding(num_embeddings: int, embedding_dim: int, padding_idx: int, learned: bool=False):
    if learned:
        if padding_idx is not None:
            num_embeddings = num_embeddings + padding_idx + 1
        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)
        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)
        if padding_idx is not None:
            nn.init.constant_(m.weight[padding_idx], 0)
    else:
        m = SinusoidalPositionalEmbedding(embedding_dim, padding_idx, init_size=num_embeddings + padding_idx + 1)
    return m


DEFAULT_MAX_SOURCE_POSITIONS = 1024


DEFAULT_MAX_TARGET_POSITIONS = 1024


DEFAULT_MIN_PARAMS_TO_WRAP = int(100000000.0)


_NAME_PARSER = '(decoder|encoder|quant_noise)_(.*)'


def safe_getattr(obj, k, default=None):
    """Returns obj[k] if it exists and is not None, otherwise returns default."""
    if OmegaConf.is_config(obj):
        return obj[k] if k in obj and obj[k] is not None else default
    return getattr(obj, k, default)


def safe_hasattr(obj, k):
    """Returns True if the given key exists and is not None."""
    return getattr(obj, k, None) is not None


class TransformerDecoderLayerBase(nn.Module):
    """Decoder layer block.

    In the original paper each operation (multi-head attention, encoder
    attention or FFN) is postprocessed with: `dropout -> add residual ->
    layernorm`. In the tensor2tensor code they suggest that learning is more
    robust when preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.decoder.normalize_before* to ``True``.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).
    """

    def __init__(self, cfg, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):
        super().__init__()
        self.embed_dim = cfg.decoder.embed_dim
        self.dropout_module = FairseqDropout(cfg.dropout, module_name=self.__class__.__name__)
        self.quant_noise = cfg.quant_noise.pq
        self.quant_noise_block_size = cfg.quant_noise.pq_block_size
        self.cross_self_attention = cfg.cross_self_attention
        self.self_attn = self.build_self_attention(self.embed_dim, cfg, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn)
        self.attn_ln = LayerNorm(self.embed_dim) if utils.safe_getattr(cfg, 'scale_attn', False) else None
        self.nh = self.self_attn.num_heads
        self.head_dim = self.self_attn.head_dim
        scale_heads = utils.safe_getattr(cfg, 'scale_heads', False)
        self.c_attn = nn.Parameter(torch.ones((self.nh,)), requires_grad=True) if scale_heads else None
        self.activation_fn = utils.get_activation_fn(activation=cfg.activation_fn)
        activation_dropout_p = cfg.activation_dropout
        if activation_dropout_p == 0:
            activation_dropout_p = cfg.relu_dropout or 0
        self.activation_dropout_module = FairseqDropout(float(activation_dropout_p), module_name=self.__class__.__name__)
        self.normalize_before = cfg.decoder.normalize_before
        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        if no_encoder_attn:
            self.encoder_attn = None
            self.encoder_attn_layer_norm = None
        else:
            self.encoder_attn = self.build_encoder_attention(self.embed_dim, cfg)
            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.ffn_layernorm = LayerNorm(cfg.decoder.ffn_embed_dim) if utils.safe_getattr(cfg, 'scale_fc', False) else None
        self.w_resid = nn.Parameter(torch.ones(self.embed_dim), requires_grad=True) if utils.safe_getattr(cfg, 'scale_resids', False) else None
        self.fc1 = self.build_fc1(self.embed_dim, cfg.decoder.ffn_embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.fc2 = self.build_fc2(cfg.decoder.ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.final_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.need_attn = True
        self.onnx_trace = False

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), q_noise, qn_block_size)

    def build_self_attention(self, embed_dim, cfg, add_bias_kv=False, add_zero_attn=False):
        return MultiheadAttention(embed_dim, cfg.decoder.attention_heads, dropout=cfg.attention_dropout, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn, self_attention=not cfg.cross_self_attention, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, xformers_att_config=cfg.decoder.xformers_att_config)

    def build_encoder_attention(self, embed_dim, cfg):
        return MultiheadAttention(embed_dim, cfg.decoder.attention_heads, kdim=cfg.encoder.embed_dim, vdim=cfg.encoder.embed_dim, dropout=cfg.attention_dropout, encoder_decoder_attention=True, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, xformers_att_config=cfg.encoder.xformers_att_config)

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def residual_connection(self, x, residual):
        return residual + x

    def forward(self, x, encoder_out: Optional[torch.Tensor]=None, encoder_padding_mask: Optional[torch.Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, prev_self_attn_state: Optional[List[torch.Tensor]]=None, prev_attn_state: Optional[List[torch.Tensor]]=None, self_attn_mask: Optional[torch.Tensor]=None, self_attn_padding_mask: Optional[torch.Tensor]=None, need_attn: bool=False, need_head_weights: bool=False):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor, optional): binary
                ByteTensor of shape `(batch, src_len)` where padding
                elements are indicated by ``1``.
            need_attn (bool, optional): return attention weights
            need_head_weights (bool, optional): return attention weights
                for each head (default: return average over heads).

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if need_head_weights:
            need_attn = True
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if prev_self_attn_state is not None:
            prev_key, prev_value = prev_self_attn_state[:2]
            saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_self_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_self_attn_state[2]
            assert incremental_state is not None
            self.self_attn._set_input_buffer(incremental_state, saved_state)
        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)
        if self.cross_self_attention and not (incremental_state is not None and _self_attn_input_buffer is not None and 'prev_key' in _self_attn_input_buffer):
            if self_attn_mask is not None:
                assert encoder_out is not None
                self_attn_mask = torch.cat((x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1)
            if self_attn_padding_mask is not None:
                if encoder_padding_mask is None:
                    assert encoder_out is not None
                    encoder_padding_mask = self_attn_padding_mask.new_zeros(encoder_out.size(1), encoder_out.size(0))
                self_attn_padding_mask = torch.cat((encoder_padding_mask, self_attn_padding_mask), dim=1)
            assert encoder_out is not None
            y = torch.cat((encoder_out, x), dim=0)
        else:
            y = x
        x, attn = self.self_attn(query=x, key=y, value=y, key_padding_mask=self_attn_padding_mask, incremental_state=incremental_state, need_weights=False, attn_mask=self_attn_mask)
        if self.c_attn is not None:
            tgt_len, bsz = x.size(0), x.size(1)
            x = x.view(tgt_len, bsz, self.nh, self.head_dim)
            x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)
            x = x.reshape(tgt_len, bsz, self.embed_dim)
        if self.attn_ln is not None:
            x = self.attn_ln(x)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if self.encoder_attn is not None and encoder_out is not None:
            residual = x
            if self.normalize_before:
                x = self.encoder_attn_layer_norm(x)
            if prev_attn_state is not None:
                prev_key, prev_value = prev_attn_state[:2]
                saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
                if len(prev_attn_state) >= 3:
                    saved_state['prev_key_padding_mask'] = prev_attn_state[2]
                assert incremental_state is not None
                self.encoder_attn._set_input_buffer(incremental_state, saved_state)
            x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
            x = self.dropout_module(x)
            x = self.residual_connection(x, residual)
            if not self.normalize_before:
                x = self.encoder_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        if self.ffn_layernorm is not None:
            x = self.ffn_layernorm(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        if self.w_resid is not None:
            residual = torch.mul(self.w_resid, residual)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.onnx_trace and incremental_state is not None:
            saved_state = self.self_attn._get_input_buffer(incremental_state)
            assert saved_state is not None
            if self_attn_padding_mask is not None:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value'], saved_state['prev_key_padding_mask']]
            else:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value']]
            return x, attn, self_attn_state
        return x, attn, None

    def make_generation_fast_(self, need_attn: bool=False, **kwargs):
        self.need_attn = need_attn


class TransformerDecoderLayer(TransformerDecoderLayerBase):

    def __init__(self, args, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):
        super().__init__(TransformerConfig.from_namespace(args), no_encoder_attn=no_encoder_attn, add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn)
        self.args = args

    def build_self_attention(self, embed_dim, args, add_bias_kv=False, add_zero_attn=False):
        return super().build_self_attention(embed_dim, TransformerConfig.from_namespace(args), add_bias_kv=add_bias_kv, add_zero_attn=add_zero_attn)

    def build_encoder_attention(self, embed_dim, args):
        return super().build_encoder_attention(embed_dim, TransformerConfig.from_namespace(args))


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position
    pos: a list of positions to be encoded: size (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000 ** omega
    pos = pos.reshape(-1)
    out = np.einsum('m,d->md', pos, omega)
    emb_sin = np.sin(out)
    emb_cos = np.cos(out)
    emb = np.concatenate([emb_sin, emb_cos], axis=1)
    return emb


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])
    emb = np.concatenate([emb_h, emb_w], axis=1)
    return emb


def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):
    """
    grid_size: int of the grid height and width
    return:
    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token:
        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)
    return pos_embed


def get_alibi(max_positions: int, attention_heads: int):

    def get_slopes(n):

        def get_slopes_power_of_2(n):
            start = 2 ** -2 ** -(math.log2(n) - 3)
            ratio = start
            return [(start * ratio ** i) for i in range(n)]
        if math.log2(n).is_integer():
            return get_slopes_power_of_2(n)
        else:
            closest_power_of_2 = 2 ** math.floor(math.log2(n))
            return get_slopes_power_of_2(closest_power_of_2) + get_slopes(2 * closest_power_of_2)[0::2][:n - closest_power_of_2]
    maxpos = max_positions
    attn_heads = attention_heads
    slopes = torch.Tensor(get_slopes(attn_heads))
    pos_bias = torch.abs(torch.arange(maxpos).unsqueeze(0) - torch.arange(maxpos).unsqueeze(1)) * -1
    alibi_bias = slopes.unsqueeze(1).unsqueeze(1) * pos_bias.unsqueeze(0).expand(attn_heads, -1, -1)
    return alibi_bias


def get_alibi_bias(alibi_biases, batch_size, time_steps, heads, dtype, device, dims=1, distance='manhattan'):
    cache_key = f'{dims}_{heads}_{distance}'
    buffered = alibi_biases.get(cache_key, None)
    target_size = heads * batch_size
    if buffered is None or buffered.size(0) < target_size or buffered.size(1) < time_steps or buffered.dtype != dtype or buffered.device != device:
        bt = max(time_steps, buffered.size(1) if buffered is not None else 0)
        bn = max(target_size, buffered.size(0) if buffered is not None else 0) // heads
        buffered = get_alibi(bt, heads, dims=dims, distance=distance).repeat(bn, 1, 1)
        alibi_biases[cache_key] = buffered
    b = buffered[:target_size, :time_steps, :time_steps]
    b = b.view(batch_size, heads, time_steps, time_steps)
    return b


class TextFeatPositionalEncoder(nn.Module):
    """
    Original encoder expects (B, T) long input. This module wraps it to take
    local_encoder output which are (B, T, D) float tensors
    """

    def __init__(self, pos_encoder):
        super().__init__()
        self.pos_encoder = pos_encoder

    def forward(self, x, padding_mask):
        return self.pos_encoder(x[..., 0])


class TextLocalEncoder(nn.Module):

    def __init__(self, vocab_size, embed_dim, max_source_positions, pad_idx, no_scale_embedding, layernorm_embedding, dropout, no_token_positional_embeddings, learned_pos):
        super().__init__()
        self.pad_idx = pad_idx
        self.dropout_module = FairseqDropout(dropout)
        self.embed_tokens = nn.Embedding(vocab_size, embed_dim, pad_idx)
        self.embed_scale = 1.0 if no_scale_embedding else math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(max_source_positions, embed_dim, pad_idx, learned=learned_pos) if not no_token_positional_embeddings else None
        self.embed_scale = 1.0 if no_scale_embedding else math.sqrt(embed_dim)
        self.layernorm_embedding = None
        if layernorm_embedding:
            self.layernorm_embedding = LayerNorm(embed_dim)

    def forward(self, src_tokens):
        x = self.embed_scale * self.embed_tokens(src_tokens)
        if self.embed_positions is not None:
            x = x + self.embed_positions(src_tokens)
        if self.layernorm_embedding is not None:
            x = self.layernorm_embedding(x)
        x = self.dropout_module(x)
        return x


class BaseRanker(nn.Module):

    def __init__(self, args, task):
        super().__init__()
        self.separator_token = task.dictionary.eos()
        self.padding_idx = task.dictionary.pad()

    def forward(self, src_tokens):
        raise NotImplementedError

    def get_segment_labels(self, src_tokens):
        segment_boundary = (src_tokens == self.separator_token).long()
        segment_labels = segment_boundary.cumsum(dim=1) - segment_boundary - (src_tokens == self.padding_idx).long()
        return segment_labels

    def get_positions(self, src_tokens, segment_labels):
        segment_positions = torch.arange(src_tokens.shape[1]).repeat(src_tokens.shape[0], 1)
        segment_boundary = (src_tokens == self.separator_token).long()
        _, col_idx = (segment_positions * segment_boundary).nonzero(as_tuple=True)
        col_idx = torch.cat([torch.zeros(1).type_as(col_idx), col_idx])
        offset = torch.cat([torch.zeros(1).type_as(segment_boundary), segment_boundary.sum(dim=1).cumsum(dim=0)[:-1]])
        segment_positions -= col_idx[segment_labels + offset.unsqueeze(1)] * (segment_labels != 0)
        padding_mask = src_tokens.ne(self.padding_idx)
        segment_positions = (segment_positions + 1) * padding_mask.type_as(segment_positions) + self.padding_idx
        return segment_positions


class RobertaClassificationHead(nn.Module):
    """Head for sentence-level classification tasks."""

    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, q_noise=0, qn_block_size=8, do_spectral_norm=False):
        super().__init__()
        self.dense = nn.Linear(input_dim, inner_dim)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.dropout = nn.Dropout(p=pooler_dropout)
        self.out_proj = apply_quant_noise_(nn.Linear(inner_dim, num_classes), q_noise, qn_block_size)
        if do_spectral_norm:
            if q_noise != 0:
                raise NotImplementedError('Attempting to use Spectral Normalization with Quant Noise. This is not officially supported')
            self.out_proj = torch.nn.utils.spectral_norm(self.out_proj)

    def forward(self, features, **kwargs):
        x = features[:, 0, :]
        x = self.dropout(x)
        x = self.dense(x)
        x = self.activation_fn(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class LayerDropModuleList(nn.ModuleList):
    """
    A LayerDrop implementation based on :class:`torch.nn.ModuleList`.

    We refresh the choice of which layers to drop every time we iterate
    over the LayerDropModuleList instance. During evaluation we always
    iterate over all layers.

    Usage::

        layers = LayerDropList(p=0.5, modules=[layer1, layer2, layer3])
        for layer in layers:  # this might iterate over layers 1 and 3
            x = layer(x)
        for layer in layers:  # this might iterate over all layers
            x = layer(x)
        for layer in layers:  # this might not iterate over any layers
            x = layer(x)

    Args:
        p (float): probability of dropping out each layer
        modules (iterable, optional): an iterable of modules to add
    """

    def __init__(self, p, modules=None):
        super().__init__(modules)
        self.p = p

    def __iter__(self):
        dropout_probs = torch.empty(len(self)).uniform_()
        for i, m in enumerate(super().__iter__()):
            if not self.training or dropout_probs[i] > self.p:
                yield m


class TransformerSentenceEncoder(nn.Module):
    """
    Implementation for a Bi-directional Transformer based Sentence Encoder used
    in BERT/XLM style pre-trained models.

    This first computes the token embedding using the token embedding matrix,
    position embeddings (if specified) and segment embeddings
    (if specified). After applying the specified number of
    TransformerEncoderLayers, it outputs all the internal states of the
    encoder as well as the final representation associated with the first
    token (usually CLS token).

    Input:
        - tokens: B x T matrix representing sentences
        - segment_labels: B x T matrix representing segment label for tokens

    Output:
        - a tuple of the following:
            - a list of internal model states used to compute the
              predictions where each tensor has shape T x B x C
            - sentence representation associated with first input token
              in format B x C.
    """

    def __init__(self, padding_idx: int, vocab_size: int, num_encoder_layers: int=6, embedding_dim: int=768, ffn_embedding_dim: int=3072, num_attention_heads: int=8, dropout: float=0.1, attention_dropout: float=0.1, activation_dropout: float=0.1, layerdrop: float=0.0, max_seq_len: int=256, num_segments: int=2, use_position_embeddings: bool=True, offset_positions_by_padding: bool=True, encoder_normalize_before: bool=False, apply_bert_init: bool=False, activation_fn: str='relu', learned_pos_embedding: bool=True, embed_scale: float=None, freeze_embeddings: bool=False, n_trans_layers_to_freeze: int=0, export: bool=False, traceable: bool=False, q_noise: float=0.0, qn_block_size: int=8) ->None:
        super().__init__()
        self.padding_idx = padding_idx
        self.vocab_size = vocab_size
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.layerdrop = layerdrop
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim
        self.num_segments = num_segments
        self.use_position_embeddings = use_position_embeddings
        self.apply_bert_init = apply_bert_init
        self.learned_pos_embedding = learned_pos_embedding
        self.traceable = traceable
        self.embed_tokens = self.build_embedding(self.vocab_size, self.embedding_dim, self.padding_idx)
        self.embed_scale = embed_scale
        if q_noise > 0:
            self.quant_noise = apply_quant_noise_(nn.Linear(self.embedding_dim, self.embedding_dim, bias=False), q_noise, qn_block_size)
        else:
            self.quant_noise = None
        self.segment_embeddings = nn.Embedding(self.num_segments, self.embedding_dim, padding_idx=None) if self.num_segments > 0 else None
        self.embed_positions = PositionalEmbedding(self.max_seq_len, self.embedding_dim, padding_idx=self.padding_idx if offset_positions_by_padding else None, learned=self.learned_pos_embedding) if self.use_position_embeddings else None
        if encoder_normalize_before:
            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)
        else:
            self.emb_layer_norm = None
        if self.layerdrop > 0.0:
            self.layers = LayerDropModuleList(p=self.layerdrop)
        else:
            self.layers = nn.ModuleList([])
        self.layers.extend([self.build_transformer_sentence_encoder_layer(embedding_dim=self.embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=self.dropout_module.p, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, export=export, q_noise=q_noise, qn_block_size=qn_block_size) for _ in range(num_encoder_layers)])
        if self.apply_bert_init:
            self.apply(init_bert_params)

        def freeze_module_params(m):
            if m is not None:
                for p in m.parameters():
                    p.requires_grad = False
        if freeze_embeddings:
            freeze_module_params(self.embed_tokens)
            freeze_module_params(self.segment_embeddings)
            freeze_module_params(self.embed_positions)
            freeze_module_params(self.emb_layer_norm)
        for layer in range(n_trans_layers_to_freeze):
            freeze_module_params(self.layers[layer])

    def build_embedding(self, vocab_size, embedding_dim, padding_idx):
        return nn.Embedding(vocab_size, embedding_dim, padding_idx)

    def build_transformer_sentence_encoder_layer(self, embedding_dim, ffn_embedding_dim, num_attention_heads, dropout, attention_dropout, activation_dropout, activation_fn, export, q_noise, qn_block_size):
        return TransformerSentenceEncoderLayer(embedding_dim=embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, export=export, q_noise=q_noise, qn_block_size=qn_block_size)

    def forward(self, tokens: torch.Tensor, segment_labels: torch.Tensor=None, last_state_only: bool=False, positions: Optional[torch.Tensor]=None, token_embeddings: Optional[torch.Tensor]=None, attn_mask: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        is_tpu = tokens.device.type == 'xla'
        padding_mask = tokens.eq(self.padding_idx)
        if not self.traceable and not is_tpu and not padding_mask.any():
            padding_mask = None
        if token_embeddings is not None:
            x = token_embeddings
        else:
            x = self.embed_tokens(tokens)
        if self.embed_scale is not None:
            x = x * self.embed_scale
        if self.embed_positions is not None:
            x = x + self.embed_positions(tokens, positions=positions)
        if self.segment_embeddings is not None and segment_labels is not None:
            x = x + self.segment_embeddings(segment_labels)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        if self.emb_layer_norm is not None:
            x = self.emb_layer_norm(x)
        x = self.dropout_module(x)
        if padding_mask is not None:
            x = x * (1 - padding_mask.unsqueeze(-1).type_as(x))
        x = x.transpose(0, 1)
        inner_states = []
        if not last_state_only:
            inner_states.append(x)
        for layer in self.layers:
            x, _ = layer(x, self_attn_padding_mask=padding_mask, self_attn_mask=attn_mask)
            if not last_state_only:
                inner_states.append(x)
        sentence_rep = x[0, :, :]
        if last_state_only:
            inner_states = [x]
        if self.traceable:
            return torch.stack(inner_states), sentence_rep
        else:
            return inner_states, sentence_rep


def update_init_roberta_model_state(state):
    """
   update the state_dict of a Roberta model for initializing
   weights of the BertRanker
   """
    for k in list(state.keys()):
        if '.lm_head.' in k or 'version' in k:
            del state[k]
            continue
        assert k.startswith('encoder.sentence_encoder.') or k.startswith('decoder.sentence_encoder.'), f'Cannot recognize parameter name {k}'
        if 'layernorm_embedding' in k:
            new_k = k.replace('.layernorm_embedding.', '.emb_layer_norm.')
            state[new_k[25:]] = state[k]
        else:
            state[k[25:]] = state[k]
        del state[k]


class BertRanker(BaseRanker):

    def __init__(self, args, task):
        super(BertRanker, self).__init__(args, task)
        init_model = getattr(args, 'pretrained_model', '')
        self.joint_layers = nn.ModuleList()
        if os.path.isfile(init_model):
            None
            x = hub_utils.from_pretrained(os.path.dirname(init_model), checkpoint_file=os.path.basename(init_model))
            in_state_dict = x['models'][0].state_dict()
            init_args = x['args'].model
            num_positional_emb = init_args.max_positions + task.dictionary.pad() + 1
            self.model = TransformerSentenceEncoder(padding_idx=task.dictionary.pad(), vocab_size=len(task.dictionary), num_encoder_layers=getattr(args, 'encoder_layers', init_args.encoder_layers), embedding_dim=init_args.encoder_embed_dim, ffn_embedding_dim=init_args.encoder_ffn_embed_dim, num_attention_heads=init_args.encoder_attention_heads, dropout=init_args.dropout, attention_dropout=init_args.attention_dropout, activation_dropout=init_args.activation_dropout, num_segments=2, max_seq_len=num_positional_emb, offset_positions_by_padding=False, encoder_normalize_before=True, apply_bert_init=True, activation_fn=init_args.activation_fn, freeze_embeddings=args.freeze_embeddings, n_trans_layers_to_freeze=args.n_trans_layers_to_freeze)
            if args.freeze_embeddings:
                for p in self.model.segment_embeddings.parameters():
                    p.requires_grad = False
            update_init_roberta_model_state(in_state_dict)
            None
            self.model.load_state_dict(in_state_dict, strict=False)
            ffn_embedding_dim = init_args.encoder_ffn_embed_dim
            num_attention_heads = init_args.encoder_attention_heads
            dropout = init_args.dropout
            attention_dropout = init_args.attention_dropout
            activation_dropout = init_args.activation_dropout
            activation_fn = init_args.activation_fn
            classifier_embed_dim = getattr(args, 'embed_dim', init_args.encoder_embed_dim)
            if classifier_embed_dim != init_args.encoder_embed_dim:
                self.transform_layer = nn.Linear(init_args.encoder_embed_dim, classifier_embed_dim)
        else:
            self.model = TransformerSentenceEncoder(padding_idx=task.dictionary.pad(), vocab_size=len(task.dictionary), num_encoder_layers=args.encoder_layers, embedding_dim=args.embed_dim, ffn_embedding_dim=args.ffn_embed_dim, num_attention_heads=args.attention_heads, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.activation_dropout, max_seq_len=task.max_positions() if task.max_positions() else args.tokens_per_sample, num_segments=2, offset_positions_by_padding=False, encoder_normalize_before=args.encoder_normalize_before, apply_bert_init=args.apply_bert_init, activation_fn=args.activation_fn)
            classifier_embed_dim = args.embed_dim
            ffn_embedding_dim = args.ffn_embed_dim
            num_attention_heads = args.attention_heads
            dropout = args.dropout
            attention_dropout = args.attention_dropout
            activation_dropout = args.activation_dropout
            activation_fn = args.activation_fn
        self.joint_classification = args.joint_classification
        if args.joint_classification == 'sent':
            if args.joint_normalize_before:
                self.joint_layer_norm = LayerNorm(classifier_embed_dim)
            else:
                self.joint_layer_norm = None
            self.joint_layers = nn.ModuleList([TransformerSentenceEncoderLayer(embedding_dim=classifier_embed_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn) for _ in range(args.num_joint_layers)])
        self.classifier = RobertaClassificationHead(classifier_embed_dim, classifier_embed_dim, 1, 'tanh', args.classifier_dropout)

    def forward(self, src_tokens, src_lengths):
        segment_labels = self.get_segment_labels(src_tokens)
        positions = self.get_positions(src_tokens, segment_labels)
        inner_states, _ = self.model(tokens=src_tokens, segment_labels=segment_labels, last_state_only=True, positions=positions)
        return inner_states[-1].transpose(0, 1)

    def sentence_forward(self, encoder_out, src_tokens=None, sentence_rep='head'):
        if sentence_rep == 'head':
            x = encoder_out[:, :1, :]
        else:
            assert src_tokens is not None, 'meanpool requires src_tokens input'
            segment_labels = self.get_segment_labels(src_tokens)
            padding_mask = src_tokens.ne(self.padding_idx)
            encoder_mask = segment_labels * padding_mask.type_as(segment_labels)
            if sentence_rep == 'meanpool':
                ntokens = torch.sum(encoder_mask, dim=1, keepdim=True)
                x = torch.sum(encoder_out * encoder_mask.unsqueeze(2), dim=1, keepdim=True) / ntokens.unsqueeze(2).type_as(encoder_out)
            else:
                encoder_out[(encoder_mask == 0).unsqueeze(2).repeat(1, 1, encoder_out.shape[-1])] = -float('inf')
                x, _ = torch.max(encoder_out, dim=1, keepdim=True)
        if hasattr(self, 'transform_layer'):
            x = self.transform_layer(x)
        return x

    def joint_forward(self, x):
        if self.joint_layer_norm:
            x = self.joint_layer_norm(x.transpose(0, 1))
            x = x.transpose(0, 1)
        for layer in self.joint_layers:
            x, _ = layer(x, self_attn_padding_mask=None)
        return x

    def classification_forward(self, x):
        return self.classifier(x)


class Predictor(nn.Module):

    def __init__(self, n_tokens, emb_dim):
        super(Predictor, self).__init__()
        self.n_tokens = n_tokens
        self.emb_dim = emb_dim
        self.padding_token = n_tokens
        self.emb = nn.Embedding(n_tokens + 1, emb_dim, padding_idx=self.padding_token)

    def inflate_input(self, batch):
        """ get a sequence of tokens, predict their durations
        and inflate them accordingly """
        batch_durs = self.forward(batch)
        batch_durs = torch.exp(batch_durs) - 1
        batch_durs = batch_durs.round()
        output = []
        for seq, durs in zip(batch, batch_durs):
            inflated_seq = []
            for token, n in zip(seq, durs):
                if token == self.padding_token:
                    break
                n = int(n.item())
                token = int(token.item())
                inflated_seq.extend([token for _ in range(n)])
            output.append(inflated_seq)
        output = torch.LongTensor(output)
        return output


def bin2freq(x, f0_min, f0_max, bins, mode):
    n_bins = len(bins) + 1
    assert x.shape[-1] == n_bins
    bins = torch.cat([torch.tensor([f0_min]), bins])
    if mode == 'mean':
        f0 = (x * bins).sum(-1, keepdims=True) / x.sum(-1, keepdims=True)
    elif mode == 'argmax':
        idx = F.one_hot(x.argmax(-1), num_classes=n_bins)
        f0 = (idx * bins).sum(-1, keepdims=True)
    else:
        raise NotImplementedError()
    return f0[..., 0]


class CnnPredictor(nn.Module):

    def __init__(self, n_tokens, emb_dim, channels, kernel, dropout, n_layers, spk_emb, gst_emb, n_bins, f0_pred, f0_log, f0_norm):
        super(CnnPredictor, self).__init__()
        self.n_tokens = n_tokens
        self.emb_dim = emb_dim
        self.f0_log = f0_log
        self.f0_pred = f0_pred
        self.padding_token = n_tokens
        self.f0_norm = f0_norm
        self.token_emb = nn.Embedding(n_tokens + 1, emb_dim, padding_idx=self.padding_token)
        self.spk_emb = spk_emb
        self.gst_emb = nn.Embedding(20, gst_emb)
        self.setup = False
        feats = emb_dim + gst_emb
        layers = [nn.Sequential(Rearrange('b t c -> b c t'), nn.Conv1d(feats, channels, kernel_size=kernel, padding=(kernel - 1) // 2), Rearrange('b c t -> b t c'), nn.ReLU(), nn.LayerNorm(channels), nn.Dropout(dropout))]
        for _ in range(n_layers - 1):
            layers += [nn.Sequential(Rearrange('b t c -> b c t'), nn.Conv1d(channels, channels, kernel_size=kernel, padding=(kernel - 1) // 2), Rearrange('b c t -> b t c'), nn.ReLU(), nn.LayerNorm(channels), nn.Dropout(dropout))]
        self.conv_layer = nn.ModuleList(layers)
        self.proj = nn.Linear(channels, n_bins)

    def forward(self, x, gst=None):
        x = self.token_emb(x)
        feats = [x]
        if gst is not None:
            gst = self.gst_emb(gst)
            gst = rearrange(gst, 'b c -> b c 1')
            gst = F.interpolate(gst, x.shape[1])
            gst = rearrange(gst, 'b c t -> b t c')
            feats.append(gst)
        x = torch.cat(feats, dim=-1)
        for i, conv in enumerate(self.conv_layer):
            if i != 0:
                x = conv(x) + x
            else:
                x = conv(x)
        x = self.proj(x)
        x = x.squeeze(-1)
        if self.f0_pred == 'mean':
            x = torch.sigmoid(x)
        elif self.f0_pred == 'argmax':
            x = torch.softmax(x, dim=-1)
        else:
            raise NotImplementedError
        return x

    def setup_f0_stats(self, f0_min, f0_max, f0_bins, speaker_stats):
        self.f0_min = f0_min
        self.f0_max = f0_max
        self.f0_bins = f0_bins
        self.speaker_stats = speaker_stats
        self.setup = True

    def inference(self, x, spk_id=None, gst=None):
        assert self.setup == True, 'make sure that `setup_f0_stats` was called before inference!'
        probs = self(x, gst)
        f0 = bin2freq(probs, self.f0_min, self.f0_max, self.f0_bins, self.f0_pred)
        for i in range(f0.shape[0]):
            mean = self.speaker_stats[spk_id[i].item()].mean_log if self.f0_log else self.speaker_stats[spk_id[i].item()].mean
            std = self.speaker_stats[spk_id[i].item()].std_log if self.f0_log else self.speaker_stats[spk_id[i].item()].std
            if self.f0_norm == 'mean':
                f0[i] = f0[i] + mean
            if self.f0_norm == 'meanstd':
                f0[i] = f0[i] * std + mean
        if self.f0_log:
            f0 = f0.exp()
        return f0


class LatentLayersKLLoss(_Loss):

    def __init__(self, args):
        super().__init__()
        self.args = args

    def forward(self, layer_samples, lang_idx, update_num, sample_size):
        prior = self.args.prior
        samples = layer_samples[lang_idx]
        eps = 1e-07
        if prior == 'uniform':
            kl_loss = (samples * (torch.log(samples + eps) - math.log(0.5))).sum(-1)
        elif prior == 'agged_posterior':
            y_t = torch.stack([x.detach() for x in layer_samples], dim=0)
            agged_q = torch.sum(y_t, dim=0)
            row_norm = agged_q.sum(-1)
            normed_agg_q = agged_q / row_norm
            kl_loss = (samples * (torch.log(samples + eps) - torch.log(normed_agg_q + eps))).sum(-1)
        else:
            raise NotImplementedError('The specified prior is not implemented.')
        kl_loss /= layer_samples[0].size()[0]
        kl_weight = min(self.args.sparsity_weight, (update_num - self.args.soft_update) * self.args.sparsity_weight / self.args.anneal_updates)
        kl_loss *= kl_weight * sample_size
        return kl_loss


class LatentLayersSparsityLoss(_Loss):

    def __init__(self, args):
        super().__init__()
        self.args = args

    def is_valid(self, update_num):
        if self.args.target_layers <= 0:
            return False
        return update_num > self.args.soft_update + self.args.anneal_updates

    def forward(self, layer_samples_list, update_num, sample_size):
        batch_loss = 0
        share_loss = 0
        global_sparsity_loss = 0
        layer_samples = torch.stack(layer_samples_list, dim=0)
        if (self.args.target_layers > 0 or self.args.share_weight > 0) and update_num > self.args.soft_update + self.args.anneal_updates:
            if update_num < self.args.anneal_updates + self.args.soft_update:
                weight_anneal = 0
            elif update_num < 2 * self.args.anneal_updates + self.args.soft_update:
                weight_anneal = (update_num - self.args.soft_update - self.args.anneal_updates) * self.args.share_weight / self.args.anneal_updates
            else:
                weight_anneal = 1
            layer_utilization = torch.sum(layer_samples, dim=0)
            layer_utilization /= layer_samples.size()[0]
            if self.args.share_weight > 0:
                share_loss = sum(-1.0 * v * math.log(v) for v in layer_utilization if v > 0)
                batch_loss += weight_anneal * self.args.share_weight * sample_size * share_loss
            if self.args.target_layers > 0:
                expeted_layers = sum(layer_utilization)
                global_sparsity_loss = (expeted_layers - self.args.target_layers) ** 2
                batch_loss += weight_anneal * self.args.share_weight * sample_size * global_sparsity_loss
        return batch_loss


class TransformerEncoderLayerBase(nn.Module):
    """Encoder layer block.

    In the original paper each operation (multi-head attention or FFN) is
    postprocessed with: `dropout -> add residual -> layernorm`. In the
    tensor2tensor code they suggest that learning is more robust when
    preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.encoder.normalize_before* to ``True``.

    Args:
        cfg (argparse.Namespace): parsed command-line arguments
    """

    def __init__(self, cfg, return_fc=False):
        super().__init__()
        self.cfg = cfg
        self.return_fc = return_fc
        self.embed_dim = cfg.encoder.embed_dim
        self.quant_noise = cfg.quant_noise.pq
        self.quant_noise_block_size = cfg.quant_noise.pq_block_size
        self.self_attn = self.build_self_attention(self.embed_dim, cfg)
        self.self_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.dropout_module = FairseqDropout(cfg.dropout, module_name=self.__class__.__name__)
        self.activation_fn = utils.get_activation_fn(activation=cfg.activation_fn)
        activation_dropout_p = cfg.activation_dropout
        if activation_dropout_p == 0:
            activation_dropout_p = cfg.relu_dropout or 0
        self.activation_dropout_module = FairseqDropout(float(activation_dropout_p), module_name=self.__class__.__name__)
        self.normalize_before = cfg.encoder.normalize_before
        self.fc1 = self.build_fc1(self.embed_dim, cfg.encoder.ffn_embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.fc2 = self.build_fc2(cfg.encoder.ffn_embed_dim, self.embed_dim, self.quant_noise, self.quant_noise_block_size)
        self.final_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        return quant_noise(nn.Linear(input_dim, output_dim), p=q_noise, block_size=qn_block_size)

    def _get_fc_rank(self, remove_num: int) ->List[int]:
        f1_filter_param = []
        for i in range(self.fc1.out_features):
            f1_filter_param.append(torch.sum(torch.abs(self.fc1.weight[i])) + torch.sum(torch.abs(self.fc2.weight[:, i])) + torch.abs(self.fc1.bias[i]))
        return sorted(range(len(f1_filter_param)), key=lambda k: f1_filter_param[k], reverse=False)[0:remove_num]

    def _prune_fc_layer(self, remove_index: List[int]):
        new_fc1_weight = []
        new_fc1_bias = []
        for i in range(self.fc1.out_features):
            if i not in remove_index:
                new_fc1_weight.append(self.fc1.weight[i])
                new_fc1_bias.append(self.fc1.bias[i])
        new_fc1_weight = torch.stack(new_fc1_weight).detach()
        new_fc1_weight.requires_grad = True
        new_fc1_bias = torch.stack(new_fc1_bias).detach()
        new_fc1_bias.requires_grad = True
        self.fc1 = quant_noise(nn.Linear(self.fc1.in_features, self.fc1.out_features - len(remove_index)), p=self.quant_noise, block_size=self.quant_noise_block_size)
        self.fc1.weight = torch.nn.Parameter(new_fc1_weight)
        self.fc1.bias = torch.nn.Parameter(new_fc1_bias)
        new_fc2_weight = []
        new_fc2_bias = []
        for i in range(self.fc2.in_features):
            if i not in remove_index:
                new_fc2_weight.append(self.fc2.weight[:, i])
        new_fc2_bias = self.fc2.bias.detach()
        new_fc2_weight = torch.stack(new_fc2_weight, dim=-1).detach()
        new_fc2_weight.requires_grad = True
        new_fc2_bias = self.fc2.bias.detach()
        new_fc2_bias.requires_grad = True
        self.fc2 = quant_noise(nn.Linear(self.fc2.in_features - len(remove_index), self.fc2.out_features), p=self.quant_noise, block_size=self.quant_noise_block_size)
        self.fc2.weight = torch.nn.Parameter(new_fc2_weight)
        self.fc2.bias = torch.nn.Parameter(new_fc2_bias)

    def build_self_attention(self, embed_dim, cfg):
        return MultiheadAttention(embed_dim, cfg.encoder.attention_heads, dropout=cfg.attention_dropout, self_attention=True, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, xformers_att_config=cfg.encoder.xformers_att_config)

    def residual_connection(self, x, residual):
        return residual + x

    def upgrade_state_dict_named(self, state_dict, name):
        """
        Rename layer norm states from `...layer_norms.0.weight` to
        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to
        `...final_layer_norm.weight`
        """
        layer_norm_map = {'0': 'self_attn_layer_norm', '1': 'final_layer_norm'}
        for old, new in layer_norm_map.items():
            for m in ('weight', 'bias'):
                k = '{}.layer_norms.{}.{}'.format(name, old, m)
                if k in state_dict:
                    state_dict['{}.{}.{}'.format(name, new, m)] = state_dict[k]
                    del state_dict[k]

    def forward(self, x, encoder_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor]=None):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, seq_len)` where padding elements are indicated by ``1``.
            attn_mask (ByteTensor): binary tensor of shape `(tgt_len, src_len)`,
                where `tgt_len` is the length of output and `src_len` is the
                length of input, though here both are equal to `seq_len`.
                `attn_mask[tgt_i, src_j] = 1` means that when calculating the
                embedding for `tgt_i`, we exclude (mask out) `src_j`. This is
                useful for strided self-attention.

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if attn_mask is not None:
            attn_mask = attn_mask.masked_fill(attn_mask, -100000000.0 if x.dtype == torch.float32 else -10000.0)
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, need_weights=False, attn_mask=attn_mask)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        fc_result = x
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.return_fc and not torch.jit.is_scripting():
            return x, fc_result
        return x


class TransformerEncoderLayer(TransformerEncoderLayerBase):

    def __init__(self, args):
        super().__init__(TransformerConfig.from_namespace(args))
        self.args = args

    def build_self_attention(self, embed_dim, args):
        return super().build_self_attention(embed_dim, TransformerConfig.from_namespace(args))


class LatentTransformerEncoderLayer(TransformerEncoderLayer):
    """Encoder layer with each (non_residual) block weighted by samples of Bernouli
    or Gumbel Signmoid samples.

    Args:
        args (argparse.Namespace): parsed command-line arguments from standard
            TransformerEncoderLayer.
        idx (int): layer index (used to retrieve samples).
        layer_select (LayerSelect, optional): instance of LayerSelect module with logits
            parameters and sampling method.
    """

    def __init__(self, args, idx, layer_select=None):
        super().__init__(args)
        self.idx = idx
        self.layer_select = layer_select

    def residual_connection(self, x, residual):
        return residual + x * self.layer_select(self.idx)


class LayerSelect(nn.Module):
    """Compute samples (from a Gumbel-Sigmoid distribution) which is used as
    either (soft) weighting or (hard) selection of residual connection.
    https://arxiv.org/abs/2009.13102
    """

    def __init__(self, num_layers, num_logits, soft_select=False, sampling_tau=5.0):
        super(LayerSelect, self).__init__()
        self.layer_logits = torch.nn.Parameter(torch.Tensor(num_logits, num_layers), requires_grad=True)
        self.hard_select = not soft_select
        self.tau = sampling_tau
        self.detach_grad = False
        self.layer_samples = [None] * num_logits

    def sample(self, logit_idx):
        """To leverage the efficiency of distributed training, samples for all
        layers are computed at once for each logit_idx. Logits are parameters
        learnt independent of each other.

        Args:
            logit_idx: The index of logit parameters used for sampling.
        """
        assert logit_idx is not None
        self.samples = self._gumbel_sigmoid(self.layer_logits[logit_idx, :].detach() if self.detach_grad else self.layer_logits[logit_idx, :], dim=-1, tau=self.tau, hard=self.hard_select)
        self.layer_samples[logit_idx] = self.samples

    def forward(self, i):
        sample = self.samples[i]
        return sample

    def _gumbel_sigmoid(self, logits, tau=1, hard=False, eps=1e-10, dim=-1, threshold=0.5):
        gumbels1 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels2 = -torch.empty_like(logits, memory_format=torch.legacy_contiguous_format).exponential_().log()
        gumbels1 = (logits + gumbels1 - gumbels2) / tau
        y_soft = gumbels1.sigmoid()
        if hard:
            y_hard = torch.zeros_like(logits, memory_format=torch.legacy_contiguous_format).masked_fill(y_soft > threshold, 1.0)
            ret = y_hard - y_soft.detach() + y_soft
        else:
            ret = y_soft
        return ret


EncoderOut = NamedTuple('EncoderOut', [('encoder_out', Tensor), ('encoder_padding_mask', Optional[Tensor]), ('encoder_embedding', Optional[Tensor]), ('encoder_states', Optional[List[Tensor]]), ('src_tokens', Optional[Tensor]), ('src_lengths', Optional[Tensor])])


class LatentTransformerDecoderLayer(TransformerDecoderLayer):
    """Decoder layer with each (non_residual) block weighted by samples of Bernouli
    or Gumbel Signmoid samples.

    Args:
        args (argparse.Namespace): parsed command-line arguments from standard
            TransformerDecoderLayer.
        idx (int): layer index (used to retrieve samples).
        layer_select (LayerSelect, optional): instance of LayerSelect module with logits
            parameters and sampling method.
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).

    """

    def __init__(self, args, idx, layer_select=None, no_encoder_attn=False, add_bias_kv=False, add_zero_attn=False):
        super().__init__(args, no_encoder_attn, add_bias_kv, add_zero_attn)
        self.idx = idx
        self.layer_select = layer_select

    def residual_connection(self, x, residual):
        return residual + x * self.layer_select(self.idx)


@with_incremental_state
class MultiheadLinearAttention(nn.Module):
    """Multi-headed linformer attention.

    Projects the key and values down to the compressed dimension, before computing self-attention.

    See "Linformer: Self-Attention with Linear Complexity" for more details.
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, compressed=1, max_seq_len=256, shared_kv_compressed=0, shared_compress_layer=None, freeze_compress=0):
        super().__init__()
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, 'embed_dim must be divisible by num_heads'
        self.scaling = self.head_dim ** -0.5
        self.self_attention = self_attention
        self.encoder_decoder_attention = encoder_decoder_attention
        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and value to be of the same size'
        self.k_proj = quant_noise(nn.Linear(self.kdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.v_proj = quant_noise(nn.Linear(self.vdim, embed_dim, bias=bias), q_noise, qn_block_size)
        self.q_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        if shared_compress_layer is None:
            self.compress_seq_len = max_seq_len // compressed
            self.compress_k = nn.Linear(max_seq_len, self.compress_seq_len, bias=False)
            if shared_kv_compressed == 0:
                self.compress_v = nn.Linear(max_seq_len, self.compress_seq_len, bias=False)
            self.layerwise_sharing = False
        else:
            self.compress_k = shared_compress_layer
            if shared_kv_compressed == 0:
                self.compress_v = shared_compress_layer
            self.layerwise_sharing = True
        self.shared_kv_compressed = shared_kv_compressed
        self.out_proj = quant_noise(nn.Linear(embed_dim, embed_dim, bias=bias), q_noise, qn_block_size)
        if add_bias_kv:
            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))
            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))
        else:
            self.bias_k = self.bias_v = None
        self.add_zero_attn = add_zero_attn
        self.reset_parameters()
        if freeze_compress == 1:
            self.compress_k.weight.requires_grad = False
            if shared_kv_compressed == 0:
                self.compress_v.weight.requires_grad = False
        self.onnx_trace = False

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def reset_parameters(self):
        if self.qkv_same_dim:
            nn.init.xavier_uniform_(self.k_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.v_proj.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.q_proj.weight, gain=1 / math.sqrt(2))
            if not self.layerwise_sharing:
                nn.init.xavier_uniform_(self.compress_k.weight, gain=1 / math.sqrt(2))
                if self.shared_kv_compressed == 0:
                    nn.init.xavier_uniform_(self.compress_v.weight, gain=1 / math.sqrt(2))
        else:
            nn.init.xavier_uniform_(self.k_proj.weight)
            nn.init.xavier_uniform_(self.v_proj.weight)
            nn.init.xavier_uniform_(self.q_proj.weight)
            if not self.layerwise_sharing:
                nn.init.xavier_uniform_(self.compress_k.weight)
                if self.shared_kv_compressed == 0:
                    nn.init.xavier_uniform_(self.compress_v.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.out_proj.bias is not None:
            nn.init.constant_(self.out_proj.bias, 0.0)
        if self.bias_k is not None:
            nn.init.xavier_normal_(self.bias_k)
        if self.bias_v is not None:
            nn.init.xavier_normal_(self.bias_v)

    def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, need_weights: bool=True, static_kv: bool=False, attn_mask: Optional[Tensor]=None, before_softmax: bool=False, need_head_weights: bool=False) ->Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time x Batch x Channel

        Args:
            key_padding_mask (ByteTensor, optional): mask to exclude
                keys that are pads, of shape `(batch, src_len)`, where
                padding elements are indicated by 1s.
            need_weights (bool, optional): return the attention weights,
                averaged over heads (default: False).
            attn_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
            before_softmax (bool, optional): return the raw attention
                weights and values before the attention softmax.
            need_head_weights (bool, optional): return the attention
                weights for each head. Implies *need_weights*. Default:
                return the average attention weights over all heads.
        """
        if need_head_weights:
            need_weights = True
        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.embed_dim
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if saved_state is not None and 'prev_key' in saved_state:
                if static_kv:
                    assert self.encoder_decoder_attention and not self.self_attention
                    key = value = None
        else:
            saved_state = None
        if self.self_attention:
            q = self.q_proj(query)
            k_input = query.permute(1, 2, 0).contiguous()
            k_input = F.linear(k_input, self.compress_k.weight[:, 0:tgt_len]).permute(2, 0, 1).contiguous()
            k = self.k_proj(k_input)
            v_input = query.permute(1, 2, 0).contiguous()
            if self.shared_kv_compressed == 0:
                v_input = F.linear(v_input, self.compress_v.weight[:, 0:tgt_len]).permute(2, 0, 1).contiguous()
            if self.shared_kv_compressed == 1:
                v_input = F.linear(v_input, self.compress_k.weight[:, 0:tgt_len]).permute(2, 0, 1).contiguous()
            v = self.v_proj(v_input)
        elif self.encoder_decoder_attention:
            q = self.q_proj(query)
            if key is None:
                assert value is None
                k = v = None
            else:
                k = self.k_proj(key)
                v = self.v_proj(key)
        else:
            assert key is not None and value is not None
            q = self.q_proj(query)
            k = self.k_proj(key)
            v = self.v_proj(value)
        q *= self.scaling
        if self.bias_k is not None:
            assert self.bias_v is not None
            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])
            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])
            if attn_mask is not None:
                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)
            if key_padding_mask is not None:
                key_padding_mask = torch.cat([key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)], dim=1)
        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if k is not None:
            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if v is not None:
            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)
        if saved_state is not None:
            if 'prev_key' in saved_state:
                _prev_key = saved_state['prev_key']
                assert _prev_key is not None
                prev_key = _prev_key.view(bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    k = prev_key
                else:
                    assert k is not None
                    k = torch.cat([prev_key, k], dim=1)
            if 'prev_value' in saved_state:
                _prev_value = saved_state['prev_value']
                assert _prev_value is not None
                prev_value = _prev_value.view(bsz * self.num_heads, -1, self.head_dim)
                if static_kv:
                    v = prev_value
                else:
                    assert v is not None
                    v = torch.cat([prev_value, v], dim=1)
            prev_key_padding_mask: Optional[Tensor] = None
            if 'prev_key_padding_mask' in saved_state:
                prev_key_padding_mask = saved_state['prev_key_padding_mask']
            assert k is not None and v is not None
            key_padding_mask = MultiheadLinearAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)
            saved_state['prev_key'] = k.view(bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_value'] = v.view(bsz, self.num_heads, -1, self.head_dim)
            saved_state['prev_key_padding_mask'] = key_padding_mask
            assert incremental_state is not None
            incremental_state = self._set_input_buffer(incremental_state, saved_state)
        assert k is not None
        src_len = k.size(1)
        if self.add_zero_attn:
            assert v is not None
            src_len += 1
            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)
            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)
            if attn_mask is not None:
                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        attn_weights = MultiheadLinearAttention.apply_sparse_mask(attn_weights, tgt_len, src_len, bsz)
        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]
        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(0)
            if self.onnx_trace:
                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)
            attn_weights += attn_mask
        if before_softmax:
            return attn_weights, v
        attn_weights_float = utils.softmax(attn_weights, dim=-1, onnx_trace=self.onnx_trace)
        attn_weights = attn_weights_float.type_as(attn_weights)
        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)
        assert v is not None
        attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]
        if self.onnx_trace and attn.size(1) == 1:
            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)
        else:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)
        attn = self.out_proj(attn)
        attn_weights: Optional[Tensor] = None
        if need_weights:
            attn_weights = attn_weights_float.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)
            if not need_head_weights:
                attn_weights = attn_weights.mean(dim=0)
        return attn, attn_weights

    @staticmethod
    def _append_prev_key_padding_mask(key_padding_mask: Optional[Tensor], prev_key_padding_mask: Optional[Tensor], batch_size: int, src_len: int, static_kv: bool) ->Optional[Tensor]:
        if prev_key_padding_mask is not None and static_kv:
            new_key_padding_mask = prev_key_padding_mask
        elif prev_key_padding_mask is not None and key_padding_mask is not None:
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), key_padding_mask.float()], dim=1)
        elif prev_key_padding_mask is not None:
            filler = torch.zeros((batch_size, src_len - prev_key_padding_mask.size(1)), device=prev_key_padding_mask.device)
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), filler.float()], dim=1)
        elif key_padding_mask is not None:
            filler = torch.zeros((batch_size, src_len - key_padding_mask.size(1)), device=key_padding_mask.device)
            new_key_padding_mask = torch.cat([filler.float(), key_padding_mask.float()], dim=1)
        else:
            new_key_padding_mask = prev_key_padding_mask
        return new_key_padding_mask

    @torch.jit.export
    def reorder_incremental_state(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order: Tensor):
        """Reorder buffered internal state (for incremental generation)."""
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            for k in input_buffer.keys():
                input_buffer_k = input_buffer[k]
                if input_buffer_k is not None:
                    if self.encoder_decoder_attention and input_buffer_k.size(0) == new_order.size(0):
                        break
                    input_buffer[k] = input_buffer_k.index_select(0, new_order)
            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
        return incremental_state

    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]) ->Dict[str, Optional[Tensor]]:
        result = self.get_incremental_state(incremental_state, 'attn_state')
        if result is not None:
            return result
        else:
            empty_result: Dict[str, Optional[Tensor]] = {}
            return empty_result

    def _set_input_buffer(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], buffer: Dict[str, Optional[Tensor]]):
        return self.set_incremental_state(incremental_state, 'attn_state', buffer)

    def apply_sparse_mask(attn_weights, tgt_len: int, src_len: int, bsz: int):
        return attn_weights

    def upgrade_state_dict_named(self, state_dict, name):
        prefix = name + '.' if name != '' else ''
        items_to_add = {}
        keys_to_remove = []
        for k in state_dict.keys():
            if k.endswith(prefix + 'in_proj_weight'):
                dim = int(state_dict[k].shape[0] / 3)
                items_to_add[prefix + 'q_proj.weight'] = state_dict[k][:dim]
                items_to_add[prefix + 'k_proj.weight'] = state_dict[k][dim:2 * dim]
                items_to_add[prefix + 'v_proj.weight'] = state_dict[k][2 * dim:]
                keys_to_remove.append(k)
                k_bias = prefix + 'in_proj_bias'
                if k_bias in state_dict.keys():
                    dim = int(state_dict[k].shape[0] / 3)
                    items_to_add[prefix + 'q_proj.bias'] = state_dict[k_bias][:dim]
                    items_to_add[prefix + 'k_proj.bias'] = state_dict[k_bias][dim:2 * dim]
                    items_to_add[prefix + 'v_proj.bias'] = state_dict[k_bias][2 * dim:]
                    keys_to_remove.append(prefix + 'in_proj_bias')
        for k in keys_to_remove:
            del state_dict[k]
        for key, value in items_to_add.items():
            state_dict[key] = value


class LinformerTransformerEncoderLayer(TransformerEncoderLayer):
    """
    Implements a Linformer Encoder Layer used in BERT/XLM style pre-trained
    models.
    """

    def __init__(self, args, shared_compress_layer):
        self.shared_compress_layer = [shared_compress_layer]
        super().__init__(args)
        self.register_buffer('version', torch.tensor(2))

    def build_self_attention(self, embed_dim, args):
        return MultiheadLinearAttention(embed_dim, args.encoder_attention_heads, dropout=args.dropout, self_attention=True, q_noise=args.quant_noise_pq, qn_block_size=args.quant_noise_pq_block_size, compressed=args.compressed, max_seq_len=args.max_positions, shared_kv_compressed=args.shared_kv_compressed, shared_compress_layer=self.shared_compress_layer[0], freeze_compress=args.freeze_compress)

    def upgrade_state_dict_named(self, state_dict, name):
        super().upgrade_state_dict_named(state_dict, name)
        prefix = name + '.' if name != '' else ''
        if utils.item(state_dict.get(f'{prefix}version', torch.tensor(1))) < 2:
            state_dict[f'{prefix}version'] = torch.tensor(1)
            if f'{prefix}shared_compress_layer.weight' in state_dict:
                self.shared_compress_layer = [torch.nn.Linear(self.shared_compress_layer[0].weight.size(1), self.shared_compress_layer[0].weight.size(0))]
                self.self_attn = self.build_self_attention(self.embed_dim, self.args)
                del state_dict[f'{prefix}shared_compress_layer.weight']
                if f'{prefix}shared_compress_layer.bias' in state_dict:
                    del state_dict[f'{prefix}shared_compress_layer.bias']


def Embedding(num_embeddings, embedding_dim, padding_idx):
    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)
    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)
    nn.init.constant_(m.weight[padding_idx], 0)
    return m


class TransformerMonotonicEncoderLayer(TransformerEncoderLayer):

    def forward(self, x, encoder_padding_mask):
        seq_len, _, _ = x.size()
        attn_mask = x.new_ones([seq_len, seq_len]).triu(1)
        attn_mask = attn_mask.masked_fill(attn_mask.bool(), float('-inf'))
        return super().forward(x, encoder_padding_mask, attn_mask)


class TransformerMonotonicDecoderLayer(TransformerDecoderLayer):

    def __init__(self, args):
        super().__init__(args)
        assert args.simul_type is not None, 'A --simul-type is needed.'
        self.encoder_attn = build_monotonic_attention(args)

    def prune_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]):
        input_buffer = self.self_attn._get_input_buffer(incremental_state)
        for key in ['prev_key', 'prev_value']:
            input_buffer_key = input_buffer[key]
            assert input_buffer_key is not None
            if input_buffer_key.size(2) > 1:
                input_buffer[key] = input_buffer_key[:, :, :-1, :]
            else:
                typed_empty_dict: Dict[str, Optional[Tensor]] = {}
                input_buffer = typed_empty_dict
                break
        assert incremental_state is not None
        self.self_attn._set_input_buffer(incremental_state, input_buffer)

    def forward(self, x, encoder_out: Optional[Tensor]=None, encoder_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, prev_self_attn_state: Optional[List[Tensor]]=None, prev_attn_state: Optional[List[Tensor]]=None, self_attn_mask: Optional[Tensor]=None, self_attn_padding_mask: Optional[Tensor]=None, need_attn: bool=False, need_head_weights: bool=False):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor, optional): binary
                ByteTensor of shape `(batch, src_len)` where padding
                elements are indicated by ``1``.
            need_attn (bool, optional): return attention weights
            need_head_weights (bool, optional): return attention weights
                for each head (default: return average over heads).

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if need_head_weights:
            need_attn = True
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if prev_self_attn_state is not None:
            prev_key, prev_value = prev_self_attn_state[:2]
            saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_self_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_self_attn_state[2]
            assert incremental_state is not None
            self.self_attn._set_input_buffer(incremental_state, saved_state)
        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)
        if self.cross_self_attention and not (incremental_state is not None and _self_attn_input_buffer is not None and 'prev_key' in _self_attn_input_buffer):
            if self_attn_mask is not None:
                assert encoder_out is not None
                self_attn_mask = torch.cat((x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1)
            if self_attn_padding_mask is not None:
                if encoder_padding_mask is None:
                    assert encoder_out is not None
                    encoder_padding_mask = self_attn_padding_mask.new_zeros(encoder_out.size(1), encoder_out.size(0))
                self_attn_padding_mask = torch.cat((encoder_padding_mask, self_attn_padding_mask), dim=1)
            assert encoder_out is not None
            y = torch.cat((encoder_out, x), dim=0)
        else:
            y = x
        x, attn = self.self_attn(query=x, key=y, value=y, key_padding_mask=self_attn_padding_mask, incremental_state=incremental_state, need_weights=False, attn_mask=self_attn_mask)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        assert self.encoder_attn is not None
        residual = x
        if self.normalize_before:
            x = self.encoder_attn_layer_norm(x)
        if prev_attn_state is not None:
            prev_key, prev_value = prev_attn_state[:2]
            saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_attn_state[2]
            assert incremental_state is not None
            self.encoder_attn._set_input_buffer(incremental_state, saved_state)
        x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.encoder_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.onnx_trace and incremental_state is not None:
            saved_state = self.self_attn._get_input_buffer(incremental_state)
            assert saved_state is not None
            if self_attn_padding_mask is not None:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value'], saved_state['prev_key_padding_mask']]
            else:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value']]
            return x, attn, self_attn_state
        return x, attn, None


TransformerMonotonicDecoderOut = NamedTuple('TransformerMonotonicDecoderOut', [('action', int), ('p_choose', Optional[Tensor]), ('attn_list', Optional[List[Optional[Dict[str, Tensor]]]]), ('encoder_out', Optional[Dict[str, List[Tensor]]]), ('encoder_padding_mask', Optional[Tensor])])


class BLSTM(nn.Module):

    def __init__(self, dim, layers=2, bi=True):
        super().__init__()
        klass = nn.LSTM
        self.lstm = klass(bidirectional=bi, num_layers=layers, hidden_size=dim, input_size=dim)
        self.linear = None
        if bi:
            self.linear = nn.Linear(2 * dim, dim)

    def forward(self, x, hidden=None):
        x, hidden = self.lstm(x, hidden)
        if self.linear:
            x = self.linear(x)
        return x, hidden


def capture_init(init):
    """capture_init.

    Decorate `__init__` with this, and you can then
    recover the *args and **kwargs passed to it in `self._init_args_kwargs`
    """

    @functools.wraps(init)
    def __init__(self, *args, **kwargs):
        self._init_args_kwargs = args, kwargs
        init(self, *args, **kwargs)
    return __init__


def sinc(t):
    """sinc.

    :param t: the input tensor
    """
    return th.where(t == 0, th.tensor(1.0, device=t.device, dtype=t.dtype), th.sin(t) / t)


def kernel_downsample2(zeros=56):
    """kernel_downsample2.

    """
    win = th.hann_window(4 * zeros + 1, periodic=False)
    winodd = win[1::2]
    t = th.linspace(-zeros + 0.5, zeros - 0.5, 2 * zeros)
    t.mul_(math.pi)
    kernel = (sinc(t) * winodd).view(1, 1, -1)
    return kernel


def downsample2(x, zeros=56):
    """
    Downsampling the input by 2 using sinc interpolation.
    Smith, Julius, and Phil Gossett. "A flexible sampling-rate conversion method."
    ICASSP'84. IEEE International Conference on Acoustics, Speech, and Signal Processing.
    Vol. 9. IEEE, 1984.
    """
    if x.shape[-1] % 2 != 0:
        x = F.pad(x, (0, 1))
    xeven = x[..., ::2]
    xodd = x[..., 1::2]
    *other, time = xodd.shape
    kernel = kernel_downsample2(zeros)
    out = xeven + F.conv1d(xodd.view(-1, 1, time), kernel, padding=zeros)[..., :-1].view(*other, time)
    return out.view(*other, -1).mul(0.5)


def rescale_conv(conv, reference):
    std = conv.weight.std().detach()
    scale = (std / reference) ** 0.5
    conv.weight.data /= scale
    if conv.bias is not None:
        conv.bias.data /= scale


def rescale_module(module, reference):
    for sub in module.modules():
        if isinstance(sub, (nn.Conv1d, nn.ConvTranspose1d)):
            rescale_conv(sub, reference)


def kernel_upsample2(zeros=56):
    """kernel_upsample2.

    """
    win = th.hann_window(4 * zeros + 1, periodic=False)
    winodd = win[1::2]
    t = th.linspace(-zeros + 0.5, zeros - 0.5, 2 * zeros)
    t *= math.pi
    kernel = (sinc(t) * winodd).view(1, 1, -1)
    return kernel


def upsample2(x, zeros=56):
    """
    Upsampling the input by 2 using sinc interpolation.
    Smith, Julius, and Phil Gossett. "A flexible sampling-rate conversion method."
    ICASSP'84. IEEE International Conference on Acoustics, Speech, and Signal Processing.
    Vol. 9. IEEE, 1984.
    """
    *other, time = x.shape
    kernel = kernel_upsample2(zeros)
    out = F.conv1d(x.view(-1, 1, time), kernel, padding=zeros)[..., 1:].view(*other, time)
    y = th.stack([x, out], dim=-1)
    return y.view(*other, -1)


class Demucs(nn.Module):
    """
    Demucs speech enhancement model.
    Args:
        - chin (int): number of input channels.
        - chout (int): number of output channels.
        - hidden (int): number of initial hidden channels.
        - depth (int): number of layers.
        - kernel_size (int): kernel size for each layer.
        - stride (int): stride for each layer.
        - causal (bool): if false, uses BiLSTM instead of LSTM.
        - resample (int): amount of resampling to apply to the input/output.
            Can be one of 1, 2 or 4.
        - growth (float): number of channels is multiplied by this for every layer.
        - max_hidden (int): maximum number of channels. Can be useful to
            control the size/speed of the model.
        - normalize (bool): if true, normalize the input.
        - glu (bool): if true uses GLU instead of ReLU in 1x1 convolutions.
        - rescale (float): controls custom weight initialization.
            See https://arxiv.org/abs/1911.13254.
        - floor (float): stability flooring when normalizing.

    """

    @capture_init
    def __init__(self, chin=1, chout=1, hidden=48, depth=5, kernel_size=8, stride=4, causal=True, resample=4, growth=2, max_hidden=10000, normalize=True, glu=True, rescale=0.1, floor=0.001):
        super().__init__()
        if resample not in [1, 2, 4]:
            raise ValueError('Resample should be 1, 2 or 4.')
        self.chin = chin
        self.chout = chout
        self.hidden = hidden
        self.depth = depth
        self.kernel_size = kernel_size
        self.stride = stride
        self.causal = causal
        self.floor = floor
        self.resample = resample
        self.normalize = normalize
        self.encoder = nn.ModuleList()
        self.decoder = nn.ModuleList()
        activation = nn.GLU(1) if glu else nn.ReLU()
        ch_scale = 2 if glu else 1
        for index in range(depth):
            encode = []
            encode += [nn.Conv1d(chin, hidden, kernel_size, stride), nn.ReLU(), nn.Conv1d(hidden, hidden * ch_scale, 1), activation]
            self.encoder.append(nn.Sequential(*encode))
            decode = []
            decode += [nn.Conv1d(hidden, ch_scale * hidden, 1), activation, nn.ConvTranspose1d(hidden, chout, kernel_size, stride)]
            if index > 0:
                decode.append(nn.ReLU())
            self.decoder.insert(0, nn.Sequential(*decode))
            chout = hidden
            chin = hidden
            hidden = min(int(growth * hidden), max_hidden)
        self.lstm = BLSTM(chin, bi=not causal)
        if rescale:
            rescale_module(self, reference=rescale)

    def valid_length(self, length):
        """
        Return the nearest valid length to use with the model so that
        there is no time steps left over in a convolutions, e.g. for all
        layers, size of the input - kernel_size % stride = 0.

        If the mixture has a valid length, the estimated sources
        will have exactly the same length.
        """
        length = math.ceil(length * self.resample)
        for _ in range(self.depth):
            length = math.ceil((length - self.kernel_size) / self.stride) + 1
            length = max(length, 1)
        for _ in range(self.depth):
            length = (length - 1) * self.stride + self.kernel_size
        length = int(math.ceil(length / self.resample))
        return int(length)

    @property
    def total_stride(self):
        return self.stride ** self.depth // self.resample

    def forward(self, mix):
        if mix.dim() == 2:
            mix = mix.unsqueeze(1)
        if self.normalize:
            mono = mix.mean(dim=1, keepdim=True)
            std = mono.std(dim=-1, keepdim=True)
            mix = mix / (self.floor + std)
        else:
            std = 1
        length = mix.shape[-1]
        x = mix
        x = F.pad(x, (0, self.valid_length(length) - length))
        if self.resample == 2:
            x = upsample2(x)
        elif self.resample == 4:
            x = upsample2(x)
            x = upsample2(x)
        skips = []
        for encode in self.encoder:
            x = encode(x)
            skips.append(x)
        x = x.permute(2, 0, 1)
        x, _ = self.lstm(x)
        x = x.permute(1, 2, 0)
        for decode in self.decoder:
            skip = skips.pop(-1)
            x = x + skip[..., :x.shape[-1]]
            x = decode(x)
        if self.resample == 2:
            x = downsample2(x)
        elif self.resample == 4:
            x = downsample2(x)
            x = downsample2(x)
        x = x[..., :length]
        return std * x


class SpeechEmbedder(nn.Module):

    def __init__(self, hp):
        super(SpeechEmbedder, self).__init__()
        self.lstm = nn.LSTM(hp['num_mels'], hp['lstm_hidden'], num_layers=hp['lstm_layers'], batch_first=True)
        self.proj = LinearNorm(hp)
        self.hp = hp

    def forward(self, mel):
        mels = mel.unfold(1, self.hp['window'], self.hp['stride'])
        mels = mels.permute(1, 2, 0)
        x, _ = self.lstm(mels)
        x = x[:, -1, :]
        x = self.proj(x)
        x = x / torch.norm(x, p=2, dim=1, keepdim=True)
        x = x.mean(dim=0)
        if x.norm(p=2) != 0:
            x = x / x.norm(p=2)
        return x


EMBEDDER_PARAMS = {'num_mels': 40, 'n_fft': 512, 'emb_dim': 256, 'lstm_hidden': 768, 'lstm_layers': 3, 'window': 80, 'stride': 40}


def set_requires_grad(nets, requires_grad=False):
    """Set requies_grad=Fasle for all the networks to avoid unnecessary
    computations
    Parameters:
        nets (network list)   -- a list of networks
        requires_grad (bool)  -- whether the networks require gradients or not
    """
    if not isinstance(nets, list):
        nets = [nets]
    for net in nets:
        if net is not None:
            for param in net.parameters():
                param.requires_grad = requires_grad


class SpkrEmbedder(nn.Module):
    RATE = 16000

    def __init__(self, embedder_path, embedder_params=EMBEDDER_PARAMS, rate=16000, hop_length=160, win_length=400, pad=False):
        super(SpkrEmbedder, self).__init__()
        embedder_pt = torch.load(embedder_path, map_location='cpu')
        self.embedder = SpeechEmbedder(embedder_params)
        self.embedder.load_state_dict(embedder_pt)
        self.embedder.eval()
        set_requires_grad(self.embedder, requires_grad=False)
        self.embedder_params = embedder_params
        self.register_buffer('mel_basis', torch.from_numpy(librosa.filters.mel(sr=self.RATE, n_fft=self.embedder_params['n_fft'], n_mels=self.embedder_params['num_mels'])))
        self.resample = None
        if rate != self.RATE:
            self.resample = torchaudio.transforms.Resample(rate, self.RATE)
        self.hop_length = hop_length
        self.win_length = win_length
        self.pad = pad

    def get_mel(self, y):
        if self.pad and y.shape[-1] < 14000:
            y = F.pad(y, (0, 14000 - y.shape[-1]))
        window = torch.hann_window(self.win_length)
        y = torch.stft(y, n_fft=self.embedder_params['n_fft'], hop_length=self.hop_length, win_length=self.win_length, window=window)
        magnitudes = torch.norm(y, dim=-1, p=2) ** 2
        mel = torch.log10(self.mel_basis @ magnitudes + 1e-06)
        return mel

    def forward(self, inputs):
        dvecs = []
        for wav in inputs:
            mel = self.get_mel(wav)
            if mel.dim() == 3:
                mel = mel.squeeze(0)
            dvecs += [self.embedder(mel)]
        dvecs = torch.stack(dvecs)
        dvec = torch.mean(dvecs, dim=0)
        dvec = dvec / torch.norm(dvec)
        return dvec


class BenchmarkingBase(nn.Module):

    def __init__(self):
        nn.Module.__init__(self)
        self.s2x_task = None

    def warm_up(self, sample, repeat):
        """Warm up the model"""
        for _i in range(repeat):
            self.forward(sample)
        logger.info(f'Model warmed up by running inference {repeat} times')

    def benchmark_run_time(self, dataset, repeat):
        """Benchmark average runtime for the model by calling benchmark_run_time_single_sample function"""
        logger.info('Starting run time benchmarking')
        time_elapsed = 0
        for i, sample in enumerate(dataset):
            time_elapsed += self.benchmark_run_time_single_sample(sample, repeat=repeat)
            if i % 100 == 0:
                logger.info(f'Benchmarked run time for {i}/{len(dataset)} samples')
        total_time_elapsed = time_elapsed / len(dataset)
        return total_time_elapsed

    def benchmark_run_time_single_sample(self, sample, repeat):
        """Benchmark average runtime for a single sample using timeit library. Units are seconds"""
        timer = timeit.Timer(lambda : self.forward(sample))
        time_elapsed = timer.timeit(repeat)
        return time_elapsed / repeat

    def count_flops(self, dataset, repeat):
        """Use PYPAPI library to count average flops for model inference.
        Note: It only works if the model is being run on cpu"""
        logger.info('Starting flop counter')
        high.start_counters([events.PAPI_DP_OPS])
        for i, sample in enumerate(dataset):
            for _r in range(repeat):
                self.forward(sample)
            if i % 100 == 0:
                logger.info(f'Counted flops for {i}/{len(dataset)} samples')
        flops = high.stop_counters()
        flops = round(flops[0] / (repeat * len(dataset)))
        return flops

    def max_memory(self, dataset, repeat):
        """Compute average max memory consumed by model inference. Units are MiB"""
        logger.info('Starting memory benchmarking')
        total_memory = 0
        for i, sample in enumerate(dataset):
            for _r in range(repeat):
                total_memory += max(memory_usage((self.forward, (sample,), {})))
            if i % 100 == 0:
                logger.info(f'Benchmarked memory for {i}/{len(dataset)} samples')
        total_memory = total_memory / (repeat * len(dataset))
        return total_memory

    def gather_all_metrics(self, dataset, repeat):
        run_time = self.benchmark_run_time(dataset, repeat)
        max_memory = self.max_memory(dataset, repeat)
        flops = self.count_flops(dataset, repeat)
        return run_time, max_memory, flops

    def dump_final_speech_output(self, dataset, output_dir, resample_fn, sample_rate, prefix=None):
        for i, sample in enumerate(dataset):
            hypo = self.forward(sample)[0]

            def to_np(x):
                return x.detach().cpu().numpy()
            try:
                wave_preds = to_np(resample_fn(hypo['waveform']))
                sf.write(f'{output_dir}/{prefix}_{i}_pred.wav', wave_preds, sample_rate)
            except Exception as e:
                raise Exception(f' Encountered {e} - Invalid waveform. Make sure the model outputs a waveform')


def _set_legacy_defaults(args, cls):
    """Helper to set default arguments based on *add_args*."""
    if not hasattr(cls, 'add_args'):
        return
    parser = argparse.ArgumentParser(argument_default=argparse.SUPPRESS, allow_abbrev=False)
    cls.add_args(parser)
    defaults = argparse.Namespace()
    for action in parser._actions:
        if action.dest is not argparse.SUPPRESS:
            if not hasattr(defaults, action.dest):
                if action.default is not argparse.SUPPRESS:
                    setattr(defaults, action.dest, action.default)
    for key, default_value in vars(defaults).items():
        if not hasattr(args, key):
            setattr(args, key, default_value)


class omegaconf_no_object_check:

    def __init__(self):
        if hasattr(_utils, 'is_primitive_type'):
            self.old_is_primitive = _utils.is_primitive_type
        else:
            self.old_is_primitive = _utils.is_primitive_type_annotation

    def __enter__(self):
        if hasattr(_utils, 'is_primitive_type'):
            _utils.is_primitive_type = lambda _: True
        else:
            _utils.is_primitive_type_annotation = lambda _: True

    def __exit__(self, type, value, traceback):
        if hasattr(_utils, 'is_primitive_type'):
            _utils.is_primitive_type = self.old_is_primitive
        else:
            _utils.is_primitive_type_annotation = self.old_is_primitive


def migrate_registry(name, value, registry, args, overrides, deletes, use_name_as_val=False):
    if value in registry:
        overrides.append('{}={}'.format(name, value))
        overrides.append('{}._name={}'.format(name, value))
        overrides.extend(_override_attr(name, registry[value], args))
    elif use_name_as_val and value is not None:
        overrides.append('{}={}'.format(name, value))
    else:
        deletes.append(name)


def get_symbols_to_strip_from_output(generator):
    if hasattr(generator, 'symbols_to_strip_from_output'):
        return generator.symbols_to_strip_from_output
    else:
        return {generator.eos}


class Processing(BenchmarkingBase):
    """Class similar to fairseq_cli/generate.py. Supports ASR, MT and ST model inference"""

    def __init__(self, args):
        super().__init__()
        self.use_cuda = not getattr(args, 'cpu', False)
        self.setUp(args)
        self.training = False
        self.s2x_task = self.task

    def setUp(self, cfg):
        if isinstance(cfg, Namespace):
            cfg = convert_namespace_to_omegaconf(cfg)
        self.task = tasks.setup_task(cfg.task)
        self.tgt_dict = self.task.target_dictionary
        logger.info('loading model(s) from {}'.format(cfg.common_eval.path))
        models, _ = checkpoint_utils.load_model_ensemble(utils.split_paths(cfg.common_eval.path), arg_overrides={}, task=self.task, suffix=cfg.checkpoint.checkpoint_suffix, strict=False, num_shards=cfg.checkpoint.checkpoint_shard_count)
        if len(models) > 1:
            raise Exception('Currently loading multiple models is not supported')
        self.model = models[0]
        if cfg.common.fp16:
            self.model.half()
        if self.use_cuda:
            self.model
        self.model.prepare_for_inference_(cfg)
        self.generator = self.task.build_generator([self.model], cfg.generation, extra_gen_cls_kwargs={})
        self.tokenizer = self.task.build_tokenizer(cfg.tokenizer)
        self.bpe = self.task.build_bpe(cfg.bpe)
        self.remove_bpe = cfg.common_eval.post_process

    def encode_source(self, src):
        """Method to generate source tokens from a string"""
        if self.tokenizer is not None:
            src = self.tokenizer.encode(src)
        if self.bpe is not None:
            src = self.bpe.encode(src)
        src_tokens = self.task.source_dictionary.encode_line(src).long()
        src_lens = src_tokens.size(0)
        return {'net_input': {'src_tokens': src_tokens.view(1, src_lens), 'src_lengths': torch.tensor([src_lens])}}

    def decode_target(self, hypos):
        """Method to decode target string from tokens"""
        hypo_str = self.tgt_dict.string(hypos[0][0]['tokens'].int().cpu(), self.remove_bpe, get_symbols_to_strip_from_output(self.generator))
        if self.bpe is not None:
            hypo_str = self.bpe.decode(hypo_str)
        if self.tokenizer is not None:
            hypo_str = self.tokenizer.decode(hypo_str)
        return hypo_str

    def forward(self, sample):
        hypos = self.task.inference_step(self.generator, [self.model], sample, prefix_tokens=None, constraints=None)
        return hypos


class VocoderHubInterface(nn.Module):
    """Vocoder interface to run vocoder models through hub. Currently we only support unit vocoder"""

    def __init__(self, cfg, model):
        super().__init__()
        self.vocoder = model
        self.vocoder.eval()
        self.sr = 16000
        self.multispkr = self.vocoder.model.multispkr
        if self.multispkr:
            logger.info('multi-speaker vocoder')
            self.num_speakers = cfg.get('num_speakers', 200)

    def get_model_input(self, text: str, speaker: Optional[int]=-1):
        units = list(map(int, text.strip().split()))
        x = {'code': torch.LongTensor(units).view(1, -1)}
        if not speaker:
            speaker = -1
        if self.multispkr:
            assert speaker < self.num_speakers, f'invalid --speaker-id ({speaker}) with total #speakers = {self.num_speakers}'
            spk = random.randint(0, self.num_speakers - 1) if speaker == -1 else speaker
            x['spkr'] = torch.LongTensor([spk]).view(1, 1)
        return x

    def get_prediction(self, sample, dur_prediction: Optional[bool]=True):
        wav = self.vocoder(sample, dur_prediction)
        return wav, self.sr

    def predict(self, text: str, speaker: Optional[int]=None, dur_prediction: Optional[bool]=True):
        sample = self.get_model_input(text, speaker)
        return self.get_prediction(sample, dur_prediction)


MODEL_DATACLASS_REGISTRY = {}


MODEL_REGISTRY = {}


ARCH_CONFIG_REGISTRY = {}


ARCH_MODEL_INV_REGISTRY = {}


ARCH_MODEL_NAME_REGISTRY = {}


ARCH_MODEL_REGISTRY = {}


def register_model_architecture(model_name, arch_name):
    """
    New model architectures can be added to fairseq with the
    :func:`register_model_architecture` function decorator. After registration,
    model architectures can be selected with the ``--arch`` command-line
    argument.

    For example::

        @register_model_architecture('lstm', 'lstm_luong_wmt_en_de')
        def lstm_luong_wmt_en_de(cfg):
            args.encoder_embed_dim = getattr(cfg.model, 'encoder_embed_dim', 1000)
            (...)

    The decorated function should take a single argument *cfg*, which is a
    :class:`omegaconf.DictConfig`. The decorated function should modify these
    arguments in-place to match the desired architecture.

    Args:
        model_name (str): the name of the Model (Model must already be
            registered)
        arch_name (str): the name of the model architecture (``--arch``)
    """

    def register_model_arch_fn(fn):
        if model_name not in MODEL_REGISTRY:
            raise ValueError('Cannot register model architecture for unknown model type ({})'.format(model_name))
        if arch_name in ARCH_MODEL_REGISTRY:
            raise ValueError('Cannot register duplicate model architecture ({})'.format(arch_name))
        if not callable(fn):
            raise ValueError('Model architecture must be callable ({})'.format(arch_name))
        ARCH_MODEL_REGISTRY[arch_name] = MODEL_REGISTRY[model_name]
        ARCH_MODEL_NAME_REGISTRY[arch_name] = model_name
        ARCH_MODEL_INV_REGISTRY.setdefault(model_name, []).append(arch_name)
        ARCH_CONFIG_REGISTRY[arch_name] = fn
        return fn
    return register_model_arch_fn


def register_model(name, dataclass=None):
    """
    New model types can be added to fairseq with the :func:`register_model`
    function decorator.

    For example::

        @register_model('lstm')
        class LSTM(FairseqEncoderDecoderModel):
            (...)

    .. note:: All models must implement the :class:`BaseFairseqModel` interface.
        Typically you will extend :class:`FairseqEncoderDecoderModel` for
        sequence-to-sequence tasks or :class:`FairseqLanguageModel` for
        language modeling tasks.

    Args:
        name (str): the name of the model
    """

    def register_model_cls(cls):
        if name in MODEL_REGISTRY:
            return MODEL_REGISTRY[name]
        if not issubclass(cls, BaseFairseqModel):
            raise ValueError('Model ({}: {}) must extend BaseFairseqModel'.format(name, cls.__name__))
        MODEL_REGISTRY[name] = cls
        if dataclass is not None and not issubclass(dataclass, FairseqDataclass):
            raise ValueError('Dataclass {} must extend FairseqDataclass'.format(dataclass))
        cls.__dataclass = dataclass
        if dataclass is not None:
            MODEL_DATACLASS_REGISTRY[name] = dataclass
            cs = ConfigStore.instance()
            node = dataclass()
            node._name = name
            cs.store(name=name, group='model', node=node, provider='fairseq')

            @register_model_architecture(name, name)
            def noop(_):
                pass
        return cls
    return register_model_cls


class GenerateWaveformFromCode(BenchmarkingBase):
    """Class to support waveform generation from code. Currently, vocoder only supports single speaker"""

    def __init__(self, args):
        super().__init__()
        with open(args.vocoder_cfg) as f:
            vocoder_cfg = json.load(f)
        self.dur_prediction = args.dur_prediction
        self.vocoder = CodeHiFiGANVocoder(args.vocoder, vocoder_cfg)

    def format_units(self, input):
        code = torch.LongTensor(list(map(int, input.strip().split()))).view(1, -1)
        return {'code': code}

    def generate_vocoder_input(self, dataset):
        return [self.format_units(sample) for sample in dataset]

    def forward(self, sample):
        return [{'waveform': self.vocoder(sample, self.dur_prediction)}]


class ApplyKmeans(object):

    def __init__(self, km_path):
        self.km_model = joblib.load(km_path)
        self.C_np = self.km_model.cluster_centers_.transpose()
        self.Cnorm_np = (self.C_np ** 2).sum(0, keepdims=True)
        self.C = torch.from_numpy(self.C_np)
        self.Cnorm = torch.from_numpy(self.Cnorm_np)
        if torch.cuda.is_available():
            self.C = self.C
            self.Cnorm = self.Cnorm

    def __call__(self, x):
        if isinstance(x, torch.Tensor):
            dist = x.pow(2).sum(1, keepdim=True) - 2 * torch.matmul(x, self.C) + self.Cnorm
            return dist.argmin(dim=1).cpu().numpy()
        else:
            dist = (x ** 2).sum(1, keepdims=True) - 2 * np.matmul(x, self.C_np) + self.Cnorm_np
            return np.argmin(dist, axis=1)


class HubertFeatureReader:
    """
    Wrapper class to run inference on HuBERT model.
    Helps extract features for a given audio file.
    """

    def __init__(self, checkpoint_path, layer, max_chunk=1600000):
        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path])
        self.model = model[0].eval()
        self.task = task
        self.layer = layer
        self.max_chunk = max_chunk

    def read_audio(self, path, ref_len=None):
        wav, sr = sf.read(path)
        if wav.ndim == 2:
            wav = wav.mean(-1)
        assert wav.ndim == 1, wav.ndim
        assert sr == self.task.cfg.sample_rate, sr
        if ref_len is not None and abs(ref_len - len(wav)) > 160:
            None
        return wav

    def get_feats(self, file_path, ref_len=None):
        x = self.read_audio(file_path, ref_len)
        with torch.no_grad():
            x = torch.from_numpy(x).float()
            if self.task.cfg.normalize:
                x = F.layer_norm(x, x.shape)
            x = x.view(1, -1)
            feat = []
            for start in range(0, x.size(1), self.max_chunk):
                x_chunk = x[:, start:start + self.max_chunk]
                feat_chunk, _ = self.model.extract_features(source=x_chunk, padding_mask=None, mask=False, output_layer=self.layer)
                feat.append(feat_chunk)
        return torch.cat(feat, 1).squeeze(0)


class HubertUnitExtractor(BenchmarkingBase):

    def __init__(self, args):
        self.feature_reader = HubertFeatureReader(args.hubert_ckpt_path, args.hubert_layer)
        self.kmeans = ApplyKmeans(args.hubert_km_path)

    def forward(self, sample):
        with torch.no_grad():
            feat = []
            for start in range(0, sample.size(1), self.feature_reader.max_chunk):
                x_chunk = sample[:, start:start + self.max_chunk]
                feat_chunk, _ = self.feature_reader.model.extract_features(source=x_chunk, padding_mask=None, mask=False, output_layer=self.layer)
                feat.append(feat_chunk)
            torch.cat(feat, 1).squeeze(0)
        return self.kmeans(feat).tolist()


class SpeechGeneration(BenchmarkingBase):
    """Class similar to examples/text_to_speech/generate_waveform.py.
    Supports models with speech generation as end goal (TTS, Direct S2ST models etc)"""

    def __init__(self, args):
        super().__init__()
        self.use_cuda = not getattr(args, 'cpu', False)
        self.setUp(args)
        self.s2x_task = self.task

    def setUp(self, args):
        if args.task == 'speech_to_speech':
            args.normalize_waveform = False
        self.task = tasks.setup_task(args)
        self.pre_tokenizer = self.task.build_tokenizer(args)
        self.bpe_tokenizer = self.task.build_bpe(args)
        try:
            self.src_dict = self.task.src_dict
        except Exception:
            self.src_dict = None
        ensemble, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([args.path], arg_overrides=ast.literal_eval(args.model_overrides), task=self.task, strict=False)
        self.model = ensemble[0]
        if self.use_cuda:
            self.model
        self.model.eval()
        self.generator = self.task.build_generator([self.model], args)

    def processTextInput(self, text):
        """Generate source tokens from text input"""
        if self.pre_tokenizer is not None:
            text = self.pre_tokenizer.encode(text)
        if self.bpe_tokenizer is not None:
            text = self.bpe_tokenizer.encode(text)
        target = self.src_dict.encode_line(text, add_if_not_exist=False, append_eos=True).long()
        target = fairseq_data_utils.collate_tokens([target], self.src_dict.pad(), self.src_dict.eos(), left_pad=False, move_eos_to_beginning=False)
        src_lengths = torch.tensor([target.size(1)], dtype=torch.long)
        prev_output_tokens = None
        sample = {'net_input': {'src_tokens': target, 'src_lengths': src_lengths, 'prev_output_tokens': prev_output_tokens}}
        sample = utils.move_to_cuda(sample) if self.use_cuda else sample
        return sample

    def forward(self, sample):
        sample['speaker'] = None
        output = self.generator.generate(self.model, sample)
        return output


class S2UT(BenchmarkingBase):
    """Class to support S2UT models. Also supports generating waveforms from the units predicted"""

    def __init__(self, s2u_args, vocoder_args=None):
        super().__init__()
        self.s2u = Processing(s2u_args)
        self.vocoder = None
        if vocoder_args:
            self.vocoder = GenerateWaveformFromCode(vocoder_args)
        self.vocoder_input = None

    def forward(self, sample):
        s2u_hypos = self.s2u(sample)
        s2u_output = self.s2u.decode_target(s2u_hypos)
        if not self.vocoder:
            return s2u_output
        units = self.vocoder.format_units(s2u_output)
        vocoder_output = self.vocoder(units)
        return vocoder_output

    def generate_s2u_outputs(self, dataset):
        return [self.s2u.decode_target(self.s2u(sample)) for sample in dataset]

    def compute_metrics(self, metric_type, dataset, repeat=None):
        """Generic function to compute metrics ignoring the io processing time"""
        if self.vocoder and not self.vocoder_input:
            self.s2u_output = self.generate_s2u_outputs(dataset)
            self.vocoder_input = self.vocoder.generate_vocoder_input(self.s2u_output)
        s2u_metrics = getattr(self.s2u, metric_type)(dataset, repeat)
        vocoder_metrics = 0
        if self.vocoder:
            vocoder_metrics = getattr(self.vocoder, metric_type)(self.vocoder_input, repeat)
        None
        if metric_type == 'max_memory':
            return max(s2u_metrics, vocoder_metrics)
        else:
            return s2u_metrics + vocoder_metrics

    def benchmark_run_time(self, dataset, repeat):
        return self.compute_metrics('benchmark_run_time', dataset, repeat)

    def count_flops(self, dataset, repeat):
        return self.compute_metrics('count_flops', dataset, repeat)

    def max_memory(self, dataset, repeat):
        return self.compute_metrics('max_memory', dataset, repeat)


class Cascaded2StageS2ST(BenchmarkingBase):
    """ST + TTS"""

    def __init__(self, s2t_args, tts_args):
        super().__init__()
        self.s2t = Processing(s2t_args)
        self.s2x_task = self.s2t.task
        self.tts = SpeechGeneration(tts_args) if tts_args else None
        self.training = False
        self.tts_inputs = None

    def forward(self, sample):
        if not self.tts:
            raise Exception('Forward function is not callable without tts. Reinitialize the class with tts_args')
        s2t_hypos = self.s2t(sample)
        s2t_output = self.s2t.decode_target(s2t_hypos)
        tts_input = self.tts.processTextInput(s2t_output)
        tts_output = self.tts(tts_input)
        return tts_output

    def generate_s2t_outputs(self, dataset):
        """Process dataset and generate s2t outputs"""
        return [self.s2t.decode_target(self.s2t(sample)) for sample in dataset]

    def generate_tts_inputs(self, dataset):
        """Process dataset and generate tts inputs"""
        return [self.tts.processTextInput(sample) for sample in dataset]

    def compute_metrics(self, metric_type, dataset, repeat=None):
        """Generic function to compute metrics ignoring the io processing time"""
        if not self.tts_inputs:
            s2t_outputs = self.generate_s2t_outputs(dataset)
            self.tts_inputs = self.generate_tts_inputs(s2t_outputs)
        s2t_metrics = getattr(self.s2t, metric_type)(dataset, repeat)
        tts_metrics = getattr(self.tts, metric_type)(self.tts_inputs, repeat)
        None
        if metric_type == 'max_memory':
            return max(s2t_metrics, tts_metrics)
        else:
            return s2t_metrics + tts_metrics

    def benchmark_run_time(self, dataset, repeat):
        return self.compute_metrics('benchmark_run_time', dataset, repeat)

    def count_flops(self, dataset, repeat):
        return self.compute_metrics('count_flops', dataset, repeat)

    def max_memory(self, dataset, repeat):
        return self.compute_metrics('max_memory', dataset, repeat)


class Cascaded3StageS2ST(Cascaded2StageS2ST):
    """ASR + MT + TTS"""

    def __init__(self, s2t_args, tts_args, mt_args):
        super().__init__(s2t_args, tts_args)
        self.mt = Processing(mt_args)
        self.mt_inputs = []

    def forward(self, sample):
        s2t_hypos = self.s2t(sample)
        s2t_output = self.s2t.decode_target(s2t_hypos)
        mt_input = self.mt.encode_source(s2t_output)
        mt_hypos = self.mt(mt_input)
        mt_output = self.mt.decode_target(mt_hypos)
        tts_input = self.tts.processTextInput(mt_output)
        tts_output = self.tts(tts_input)
        return tts_output

    def generate_mt_inputs(self, dataset):
        """Process dataset to generate mt model inputs"""
        return [self.mt.encode_source(sample) for sample in dataset]

    def generate_mt_outputs(self, dataset):
        """Process dataset to generate mt model outputs"""
        return [self.mt.decode_target(self.mt(sample)) for sample in dataset]

    def compute_metrics(self, metric_type, dataset, repeat=None):
        """Generic function to compute metrics ignoring the io processing time"""
        if not self.tts_inputs:
            s2t_outputs = self.generate_s2t_outputs(dataset)
            self.mt_inputs = self.generate_mt_inputs(s2t_outputs)
            mt_outputs = self.generate_mt_outputs(self.mt_inputs)
            self.tts_inputs = self.generate_tts_inputs(mt_outputs)
        s2t_metrics = getattr(self.s2t, metric_type)(dataset, repeat)
        mt_metrics = getattr(self.mt, metric_type)(self.mt_inputs, repeat)
        tts_metrics = getattr(self.tts, metric_type)(self.tts_inputs, repeat)
        None
        if metric_type == 'max_memory':
            return max(s2t_metrics, mt_metrics, tts_metrics)
        else:
            return s2t_metrics + mt_metrics + tts_metrics


class MultiDecoderSequenceGenerator(nn.Module):

    def __init__(self, models, tgt_dict, tgt_dict_mt, beam_size=1, beam_size_mt=1, max_len_a=0, max_len_b=200, max_len_a_mt=0, max_len_b_mt=200, max_len=0, min_len=1, normalize_scores=True, len_penalty=1.0, len_penalty_mt=1.0, unk_penalty=0.0, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, eos=None, eos_mt=None, symbols_to_strip_from_output=None, lm_model=None, lm_weight=1.0):
        """Generates translations of a given source sentence.

        Args:
            models (List[~fairseq.models.FairseqModel]): ensemble of models,
                currently support fairseq.models.TransformerModel for scripting
            beam_size (int, optional): beam width (default: 1)
            max_len_a/b (int, optional): generate sequences of maximum length
                ax + b, where x is the source length for the second pass
            max_len_a_mt/b_mt (int, optional): generate sequences of maximum length
                ax + b, where x is the source length for the first pass
            max_len (int, optional): the maximum length of the generated output
                (not including end-of-sentence)
            min_len (int, optional): the minimum length of the generated output
                (not including end-of-sentence)
            normalize_scores (bool, optional): normalize scores by the length
                of the output (default: True)
            len_penalty (float, optional): length penalty in the second pass, where <1.0 favors
                shorter, >1.0 favors longer sentences (default: 1.0)
            len_penalty (float, optional): length penalty in the first pass, where <1.0 favors
                shorter, >1.0 favors longer sentences (default: 1.0)
            unk_penalty (float, optional): unknown word penalty, where <0
                produces more unks, >0 produces fewer (default: 0.0)
            temperature (float, optional): temperature, where values
                >1.0 produce more uniform samples and values <1.0 produce
                sharper samples (default: 1.0)
            match_source_len (bool, optional): outputs should match the source
                length (default: False)
        """
        super().__init__()
        self.generator = SequenceGenerator(models, tgt_dict, beam_size=beam_size, max_len_a=max_len_a, max_len_b=max_len_b, max_len=max_len, min_len=min_len, normalize_scores=normalize_scores, len_penalty=len_penalty, unk_penalty=unk_penalty, temperature=temperature, match_source_len=match_source_len, no_repeat_ngram_size=no_repeat_ngram_size, search_strategy=search.BeamSearch(tgt_dict), eos=eos, symbols_to_strip_from_output=symbols_to_strip_from_output, lm_model=lm_model, lm_weight=lm_weight)
        self.eos = self.generator.eos
        self.generator_mt = SequenceGenerator(models, tgt_dict_mt, beam_size=beam_size_mt, max_len_a=max_len_a_mt, max_len_b=max_len_b_mt, max_len=max_len, min_len=min_len, normalize_scores=normalize_scores, len_penalty=len_penalty_mt, unk_penalty=unk_penalty, temperature=temperature, match_source_len=match_source_len, no_repeat_ngram_size=no_repeat_ngram_size, search_strategy=search.BeamSearch(tgt_dict_mt), eos=eos_mt, symbols_to_strip_from_output=symbols_to_strip_from_output)

    @torch.no_grad()
    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs) ->List[List[Dict[str, Tensor]]]:
        """Generate translations. Match the api of other fairseq generators.

        Args:
            models (List[~fairseq.models.FairseqModel]): ensemble of models
            sample (dict): batch
            prefix_tokens (torch.LongTensor, optional): force decoder to begin
                with these tokens
            constraints (torch.LongTensor, optional): force decoder to include
                the list of constraints
            bos_token (int, optional): beginning of sentence token
                (default: self.eos)
        """
        return self._generate(sample, **kwargs)

    def _generate(self, sample: Dict[str, Dict[str, Tensor]], prefix_tokens: Optional[Tensor]=None, constraints: Optional[Tensor]=None, bos_token: Optional[int]=None):
        net_input = sample['net_input']
        if 'src_tokens' in net_input:
            src_tokens = net_input['src_tokens']
            if 'src_lengths' in net_input:
                src_lengths = net_input['src_lengths']
            else:
                src_lengths = (src_tokens.ne(self.generator.eos) & src_tokens.ne(self.generator.pad)).long().sum(dim=1)
        else:
            raise Exception('expected src_tokens or source in net input. input keys: ' + str(net_input.keys()))
        if constraints is not None and not self.generator.search.supports_constraints:
            raise NotImplementedError("Target-side constraints were provided, but search method doesn't support them")
        self.generator.search.init_constraints(constraints, self.generator.beam_size)
        self.generator_mt.search.init_constraints(constraints, self.generator_mt.beam_size)
        with torch.autograd.profiler.record_function('EnsembleModel: forward_encoder'):
            encoder_outs = self.generator.model.forward_encoder(net_input)
        single_model = self.generator.model.single_model
        mt_decoder = getattr(single_model, f'{single_model.mt_task_name}_decoder')
        finalized_mt = self.generator_mt.generate_decoder(encoder_outs, src_tokens, src_lengths, sample, prefix_tokens, constraints, bos_token, aux_task_name=single_model.mt_task_name)
        max_tgt_len = max([len(hypo[0]['tokens']) for hypo in finalized_mt])
        prev_output_tokens_mt = src_tokens.new_zeros(src_tokens.shape[0], max_tgt_len).fill_(mt_decoder.padding_idx).int()
        for i, hypo in enumerate(finalized_mt):
            i_beam = 0
            tmp = hypo[i_beam]['tokens'].int()
            prev_output_tokens_mt[i, 0] = self.generator_mt.eos
            if tmp[-1] == self.generator_mt.eos:
                tmp = tmp[:-1]
            prev_output_tokens_mt[i, 1:len(tmp) + 1] = tmp
            text = ''.join([self.generator_mt.tgt_dict[c] for c in tmp])
            text = text.replace('_', ' ')
            text = text.replace('', ' ')
            text = text.replace('<unk>', ' ')
            text = text.replace('<s>', '')
            text = text.replace('</s>', '')
            if len(text) > 0 and text[0] == ' ':
                text = text[1:]
            sample_id = sample['id'].tolist()[i]
            None
        x = mt_decoder(prev_output_tokens_mt, encoder_out=encoder_outs[0], features_only=True)[0].transpose(0, 1)
        if getattr(single_model, 'proj', None) is not None:
            x = single_model.proj(x)
        mt_decoder_padding_mask = None
        if prev_output_tokens_mt.eq(mt_decoder.padding_idx).any():
            mt_decoder_padding_mask = prev_output_tokens_mt.eq(mt_decoder.padding_idx)
        if getattr(single_model, 'synthesizer_encoder', None) is not None:
            t2u_encoder_out = single_model.synthesizer_encoder(x, mt_decoder_padding_mask)
        else:
            t2u_encoder_out = {'encoder_out': [x], 'encoder_padding_mask': [mt_decoder_padding_mask] if mt_decoder_padding_mask is not None else [], 'encoder_embedding': [], 'encoder_states': [], 'src_tokens': [], 'src_lengths': []}
        if getattr(single_model, 't2u_augmented_cross_attn', False):
            encoder_outs_aug = [t2u_encoder_out]
        else:
            encoder_outs = [t2u_encoder_out]
            encoder_outs_aug = None
        finalized = self.generator.generate_decoder(encoder_outs, src_tokens, src_lengths, sample, prefix_tokens, constraints, bos_token, encoder_outs_aug=encoder_outs_aug)
        return finalized


class ChannelNorm(nn.Module):

    def __init__(self, num_features, epsilon=1e-05, affine=True):
        super(ChannelNorm, self).__init__()
        if affine:
            self.weight = nn.parameter.Parameter(torch.Tensor(1, num_features, 1))
            self.bias = nn.parameter.Parameter(torch.Tensor(1, num_features, 1))
        else:
            self.weight = None
            self.bias = None
        self.epsilon = epsilon
        self.p = 0
        self.affine = affine
        self.reset_parameters()

    def reset_parameters(self):
        if self.affine:
            torch.nn.init.ones_(self.weight)
            torch.nn.init.zeros_(self.bias)

    def forward(self, x):
        cum_mean = x.mean(dim=1, keepdim=True)
        cum_var = x.var(dim=1, keepdim=True)
        x = (x - cum_mean) * torch.rsqrt(cum_var + self.epsilon)
        if self.weight is not None:
            x = x * self.weight + self.bias
        return x


class CPCEncoder(nn.Module):

    def __init__(self, hidden_dim=512):
        super(CPCEncoder, self).__init__()
        self.conv0 = nn.Conv1d(1, hidden_dim, 10, stride=5, padding=3)
        self.batchNorm0 = ChannelNorm(hidden_dim)
        self.conv1 = nn.Conv1d(hidden_dim, hidden_dim, 8, stride=4, padding=2)
        self.batchNorm1 = ChannelNorm(hidden_dim)
        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, 4, stride=2, padding=1)
        self.batchNorm2 = ChannelNorm(hidden_dim)
        self.conv3 = nn.Conv1d(hidden_dim, hidden_dim, 4, stride=2, padding=1)
        self.batchNorm3 = ChannelNorm(hidden_dim)
        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, 4, stride=2, padding=1)
        self.batchNorm4 = ChannelNorm(hidden_dim)
        self.DOWNSAMPLING = 160

    def get_output_dim(self):
        return self.conv4.out_channels

    def forward(self, x):
        x = F.relu(self.batchNorm0(self.conv0(x)))
        x = F.relu(self.batchNorm1(self.conv1(x)))
        x = F.relu(self.batchNorm2(self.conv2(x)))
        x = F.relu(self.batchNorm3(self.conv3(x)))
        x = F.relu(self.batchNorm4(self.conv4(x)))
        return x


class CPCAR(nn.Module):

    def __init__(self, dim_encoded, dim_output, keep_hidden, num_layers):
        super(CPCAR, self).__init__()
        self.baseNet = nn.LSTM(dim_encoded, dim_output, num_layers=num_layers, batch_first=True)
        self.hidden = None
        self.keep_hidden = keep_hidden

    def get_output_dim(self):
        return self.baseNet.hidden_size

    def forward(self, x):
        try:
            self.baseNet.flatten_parameters()
        except RuntimeError:
            pass
        x, h = self.baseNet(x, self.hidden)
        if self.keep_hidden:
            if isinstance(h, tuple):
                self.hidden = tuple(x.detach() for x in h)
            else:
                self.hidden = h.detach()
        return x


class CPCModel(nn.Module):

    def __init__(self, encoder, ar_net):
        super(CPCModel, self).__init__()
        self.gEncoder = encoder
        self.gAR = ar_net
        self.config = None

    def forward(self, x, label):
        encoded = self.gEncoder(x).permute(0, 2, 1)
        cpc_feature = self.gAR(encoded)
        return cpc_feature, encoded, label

    def extract_features(self, source, get_encoded=False, norm_output=False):
        cpc_feature, encoded, _ = self.forward(source, None)
        if get_encoded:
            cpc_feature = encoded
        if norm_output:
            mean = cpc_feature.mean(dim=1, keepdim=True)
            var = cpc_feature.var(dim=1, keepdim=True)
            cpc_feature = (cpc_feature - mean) / torch.sqrt(var + 1e-08)
        return cpc_feature


class WaveGlowLoss(torch.nn.Module):

    def __init__(self, sigma=1.0):
        super(WaveGlowLoss, self).__init__()
        self.sigma = sigma

    def forward(self, model_output):
        z, log_s_list, log_det_W_list = model_output
        for i, log_s in enumerate(log_s_list):
            if i == 0:
                log_s_total = torch.sum(log_s)
                log_det_W_total = log_det_W_list[i]
            else:
                log_s_total = log_s_total + torch.sum(log_s)
                log_det_W_total += log_det_W_list[i]
        loss = torch.sum(z * z) / (2 * self.sigma * self.sigma) - log_s_total - log_det_W_total
        return loss / (z.size(0) * z.size(1) * z.size(2))


class Invertible1x1Conv(torch.nn.Module):
    """
    The layer outputs both the convolution, and the log determinant
    of its weight matrix.  If reverse=True it does convolution with
    inverse
    """

    def __init__(self, c):
        super(Invertible1x1Conv, self).__init__()
        self.conv = torch.nn.Conv1d(c, c, kernel_size=1, stride=1, padding=0, bias=False)
        _qr = torch.linalg.qr if torch.__version__ >= '1.8' else torch.qr
        W = _qr(torch.FloatTensor(c, c).normal_())[0]
        if torch.det(W) < 0:
            W[:, 0] = -1 * W[:, 0]
        W = W.view(c, c, 1)
        self.conv.weight.data = W

    def forward(self, z, reverse=False):
        batch_size, group_size, n_of_groups = z.size()
        W = self.conv.weight.squeeze()
        if reverse:
            if not hasattr(self, 'W_inverse'):
                W_inverse = W.float().inverse()
                W_inverse = Variable(W_inverse[..., None])
                if z.type() == 'torch.cuda.HalfTensor':
                    W_inverse = W_inverse.half()
                self.W_inverse = W_inverse
            z = F.conv1d(z, self.W_inverse, bias=None, stride=1, padding=0)
            return z
        else:
            log_det_W = batch_size * n_of_groups * torch.logdet(W)
            z = self.conv(z)
            return z, log_det_W


@torch.jit.script
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
    acts = t_act * s_act
    return acts


class WN(torch.nn.Module):
    """
    This is the WaveNet like layer for the affine coupling.  The primary difference
    from WaveNet is the convolutions need not be causal.  There is also no dilation
    size reset.  The dilation only doubles on each layer
    """

    def __init__(self, n_in_channels, n_mel_channels, n_layers, n_channels, kernel_size):
        super(WN, self).__init__()
        assert kernel_size % 2 == 1
        assert n_channels % 2 == 0
        self.n_layers = n_layers
        self.n_channels = n_channels
        self.in_layers = torch.nn.ModuleList()
        self.res_skip_layers = torch.nn.ModuleList()
        start = torch.nn.Conv1d(n_in_channels, n_channels, 1)
        start = torch.nn.utils.weight_norm(start, name='weight')
        self.start = start
        end = torch.nn.Conv1d(n_channels, 2 * n_in_channels, 1)
        end.weight.data.zero_()
        end.bias.data.zero_()
        self.end = end
        cond_layer = torch.nn.Conv1d(n_mel_channels, 2 * n_channels * n_layers, 1)
        self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name='weight')
        for i in range(n_layers):
            dilation = 2 ** i
            padding = int((kernel_size * dilation - dilation) / 2)
            in_layer = torch.nn.Conv1d(n_channels, 2 * n_channels, kernel_size, dilation=dilation, padding=padding)
            in_layer = torch.nn.utils.weight_norm(in_layer, name='weight')
            self.in_layers.append(in_layer)
            if i < n_layers - 1:
                res_skip_channels = 2 * n_channels
            else:
                res_skip_channels = n_channels
            res_skip_layer = torch.nn.Conv1d(n_channels, res_skip_channels, 1)
            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name='weight')
            self.res_skip_layers.append(res_skip_layer)

    def forward(self, forward_input):
        audio, spect = forward_input
        audio = self.start(audio)
        output = torch.zeros_like(audio)
        n_channels_tensor = torch.IntTensor([self.n_channels])
        spect = self.cond_layer(spect)
        for i in range(self.n_layers):
            spect_offset = i * 2 * self.n_channels
            acts = fused_add_tanh_sigmoid_multiply(self.in_layers[i](audio), spect[:, spect_offset:spect_offset + 2 * self.n_channels, :], n_channels_tensor)
            res_skip_acts = self.res_skip_layers[i](acts)
            if i < self.n_layers - 1:
                audio = audio + res_skip_acts[:, :self.n_channels, :]
                output = output + res_skip_acts[:, self.n_channels:, :]
            else:
                output = output + res_skip_acts
        return self.end(output)


def remove(conv_list):
    new_conv_list = torch.nn.ModuleList()
    for old_conv in conv_list:
        old_conv = torch.nn.utils.remove_weight_norm(old_conv)
        new_conv_list.append(old_conv)
    return new_conv_list


class WaveGlow(torch.nn.Module):

    def __init__(self, n_mel_channels, n_flows, n_group, n_early_every, n_early_size, WN_config):
        super(WaveGlow, self).__init__()
        self.upsample = torch.nn.ConvTranspose1d(n_mel_channels, n_mel_channels, 1024, stride=256)
        assert n_group % 2 == 0
        self.n_flows = n_flows
        self.n_group = n_group
        self.n_early_every = n_early_every
        self.n_early_size = n_early_size
        self.WN = torch.nn.ModuleList()
        self.convinv = torch.nn.ModuleList()
        n_half = int(n_group / 2)
        n_remaining_channels = n_group
        for k in range(n_flows):
            if k % self.n_early_every == 0 and k > 0:
                n_half = n_half - int(self.n_early_size / 2)
                n_remaining_channels = n_remaining_channels - self.n_early_size
            self.convinv.append(Invertible1x1Conv(n_remaining_channels))
            self.WN.append(WN(n_half, n_mel_channels * n_group, **WN_config))
        self.n_remaining_channels = n_remaining_channels

    def forward(self, forward_input):
        """
        forward_input[0] = mel_spectrogram:  batch x n_mel_channels x frames
        forward_input[1] = audio: batch x time
        """
        spect, audio = forward_input
        spect = self.upsample(spect)
        assert spect.size(2) >= audio.size(1)
        if spect.size(2) > audio.size(1):
            spect = spect[:, :, :audio.size(1)]
        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)
        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)
        audio = audio.unfold(1, self.n_group, self.n_group).permute(0, 2, 1)
        output_audio = []
        log_s_list = []
        log_det_W_list = []
        for k in range(self.n_flows):
            if k % self.n_early_every == 0 and k > 0:
                output_audio.append(audio[:, :self.n_early_size, :])
                audio = audio[:, self.n_early_size:, :]
            audio, log_det_W = self.convinv[k](audio)
            log_det_W_list.append(log_det_W)
            n_half = int(audio.size(1) / 2)
            audio_0 = audio[:, :n_half, :]
            audio_1 = audio[:, n_half:, :]
            output = self.WN[k]((audio_0, spect))
            log_s = output[:, n_half:, :]
            b = output[:, :n_half, :]
            audio_1 = torch.exp(log_s) * audio_1 + b
            log_s_list.append(log_s)
            audio = torch.cat([audio_0, audio_1], 1)
        output_audio.append(audio)
        return torch.cat(output_audio, 1), log_s_list, log_det_W_list

    def infer(self, spect, sigma=1.0):
        spect = self.upsample(spect)
        time_cutoff = self.upsample.kernel_size[0] - self.upsample.stride[0]
        spect = spect[:, :, :-time_cutoff]
        spect = spect.unfold(2, self.n_group, self.n_group).permute(0, 2, 1, 3)
        spect = spect.contiguous().view(spect.size(0), spect.size(1), -1).permute(0, 2, 1)
        if spect.type() == 'torch.cuda.HalfTensor':
            audio = torch.HalfTensor(spect.size(0), self.n_remaining_channels, spect.size(2)).normal_()
        else:
            audio = torch.FloatTensor(spect.size(0), self.n_remaining_channels, spect.size(2)).normal_()
        audio = torch.autograd.Variable(sigma * audio)
        for k in reversed(range(self.n_flows)):
            n_half = int(audio.size(1) / 2)
            audio_0 = audio[:, :n_half, :]
            audio_1 = audio[:, n_half:, :]
            output = self.WN[k]((audio_0, spect))
            s = output[:, n_half:, :]
            b = output[:, :n_half, :]
            audio_1 = (audio_1 - b) / torch.exp(s)
            audio = torch.cat([audio_0, audio_1], 1)
            audio = self.convinv[k](audio, reverse=True)
            if k % self.n_early_every == 0 and k > 0:
                if spect.type() == 'torch.cuda.HalfTensor':
                    z = torch.HalfTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()
                else:
                    z = torch.FloatTensor(spect.size(0), self.n_early_size, spect.size(2)).normal_()
                audio = torch.cat((sigma * z, audio), 1)
        audio = audio.permute(0, 2, 1).contiguous().view(audio.size(0), -1).data
        return audio

    @staticmethod
    def remove_weightnorm(model):
        waveglow = model
        for WN in waveglow.WN:
            WN.start = torch.nn.utils.remove_weight_norm(WN.start)
            WN.in_layers = remove(WN.in_layers)
            WN.cond_layer = torch.nn.utils.remove_weight_norm(WN.cond_layer)
            WN.res_skip_layers = remove(WN.res_skip_layers)
        return waveglow


def window_sumsquare(window, n_frames, hop_length=200, win_length=800, n_fft=800, dtype=np.float32, norm=None):
    """
    # from librosa 0.6
    Compute the sum-square envelope of a window function at a given hop length.

    This is used to estimate modulation effects induced by windowing
    observations in short-time fourier transforms.

    Parameters
    ----------
    window : string, tuple, number, callable, or list-like
        Window specification, as in `get_window`

    n_frames : int > 0
        The number of analysis frames

    hop_length : int > 0
        The number of samples to advance between frames

    win_length : [optional]
        The length of the window function.  By default, this matches `n_fft`.

    n_fft : int > 0
        The length of each analysis frame.

    dtype : np.dtype
        The data type of the output

    Returns
    -------
    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`
        The sum-squared envelope of the window function
    """
    if win_length is None:
        win_length = n_fft
    n = n_fft + hop_length * (n_frames - 1)
    x = np.zeros(n, dtype=dtype)
    win_sq = get_window(window, win_length, fftbins=True)
    win_sq = librosa_util.normalize(win_sq, norm=norm) ** 2
    win_sq = librosa_util.pad_center(win_sq, n_fft)
    for i in range(n_frames):
        sample = i * hop_length
        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]
    return x


class STFT(torch.nn.Module):
    """adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft"""

    def __init__(self, filter_length=800, hop_length=200, win_length=800, window='hann'):
        super(STFT, self).__init__()
        self.filter_length = filter_length
        self.hop_length = hop_length
        self.win_length = win_length
        self.window = window
        self.forward_transform = None
        scale = self.filter_length / self.hop_length
        fourier_basis = np.fft.fft(np.eye(self.filter_length))
        cutoff = int(self.filter_length / 2 + 1)
        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]), np.imag(fourier_basis[:cutoff, :])])
        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])
        inverse_basis = torch.FloatTensor(np.linalg.pinv(scale * fourier_basis).T[:, None, :])
        if window is not None:
            assert filter_length >= win_length
            fft_window = get_window(window, win_length, fftbins=True)
            fft_window = pad_center(fft_window, filter_length)
            fft_window = torch.from_numpy(fft_window).float()
            forward_basis *= fft_window
            inverse_basis *= fft_window
        self.register_buffer('forward_basis', forward_basis.float())
        self.register_buffer('inverse_basis', inverse_basis.float())

    def transform(self, input_data):
        num_batches = input_data.size(0)
        num_samples = input_data.size(1)
        self.num_samples = num_samples
        input_data = input_data.view(num_batches, 1, num_samples)
        input_data = F.pad(input_data.unsqueeze(1), (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0), mode='reflect')
        input_data = input_data.squeeze(1)
        forward_transform = F.conv1d(input_data, Variable(self.forward_basis, requires_grad=False), stride=self.hop_length, padding=0)
        cutoff = int(self.filter_length / 2 + 1)
        real_part = forward_transform[:, :cutoff, :]
        imag_part = forward_transform[:, cutoff:, :]
        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)
        phase = torch.autograd.Variable(torch.atan2(imag_part.data, real_part.data))
        return magnitude, phase

    def inverse(self, magnitude, phase):
        recombine_magnitude_phase = torch.cat([magnitude * torch.cos(phase), magnitude * torch.sin(phase)], dim=1)
        inverse_transform = F.conv_transpose1d(recombine_magnitude_phase, Variable(self.inverse_basis, requires_grad=False), stride=self.hop_length, padding=0)
        if self.window is not None:
            window_sum = window_sumsquare(self.window, magnitude.size(-1), hop_length=self.hop_length, win_length=self.win_length, n_fft=self.filter_length, dtype=np.float32)
            approx_nonzero_indices = torch.from_numpy(np.where(window_sum > tiny(window_sum))[0])
            window_sum = torch.autograd.Variable(torch.from_numpy(window_sum), requires_grad=False)
            window_sum = window_sum if magnitude.is_cuda else window_sum
            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]
            inverse_transform *= float(self.filter_length) / self.hop_length
        inverse_transform = inverse_transform[:, :, int(self.filter_length / 2):]
        inverse_transform = inverse_transform[:, :, :-int(self.filter_length / 2)]
        return inverse_transform

    def forward(self, input_data):
        self.magnitude, self.phase = self.transform(input_data)
        reconstruction = self.inverse(self.magnitude, self.phase)
        return reconstruction


def dynamic_range_compression(x, C=1, clip_val=1e-05):
    """
    PARAMS
    ------
    C: compression factor
    """
    return torch.log(torch.clamp(x, min=clip_val) * C)


def dynamic_range_decompression(x, C=1):
    """
    PARAMS
    ------
    C: compression factor used to compress
    """
    return torch.exp(x) / C


class TacotronSTFT(torch.nn.Module):

    def __init__(self, filter_length=1024, hop_length=256, win_length=1024, n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0, mel_fmax=8000.0):
        super(TacotronSTFT, self).__init__()
        self.n_mel_channels = n_mel_channels
        self.sampling_rate = sampling_rate
        self.stft_fn = STFT(filter_length, hop_length, win_length)
        mel_basis = librosa_mel_fn(sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)
        mel_basis = torch.from_numpy(mel_basis).float()
        self.register_buffer('mel_basis', mel_basis)

    def spectral_normalize(self, magnitudes):
        output = dynamic_range_compression(magnitudes)
        return output

    def spectral_de_normalize(self, magnitudes):
        output = dynamic_range_decompression(magnitudes)
        return output

    def mel_spectrogram(self, y):
        """Computes mel-spectrograms from a batch of waves
        PARAMS
        ------
        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]

        RETURNS
        -------
        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)
        """
        assert torch.min(y.data) >= -1
        assert torch.max(y.data) <= 1
        magnitudes, phases = self.stft_fn.transform(y)
        magnitudes = magnitudes.data
        mel_output = torch.matmul(self.mel_basis, magnitudes)
        mel_output = self.spectral_normalize(mel_output)
        return mel_output


class Prenet(nn.Module):

    def __init__(self, in_dim, n_layers, n_units, dropout):
        super().__init__()
        self.layers = nn.ModuleList(nn.Sequential(nn.Linear(in_dim if i == 0 else n_units, n_units), nn.ReLU()) for i in range(n_layers))
        self.dropout = dropout

    def forward(self, x):
        for layer in self.layers:
            x = F.dropout(layer(x), p=self.dropout)
        return x


class Postnet(nn.Module):

    def __init__(self, in_dim, n_channels, kernel_size, n_layers, dropout):
        super(Postnet, self).__init__()
        self.convolutions = nn.ModuleList()
        assert kernel_size % 2 == 1
        for i in range(n_layers):
            cur_layers = [nn.Conv1d(in_dim if i == 0 else n_channels, n_channels if i < n_layers - 1 else in_dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2), nn.BatchNorm1d(n_channels if i < n_layers - 1 else in_dim)] + ([nn.Tanh()] if i < n_layers - 1 else []) + [nn.Dropout(dropout)]
            nn.init.xavier_uniform_(cur_layers[0].weight, torch.nn.init.calculate_gain('tanh' if i < n_layers - 1 else 'linear'))
            self.convolutions.append(nn.Sequential(*cur_layers))

    def forward(self, x):
        x = x.transpose(1, 2)
        for conv in self.convolutions:
            x = conv(x)
        return x.transpose(1, 2)


class Encoder(nn.Module):
    """Encoder module:
        - Three 1-d convolution banks
        - Bidirectional LSTM
    """

    def __init__(self, hparams):
        super(Encoder, self).__init__()
        convolutions = []
        for _ in range(hparams.encoder_n_convolutions):
            conv_layer = nn.Sequential(ConvNorm(hparams.encoder_embedding_dim, hparams.encoder_embedding_dim, kernel_size=hparams.encoder_kernel_size, stride=1, padding=int((hparams.encoder_kernel_size - 1) / 2), dilation=1, w_init_gain='relu'), nn.BatchNorm1d(hparams.encoder_embedding_dim))
            convolutions.append(conv_layer)
        self.convolutions = nn.ModuleList(convolutions)
        self.lstm = nn.LSTM(hparams.encoder_embedding_dim, int(hparams.encoder_embedding_dim / 2), 1, batch_first=True, bidirectional=True)

    def forward(self, x, input_lengths):
        for conv in self.convolutions:
            x = F.dropout(F.relu(conv(x)), 0.5, self.training)
        x = x.transpose(1, 2)
        input_lengths = input_lengths.cpu().numpy()
        x = nn.utils.rnn.pack_padded_sequence(x, input_lengths, batch_first=True)
        self.lstm.flatten_parameters()
        outputs, _ = self.lstm(x)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)
        return outputs

    def inference(self, x):
        for conv in self.convolutions:
            x = F.dropout(F.relu(conv(x)), 0.5, self.training)
        x = x.transpose(1, 2)
        self.lstm.flatten_parameters()
        outputs, _ = self.lstm(x)
        return outputs


class AudioEncoder(nn.Module):

    def __init__(self, hparams):
        super(AudioEncoder, self).__init__()
        assert hparams.lat_dim > 0
        convolutions = []
        inp_dim = hparams.n_mel_channels
        for _ in range(hparams.lat_n_convolutions):
            conv_layer = nn.Sequential(ConvNorm(inp_dim, hparams.lat_n_filters, kernel_size=hparams.lat_kernel_size, stride=1, padding=int((hparams.lat_kernel_size - 1) / 2), dilation=1, w_init_gain='tanh'), nn.BatchNorm1d(hparams.lat_n_filters))
            inp_dim = hparams.lat_n_filters
            convolutions.append(conv_layer)
        self.convolutions = nn.ModuleList(convolutions)
        self.lstm = nn.LSTM(hparams.lat_n_filters, int(hparams.lat_n_filters / 2), hparams.lat_n_blstms, batch_first=True, bidirectional=True)
        self.pool = GlobalAvgPool()
        self.mu_proj = LinearNorm(hparams.lat_n_filters, hparams.lat_dim)
        self.logvar_proj = LinearNorm(hparams.lat_n_filters, hparams.lat_dim)
        self.lat_dim = hparams.lat_dim

    def forward(self, x, lengths):
        """
        Args:
            x (torch.Tensor): (B, F, T)
        """
        for conv in self.convolutions:
            x = F.dropout(F.tanh(conv(x)), 0.5, self.training)
        x = x.transpose(1, 2)
        max_len = x.size(1)
        assert max_len == torch.max(lengths).item()
        lengths, perm_idx = lengths.sort(0, descending=True)
        x = x[perm_idx]
        x = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True)
        self.lstm.flatten_parameters()
        outputs, _ = self.lstm(x)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)
        _, unperm_idx = perm_idx.sort(0)
        outputs = outputs[unperm_idx]
        lengths = lengths[unperm_idx]
        outputs = self.pool(outputs, lengths)
        mu = self.mu_proj(outputs)
        logvar = self.logvar_proj(outputs)
        z = distr.Normal(mu, logvar).rsample()
        return z, mu, logvar


class Decoder(nn.Module):

    def __init__(self, hparams):
        super(Decoder, self).__init__()
        self.n_mel_channels = hparams.n_mel_channels
        self.n_frames_per_step = hparams.n_frames_per_step
        self.encoder_embedding_dim = hparams.encoder_embedding_dim
        self.obs_dim = hparams.obs_dim
        self.lat_dim = hparams.lat_dim
        self.attention_rnn_dim = hparams.attention_rnn_dim
        self.decoder_rnn_dim = hparams.decoder_rnn_dim
        self.prenet_dim = hparams.prenet_dim
        self.max_decoder_steps = hparams.max_decoder_steps
        self.gate_threshold = hparams.gate_threshold
        self.p_attention_dropout = hparams.p_attention_dropout
        self.p_decoder_dropout = hparams.p_decoder_dropout
        self.prenet = Prenet(hparams.n_mel_channels * hparams.n_frames_per_step, [hparams.prenet_dim, hparams.prenet_dim])
        self.attention_rnn = nn.LSTMCell(hparams.prenet_dim + hparams.encoder_embedding_dim, hparams.attention_rnn_dim)
        self.attention_layer = Attention(hparams.attention_rnn_dim, hparams.encoder_embedding_dim, hparams.attention_dim, hparams.attention_location_n_filters, hparams.attention_location_kernel_size)
        encoder_tot_dim = hparams.encoder_embedding_dim + hparams.lat_dim + hparams.obs_dim
        self.decoder_rnn = nn.LSTMCell(hparams.attention_rnn_dim + encoder_tot_dim, hparams.decoder_rnn_dim, 1)
        self.linear_projection = LinearNorm(hparams.decoder_rnn_dim + encoder_tot_dim, hparams.n_mel_channels * hparams.n_frames_per_step)
        self.gate_layer = LinearNorm(hparams.decoder_rnn_dim + encoder_tot_dim, 1, bias=True, w_init_gain='sigmoid')

    def get_go_frame(self, memory):
        """ Gets all zeros frames to use as first decoder input
        PARAMS
        ------
        memory: decoder outputs

        RETURNS
        -------
        decoder_input: all zeros frames
        """
        B = memory.size(0)
        decoder_input = Variable(memory.data.new(B, self.n_mel_channels * self.n_frames_per_step).zero_())
        return decoder_input

    def initialize_decoder_states(self, memory, obs_and_lat, mask):
        """ Initializes attention rnn states, decoder rnn states, attention
        weights, attention cumulative weights, attention context, stores memory
        and stores processed memory
        PARAMS
        ------
        memory: Encoder outputs
        obs_and_lat: Observed and latent attribute embeddings
        mask: Mask for padded data if training, expects None for inference
        """
        B = memory.size(0)
        MAX_TIME = memory.size(1)
        self.attention_hidden = Variable(memory.data.new(B, self.attention_rnn_dim).zero_())
        self.attention_cell = Variable(memory.data.new(B, self.attention_rnn_dim).zero_())
        self.decoder_hidden = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())
        self.decoder_cell = Variable(memory.data.new(B, self.decoder_rnn_dim).zero_())
        self.attention_weights = Variable(memory.data.new(B, MAX_TIME).zero_())
        self.attention_weights_cum = Variable(memory.data.new(B, MAX_TIME).zero_())
        self.attention_context = Variable(memory.data.new(B, self.encoder_embedding_dim).zero_())
        self.memory = memory
        self.processed_memory = self.attention_layer.memory_layer(memory)
        self.obs_and_lat = obs_and_lat
        self.mask = mask

    def parse_decoder_inputs(self, decoder_inputs):
        """ Prepares decoder inputs, i.e. mel outputs
        PARAMS
        ------
        decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs

        RETURNS
        -------
        inputs: processed decoder inputs

        """
        decoder_inputs = decoder_inputs.transpose(1, 2)
        decoder_inputs = decoder_inputs.view(decoder_inputs.size(0), int(decoder_inputs.size(1) / self.n_frames_per_step), -1)
        decoder_inputs = decoder_inputs.transpose(0, 1)
        return decoder_inputs

    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):
        """ Prepares decoder outputs for output
        PARAMS
        ------
        mel_outputs:
        gate_outputs: gate output energies
        alignments:

        RETURNS
        -------
        mel_outputs:
        gate_outpust: gate output energies
        alignments:
        """
        alignments = torch.stack(alignments).transpose(0, 1)
        gate_outputs = torch.stack(gate_outputs).transpose(0, 1)
        gate_outputs = gate_outputs.contiguous()
        mel_outputs = torch.stack(mel_outputs).transpose(0, 1).contiguous()
        mel_outputs = mel_outputs.view(mel_outputs.size(0), -1, self.n_mel_channels)
        mel_outputs = mel_outputs.transpose(1, 2)
        return mel_outputs, gate_outputs, alignments

    def decode(self, decoder_input):
        """ Decoder step using stored states, attention and memory
        PARAMS
        ------
        decoder_input: previous mel output

        RETURNS
        -------
        mel_output:
        gate_output: gate output energies
        attention_weights:
        """
        cell_input = torch.cat((decoder_input, self.attention_context), -1)
        self.attention_hidden, self.attention_cell = self.attention_rnn(cell_input, (self.attention_hidden, self.attention_cell))
        self.attention_hidden = F.dropout(self.attention_hidden, self.p_attention_dropout, self.training)
        attention_weights_cat = torch.cat((self.attention_weights.unsqueeze(1), self.attention_weights_cum.unsqueeze(1)), dim=1)
        self.attention_context, self.attention_weights = self.attention_layer(self.attention_hidden, self.memory, self.processed_memory, attention_weights_cat, self.mask)
        self.attention_weights_cum += self.attention_weights
        decoder_input = torch.cat((self.attention_hidden, self.attention_context), -1)
        if self.obs_and_lat is not None:
            decoder_input = torch.cat((decoder_input, self.obs_and_lat), -1)
        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(decoder_input, (self.decoder_hidden, self.decoder_cell))
        self.decoder_hidden = F.dropout(self.decoder_hidden, self.p_decoder_dropout, self.training)
        decoder_hidden_attention_context = torch.cat((self.decoder_hidden, self.attention_context), dim=1)
        if self.obs_and_lat is not None:
            decoder_hidden_attention_context = torch.cat((decoder_hidden_attention_context, self.obs_and_lat), dim=1)
        decoder_output = self.linear_projection(decoder_hidden_attention_context)
        gate_prediction = self.gate_layer(decoder_hidden_attention_context)
        return decoder_output, gate_prediction, self.attention_weights

    def forward(self, memory, obs_and_lat, decoder_inputs, memory_lengths):
        """ Decoder forward pass for training
        PARAMS
        ------
        memory: Encoder outputs
        obs_and_lat: Observed and latent attribute embeddings
        decoder_inputs: Decoder inputs for teacher forcing. i.e. mel-specs
        memory_lengths: Encoder output lengths for attention masking.

        RETURNS
        -------
        mel_outputs: mel outputs from the decoder
        gate_outputs: gate outputs from the decoder
        alignments: sequence of attention weights from the decoder
        """
        decoder_input = self.get_go_frame(memory).unsqueeze(0)
        decoder_inputs = self.parse_decoder_inputs(decoder_inputs)
        decoder_inputs = torch.cat((decoder_input, decoder_inputs), dim=0)
        decoder_inputs = self.prenet(decoder_inputs)
        self.initialize_decoder_states(memory, obs_and_lat, mask=~get_mask_from_lengths(memory_lengths))
        mel_outputs, gate_outputs, alignments = [], [], []
        while len(mel_outputs) < decoder_inputs.size(0) - 1:
            decoder_input = decoder_inputs[len(mel_outputs)]
            mel_output, gate_output, attention_weights = self.decode(decoder_input)
            mel_outputs += [mel_output.squeeze(1)]
            gate_outputs += [gate_output.squeeze()]
            alignments += [attention_weights]
        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(mel_outputs, gate_outputs, alignments)
        return mel_outputs, gate_outputs, alignments

    def inference(self, memory, obs_and_lat, ret_has_eos=False):
        """ Decoder inference
        PARAMS
        ------
        memory: Encoder outputs
        obs_and_lat: Observed and latent attribute embeddings

        RETURNS
        -------
        mel_outputs: mel outputs from the decoder
        gate_outputs: gate outputs from the decoder
        alignments: sequence of attention weights from the decoder
        """
        decoder_input = self.get_go_frame(memory)
        self.initialize_decoder_states(memory, obs_and_lat, mask=None)
        mel_outputs, gate_outputs, alignments = [], [], []
        has_eos = False
        while True:
            decoder_input = self.prenet(decoder_input)
            mel_output, gate_output, alignment = self.decode(decoder_input)
            mel_outputs += [mel_output.squeeze(1)]
            gate_outputs += [gate_output]
            alignments += [alignment]
            if torch.sigmoid(gate_output.data) > self.gate_threshold:
                has_eos = True
                break
            elif len(mel_outputs) == self.max_decoder_steps:
                break
            decoder_input = mel_output
        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(mel_outputs, gate_outputs, alignments)
        if ret_has_eos:
            return mel_outputs, gate_outputs, alignments, has_eos
        else:
            return mel_outputs, gate_outputs, alignments


def to_gpu(x):
    x = x.contiguous()
    if torch.cuda.is_available():
        x = x
    return torch.autograd.Variable(x)


class Tacotron2(nn.Module):

    def __init__(self, hparams):
        super(Tacotron2, self).__init__()
        self.mask_padding = hparams.mask_padding
        self.fp16_run = hparams.fp16_run
        self.n_mel_channels = hparams.n_mel_channels
        self.n_frames_per_step = hparams.n_frames_per_step
        self.embedding = nn.Embedding(hparams.n_symbols, hparams.symbols_embedding_dim)
        std = sqrt(2.0 / (hparams.n_symbols + hparams.symbols_embedding_dim))
        val = sqrt(3.0) * std
        self.embedding.weight.data.uniform_(-val, val)
        self.obs_embedding = None
        if hparams.obs_dim > 0:
            self.obs_embedding = nn.Embedding(hparams.obs_n_class, hparams.obs_dim)
            std = sqrt(2.0 / (hparams.obs_n_class + hparams.obs_dim))
            val = sqrt(3.0) * std
            self.obs_embedding.weight.data.uniform_(-val, val)
        self.encoder = Encoder(hparams)
        self.decoder = Decoder(hparams)
        self.postnet = Postnet(hparams)
        self.lat_encoder = None
        if hparams.lat_dim > 0:
            self.lat_encoder = AudioEncoder(hparams)

    def parse_batch(self, batch):
        text_padded, input_lengths, obs_labels, mel_padded, gate_padded, output_lengths = batch
        text_padded = to_gpu(text_padded).long()
        input_lengths = to_gpu(input_lengths).long()
        obs_labels = to_gpu(obs_labels).long()
        max_len = torch.max(input_lengths.data).item()
        mel_padded = to_gpu(mel_padded).float()
        gate_padded = to_gpu(gate_padded).float()
        output_lengths = to_gpu(output_lengths).long()
        return (text_padded, input_lengths, obs_labels, mel_padded, max_len, output_lengths), (mel_padded, gate_padded)

    def parse_output(self, outputs, output_lengths=None):
        if self.mask_padding and output_lengths is not None:
            mask = ~get_mask_from_lengths(output_lengths)
            mask = mask.expand(self.n_mel_channels, mask.size(0), mask.size(1))
            mask = mask.permute(1, 0, 2)
            outputs[0].data.masked_fill_(mask, 0.0)
            outputs[1].data.masked_fill_(mask, 0.0)
            outputs[2].data.masked_fill_(mask[:, 0, :], 1000.0)
        return outputs

    def forward(self, inputs):
        text_inputs, text_lengths, obs_labels, mels, max_len, output_lengths = inputs
        text_lengths, output_lengths = text_lengths.data, output_lengths.data
        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)
        encoder_outputs = self.encoder(embedded_inputs, text_lengths)
        obs = None
        if self.obs_embedding is not None:
            obs = self.obs_embedding(obs_labels)
        lat, lat_mu, lat_logvar = None, None, None
        if self.lat_encoder is not None:
            lat, lat_mu, lat_logvar = self.lat_encoder(mels, output_lengths)
        obs_and_lat = [x for x in [obs, lat] if x is not None]
        if bool(obs_and_lat):
            obs_and_lat = torch.cat(obs_and_lat, dim=-1)
        else:
            obs_and_lat = None
        mel_outputs, gate_outputs, alignments = self.decoder(encoder_outputs, obs_and_lat, mels, memory_lengths=text_lengths)
        mel_outputs_postnet = self.postnet(mel_outputs)
        mel_outputs_postnet = mel_outputs + mel_outputs_postnet
        return self.parse_output([mel_outputs, mel_outputs_postnet, gate_outputs, alignments, lat_mu, lat_logvar], output_lengths)

    def inference(self, inputs, obs_labels=None, lat=None, ret_has_eos=False):
        embedded_inputs = self.embedding(inputs).transpose(1, 2)
        encoder_outputs = self.encoder.inference(embedded_inputs)
        if obs_labels is None:
            obs_labels = torch.LongTensor(len(inputs))
            obs_labels = obs_labels.zero_()
        obs = None
        if self.obs_embedding is not None:
            obs = self.obs_embedding(obs_labels)
        if self.lat_encoder is not None:
            if lat is None:
                lat = torch.FloatTensor(len(inputs), self.lat_encoder.lat_dim)
                lat = lat.zero_().type(encoder_outputs.type())
        obs_and_lat = [x for x in [obs, lat] if x is not None]
        if bool(obs_and_lat):
            obs_and_lat = torch.cat(obs_and_lat, dim=-1)
        else:
            obs_and_lat = None
        mel_outputs, gate_outputs, alignments, has_eos = self.decoder.inference(encoder_outputs, obs_and_lat, ret_has_eos=True)
        mel_outputs_postnet = self.postnet(mel_outputs)
        mel_outputs_postnet = mel_outputs + mel_outputs_postnet
        outputs = self.parse_output([mel_outputs, mel_outputs_postnet, gate_outputs, alignments])
        if ret_has_eos:
            return outputs + [has_eos]
        else:
            return outputs


class Denoiser(torch.nn.Module):
    """ Removes model bias from audio produced with waveglow """

    def __init__(self, waveglow, filter_length=1024, n_overlap=4, win_length=1024, mode='zeros'):
        super(Denoiser, self).__init__()
        self.stft = STFT(filter_length=filter_length, hop_length=int(filter_length / n_overlap), win_length=win_length)
        if mode == 'zeros':
            mel_input = torch.zeros((1, 80, 88), dtype=waveglow.upsample.weight.dtype, device=waveglow.upsample.weight.device)
        elif mode == 'normal':
            mel_input = torch.randn((1, 80, 88), dtype=waveglow.upsample.weight.dtype, device=waveglow.upsample.weight.device)
        else:
            raise Exception('Mode {} if not supported'.format(mode))
        with torch.no_grad():
            bias_audio = waveglow.infer(mel_input, sigma=0.0).float()
            bias_spec, _ = self.stft.transform(bias_audio)
        self.register_buffer('bias_spec', bias_spec[:, :, 0][:, :, None])

    def forward(self, audio, strength=0.1):
        audio_spec, audio_angles = self.stft.transform(audio.float())
        audio_spec_denoised = audio_spec - self.bias_spec * strength
        audio_spec_denoised = torch.clamp(audio_spec_denoised, 0.0)
        audio_denoised = self.stft.inverse(audio_spec_denoised, audio_angles)
        return audio_denoised


class Naive_F0_Decoder(torch.nn.Module):

    def __init__(self, bounds_path, n_units=32):
        super().__init__()
        bounds = torch.load(bounds_path)
        bounds = torch.from_numpy(bounds[n_units])
        assert bounds.ndim == 1
        pad = torch.tensor([-5.0, -5.0])
        centers = torch.cat([bounds[0:1], 0.5 * (bounds[1:] + bounds[:-1]), bounds[-1:], pad[:]])
        self.embedding = torch.nn.Embedding.from_pretrained(centers.unsqueeze(-1), freeze=True)
        self.max_n = self.embedding.weight.numel()

    def forward(self, discrete_f0: torch.Tensor):
        in_bounds = (0 <= discrete_f0).all() and (discrete_f0 < self.max_n).all()
        if not in_bounds:
            warnings.warn(f'F0 contains some weird outputs: discrete_f0.max().item()={discrete_f0.max().item()} discrete_f0.min().item()={discrete_f0.min().item()}; while we have embeddings for {self.max_n} values. Assuming this is a no-prosody model -- but be careful!')
            mask = discrete_f0 >= self.max_n
            discrete_f0 = discrete_f0.masked_fill(mask, self.max_n - 1)
        return self.embedding(discrete_f0).squeeze(-1)


class MeanPoolGatingNetwork(torch.nn.Module):
    """A simple mean-pooling gating network for selecting experts.

    This module applies mean pooling over an encoder's output and returns
    reponsibilities for each expert. The encoder format is expected to match
    :class:`fairseq.models.transformer.TransformerEncoder`.
    """

    def __init__(self, embed_dim, num_experts, dropout=None):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_experts = num_experts
        self.fc1 = torch.nn.Linear(embed_dim, embed_dim)
        self.dropout = torch.nn.Dropout(dropout) if dropout is not None else None
        self.fc2 = torch.nn.Linear(embed_dim, num_experts)

    def forward(self, encoder_out):
        if not ('encoder_out' in encoder_out and 'encoder_padding_mask' in encoder_out and encoder_out['encoder_out'][0].size(2) == self.embed_dim):
            raise ValueError('Unexpected format for encoder_out')
        encoder_padding_mask = encoder_out['encoder_padding_mask'][0]
        encoder_out = encoder_out['encoder_out'][0].transpose(0, 1)
        if encoder_padding_mask is not None:
            encoder_out = encoder_out.clone()
            encoder_out[encoder_padding_mask] = 0
            ntokens = torch.sum(~encoder_padding_mask, dim=1, keepdim=True)
            x = torch.sum(encoder_out, dim=1) / ntokens.type_as(encoder_out)
        else:
            x = torch.mean(encoder_out, dim=1)
        x = torch.tanh(self.fc1(x))
        if self.dropout is not None:
            x = self.dropout(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=-1, dtype=torch.float32).type_as(x)


class SegmentationType(Enum):
    NONE = auto()
    RANDOM = auto()
    UNIFORM_RANDOM = auto()
    UNIFORM_RANDOM_JOIN = auto()
    JOIN = auto()


LRELU_SLOPE = 0.1


def get_padding(kernel_size, dilation=1):
    return (kernel_size * dilation - dilation) // 2


def init_weights(m, mean=0.0, std=0.01):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        m.weight.data.normal_(mean, std)


class ResBlock(torch.nn.Module):

    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):
        super(ResBlock, self).__init__()
        self.convs1 = nn.ModuleList([weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0], padding=get_padding(kernel_size, dilation[0]))), weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1], padding=get_padding(kernel_size, dilation[1]))), weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2], padding=get_padding(kernel_size, dilation[2])))])
        self.convs1.apply(init_weights)
        self.convs2 = nn.ModuleList([weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))), weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1))), weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1, padding=get_padding(kernel_size, 1)))])
        self.convs2.apply(init_weights)

    def forward(self, x):
        for c1, c2 in zip(self.convs1, self.convs2):
            xt = F.leaky_relu(x, LRELU_SLOPE)
            xt = c1(xt)
            xt = F.leaky_relu(xt, LRELU_SLOPE)
            xt = c2(xt)
            x = xt + x
        return x

    def remove_weight_norm(self):
        for layer in self.convs1:
            remove_weight_norm(layer)
        for layer in self.convs2:
            remove_weight_norm(layer)


class Generator(torch.nn.Module):

    def __init__(self, cfg):
        super(Generator, self).__init__()
        self.num_kernels = len(cfg['resblock_kernel_sizes'])
        self.num_upsamples = len(cfg['upsample_rates'])
        self.conv_pre = weight_norm(Conv1d(cfg.get('model_in_dim', 80), cfg['upsample_initial_channel'], 7, 1, padding=3))
        self.ups = nn.ModuleList()
        for i, (u, k) in enumerate(zip(cfg['upsample_rates'], cfg['upsample_kernel_sizes'])):
            self.ups.append(weight_norm(ConvTranspose1d(cfg['upsample_initial_channel'] // 2 ** i, cfg['upsample_initial_channel'] // 2 ** (i + 1), k, u, padding=(k - u) // 2)))
        self.resblocks = nn.ModuleList()
        for i in range(len(self.ups)):
            ch = cfg['upsample_initial_channel'] // 2 ** (i + 1)
            for k, d in zip(cfg['resblock_kernel_sizes'], cfg['resblock_dilation_sizes']):
                self.resblocks.append(ResBlock(ch, k, d))
        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))
        self.ups.apply(init_weights)
        self.conv_post.apply(init_weights)

    def forward(self, x):
        x = self.conv_pre(x)
        for i in range(self.num_upsamples):
            x = F.leaky_relu(x, LRELU_SLOPE)
            x = self.ups[i](x)
            xs = None
            for j in range(self.num_kernels):
                if xs is None:
                    xs = self.resblocks[i * self.num_kernels + j](x)
                else:
                    xs += self.resblocks[i * self.num_kernels + j](x)
            x = xs / self.num_kernels
        x = F.leaky_relu(x)
        x = self.conv_post(x)
        x = torch.tanh(x)
        return x

    def remove_weight_norm(self):
        None
        for layer in self.ups:
            remove_weight_norm(layer)
        for layer in self.resblocks:
            layer.remove_weight_norm()
        remove_weight_norm(self.conv_pre)
        remove_weight_norm(self.conv_post)


class PretrainedWav2VecModel(nn.Module):

    def __init__(self, fname):
        super().__init__()
        model, cfg, task = fairseq.checkpoint_utils.load_model_ensemble_and_task([fname])
        model = model[0]
        model.eval()
        self.model = model

    def forward(self, x):
        with torch.no_grad():
            z = self.model.feature_extractor(x)
            if isinstance(z, tuple):
                z = z[0]
            c = self.model.feature_aggregator(z)
        return z, c


class FairseqEncoder(nn.Module):
    """Base class for encoders."""

    def __init__(self, dictionary):
        super().__init__()
        self.dictionary = dictionary

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (LongTensor): lengths of each source sentence of shape
                `(batch)`
        """
        raise NotImplementedError

    def forward_torchscript(self, net_input: Dict[str, Tensor]):
        """A TorchScript-compatible version of forward.

        Encoders which use additional arguments may want to override
        this method for TorchScript compatibility.
        """
        if torch.jit.is_scripting():
            return self.forward(src_tokens=net_input['src_tokens'], src_lengths=net_input['src_lengths'])
        else:
            return self.forward_non_torchscript(net_input)

    @torch.jit.unused
    def forward_non_torchscript(self, net_input: Dict[str, Tensor]):
        encoder_input = {k: v for k, v in net_input.items() if k != 'prev_output_tokens'}
        return self.forward(**encoder_input)

    def reorder_encoder_out(self, encoder_out, new_order):
        """
        Reorder encoder output according to `new_order`.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            `encoder_out` rearranged according to `new_order`
        """
        raise NotImplementedError

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        return 1000000.0

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade old state dicts to work with newer code."""
        return state_dict

    def set_num_updates(self, num_updates):
        """State from trainer to pass along to model at every update."""

        def _apply(m):
            if hasattr(m, 'set_num_updates') and m != self:
                m.set_num_updates(num_updates)
        self.apply(_apply)


def check_type(module, expected_type):
    if hasattr(module, 'unwrapped_module'):
        assert isinstance(module.unwrapped_module, expected_type), f'{type(module.unwrapped_module)} != {expected_type}'
    else:
        assert isinstance(module, expected_type), f'{type(module)} != {expected_type}'


def lengths_to_padding_mask(lengths: Tensor) ->Tensor:
    """Convert lengths of shape (B, ) to padding mask."""
    batch_size = lengths.shape[0]
    max_length = int(torch.max(lengths).item())
    padding_mask = torch.arange(max_length, device=lengths.device, dtype=lengths.dtype).expand(batch_size, max_length) >= lengths.unsqueeze(1)
    return padding_mask


def lengths_to_mask(lens):
    return ~lengths_to_padding_mask(lens)


def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=None, reduce=True):
    if target.dim() == lprobs.dim() - 1:
        target = target.unsqueeze(-1)
    nll_loss = -lprobs.gather(dim=-1, index=target)
    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)
    if ignore_index is not None:
        pad_mask = target.eq(ignore_index)
        nll_loss.masked_fill_(pad_mask, 0.0)
        smooth_loss.masked_fill_(pad_mask, 0.0)
    else:
        nll_loss = nll_loss.squeeze(-1)
        smooth_loss = smooth_loss.squeeze(-1)
    if reduce:
        nll_loss = nll_loss.sum()
        smooth_loss = smooth_loss.sum()
    eps_i = epsilon / (lprobs.size(-1) - 1)
    loss = (1.0 - epsilon - eps_i) * nll_loss + eps_i * smooth_loss
    return loss, nll_loss


def compute_kl_loss(model, net_output, pad_mask=None, reduce=True):
    net_prob = model.get_normalized_probs(net_output, log_probs=True)
    net_prob_tec = model.get_normalized_probs(net_output, log_probs=False)
    net_prob = net_prob.view(-1, net_prob.size(-1))
    net_prob_tec = net_prob_tec.view(-1, net_prob_tec.size(-1))
    p, q = torch.split(net_prob, net_prob.size(0) // 2, dim=0)
    p_tec, q_tec = torch.split(net_prob_tec, net_prob_tec.size(0) // 2, dim=0)
    p_loss = torch.nn.functional.kl_div(p, q_tec, reduction='none')
    q_loss = torch.nn.functional.kl_div(q, p_tec, reduction='none')
    if pad_mask is not None:
        p_loss.masked_fill_(pad_mask, 0.0)
        q_loss.masked_fill_(pad_mask, 0.0)
    if reduce:
        p_loss = p_loss.sum()
        q_loss = q_loss.sum()
    loss = (p_loss + q_loss) / 2
    return loss


def duplicate_input(sample):
    if 'net_input' in sample.keys():
        sample_input = sample['net_input']
    else:
        sample_input = sample
    for k, v in sample_input.items():
        if isinstance(v, torch.Tensor):
            sample_input[k] = torch.cat([v, v.clone()], dim=0)
    if 'net_input' in sample.keys():
        sample['net_input'] = sample_input
    else:
        sample = sample_input
    return sample


def compute_cross_entropy_loss(logits, targets, ignore_index=-100):
    """
    Function to compute the cross entropy loss. The default value of
    ignore_index is the same as the default value for F.cross_entropy in
    pytorch.
    """
    assert logits.size(0) == targets.size(-1), "Logits and Targets tensor shapes don't match up"
    loss = F.nll_loss(F.log_softmax(logits, -1, dtype=torch.float32), targets, reduction='sum', ignore_index=ignore_index)
    return loss


def safe_round(number, ndigits):
    if hasattr(number, '__round__'):
        return round(number, ndigits)
    elif torch is not None and torch.is_tensor(number) and number.numel() == 1:
        return safe_round(number.item(), ndigits)
    elif np is not None and np.ndim(number) == 0 and hasattr(number, 'item'):
        return safe_round(number.item(), ndigits)
    else:
        return number


def simple_accuracy(preds, labels):
    return (preds == labels).mean()


def acc_and_f1(preds, labels):
    acc = simple_accuracy(preds, labels)
    f1 = f1_score(y_true=labels, y_pred=preds)
    return {'acc': acc, 'f1': f1, 'acc_and_f1': (acc + f1) / 2}


def matthews_corrcoef(preds, labels):
    mcc = _matthews_corrcoef(labels, preds)
    return mcc


def pearson_and_spearman(preds, labels):
    pearson_corr = pearsonr(preds, labels)[0]
    spearman_corr = spearmanr(preds, labels)[0]
    return {'pearson': pearson_corr, 'spearmanr': spearman_corr, 'corr': (pearson_corr + spearman_corr) / 2}


def post_process(sentence: str, symbol: str):
    if symbol == 'sentencepiece':
        sentence = sentence.replace(' ', '').replace('', ' ').strip()
    elif symbol == 'wordpiece':
        sentence = sentence.replace(' ', '').replace('_', ' ').strip()
    elif symbol == 'letter':
        sentence = sentence.replace(' ', '').replace('|', ' ').strip()
    elif symbol == 'silence':
        import re
        sentence = sentence.replace('<SIL>', '')
        sentence = re.sub(' +', ' ', sentence).strip()
    elif symbol == '_EOW':
        sentence = sentence.replace(' ', '').replace('_EOW', ' ').strip()
    elif symbol in {'subword_nmt', '@@ ', '@@'}:
        if symbol == 'subword_nmt':
            symbol = '@@ '
        sentence = (sentence + ' ').replace(symbol, '').rstrip()
    elif symbol == 'none':
        pass
    elif symbol is not None:
        raise NotImplementedError(f'Unknown post_process option: {symbol}')
    return sentence


class MultitaskCriterion:

    def __init__(self, multitask_tasks, rdrop_alpha=0.0):
        self.rdrop_alpha = rdrop_alpha
        self.rdrop_alpha_mtl = rdrop_alpha
        self.multitask_criterion = OrderedDict()
        self.multitask_loss_weight = OrderedDict()
        for task_name, task_obj in multitask_tasks.items():
            if task_obj.args.get_loss_weight(0) == 0:
                logger.info(f'Skip {task_name} loss criterion')
                continue
            rdrop_alpha_task = task_obj.args.rdrop_alpha
            if rdrop_alpha_task is None:
                rdrop_alpha_task = rdrop_alpha
            self.rdrop_alpha_mtl = rdrop_alpha_task
            logger.info(f'rdrop_alpha is set to {rdrop_alpha_task} for {task_name}')
            if task_obj.args.decoder_type == 'ctc':
                self.multitask_criterion[task_name] = CtcCriterion(task_obj.args.criterion_cfg, task_obj, rdrop_alpha=rdrop_alpha_task)
            else:
                self.multitask_criterion[task_name] = RdropLabelSmoothedCrossEntropyCriterion(task_obj, task_obj.args.criterion_cfg.sentence_avg, label_smoothing=task_obj.args.criterion_cfg.label_smoothing, rdrop_alpha=rdrop_alpha_task)

    def set_multitask_loss_weight(self, task_name, weight=0.0):
        self.multitask_loss_weight[task_name] = weight

    def get_multitask_loss(self, model, sample, model_out):
        logging_output = {}
        loss = 0.0
        for task_name, task_criterion in self.multitask_criterion.items():
            layer_id = task_criterion.task.args.input_layer
            if isinstance(task_criterion, CtcCriterion):
                if task_criterion.task.args.input_from == 'encoder':
                    if len(model_out['encoder_padding_mask']) > 0:
                        non_padding_mask = ~model_out['encoder_padding_mask'][0]
                        input_lengths = non_padding_mask.long().sum(-1)
                    else:
                        out = model_out['encoder_states'][layer_id]
                        input_lengths = out.new_full((out.shape[1],), out.shape[0]).long()
                    task_sample = {'net_input': {'src_tokens': model_out['encoder_states'][layer_id], 'src_lengths': input_lengths}, 'id': sample['id']}
                else:
                    task_sample = {'net_input': {'src_tokens': model_out['inner_states'][layer_id], 'src_lengths': sample['target_lengths']}, 'id': sample['id']}
            else:
                task_sample = {'net_input': {'src_tokens': sample['multitask'][task_name]['net_input']['prev_output_tokens'], 'encoder_out': {'encoder_out': [model_out['encoder_states'][layer_id]], 'encoder_padding_mask': model_out['encoder_padding_mask']}}}
            for key in ['target', 'target_lengths', 'ntokens']:
                task_sample[key] = sample['multitask'][task_name][key]
            if task_name == getattr(model, 'mt_task_name', None):
                decoder_out = model_out['mt_decoder_out']
            else:
                decoder_out = None
            task_loss, task_sample_size, task_logging_output = task_criterion(model.multitask_decoders[task_name], task_sample, net_output=decoder_out)
            loss = loss + self.multitask_loss_weight[task_name] * task_loss
            task_logging_output['loss_weight'] = self.multitask_loss_weight[task_name]
            logging_output[task_name] = task_logging_output
        return loss, logging_output

    @classmethod
    def reduce_metrics(cls, logging_outputs) ->None:
        for task_name in logging_outputs[0]['multitask'].keys():
            loss_sum = sum(log['multitask'][task_name].get('loss', 0) for log in logging_outputs)
            sample_size = sum(log['multitask'][task_name].get('sample_size', 0) for log in logging_outputs)
            metrics.log_scalar(f'multitask_{task_name}_loss', loss_sum / sample_size / math.log(2), sample_size, round=3)
            loss_weight = logging_outputs[0]['multitask'][task_name].get('loss_weight', 0)
            metrics.log_scalar(f'multitask_{task_name}_loss_weight', loss_weight, weight=0, priority=250)


def mae_loss(pred, targ, mask, reduce=True):
    if pred.ndim == 3:
        pred = pred.squeeze(2)
    else:
        assert pred.ndim == 2
    loss = (pred.float() - targ.float()).abs() * (~mask).float()
    loss = loss.sum() if reduce else loss.view(-1)
    return loss


def nll_loss(pred, targ, mask, reduce=True):
    lprob = F.log_softmax(pred, dim=-1)
    loss = F.nll_loss(lprob.view(-1, lprob.size(-1)), targ.view(-1), reduction='none')
    loss = loss * (~mask).float().view(-1)
    loss = loss.sum() if reduce else loss.view(-1)
    return loss


class GuidedAttentionLoss(torch.nn.Module):
    """
    Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
    Networks with Guided Attention (https://arxiv.org/abs/1710.08969)
    """

    def __init__(self, sigma):
        super().__init__()
        self.sigma = sigma

    @staticmethod
    @lru_cache(maxsize=8)
    def _get_weight(s_len, t_len, sigma):
        grid_x, grid_y = torch.meshgrid(torch.arange(t_len), torch.arange(s_len))
        grid_x = grid_x
        grid_y = grid_y
        w = (grid_y.float() / s_len - grid_x.float() / t_len) ** 2
        return 1.0 - torch.exp(-w / (2 * sigma ** 2))

    def _get_weights(self, src_lens, tgt_lens):
        bsz, max_s_len, max_t_len = len(src_lens), max(src_lens), max(tgt_lens)
        weights = torch.zeros((bsz, max_t_len, max_s_len))
        for i, (s_len, t_len) in enumerate(zip(src_lens, tgt_lens)):
            weights[i, :t_len, :s_len] = self._get_weight(s_len, t_len, self.sigma)
        return weights

    @staticmethod
    def _get_masks(src_lens, tgt_lens):
        in_masks = lengths_to_mask(src_lens)
        out_masks = lengths_to_mask(tgt_lens)
        return out_masks.unsqueeze(2) & in_masks.unsqueeze(1)

    def forward(self, attn, src_lens, tgt_lens, reduction='mean'):
        weights = self._get_weights(src_lens, tgt_lens)
        masks = self._get_masks(src_lens, tgt_lens)
        loss = (weights * attn.transpose(1, 2)).masked_select(masks)
        loss = torch.sum(loss) if reduction == 'sum' else torch.mean(loss)
        return loss


def get_fourier_basis(n_fft: int) ->torch.Tensor:
    basis = np.fft.fft(np.eye(n_fft))
    basis = np.vstack([np.real(basis[:n_fft // 2 + 1, :]), np.imag(basis[:n_fft // 2 + 1, :])])
    return torch.from_numpy(basis).float()


class TTSSpectrogram(torch.nn.Module):

    def __init__(self, n_fft: int, win_length: int, hop_length: int, window_fn: callable=torch.hann_window, return_phase: bool=False) ->None:
        super(TTSSpectrogram, self).__init__()
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.return_phase = return_phase
        basis = get_fourier_basis(n_fft).unsqueeze(1)
        basis *= get_window(window_fn, n_fft, win_length)
        self.register_buffer('basis', basis)

    def forward(self, waveform: torch.Tensor) ->Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        padding = self.n_fft // 2, self.n_fft // 2
        x = F.pad(waveform.unsqueeze(1), padding, mode='reflect')
        x = F.conv1d(x, self.basis, stride=self.hop_length)
        real_part = x[:, :self.n_fft // 2 + 1, :]
        imag_part = x[:, self.n_fft // 2 + 1:, :]
        magnitude = torch.sqrt(real_part ** 2 + imag_part ** 2)
        if self.return_phase:
            phase = torch.atan2(imag_part, real_part)
            return magnitude, phase
        return magnitude


class TTSMelScale(torch.nn.Module):

    def __init__(self, n_mels: int, sample_rate: int, f_min: float, f_max: float, n_stft: int) ->None:
        super(TTSMelScale, self).__init__()
        basis = get_mel_filters(sample_rate, (n_stft - 1) * 2, n_mels, f_min, f_max)
        self.register_buffer('basis', basis)

    def forward(self, specgram: torch.Tensor) ->torch.Tensor:
        return torch.matmul(self.basis, specgram)


class ModuleProxyWrapper(nn.Module):
    """
    Wrap a DistributedDataParallel module and forward requests for missing
    attributes to the module wrapped by DDP (the twice-wrapped module).
    Also forward calls to :func:`state_dict` and :func:`load_state_dict`.

    Usage::

        module.xyz = "hello world"
        wrapped_module = DistributedDataParallel(module, **ddp_args)
        wrapped_module = ModuleProxyWrapper(wrapped_module)
        assert wrapped_module.xyz == "hello world"
        assert wrapped_module.state_dict().keys() == module.state_dict().keys()

    Args:
        module (nn.Module): module to wrap
    """

    def __init__(self, module: nn.Module):
        super().__init__()
        assert hasattr(module, 'module'), 'ModuleProxyWrapper expects input to wrap another module'
        self.module = module

    def __getattr__(self, name):
        """Forward missing attributes to twice-wrapped module."""
        try:
            return super().__getattr__(name)
        except AttributeError:
            try:
                return getattr(self.module, name)
            except AttributeError:
                return getattr(self.module.module, name)

    def state_dict(self, *args, **kwargs):
        """Forward to the twice-wrapped module."""
        return self.module.module.state_dict(*args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        """Forward to the twice-wrapped module."""
        return self.module.module.load_state_dict(*args, **kwargs)

    def forward(self, *args, **kwargs):
        return self.module(*args, **kwargs)


class TPUDistributedDataParallel(nn.Module):

    def __init__(self, module, process_group):
        super().__init__()
        self.module = module
        self.process_group = process_group
        self.world_size = utils.get_world_size(self.process_group)

    def forward(self, *inputs, **kwargs):
        return self.module(*inputs, **kwargs)

    def all_reduce_grads(self):
        gradients = []
        for p in self.parameters():
            if not p.requires_grad:
                continue
            if p.grad is None:
                p.grad = torch.zeros_like(p)
            if p.grad.requires_grad:
                raise RuntimeError("TPUDistributedDataParallel only works with gradients that don't require grad")
            gradients.append(p.grad)
        xm.all_reduce('sum', gradients, scale=1.0 / self.world_size, groups=self.process_group[1])


class GeneratorHubInterface(nn.Module):
    """
    PyTorch Hub interface for generating sequences from a pre-trained
    translation or language model.
    """

    def __init__(self, cfg, task, models):
        super().__init__()
        self.cfg = cfg
        self.task = task
        self.models = nn.ModuleList(models)
        self.src_dict = task.source_dictionary
        self.tgt_dict = task.target_dictionary
        for model in self.models:
            model.prepare_for_inference_(cfg)
        self.align_dict = utils.load_align_dict(cfg.generation.replace_unk)
        self.tokenizer = encoders.build_tokenizer(cfg.tokenizer)
        self.bpe = encoders.build_bpe(cfg.bpe)
        self.max_positions = utils.resolve_max_positions(self.task.max_positions(), *[model.max_positions() for model in models])
        self.register_buffer('_float_tensor', torch.tensor([0], dtype=torch.float))

    @property
    def device(self):
        return self._float_tensor.device

    def translate(self, sentences: List[str], beam: int=5, verbose: bool=False, **kwargs) ->List[str]:
        return self.sample(sentences, beam, verbose, **kwargs)

    def sample(self, sentences: List[str], beam: int=1, verbose: bool=False, **kwargs) ->List[str]:
        if isinstance(sentences, str):
            return self.sample([sentences], beam=beam, verbose=verbose, **kwargs)[0]
        tokenized_sentences = [self.encode(sentence) for sentence in sentences]
        batched_hypos = self.generate(tokenized_sentences, beam, verbose, **kwargs)
        return [self.decode(hypos[0]['tokens']) for hypos in batched_hypos]

    def score(self, sentences: List[str], replace_newline_with_eos: bool=False, **kwargs):
        if isinstance(sentences, str):
            return self.score([sentences], replace_newline_with_eos=replace_newline_with_eos, **kwargs)[0]

        def encode(sentence):
            if replace_newline_with_eos:
                return torch.cat([self.encode(line) for line in sentence.splitlines()])
            else:
                return self.encode(sentence)
        tokenized_sentences = [encode(sentence) for sentence in sentences]
        return [hypos[0] for hypos in self.generate(tokenized_sentences, score_reference=True, **kwargs)]

    def generate(self, tokenized_sentences: List[torch.LongTensor], beam: int=5, verbose: bool=False, skip_invalid_size_inputs=False, inference_step_args=None, prefix_allowed_tokens_fn=None, **kwargs) ->List[List[Dict[str, torch.Tensor]]]:
        if torch.is_tensor(tokenized_sentences) and tokenized_sentences.dim() == 1:
            return self.generate(tokenized_sentences.unsqueeze(0), beam=beam, verbose=verbose, **kwargs)[0]
        gen_args = copy.deepcopy(self.cfg.generation)
        with open_dict(gen_args):
            gen_args.beam = beam
            for k, v in kwargs.items():
                setattr(gen_args, k, v)
        generator = self.task.build_generator(self.models, gen_args, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)
        inference_step_args = inference_step_args or {}
        results = []
        for batch in self._build_batches(tokenized_sentences, skip_invalid_size_inputs):
            batch = utils.apply_to_sample(lambda t: t, batch)
            translations = self.task.inference_step(generator, self.models, batch, **inference_step_args)
            for id, hypos in zip(batch['id'].tolist(), translations):
                results.append((id, hypos))
        outputs = [hypos for _, hypos in sorted(results, key=lambda x: x[0])]
        if verbose:

            def getarg(name, default):
                return getattr(gen_args, name, getattr(self.cfg, name, default))
            for source_tokens, target_hypotheses in zip(tokenized_sentences, outputs):
                src_str_with_unk = self.string(source_tokens)
                logger.info('S\t{}'.format(src_str_with_unk))
                for hypo in target_hypotheses:
                    hypo_str = self.decode(hypo['tokens'])
                    logger.info('H\t{}\t{}'.format(hypo['score'], hypo_str))
                    logger.info('P\t{}'.format(' '.join(map(lambda x: '{:.4f}'.format(x), hypo['positional_scores'].tolist()))))
                    if hypo['alignment'] is not None and getarg('print_alignment', False):
                        logger.info('A\t{}'.format(' '.join(['{}-{}'.format(src_idx, tgt_idx) for src_idx, tgt_idx in hypo['alignment']])))
        return outputs

    def encode(self, sentence: str) ->torch.LongTensor:
        sentence = self.tokenize(sentence)
        sentence = self.apply_bpe(sentence)
        return self.binarize(sentence)

    def decode(self, tokens: torch.LongTensor) ->str:
        sentence = self.string(tokens)
        sentence = self.remove_bpe(sentence)
        return self.detokenize(sentence)

    def tokenize(self, sentence: str) ->str:
        if self.tokenizer is not None:
            sentence = self.tokenizer.encode(sentence)
        return sentence

    def detokenize(self, sentence: str) ->str:
        if self.tokenizer is not None:
            sentence = self.tokenizer.decode(sentence)
        return sentence

    def apply_bpe(self, sentence: str) ->str:
        if self.bpe is not None:
            sentence = self.bpe.encode(sentence)
        return sentence

    def remove_bpe(self, sentence: str) ->str:
        if self.bpe is not None:
            sentence = self.bpe.decode(sentence)
        return sentence

    def binarize(self, sentence: str) ->torch.LongTensor:
        return self.src_dict.encode_line(sentence, add_if_not_exist=False).long()

    def string(self, tokens: torch.LongTensor) ->str:
        return self.tgt_dict.string(tokens)

    def _build_batches(self, tokens: List[List[int]], skip_invalid_size_inputs: bool) ->Iterator[Dict[str, Any]]:
        lengths = torch.LongTensor([t.numel() for t in tokens])
        batch_iterator = self.task.get_batch_iterator(dataset=self.task.build_dataset_for_inference(tokens, lengths), max_tokens=self.cfg.dataset.max_tokens, max_sentences=self.cfg.dataset.batch_size, max_positions=self.max_positions, ignore_invalid_inputs=skip_invalid_size_inputs, disable_iterator_cache=True).next_epoch_itr(shuffle=False)
        return batch_iterator


class TransformerEncoderEmbedding(nn.Module):
    """Encoder Embedding + Positional Embedding"""

    def __init__(self, args, embed_tokens):
        super().__init__()
        self.dropout = args.dropout
        self.max_source_positions = args.max_source_positions
        self.embed_tokens = embed_tokens
        if isinstance(embed_tokens, nn.ModuleList):
            self.padding_idx = embed_tokens[0].padding_idx
            embed_dim = sum(e.embedding_dim for e in embed_tokens)
        else:
            self.padding_idx = embed_tokens.padding_idx
            embed_dim = embed_tokens.embedding_dim
        self.embed_scale = math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(args.max_source_positions, embed_dim, self.padding_idx, learned=args.encoder_learned_pos) if not args.no_token_positional_embeddings else None
        if getattr(args, 'layernorm_embedding', False):
            self.layernorm_embedding = LayerNorm(embed_dim)
        else:
            self.layernorm_embedding = None

    def forward(self, input):
        src_tokens = input[0]
        prev_output_tokens = input[2]
        if isinstance(self.embed_tokens, nn.ModuleList):
            x_embed_list = []
            for embed_tokens_part in self.embed_tokens:
                x_embed_list.append(embed_tokens_part(src_tokens))
            embedded = torch.cat(x_embed_list, dim=-1)
        else:
            embedded = self.embed_tokens(src_tokens)
        x = embed = self.embed_scale * embedded
        if self.embed_positions is not None:
            x = embed + self.embed_positions(src_tokens)
        if self.layernorm_embedding:
            x = self.layernorm_embedding(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = x.transpose(0, 1)
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        return x, encoder_padding_mask, prev_output_tokens


class TransformerEncoderLayerNorm(nn.Module):
    """
    Layer norm at the the end of all encoder layers if
    args.encoder_enormalize_before = True
    """

    def __init__(self, args, embed_dim):
        super().__init__()
        if args.encoder_normalize_before:
            self.layer_norm = LayerNorm(embed_dim)
        else:
            self.layer_norm = None

    def forward(self, input):
        x = input[0]
        encoder_padding_mask = input[1]
        prev_output_tokens = input[2]
        if self.layer_norm:
            x = self.layer_norm(x)
        return x, encoder_padding_mask, prev_output_tokens


class TransformerDecoderEmbedding(nn.Module):
    """Decoder Embedding + Positional Embedding"""

    def __init__(self, args, embed_tokens):
        super().__init__()
        self.dropout = args.dropout
        self.share_input_output_embed = args.share_decoder_input_output_embed
        input_embed_dim = sum(e.embedding_dim for e in embed_tokens) if isinstance(embed_tokens, nn.ModuleList) else embed_tokens.embedding_dim
        embed_dim = args.decoder_embed_dim
        self.output_embed_dim = args.decoder_output_dim
        padding_idx = embed_tokens[0].padding_idx if isinstance(embed_tokens, nn.ModuleList) else embed_tokens.padding_idx
        self.max_target_positions = args.max_target_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None
        self.embed_positions = PositionalEmbedding(args.max_target_positions, embed_dim, padding_idx, learned=args.decoder_learned_pos) if not args.no_token_positional_embeddings else None

    def forward(self, input):
        mt_task = False
        if isinstance(input, tuple):
            if len(input) == 3:
                encoder_out = input[0]
                encoder_padding_mask = input[1]
                prev_output_tokens = input[2]
                incremental_state = None
                mt_task = True
            else:
                prev_output_tokens = input[0]
                encoder_out = None
                encoder_padding_mask = None
                incremental_state = None
        else:
            prev_output_tokens = input
            encoder_out = None
            encoder_padding_mask = None
            incremental_state = None
        positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state) if self.embed_positions is not None else None
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        if isinstance(self.embed_tokens, nn.ModuleList):
            x_embed_list = []
            for embed_tokens_part in self.embed_tokens:
                x_embed_list.append(embed_tokens_part(prev_output_tokens))
            x = self.embed_scale * torch.cat(x_embed_list, dim=-1)
        else:
            x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = x.transpose(0, 1)
        if mt_task:
            return x, encoder_out, encoder_padding_mask
        return x


class TiedLinear(nn.Module):

    def __init__(self, weight, transpose):
        super().__init__()
        self.weight = weight
        self.transpose = transpose

    def forward(self, input):
        return F.linear(input, self.weight.t() if self.transpose else self.weight)


class TiedHeadModule(nn.Module):

    def __init__(self, weights, input_dim, num_classes, q_noise, qn_block_size):
        super().__init__()
        tied_emb, _ = weights
        self.num_words, emb_dim = tied_emb.size()
        self.word_proj = quant_noise(TiedLinear(tied_emb, transpose=False), q_noise, qn_block_size)
        if input_dim != emb_dim:
            self.word_proj = nn.Sequential(quant_noise(nn.Linear(input_dim, emb_dim, bias=False), q_noise, qn_block_size), self.word_proj)
        self.class_proj = quant_noise(nn.Linear(input_dim, num_classes, bias=False), q_noise, qn_block_size)
        self.out_dim = self.num_words + num_classes
        self.register_buffer('_float_tensor', torch.FloatTensor(1))

    def forward(self, input):
        inp_sz = functools.reduce(operator.mul, input.shape[:-1], 1)
        out = self._float_tensor.new(inp_sz, self.out_dim)
        out[:, :self.num_words] = self.word_proj(input.view(inp_sz, -1))
        out[:, self.num_words:] = self.class_proj(input.view(inp_sz, -1))
        return out


class AdaptiveSoftmax(nn.Module):
    """
    This is an implementation of the efficient softmax approximation for
    graphical processing units (GPU), described in the paper "Efficient softmax
    approximation for GPUs" (http://arxiv.org/abs/1609.04309).
    """

    def __init__(self, vocab_size, input_dim, cutoff, dropout, factor=4.0, adaptive_inputs=None, tie_proj=False, q_noise=0, qn_block_size=8):
        super().__init__()
        if vocab_size > cutoff[-1]:
            cutoff = cutoff + [vocab_size]
        else:
            assert vocab_size == cutoff[-1], 'cannot specify cutoff larger than vocab size'
        output_dim = cutoff[0] + len(cutoff) - 1
        self.vocab_size = vocab_size
        self.cutoff = cutoff
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.input_dim = input_dim
        self.factor = factor
        self.q_noise = q_noise
        self.qn_block_size = qn_block_size
        self.lsm = nn.LogSoftmax(dim=1)
        if adaptive_inputs is not None:
            self.head = TiedHeadModule(adaptive_inputs.weights_for_band(0), input_dim, len(cutoff) - 1, self.q_noise, self.qn_block_size)
        else:
            self.head = quant_noise(nn.Linear(input_dim, output_dim, bias=False), self.q_noise, self.qn_block_size)
        self._make_tail(adaptive_inputs, tie_proj)

        def init_weights(m):
            if hasattr(m, 'weight') and not isinstance(m, TiedLinear) and not isinstance(m, TiedHeadModule):
                nn.init.xavier_uniform_(m.weight)
        self.apply(init_weights)
        self.register_buffer('version', torch.LongTensor([1]))

    def _make_tail(self, adaptive_inputs=None, tie_proj=False):
        self.tail = nn.ModuleList()
        for i in range(len(self.cutoff) - 1):
            dim = int(self.input_dim // self.factor ** (i + 1))
            tied_emb, tied_proj = adaptive_inputs.weights_for_band(i + 1) if adaptive_inputs is not None else (None, None)
            if tied_proj is not None:
                if tie_proj:
                    proj = quant_noise(TiedLinear(tied_proj, transpose=True), self.q_noise, self.qn_block_size)
                else:
                    proj = quant_noise(nn.Linear(tied_proj.size(0), tied_proj.size(1), bias=False), self.q_noise, self.qn_block_size)
            else:
                proj = quant_noise(nn.Linear(self.input_dim, dim, bias=False), self.q_noise, self.qn_block_size)
            if tied_emb is None:
                out_proj = nn.Linear(dim, self.cutoff[i + 1] - self.cutoff[i], bias=False)
            else:
                out_proj = TiedLinear(tied_emb, transpose=False)
            m = nn.Sequential(proj, nn.Dropout(self.dropout_module.p), quant_noise(out_proj, self.q_noise, self.qn_block_size))
            self.tail.append(m)

    def upgrade_state_dict_named(self, state_dict, name):
        version_name = name + '.version'
        if version_name not in state_dict:
            raise Exception('This version of the model is no longer supported')

    def adapt_target(self, target):
        """
        In order to be efficient, the AdaptiveSoftMax does not compute the
        scores for all the word of the vocabulary for all the examples. It is
        thus necessary to call the method adapt_target of the AdaptiveSoftMax
        layer inside each forward pass.
        """
        target = target.view(-1)
        new_target = [target.clone()]
        target_idxs = []
        for i in range(len(self.cutoff) - 1):
            mask = target.ge(self.cutoff[i]).mul(target.lt(self.cutoff[i + 1]))
            new_target[0][mask] = self.cutoff[0] + i
            if mask.any():
                target_idxs.append(mask.nonzero(as_tuple=False).squeeze(1))
                new_target.append(target[mask].add(-self.cutoff[i]))
            else:
                target_idxs.append(None)
                new_target.append(None)
        return new_target, target_idxs

    def forward(self, input, target):
        """
        Args:
            input: (b x t x d)
            target: (b x t)
        Returns:
            2 lists: output for each cutoff section and new targets by cut off
        """
        input = input.contiguous().view(-1, input.size(-1))
        input = self.dropout_module(input)
        new_target, target_idxs = self.adapt_target(target)
        output = [self.head(input)]
        for i in range(len(target_idxs)):
            if target_idxs[i] is not None:
                output.append(self.tail[i](input.index_select(0, target_idxs[i])))
            else:
                output.append(None)
        return output, new_target

    def get_log_prob(self, input, target):
        """
        Computes the log probabilities for all the words of the vocabulary,
        given a 2D tensor of hidden vectors.
        """
        bsz, length, dim = input.size()
        input = input.contiguous().view(-1, dim)
        if target is not None:
            _, target_idxs = self.adapt_target(target)
        else:
            target_idxs = None
        head_y = self.head(input)
        log_probs = head_y.new_zeros(input.size(0), self.vocab_size)
        head_sz = self.cutoff[0] + len(self.tail)
        log_probs[:, :head_sz] = self.lsm(head_y)
        tail_priors = log_probs[:, self.cutoff[0]:head_sz].clone()
        for i in range(len(self.tail)):
            start = self.cutoff[i]
            end = self.cutoff[i + 1]
            if target_idxs is None:
                tail_out = log_probs[:, start:end]
                tail_out.copy_(self.tail[i](input))
                log_probs[:, start:end] = self.lsm(tail_out).add_(tail_priors[:, i, None])
            elif target_idxs[i] is not None:
                idxs = target_idxs[i]
                tail_out = log_probs[idxs, start:end]
                tail_out.copy_(self.tail[i](input[idxs]))
                log_probs[idxs, start:end] = self.lsm(tail_out).add_(tail_priors[idxs, i, None])
        log_probs = log_probs.view(bsz, length, -1)
        return log_probs


class TransformerDecoderOutputLayer(nn.Module):

    def __init__(self, args, embed_tokens, dictionary):
        super().__init__()
        self.share_input_output_embed = args.share_decoder_input_output_embed
        self.embed_tokens = embed_tokens
        self.output_embed_dim = args.decoder_output_dim
        embed_dim = args.decoder_embed_dim
        self.project_out_dim = Linear(embed_dim, self.output_embed_dim, bias=False) if embed_dim != self.output_embed_dim and not args.tie_adaptive_weights else None
        self.adaptive_softmax = None
        if args.adaptive_softmax_cutoff is not None:
            assert not isinstance(embed_tokens, nn.ModuleList)
            self.adaptive_softmax = AdaptiveSoftmax(len(dictionary), self.output_embed_dim, options.eval_str_list(args.adaptive_softmax_cutoff, type=int), dropout=args.adaptive_softmax_dropout, adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None, factor=args.adaptive_softmax_factor, tie_proj=args.tie_adaptive_proj)
        elif not self.share_input_output_embed:
            self.embed_tokens = nn.Parameter(torch.Tensor(len(dictionary), self.output_embed_dim))
            nn.init.normal_(self.embed_tokens, mean=0, std=self.output_embed_dim ** -0.5)
        if args.decoder_normalize_before and not getattr(args, 'no_decoder_final_norm', False):
            self.layer_norm = LayerNorm(embed_dim)
        else:
            self.layer_norm = None

    def forward(self, input, apply_final_proj=True):
        if isinstance(input, tuple):
            x = input[0]
        else:
            x = input
        if self.layer_norm:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)
        if apply_final_proj:
            x = self.output_layer(x)
        return x

    def output_layer(self, features, **kwargs):
        """Project features to the vocabulary size."""
        if self.adaptive_softmax is None:
            if self.share_input_output_embed:
                if isinstance(self.embed_tokens, nn.ModuleList):
                    output = None
                    for i, emb in enumerate(self.embed_tokens):
                        sidx = i * emb.embedding_dim
                        eidx = (i + 1) * emb.embedding_dim
                        if output is None:
                            output = F.linear(features[:, :, sidx:eidx], emb.weight)
                        else:
                            output += F.linear(features[:, :, sidx:eidx], emb.weight)
                    return output
                else:
                    return F.linear(features, self.embed_tokens.weight)
            else:
                return F.linear(features, self.embed_tokens)
        else:
            return features


class ModelParallelRobertaLMHead(nn.Module):
    """Head for masked language modeling."""

    def __init__(self, embed_dim, output_dim, activation_fn, weight=None):
        super().__init__()
        self.dense = ColumnParallelLinear(embed_dim, embed_dim, gather_output=True)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.layer_norm = LayerNorm(embed_dim)
        if weight is None:
            weight = nn.Linear(embed_dim, output_dim, bias=False).weight
        self.weight = weight
        self.bias = nn.Parameter(torch.zeros(output_dim))

    def forward(self, features, masked_tokens=None, **kwargs):
        if masked_tokens is not None:
            features = features[masked_tokens, :]
        x = self.dense(features)
        x = self.activation_fn(x)
        x = self.layer_norm(x)
        x = copy_to_model_parallel_region(x)
        x = F.linear(x, self.weight)
        x = gather_from_model_parallel_region(x).contiguous()
        x = x + self.bias
        return x


class ModelParallelRobertaClassificationHead(nn.Module):
    """Head for sentence-level classification tasks."""

    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout):
        super().__init__()
        self.dense = ColumnParallelLinear(input_dim, inner_dim, gather_output=True)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.dropout = nn.Dropout(p=pooler_dropout)
        self.out_proj = nn.Linear(inner_dim, num_classes)

    def forward(self, features, **kwargs):
        x = features[:, 0, :]
        x = self.dropout(x)
        x = self.dense(x)
        x = self.activation_fn(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


def get_model_parallel_group():
    global _USE_MEGATRON
    if _USE_MEGATRON:
        return mpu.get_model_parallel_group()
    else:
        return None


def use_xla():
    global _USE_XLA
    return _USE_XLA


def get_global_rank():
    if use_xla():
        return xm.get_ordinal()
    elif torch.distributed.is_initialized():
        return torch.distributed.get_rank()
    else:
        return 0


def _find_my_group_index(grouped_ranks):
    my_rank = get_global_rank()
    for i, group in enumerate(grouped_ranks):
        if my_rank in group:
            return i
    raise RuntimeError


def _find_my_group(grouped_ranks):
    index = _find_my_group_index(grouped_ranks)
    return grouped_ranks[index]


def get_world_size(group):
    if use_xla():
        assert group[0] == 'tpu'
        my_group = _find_my_group(group[1])
        return len(my_group)
    elif torch.distributed.is_initialized():
        return dist.get_world_size(group=group)
    else:
        return 1


def get_model_parallel_world_size():
    """Return world size for the model parallel group."""
    return get_world_size(get_model_parallel_group())


@with_incremental_state
class ModelParallelMultiheadAttention(nn.Module):
    """Model parallel Multi-headed attention.
    This performs the Multi-headed attention over multiple gpus.

    See "Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf" for more details.
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, self_attention=False, encoder_decoder_attention=False):
        super().__init__()
        if not has_megatron_submodule:
            raise ImportError('\n\nPlease install the megatron submodule:\n\n  git submodule update --init fairseq/model_parallel/megatron')
        self.embed_dim = embed_dim
        self.kdim = kdim if kdim is not None else embed_dim
        self.vdim = vdim if vdim is not None else embed_dim
        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim
        self.model_parallel_size = get_model_parallel_world_size()
        self.num_heads_partition = num_heads // self.model_parallel_size
        assert self.num_heads_partition * self.model_parallel_size == num_heads, 'Number of heads must be divisible by model parallel size'
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, 'embed_dim must be divisible by num_heads'
        self.scaling = self.head_dim ** -0.5
        self.self_attention = self_attention
        self.encoder_decoder_attention = encoder_decoder_attention
        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and value to be of the same size'
        self.k_proj = ColumnParallelLinear(self.kdim, embed_dim, bias=bias, gather_output=False)
        self.v_proj = ColumnParallelLinear(self.vdim, embed_dim, bias=bias, gather_output=False)
        self.q_proj = ColumnParallelLinear(embed_dim, embed_dim, bias=bias, gather_output=False)
        self.out_proj = RowParallelLinear(embed_dim, embed_dim, bias=bias, input_is_parallel=True)

    def forward(self, query, key: Optional[Tensor], value: Optional[Tensor], key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, static_kv: bool=False, attn_mask: Optional[Tensor]=None, **unused_kwargs) ->Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time x Batch x Channel

        Args:
            key_padding_mask (ByteTensor, optional): mask to exclude
                keys that are pads, of shape `(batch, src_len)`, where
                padding elements are indicated by 1s.
            attn_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
        """
        tgt_len, bsz, embed_dim = query.size()
        assert embed_dim == self.embed_dim
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        is_tpu = query.device.type == 'xla'
        if incremental_state is not None:
            saved_state = self._get_input_buffer(incremental_state)
            if saved_state is not None and 'prev_key' in saved_state:
                if static_kv:
                    assert self.encoder_decoder_attention and not self.self_attention
                    key = value = None
        else:
            saved_state = None
        if self.self_attention:
            q = self.q_proj(query)
            k = self.k_proj(query)
            v = self.v_proj(query)
        elif self.encoder_decoder_attention:
            q = self.q_proj(query)
            if key is None:
                assert value is None
                k = v = None
            else:
                k = self.k_proj(key)
                v = self.v_proj(key)
        else:
            assert key is not None and value is not None
            q = self.q_proj(query)
            k = self.k_proj(key)
            v = self.v_proj(value)
        q *= self.scaling
        q = q.contiguous().view(tgt_len, bsz * self.num_heads_partition, self.head_dim).transpose(0, 1)
        if k is not None:
            k = k.contiguous().view(-1, bsz * self.num_heads_partition, self.head_dim).transpose(0, 1)
        if v is not None:
            v = v.contiguous().view(-1, bsz * self.num_heads_partition, self.head_dim).transpose(0, 1)
        if saved_state is not None:
            if 'prev_key' in saved_state:
                _prev_key = saved_state['prev_key']
                assert _prev_key is not None
                prev_key = _prev_key.view(bsz * self.num_heads_partition, -1, self.head_dim)
                if static_kv:
                    k = prev_key
                else:
                    assert k is not None
                    k = torch.cat([prev_key, k], dim=1)
            if 'prev_value' in saved_state:
                _prev_value = saved_state['prev_value']
                assert _prev_value is not None
                prev_value = _prev_value.view(bsz * self.num_heads_partition, -1, self.head_dim)
                if static_kv:
                    v = prev_value
                else:
                    assert v is not None
                    v = torch.cat([prev_value, v], dim=1)
            prev_key_padding_mask: Optional[Tensor] = None
            if 'prev_key_padding_mask' in saved_state:
                prev_key_padding_mask = saved_state['prev_key_padding_mask']
            assert k is not None and v is not None
            key_padding_mask = ModelParallelMultiheadAttention._append_prev_key_padding_mask(key_padding_mask=key_padding_mask, prev_key_padding_mask=prev_key_padding_mask, batch_size=bsz, src_len=k.size(1), static_kv=static_kv)
            saved_state['prev_key'] = k.view(bsz, self.num_heads_partition, -1, self.head_dim)
            saved_state['prev_value'] = v.view(bsz, self.num_heads_partition, -1, self.head_dim)
            saved_state['prev_key_padding_mask'] = key_padding_mask
            assert incremental_state is not None
            incremental_state = self._set_input_buffer(incremental_state, saved_state)
        assert k is not None
        src_len = k.size(1)
        if key_padding_mask is not None and key_padding_mask.dim() == 0:
            key_padding_mask = None
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == src_len
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        assert list(attn_weights.size()) == [bsz * self.num_heads_partition, tgt_len, src_len]
        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(0)
            attn_weights += attn_mask
        if key_padding_mask is not None:
            attn_weights = attn_weights.view(bsz, self.num_heads_partition, tgt_len, src_len)
            if not is_tpu:
                attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
            else:
                attn_weights = attn_weights.transpose(0, 2)
                attn_weights = attn_weights.masked_fill(key_padding_mask, float('-inf'))
                attn_weights = attn_weights.transpose(0, 2)
            attn_weights = attn_weights.view(bsz * self.num_heads_partition, tgt_len, src_len)
        attn_weights_float = utils.softmax(attn_weights, dim=-1)
        attn_weights = attn_weights_float.type_as(attn_weights)
        with get_cuda_rng_tracker().fork():
            attn_probs = self.dropout_module(attn_weights)
        assert v is not None
        attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz * self.num_heads_partition, tgt_len, self.head_dim]
        embed_dim_partition = embed_dim // self.model_parallel_size
        attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim_partition)
        attn = self.out_proj(attn)
        attn_weights: Optional[Tensor] = None
        return attn, attn_weights

    @staticmethod
    def _append_prev_key_padding_mask(key_padding_mask: Optional[Tensor], prev_key_padding_mask: Optional[Tensor], batch_size: int, src_len: int, static_kv: bool) ->Optional[Tensor]:
        if prev_key_padding_mask is not None and static_kv:
            new_key_padding_mask = prev_key_padding_mask
        elif prev_key_padding_mask is not None and key_padding_mask is not None:
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), key_padding_mask.float()], dim=1)
        elif prev_key_padding_mask is not None:
            filler = torch.zeros(batch_size, src_len - prev_key_padding_mask.size(1))
            if prev_key_padding_mask.is_cuda:
                filler = filler
            new_key_padding_mask = torch.cat([prev_key_padding_mask.float(), filler.float()], dim=1)
        elif key_padding_mask is not None:
            filler = torch.zeros(batch_size, src_len - key_padding_mask.size(1))
            if key_padding_mask.is_cuda:
                filler = filler
            new_key_padding_mask = torch.cat([filler.float(), key_padding_mask.float()], dim=1)
        else:
            new_key_padding_mask = prev_key_padding_mask
        return new_key_padding_mask

    def reorder_incremental_state(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], new_order):
        """Reorder buffered internal state (for incremental generation)."""
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            for k in input_buffer.keys():
                if input_buffer[k] is not None:
                    input_buffer[k] = input_buffer[k].index_select(0, new_order)
            incremental_state = self._set_input_buffer(incremental_state, input_buffer)
        return incremental_state

    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]) ->Dict[str, Optional[Tensor]]:
        result = self.get_incremental_state(incremental_state, 'attn_state')
        if result is not None:
            return result
        else:
            empty_result: Dict[str, Optional[Tensor]] = {}
            return empty_result

    def _set_input_buffer(self, incremental_state: Dict[str, Dict[str, Optional[Tensor]]], buffer: Dict[str, Optional[Tensor]]):
        return self.set_incremental_state(incremental_state, 'attn_state', buffer)


class ModelParallelTransformerEncoderLayer(TransformerEncoderLayer):
    """Encoder layer block over multiple gpus.

    See "Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf" for more details.
    """

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        if q_noise > 0:
            raise NotImplementedError
        return ColumnParallelLinear(input_dim, output_dim, gather_output=False)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        if q_noise > 0:
            raise NotImplementedError
        return RowParallelLinear(input_dim, output_dim, input_is_parallel=True)

    def build_self_attention(self, embed_dim, args, **unused_kwargs):
        return ModelParallelMultiheadAttention(embed_dim, args.encoder_attention_heads, dropout=args.attention_dropout, self_attention=True)


class ModelParallelTransformerDecoderLayer(TransformerDecoderLayer):
    """Decoder layer block.

    See "Megatron-LM: https://arxiv.org/pdf/1909.08053.pdf" for more details.
    """

    def build_fc1(self, input_dim, output_dim, q_noise, qn_block_size):
        if q_noise > 0:
            raise NotImplementedError
        return ColumnParallelLinear(input_dim, output_dim, gather_output=False)

    def build_fc2(self, input_dim, output_dim, q_noise, qn_block_size):
        if q_noise > 0:
            raise NotImplementedError
        return RowParallelLinear(input_dim, output_dim, input_is_parallel=True)

    def build_self_attention(self, embed_dim, args, **unused_kwargs):
        return ModelParallelMultiheadAttention(embed_dim=embed_dim, num_heads=args.decoder_attention_heads, dropout=args.attention_dropout, self_attention=not getattr(args, 'cross_self_attention', False))

    def build_encoder_attention(self, embed_dim, args, **unused_kwargs):
        return ModelParallelMultiheadAttention(embed_dim=embed_dim, num_heads=args.decoder_attention_heads, kdim=getattr(args, 'encoder_embed_dim', None), vdim=getattr(args, 'encoder_embed_dim', None), dropout=args.attention_dropout, encoder_decoder_attention=True)


class BARTHubInterface(GeneratorHubInterface):
    """A simple PyTorch Hub interface to BART.

    Usage: https://github.com/pytorch/fairseq/tree/main/examples/bart
    """

    def __init__(self, cfg, task, model):
        super().__init__(cfg, task, [model])
        self.model = self.models[0]

    def encode(self, sentence: str, *addl_sentences, no_separator=True) ->torch.LongTensor:
        """
        BPE-encode a sentence (or multiple sentences).

        Every sequence begins with a beginning-of-sentence (`<s>`) symbol.
        Every sentence ends with an end-of-sentence (`</s>`).

        Example (single sentence): `<s> a b c </s>`
        Example (sentence pair): `<s> d e f </s> 1 2 3 </s>`

        The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE
        requires leading spaces. For example::

            >>> bart.encode('Hello world').tolist()
            [0, 31414, 232, 2]
            >>> bart.encode(' world').tolist()
            [0, 232, 2]
            >>> bart.encode('world').tolist()
            [0, 8331, 2]
        """
        tokens = self.bpe.encode(sentence)
        if len(tokens.split(' ')) > min(self.max_positions) - 2:
            tokens = ' '.join(tokens.split(' ')[:min(self.max_positions) - 2])
        bpe_sentence = '<s> ' + tokens + ' </s>'
        for s in addl_sentences:
            bpe_sentence += ' </s>' if not no_separator else ''
            bpe_sentence += ' ' + self.bpe.encode(s) + ' </s>'
        tokens = self.task.source_dictionary.encode_line(bpe_sentence, append_eos=False)
        return tokens.long()

    def decode(self, tokens: torch.LongTensor):
        assert tokens.dim() == 1
        tokens = tokens.cpu().numpy()
        if tokens[0] == self.task.source_dictionary.bos():
            tokens = tokens[1:]
        eos_mask = tokens == self.task.source_dictionary.eos()
        doc_mask = eos_mask[1:] & eos_mask[:-1]
        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)
        sentences = [self.bpe.decode(self.task.source_dictionary.string(s)) for s in sentences]
        if len(sentences) == 1:
            return sentences[0]
        return sentences

    def _build_sample(self, src_tokens: List[torch.LongTensor]):
        dataset = self.task.build_dataset_for_inference(src_tokens, [x.numel() for x in src_tokens])
        sample = dataset.collater(dataset)
        sample = utils.apply_to_sample(lambda tensor: tensor, sample)
        return sample

    def generate(self, tokenized_sentences: List[torch.LongTensor], *args, inference_step_args=None, skip_invalid_size_inputs=False, **kwargs) ->List[List[Dict[str, torch.Tensor]]]:
        inference_step_args = inference_step_args or {}
        if 'prefix_tokens' in inference_step_args:
            raise NotImplementedError('prefix generation not implemented for BART')
        res = []
        for batch in self._build_batches(tokenized_sentences, skip_invalid_size_inputs):
            src_tokens = batch['net_input']['src_tokens']
            inference_step_args['prefix_tokens'] = src_tokens.new_full((src_tokens.size(0), 1), fill_value=self.task.source_dictionary.bos())
            results = super().generate(src_tokens, *args, inference_step_args=inference_step_args, skip_invalid_size_inputs=skip_invalid_size_inputs, **kwargs)
            for id, hypos in zip(batch['id'].tolist(), results):
                res.append((id, hypos))
        res = [hypos for _, hypos in sorted(res, key=lambda x: x[0])]
        return res

    def extract_features(self, tokens: torch.LongTensor, return_all_hiddens: bool=False) ->torch.Tensor:
        if tokens.dim() == 1:
            tokens = tokens.unsqueeze(0)
        if tokens.size(-1) > min(self.model.max_positions()):
            raise ValueError('tokens exceeds maximum length: {} > {}'.format(tokens.size(-1), self.model.max_positions()))
        tokens,
        prev_output_tokens = tokens.clone()
        prev_output_tokens[:, 0] = tokens.gather(1, (tokens.ne(self.task.source_dictionary.pad()).sum(dim=1) - 1).unsqueeze(-1)).squeeze()
        prev_output_tokens[:, 1:] = tokens[:, :-1]
        features, extra = self.model(src_tokens=tokens, src_lengths=None, prev_output_tokens=prev_output_tokens, features_only=True, return_all_hiddens=return_all_hiddens)
        if return_all_hiddens:
            inner_states = extra['inner_states']
            return [inner_state.transpose(0, 1) for inner_state in inner_states]
        else:
            return features

    def register_classification_head(self, name: str, num_classes: int=None, embedding_size: int=None, **kwargs):
        self.model.register_classification_head(name, num_classes=num_classes, embedding_size=embedding_size, **kwargs)

    def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool=False):
        if tokens.dim() == 1:
            tokens = tokens.unsqueeze(0)
        features = self.extract_features(tokens)
        sentence_representation = features[tokens.eq(self.task.source_dictionary.eos()), :].view(features.size(0), -1, features.size(-1))[:, -1, :]
        logits = self.model.classification_heads[head](sentence_representation)
        if return_logits:
            return logits
        return F.log_softmax(logits, dim=-1)

    def fill_mask(self, masked_inputs: List[str], topk: int=5, match_source_len: bool=True, **generate_kwargs):
        masked_token = '<mask>'
        batch_tokens = []
        for masked_input in masked_inputs:
            assert masked_token in masked_input, 'please add one {} token for the input'.format(masked_token)
            text_spans = masked_input.split(masked_token)
            text_spans_bpe = ' {0} '.format(masked_token).join([self.bpe.encode(text_span.rstrip()) for text_span in text_spans]).strip()
            tokens = self.task.source_dictionary.encode_line('<s> ' + text_spans_bpe + ' </s>', append_eos=False, add_if_not_exist=False).long()
            batch_tokens.append(tokens)
        generate_kwargs['beam'] = max(topk, generate_kwargs.get('beam', -1))
        generate_kwargs['match_source_len'] = match_source_len
        batch_hypos = self.generate(batch_tokens, **generate_kwargs)
        return [[(self.decode(hypo['tokens']), hypo['score']) for hypo in hypos[:topk]] for hypos in batch_hypos]


class BARTClassificationHead(nn.Module):
    """Head for sentence-level classification tasks."""

    def __init__(self, input_dim, inner_dim, num_classes, activation_fn, pooler_dropout, do_spectral_norm=False):
        super().__init__()
        self.dense = nn.Linear(input_dim, inner_dim)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.dropout = nn.Dropout(p=pooler_dropout)
        self.out_proj = nn.Linear(inner_dim, num_classes)
        if do_spectral_norm:
            self.out_proj = torch.nn.utils.spectral_norm(self.out_proj)

    def forward(self, features, **kwargs):
        x = features
        x = self.dropout(x)
        x = self.dense(x)
        x = self.activation_fn(x)
        x = self.dropout(x)
        x = self.out_proj(x)
        return x


class Downsample(nn.Module):
    """
    Selects every nth element, where n is the index
    """

    def __init__(self, index):
        super().__init__()
        self.index = index

    def forward(self, x):
        return x[::self.index + 1]


def GatedLinear(in_features, out_features, dropout=0.0, bias=True):
    """Weight-normalized Linear layer (input: B x T x C) with interspersed GLU units"""
    return nn.Sequential(Linear(in_features, out_features * 4, dropout, bias), nn.GLU(), Linear(out_features * 2, out_features * 2, dropout, bias), nn.GLU(), Linear(out_features, out_features, dropout, bias))


class ScalarBias(torch.autograd.Function):
    """
    Adds a vector of scalars, used in self-attention mechanism to allow
    the model to optionally attend to this vector instead of the past
    """

    @staticmethod
    def forward(ctx, input, dim, bias_init):
        size = list(input.size())
        size[dim] += 1
        output = input.new(*size).fill_(bias_init)
        output.narrow(dim, 1, size[dim] - 1).copy_(input)
        ctx.dim = dim
        return output

    @staticmethod
    def backward(ctx, grad):
        return grad.narrow(ctx.dim, 1, grad.size(ctx.dim) - 1), None, None


def scalar_bias(input, dim, bias_init=0):
    return ScalarBias.apply(input, dim, bias_init)


class SingleHeadAttention(nn.Module):
    """
    Single-head attention that supports Gating and Downsampling
    """

    def __init__(self, out_channels, embed_dim, head_dim, head_index, dropout=0.0, bias=True, project_input=True, gated=False, downsample=False, num_heads=1):
        super().__init__()
        self.embed_dim = embed_dim
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.head_index = head_index
        self.head_dim = head_dim
        self.project_input = project_input
        self.gated = gated
        self.downsample = downsample
        self.num_heads = num_heads
        self.projection = None
        k_layers = []
        v_layers = []
        if self.downsample:
            k_layers.append(Downsample(self.head_index))
            v_layers.append(Downsample(self.head_index))
            out_proj_size = self.head_dim
        else:
            out_proj_size = self.head_dim * self.num_heads
        if self.gated:
            k_layers.append(GatedLinear(self.embed_dim, out_proj_size, bias=bias))
            self.in_proj_q = GatedLinear(self.embed_dim, out_proj_size, bias=bias)
            v_layers.append(GatedLinear(self.embed_dim, out_proj_size, bias=bias))
        else:
            k_layers.append(Linear(self.embed_dim, out_proj_size, bias=bias))
            self.in_proj_q = Linear(self.embed_dim, out_proj_size, bias=bias)
            v_layers.append(Linear(self.embed_dim, out_proj_size, bias=bias))
        self.in_proj_k = nn.Sequential(*k_layers)
        self.in_proj_v = nn.Sequential(*v_layers)
        if self.downsample:
            self.out_proj = Linear(out_proj_size, self.head_dim, bias=bias)
        else:
            self.out_proj = Linear(out_proj_size, out_channels, bias=bias)
        self.scaling = self.head_dim ** -0.5

    def forward(self, query, key, value, mask_future_timesteps=False, key_padding_mask=None, use_scalar_bias=False):
        """Input shape: Time x Batch x Channel
        Self-attention can be implemented by passing in the same arguments for
        query, key and value. Future timesteps can be masked with the
        `mask_future_timesteps` argument. Padding elements can be excluded from
        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:
        batch x src_len, where padding elements are indicated by 1s.
        """
        src_len, bsz, out_channels = key.size()
        tgt_len = query.size(0)
        assert list(query.size()) == [tgt_len, bsz, out_channels]
        assert key.size() == value.size()
        if key_padding_mask is not None:
            assert key_padding_mask.size(0) == bsz
            assert key_padding_mask.size(1) == src_len
        if self.downsample:
            size = bsz
        else:
            size = bsz * self.num_heads
        k = key
        v = value
        q = query
        if self.project_input:
            q = self.in_proj_q(q)
            k = self.in_proj_k(k)
            v = self.in_proj_v(v)
            src_len = k.size()[0]
        q *= self.scaling
        if not self.downsample:
            q = q.view(tgt_len, size, self.head_dim)
            k = k.view(src_len, size, self.head_dim)
            v = v.view(src_len, size, self.head_dim)
        q = q.transpose(0, 1)
        k = k.transpose(0, 1)
        v = v.transpose(0, 1)
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        if mask_future_timesteps:
            assert query.size() == key.size(), 'mask_future_timesteps only applies to self-attention'
            attn_weights *= torch.tril(attn_weights.data.new([1]).expand(tgt_len, tgt_len).clone(), diagonal=-1)[:, ::self.head_index + 1 if self.downsample else 1].unsqueeze(0)
            attn_weights += torch.triu(attn_weights.data.new([-math.inf]).expand(tgt_len, tgt_len).clone(), diagonal=0)[:, ::self.head_index + 1 if self.downsample else 1].unsqueeze(0)
        tgt_size = tgt_len
        if use_scalar_bias:
            attn_weights = scalar_bias(attn_weights, 2)
            v = scalar_bias(v, 1)
            tgt_size += 1
        if key_padding_mask is not None:
            if key_padding_mask.max() > 0:
                if self.downsample:
                    attn_weights = attn_weights.view(bsz, 1, tgt_len, src_len)
                else:
                    attn_weights = attn_weights.view(size, self.num_heads, tgt_len, src_len)
                attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), -math.inf)
                attn_weights = attn_weights.view(size, tgt_len, src_len)
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_weights = self.dropout_module(attn_weights)
        attn = torch.bmm(attn_weights, v)
        if self.downsample:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.head_dim)
        else:
            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, self.embed_dim)
        attn = self.out_proj(attn)
        return attn, attn_weights


class DownsampledMultiHeadAttention(nn.ModuleList):
    """
    Multi-headed attention with Gating and Downsampling
    """

    def __init__(self, out_channels, embed_dim, num_heads, dropout=0.0, bias=True, project_input=True, gated=False, downsample=False):
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.downsample = downsample
        self.gated = gated
        self.project_input = project_input
        assert self.head_dim * num_heads == embed_dim
        if self.downsample:
            attention_heads = []
            for index in range(self.num_heads):
                attention_heads.append(SingleHeadAttention(out_channels, self.embed_dim, self.head_dim, index, dropout, bias, self.project_input, self.gated, self.downsample, self.num_heads))
            super().__init__(modules=attention_heads)
            self.out_proj = Linear(embed_dim, out_channels, bias=bias)
        else:
            super().__init__()
            self.attention_module = SingleHeadAttention(out_channels, self.embed_dim, self.head_dim, 1, dropout, bias, self.project_input, self.gated, self.downsample, self.num_heads)

    def forward(self, query, key, value, mask_future_timesteps=False, key_padding_mask=None, use_scalar_bias=False):
        src_len, bsz, embed_dim = key.size()
        tgt_len = query.size(0)
        assert embed_dim == self.embed_dim
        assert list(query.size()) == [tgt_len, bsz, embed_dim]
        assert key.size() == value.size()
        tgt_size = tgt_len
        if use_scalar_bias:
            tgt_size += 1
        attn = []
        attn_weights = []
        if self.downsample:
            for attention_head_number in range(self.num_heads):
                _attn, _attn_weight = self[attention_head_number](query, key, value, mask_future_timesteps, key_padding_mask, use_scalar_bias)
                attn.append(_attn)
                attn_weights.append(_attn_weight)
            full_attn = torch.cat(attn, dim=2)
            full_attn = self.out_proj(full_attn)
            return full_attn, attn_weights[0].clone()
        else:
            _attn, _attn_weight = self.attention_module(query, key, value, mask_future_timesteps, key_padding_mask, use_scalar_bias)
            attn.append(_attn)
            attn_weights.append(_attn_weight)
            full_attn = torch.cat(attn, dim=2)
            full_attn_weights = torch.cat(attn_weights)
            full_attn_weights = full_attn_weights.view(bsz, self.num_heads, tgt_size, src_len)
            full_attn_weights = full_attn_weights.sum(dim=1) / self.num_heads
            return full_attn, full_attn_weights


class ConvTBC(torch.nn.Module):
    """1D convolution over an input of shape (time x batch x channel)

    The implementation uses gemm to perform the convolution. This implementation
    is faster than cuDNN for small kernel sizes.
    """

    def __init__(self, in_channels, out_channels, kernel_size, padding=0):
        super(ConvTBC, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _single(kernel_size)
        self.padding = _single(padding)
        self.weight = torch.nn.Parameter(torch.Tensor(self.kernel_size[0], in_channels, out_channels))
        self.bias = torch.nn.Parameter(torch.Tensor(out_channels))
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_normal_(self.weight)
        nn.init.zeros_(self.bias)

    def conv_tbc(self, input: Tensor):
        return torch.conv_tbc(input.contiguous(), self.weight, self.bias, self.padding[0])

    def forward(self, input: Tensor):
        return self.conv_tbc(input)

    def __repr__(self):
        s = '{name}({in_channels}, {out_channels}, kernel_size={kernel_size}, padding={padding}'
        if self.bias is None:
            s += ', bias=False'
        s += ')'
        return s.format(name=self.__class__.__name__, **self.__dict__)


@with_incremental_state
class LinearizedConvolution(ConvTBC):
    """An optimized version of nn.Conv1d.

    At training time, this module uses ConvTBC, which is an optimized version
    of Conv1d. At inference time, it optimizes incremental generation (i.e.,
    one time step at a time) by replacing the convolutions with linear layers.
    Note that the input order changes from training to inference.
    """

    def __init__(self, in_channels, out_channels, kernel_size, **kwargs):
        super().__init__(in_channels, out_channels, kernel_size, **kwargs)
        self._linearized_weight = None
        self.register_backward_hook(self._clear_linearized_weight)

    def state_dict(self, destination=None, prefix='', keep_vars=False):
        state = ConvTBC.state_dict(self, destination, prefix, keep_vars=keep_vars)
        if prefix + '_linearized_weight' in state:
            del state[prefix + '_linearized_weight']
        return state

    def upgrade_state_dict_named(self, state_dict, name):
        prefix = name + '.' if name != '' else ''
        if prefix + '_linearized_weight' in state_dict:
            del state_dict[prefix + '_linearized_weight']

    @torch.jit.export
    def forward(self, input, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None):
        """
        Args:
            incremental_state: Used to buffer signal; if not None, then input is
                expected to contain a single frame. If the input order changes
                between time steps, call reorder_incremental_state.
        Input:
            Time x Batch x Channel during training
            Batch x Time x Channel during inference
        """
        if incremental_state is None:
            output = self.conv_tbc(input)
            if self.kernel_size[0] > 1 and self.padding[0] > 0:
                output = output[:-self.padding[0], :, :]
            return output
        weight = self._get_linearized_weight()
        kw = self.kernel_size[0]
        bsz = input.size(0)
        if kw > 1:
            input = input.data
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is None:
                input_buffer = input.new(bsz, kw, input.size(2)).zero_()
                self._set_input_buffer(incremental_state, input_buffer)
            else:
                input_buffer[:, :-1, :] = input_buffer[:, 1:, :].clone()
            input_buffer[:, -1, :] = input[:, -1, :]
            input = input_buffer
        with torch.no_grad():
            output = F.linear(input.view(bsz, -1), weight, self.bias)
        return output.view(bsz, 1, -1)

    @torch.jit.unused
    def reorder_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_order):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(0, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    @torch.jit.unused
    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]):
        return utils.get_incremental_state(self, incremental_state, 'input_buffer')

    @torch.jit.unused
    def _set_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_buffer):
        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)

    @torch.jit.unused
    def _get_linearized_weight(self):
        if self._linearized_weight is None:
            kw = self.kernel_size[0]
            weight = self.weight.transpose(2, 1).transpose(1, 0).contiguous()
            assert weight.size() == (self.out_channels, kw, self.in_channels)
            return weight.view(self.out_channels, -1)
        return self._linearized_weight

    @torch.jit.unused
    def _clear_linearized_weight(self, *args):
        self._linearized_weight = None


def LinearizedConv1d(in_channels, out_channels, kernel_size, dropout=0.0, **kwargs):
    """Weight-normalized Conv1d layer optimized for decoding"""
    m = LinearizedConvolution(in_channels, out_channels, kernel_size, **kwargs)
    std = math.sqrt(4 * (1.0 - dropout) / (m.kernel_size[0] * in_channels))
    m.weight.data.normal_(mean=0, std=std)
    m.bias.data.zero_()
    return m


class DepthWiseConv1d(nn.Module):

    def __init__(self, dim_in, dim_out, kernel_size, stride=1, bias=True, causal=False):
        super().__init__()
        self.padding = (kernel_size - 1, 0) if causal else (kernel_size // 2, kernel_size // 2)
        self.net = nn.Sequential(nn.Conv1d(dim_in, dim_in, kernel_size=kernel_size, groups=dim_in, stride=stride, bias=bias), nn.Conv1d(dim_in, dim_out, 1, bias=bias))

    def forward(self, x):
        x = F.pad(x, self.padding, value=0.0)
        return self.net(x)


KMEAN_INIT_ITERS = 10


def expand_dim(t, dim, k):
    t = t.unsqueeze(dim)
    expand_shape = [-1] * len(t.shape)
    expand_shape[dim] = k
    return t.expand(*expand_shape)


def batched_index_select(values, indices):
    last_dim = values.shape[-1]
    return values.gather(2, expand_dim(indices, -1, last_dim))


def exists(val):
    return val is not None


def default(x, d):
    if not exists(x):
        return d if not isfunction(d) else d()
    return x


def similarity(x, means):
    return torch.einsum('bhld,hcd->bhlc', x, means)


def dists_and_buckets(x, means):
    dists = similarity(x, means)
    _, buckets = torch.max(dists, dim=-1)
    return dists, buckets


def ema(old, new, decay):
    if not exists(old):
        return new
    return old * decay + new * (1 - decay)


def is_empty(t):
    return t.nelement() == 0


def ema_inplace(moving_avg, new, decay):
    if is_empty(moving_avg):
        moving_avg.data.copy_(new)
        return
    moving_avg.data.mul_(decay).add_(new, alpha=1 - decay)


def batched_bincount(index, num_classes, dim=-1):
    shape = list(index.shape)
    shape[dim] = num_classes
    out = index.new_zeros(shape)
    out.scatter_add_(dim, index, torch.ones_like(index, dtype=index.dtype))
    return out


def kmeans_iter(x, means, buckets=None):
    b, h, _, d, dtype, num_clusters = *x.shape, x.dtype, means.shape[1]
    if not exists(buckets):
        _, buckets = dists_and_buckets(x, means)
    bins = batched_bincount(buckets, num_clusters).sum(0, keepdim=True)
    zero_mask = bins.long() == 0
    means_ = buckets.new_zeros(b, h, num_clusters, d, dtype=dtype)
    means_.scatter_add_(-2, expand_dim(buckets, -1, d), x)
    means_ = F.normalize(means_.sum(0, keepdim=True), dim=-1).type(dtype)
    means = torch.where(zero_mask.unsqueeze(-1), means, means_)
    means = means.squeeze(0)
    return means


class Kmeans(nn.Module):

    def __init__(self, num_heads, head_dim, num_clusters, ema_decay=0.999, commitment=0.0001):
        super().__init__()
        self.commitment = commitment
        self.ema_decay = ema_decay
        self.register_buffer('means', torch.randn(num_heads, num_clusters, head_dim))
        self.register_buffer('initted', torch.tensor(False))
        self.num_new_means = 0
        self.new_means = None

    @torch.no_grad()
    def init(self, x):
        if self.initted:
            return
        _, h, _, d, device, _ = *x.shape, x.device, x.dtype
        num_clusters = self.means.shape[1]
        means = x.transpose(0, 1).contiguous().view(h, -1, d)
        num_samples = means.shape[1]
        if num_samples >= num_clusters:
            indices = torch.randperm(num_samples, device=device)[:num_clusters]
        else:
            indices = torch.randint(0, num_samples, (num_clusters,), device=device)
        means = means[:, indices]
        for _ in range(KMEAN_INIT_ITERS):
            means = kmeans_iter(x, means)
        self.num_new_means = 0
        self.means.data.copy_(means)
        self.initted.data.copy_(torch.tensor(True))

    @torch.no_grad()
    def update(self, new_means=None):
        new_means = default(new_means, self.new_means)
        assert exists(new_means), 'new kmeans has not been supplied'
        ema_inplace(self.means, new_means, self.ema_decay)
        del self.new_means
        self.new_means = None
        self.num_new_means = 0

    def forward(self, x, update_means=False):
        self.init(x)
        b, dtype = x.shape[0], x.dtype
        means = self.means.type(dtype)
        x = F.normalize(x, 2, dim=-1).type(dtype)
        with torch.no_grad():
            dists, buckets = dists_and_buckets(x, means)
        routed_means = batched_index_select(expand_dim(means, 0, b), buckets)
        loss = F.mse_loss(x, routed_means) * self.commitment
        if update_means:
            with torch.no_grad():
                means = kmeans_iter(x, means, buckets)
            self.new_means = ema(self.new_means, means, self.num_new_means / (self.num_new_means + 1))
            self.num_new_means += 1
        return dists, loss


TOKEN_SELF_ATTN_VALUE = -50000.0


def distribution(dists, window_size):
    _, topk_indices = dists.topk(k=window_size, dim=-2)
    indices = topk_indices.transpose(-2, -1)
    return indices.reshape(*indices.size()[:2], -1)


def max_neg_value(tensor):
    return -torch.finfo(tensor.dtype).max


def scatter_mean(src, t, index, dim, eps=1e-05):
    numer = src.scatter_add(dim, index, t)
    denom = src.scatter_add(dim, index, torch.ones_like(t))
    return numer / (denom + eps)


def split_at_index(dim, index, t):
    pre_slices = (slice(None),) * dim
    l = *pre_slices, slice(None, index)
    r = *pre_slices, slice(index, None)
    return t[l], t[r]


class KmeansAttention(nn.Module):

    def __init__(self, num_clusters, window_size, num_heads, head_dim, causal=False, dropout=0.0, ema_decay=0.999, commitment=0.0001, context_window_size=None, receives_context=False, num_mem_kv=0, shared_qk=False):
        super().__init__()
        self.num_heads = num_heads
        self.num_clusters = num_clusters
        self.head_dim = head_dim
        self.window_size = window_size
        self.context_window_size = default(context_window_size, window_size)
        self.causal = causal
        self.shared_qk = shared_qk
        self.receives_context = receives_context
        self.kmeans = Kmeans(num_heads, head_dim, num_clusters, ema_decay, commitment)
        self.dropout = nn.Dropout(dropout)
        self.num_mem_kv = max(num_mem_kv, 1 if causal and not shared_qk else 0)
        self.mem_key = nn.Parameter(torch.randn(num_heads, num_clusters, self.num_mem_kv, head_dim))
        self.mem_value = nn.Parameter(torch.randn(num_heads, num_clusters, self.num_mem_kv, head_dim))

    def forward(self, q, k, v, query_mask=None, key_mask=None, **kwargs):
        b, h, t, d, kv_t, wsz, c_wsz, nc, device, dtype = *q.shape, k.shape[2], self.window_size, self.context_window_size, self.num_clusters, q.device, q.dtype
        is_reverse = kwargs.pop('_reverse', False)
        out = torch.zeros_like(q, dtype=dtype)
        update_kmeans = self.training and not is_reverse
        key_mask = default(key_mask, query_mask) if not self.receives_context else key_mask
        kv_wsz = wsz if not self.receives_context else c_wsz
        wsz = min(wsz, t)
        kv_wsz = min(kv_wsz, kv_t)
        if not self.shared_qk or self.receives_context:
            dists, aux_loss = self.kmeans(torch.cat((q, k), dim=2), update_kmeans)
            q_dists, k_dists = split_at_index(2, t, dists)
            indices = distribution(q_dists, wsz)
            kv_indices = distribution(k_dists, kv_wsz)
        else:
            dists, aux_loss = self.kmeans(q, update_kmeans)
            k = F.normalize(k, dim=-1)
            indices = distribution(dists, wsz)
            kv_indices = indices
        q = batched_index_select(q, indices)
        k = batched_index_select(k, kv_indices)
        v = batched_index_select(v, kv_indices)
        reshape_with_window = lambda x: x.reshape(b, h, nc, -1, d)
        q, k, v = map(reshape_with_window, (q, k, v))
        m_k, m_v = map(lambda x: expand_dim(x, 0, b), (self.mem_key, self.mem_value))
        k, v = map(lambda x: torch.cat(x, dim=3), ((m_k, k), (m_v, v)))
        dots = torch.einsum('bhnid,bhnjd->bhnij', q, k) * d ** -0.5
        mask_value = max_neg_value(dots)
        if exists(query_mask) or exists(key_mask):
            query_mask = default(query_mask, lambda : torch.ones((b, t), device=device).bool())
            key_mask = default(key_mask, lambda : torch.ones((b, kv_t), device=device).bool())
            q_mask = expand_dim(query_mask, 1, h).gather(2, indices)
            kv_mask = expand_dim(key_mask, 1, h).gather(2, kv_indices)
            q_mask, kv_mask = map(lambda t: t.reshape(b, h, nc, -1), (q_mask, kv_mask))
            mask = q_mask[:, :, :, :, None] * kv_mask[:, :, :, None, :]
            mask = F.pad(mask, (self.num_mem_kv, 0), value=1)
            dots.masked_fill_(~mask, mask_value)
            del mask
        if self.causal:
            q_mask, kv_mask = map(lambda t: t.reshape(b, h, nc, -1), (indices, kv_indices))
            mask = q_mask[:, :, :, :, None] >= kv_mask[:, :, :, None, :]
            mask = F.pad(mask, (self.num_mem_kv, 0), value=1)
            dots.masked_fill_(~mask, mask_value)
            del mask
        if self.shared_qk:
            q_mask, kv_mask = map(lambda t: t.reshape(b, h, nc, -1), (indices, kv_indices))
            mask = q_mask[:, :, :, :, None] == kv_mask[:, :, :, None, :]
            mask = F.pad(mask, (self.num_mem_kv, 0), value=0)
            dots.masked_fill_(mask, TOKEN_SELF_ATTN_VALUE)
            del mask
        dots = dots.softmax(dim=-1)
        dots = self.dropout(dots)
        bo = torch.einsum('bhcij,bhcjd->bhcid', dots, v)
        so = torch.reshape(bo, (b, h, -1, bo.shape[-1])).type(dtype)
        out = scatter_mean(out, so, indices.unsqueeze(-1).expand_as(so), -2)
        return out, aux_loss


def reshape_dim(t, dim, split_dims):
    shape = list(t.shape)
    num_dims = len(shape)
    dim = (dim + num_dims) % num_dims
    shape[dim:dim + 1] = split_dims
    return t.reshape(shape)


def to(t):
    return {'device': t.device, 'dtype': t.dtype}


class SelfAttention(nn.Module):

    def __init__(self, dim, max_seq_len, heads, local_attn_heads, window_size, dim_head=None, local_attn_window_size=None, local_attn_radius_blocks=1, causal=False, attn_dropout=0.0, dropout=0.0, kmeans_ema_decay=0.999, commitment_factor=0.0001, receives_context=False, context_window_size=None, rel_pos_emb=True, num_mem_kv=0, shared_qk=False, conv_query_kernel=9):
        super().__init__()
        assert dim_head or dim % heads == 0, 'hidden dimension must be divisible by number of heads'
        assert max_seq_len % window_size == 0, 'maximum sequence length must be divisible by the target window size'
        assert local_attn_heads <= heads, 'number of local attention heads must be less than total heads'
        assert not (receives_context and local_attn_heads > 0), 'local attention cannot be used for self attention with context'
        assert not (receives_context and causal), 'contextual attention layer cannot be causal'
        local_attn_window_size = default(local_attn_window_size, window_size)
        context_window_size = default(context_window_size, window_size)
        self.shared_qk = shared_qk
        self.receives_context = receives_context
        self.heads = heads
        self.local_attn_heads = local_attn_heads
        self.global_attn_heads = heads - local_attn_heads
        self.causal = causal
        self.window_size = window_size
        dim_head = default(dim_head, dim // heads)
        dim_heads = dim_head * heads
        self.dim_head = dim_head
        num_clusters = max_seq_len // window_size
        local_dim_heads = dim_head * self.local_attn_heads
        if self.local_attn_heads > 0:
            rel_pos_emb_config = (dim_head, local_attn_heads) if rel_pos_emb else None
            self.local_attn = LocalAttention(dim=dim_head, window_size=local_attn_window_size, causal=causal, dropout=attn_dropout, rel_pos_emb_config=rel_pos_emb_config, look_backward=local_attn_radius_blocks, look_forward=0 if causal else local_attn_radius_blocks)
            self.local_to_qkv = nn.Linear(dim, 3 * local_dim_heads)
        global_dim_heads = dim_head * self.global_attn_heads
        if self.global_attn_heads > 0:
            self.global_attn = KmeansAttention(num_clusters, window_size, self.global_attn_heads, dim_head, causal=causal, dropout=attn_dropout, ema_decay=kmeans_ema_decay, commitment=commitment_factor, receives_context=receives_context, num_mem_kv=num_mem_kv, shared_qk=shared_qk)
        self.to_q = nn.Sequential(Rearrange('b n c -> b c n'), DepthWiseConv1d(dim, global_dim_heads, conv_query_kernel, causal=causal), Rearrange('b c n -> b n c'))
        self.to_v = nn.Linear(dim, global_dim_heads, bias=False)
        if not self.shared_qk:
            self.to_k = nn.Linear(dim, global_dim_heads, bias=False)
        self.to_out = nn.Linear(dim_heads, dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, context=None, key_padding_mask=None, context_mask=None, pos_emb=None, **kwargs):
        assert not (self.receives_context and not exists(context)), 'context must be passed if self attention is set to receive context'
        input_mask = key_padding_mask
        x = query.transpose(0, 1)
        b, t, _, h, dh = *x.shape, self.heads, self.dim_head
        has_local, has_global = map(lambda x: x > 0, (self.local_attn_heads, self.global_attn_heads))
        split_heads = lambda v: reshape_dim(v, -1, (-1, dh)).transpose(1, 2).contiguous()
        if has_local:
            local_qkv = self.local_to_qkv(x).chunk(3, dim=-1)
            lq, lk, lv = map(split_heads, local_qkv)
        if has_global:
            kv_input = x if not self.receives_context else context
            q, v = self.to_q(x), self.to_v(kv_input)
            if not self.shared_qk:
                k = self.to_k(kv_input)
            else:
                k = self.to_q(kv_input) if self.receives_context else q
            q, k, v = map(split_heads, (q, k, v))
        out = []
        total_loss = torch.tensor(0.0, requires_grad=True, **to(x))
        if has_local:
            local_out = self.local_attn(lq, lk, lv, input_mask=input_mask)
            out.append(local_out)
        if has_global:
            if not self.receives_context and exists(pos_emb):
                q, k = apply_rotary_pos_emb(q, k, pos_emb)
            global_out, loss = self.global_attn(q, k, v, query_mask=input_mask, key_mask=context_mask)
            total_loss = total_loss + loss
            out.append(global_out)
        out = torch.cat(out, dim=1)
        out = out.reshape(b, h, t, -1).transpose(1, 2).reshape(b, t, -1)
        out = self.dropout(out.transpose(0, 1))
        return out, total_loss


@with_incremental_state
class FConvDecoder(FairseqDecoder):
    """Convolutional decoder"""

    def __init__(self, dictionary, embed_dim=512, out_embed_dim=256, max_positions=1024, convolutions=((512, 3),) * 8, attention=True, dropout=0.1, selfattention=False, attention_nheads=1, selfattention_nheads=1, project_input=False, gated_attention=False, downsample=False, pretrained=False, trained_decoder=None):
        super().__init__(dictionary)
        self.register_buffer('version', torch.Tensor([2]))
        self.pretrained = pretrained
        self.pretrained_decoder = trained_decoder
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.need_attn = True
        in_channels = convolutions[0][0]

        def expand_bool_array(val):
            if isinstance(val, bool):
                return [val] * len(convolutions)
            return val
        attention = expand_bool_array(attention)
        selfattention = expand_bool_array(selfattention)
        if not isinstance(attention, list) or len(attention) != len(convolutions):
            raise ValueError('Attention is expected to be a list of booleans of length equal to the number of layers.')
        num_embeddings = len(dictionary)
        padding_idx = dictionary.pad()
        self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx)
        self.embed_positions = PositionalEmbedding(max_positions, embed_dim, padding_idx)
        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)
        self.projections = nn.ModuleList()
        self.convolutions = nn.ModuleList()
        self.attention = nn.ModuleList()
        self.selfattention = nn.ModuleList()
        self.attproj = nn.ModuleList()
        for i, (out_channels, kernel_size) in enumerate(convolutions):
            self.projections.append(Linear(in_channels, out_channels) if in_channels != out_channels else None)
            self.convolutions.append(LinearizedConv1d(in_channels, out_channels * 2, kernel_size, padding=kernel_size - 1, dropout=dropout))
            self.attention.append(DownsampledMultiHeadAttention(out_channels, embed_dim, attention_nheads, project_input=project_input, gated=False, downsample=False) if attention[i] else None)
            self.attproj.append(Linear(out_channels, embed_dim, dropout=dropout) if attention[i] else None)
            self.selfattention.append(SelfAttention(out_channels, embed_dim, selfattention_nheads, project_input=project_input, gated=gated_attention, downsample=downsample) if selfattention[i] else None)
            in_channels = out_channels
        self.fc2 = Linear(in_channels, out_embed_dim)
        self.fc3 = Linear(out_embed_dim, num_embeddings, dropout=dropout)
        if self.pretrained:
            self.gate1 = nn.Sequential(Linear(out_embed_dim * 2, out_embed_dim), nn.Sigmoid())
            self.gate2 = nn.Sequential(Linear(out_embed_dim * 2, out_embed_dim), nn.Sigmoid())
            self.joining = nn.Sequential(Linear(out_embed_dim * 2, out_embed_dim * 2), LayerNorm(out_embed_dim * 2), nn.GLU(), Linear(out_embed_dim, out_embed_dim * 2), LayerNorm(out_embed_dim * 2), nn.GLU(), Linear(out_embed_dim, out_embed_dim), LayerNorm(out_embed_dim))
            self.pretrained_outputs = {}

            def save_output():

                def hook(a, b, output):
                    self.pretrained_outputs['out'] = output
                return hook
            self.pretrained_decoder.fc2.register_forward_hook(save_output())

    def forward(self, prev_output_tokens, encoder_out):
        trained_encoder_out = encoder_out['pretrained'] if self.pretrained else None
        encoder_out = encoder_out['encoder']['encoder_out']
        encoder_a, encoder_b = self._split_encoder_out(encoder_out)
        positions = self.embed_positions(prev_output_tokens)
        x = self.embed_tokens(prev_output_tokens) + positions
        x = self.dropout_module(x)
        target_embedding = x.transpose(0, 1)
        x = self.fc1(x)
        x = x.transpose(0, 1)
        avg_attn_scores = None
        for proj, conv, attention, selfattention, attproj in zip(self.projections, self.convolutions, self.attention, self.selfattention, self.attproj):
            residual = x if proj is None else proj(x)
            x = self.dropout_module(x)
            x = conv(x)
            x = F.glu(x, dim=2)
            if attention is not None:
                r = x
                x, attn_scores = attention(attproj(x) + target_embedding, encoder_a, encoder_b)
                x = x + r
                if not self.training and self.need_attn:
                    if avg_attn_scores is None:
                        avg_attn_scores = attn_scores
                    else:
                        avg_attn_scores.add_(attn_scores)
            if selfattention is not None:
                x = selfattention(x)
            x = (x + residual) * math.sqrt(0.5)
        x = x.transpose(0, 1)
        x = self.fc2(x)
        x = self.dropout_module(x)
        if not self.pretrained:
            x = self.fc3(x)
        if self.pretrained:
            trained_x, _ = self.pretrained_decoder.forward(prev_output_tokens, trained_encoder_out)
            y = torch.cat([x, self.pretrained_outputs['out']], dim=-1)
            gate1 = self.gate1(y)
            gate2 = self.gate2(y)
            gated_x1 = gate1 * x
            gated_x2 = gate2 * self.pretrained_outputs['out']
            fusion = torch.cat([gated_x1, gated_x2], dim=-1)
            fusion = self.joining(fusion)
            fusion_output = self.fc3(fusion)
            return fusion_output, avg_attn_scores
        else:
            return x, avg_attn_scores

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        return self.embed_positions.max_positions

    def make_generation_fast_(self, need_attn=False, **kwargs):
        self.need_attn = need_attn

    def _split_encoder_out(self, encoder_out):
        """Split and transpose encoder outputs."""
        encoder_a, encoder_b = encoder_out
        encoder_a = encoder_a.transpose(0, 1).contiguous()
        encoder_b = encoder_b.transpose(0, 1).contiguous()
        result = encoder_a, encoder_b
        return result


class FConvEncoder(FairseqEncoder):
    """Convolutional encoder"""

    def __init__(self, dictionary, embed_dim=512, max_positions=1024, convolutions=((512, 3),) * 20, dropout=0.1, attention=False, attention_nheads=1):
        super().__init__(dictionary)
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.num_attention_layers = None
        num_embeddings = len(dictionary)
        self.padding_idx = dictionary.pad()
        self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)
        self.embed_positions = PositionalEmbedding(max_positions, embed_dim, self.padding_idx)

        def expand_bool_array(val):
            if isinstance(val, bool):
                return [val] * len(convolutions)
            return val
        attention = expand_bool_array(attention)
        in_channels = convolutions[0][0]
        self.fc1 = Linear(embed_dim, in_channels, dropout=dropout)
        self.projections = nn.ModuleList()
        self.convolutions = nn.ModuleList()
        self.attention = nn.ModuleList()
        self.attproj = nn.ModuleList()
        for i, (out_channels, kernel_size) in enumerate(convolutions):
            self.projections.append(Linear(in_channels, out_channels) if in_channels != out_channels else None)
            self.convolutions.append(ConvTBC(in_channels, out_channels * 2, kernel_size, dropout=dropout))
            self.attention.append(SelfAttention(out_channels, embed_dim, attention_nheads) if attention[i] else None)
            in_channels = out_channels
        self.fc2 = Linear(in_channels, embed_dim)

    def forward(self, src_tokens, src_lengths):
        x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens)
        x = self.dropout_module(x)
        input_embedding = x.transpose(0, 1)
        x = self.fc1(x)
        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()
        if not encoder_padding_mask.any():
            encoder_padding_mask = None
        x = x.transpose(0, 1)
        for proj, conv, attention in zip(self.projections, self.convolutions, self.attention):
            residual = x if proj is None else proj(x)
            if encoder_padding_mask is not None:
                x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)
            x = self.dropout_module(x)
            padding_l = (conv.kernel_size[0] - 1) // 2
            padding_r = conv.kernel_size[0] // 2
            x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r))
            x = conv(x)
            x = F.glu(x, dim=2)
            if attention is not None:
                x = attention(x)
            x = (x + residual) * math.sqrt(0.5)
        x = x.transpose(1, 0)
        x = self.fc2(x)
        if encoder_padding_mask is not None:
            encoder_padding_mask = encoder_padding_mask.t()
            x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0)
        x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers))
        y = (x + input_embedding.transpose(0, 1)) * math.sqrt(0.5)
        return {'encoder_out': (x, y), 'encoder_padding_mask': encoder_padding_mask}

    def reorder_encoder_out(self, encoder_out, new_order):
        encoder_out['encoder_out'] = tuple(eo.index_select(0, new_order) for eo in encoder_out['encoder_out'])
        if encoder_out['encoder_padding_mask'] is not None:
            encoder_out['encoder_padding_mask'] = encoder_out['encoder_padding_mask'].index_select(0, new_order)
        if 'pretrained' in encoder_out:
            encoder_out['pretrained']['encoder_out'] = tuple(eo.index_select(0, new_order) for eo in encoder_out['pretrained']['encoder_out'])
        return encoder_out

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        return self.embed_positions.max_positions


class AttentionLayer(nn.Module):

    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):
        super().__init__()
        self.input_proj = Linear(input_embed_dim, source_embed_dim, bias=bias)
        self.output_proj = Linear(input_embed_dim + source_embed_dim, output_embed_dim, bias=bias)

    def forward(self, input, source_hids, encoder_padding_mask):
        x = self.input_proj(input)
        attn_scores = (source_hids * x.unsqueeze(0)).sum(dim=2)
        if encoder_padding_mask is not None:
            attn_scores = attn_scores.float().masked_fill_(encoder_padding_mask, float('-inf')).type_as(attn_scores)
        attn_scores = F.softmax(attn_scores, dim=0)
        x = (attn_scores.unsqueeze(2) * source_hids).sum(dim=0)
        x = torch.tanh(self.output_proj(torch.cat((x, input), dim=1)))
        return x, attn_scores


class CompositeEncoder(FairseqEncoder):
    """
    A wrapper around a dictionary of :class:`FairseqEncoder` objects.

    We run forward on each encoder and return a dictionary of outputs. The first
    encoder's dictionary is used for initialization.

    Args:
        encoders (dict): a dictionary of :class:`FairseqEncoder` objects.
    """

    def __init__(self, encoders):
        super().__init__(next(iter(encoders.values())).dictionary)
        self.encoders = encoders
        for key in self.encoders:
            self.add_module(key, self.encoders[key])

    def forward(self, src_tokens, src_lengths):
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (LongTensor): lengths of each source sentence of shape
                `(batch)`

        Returns:
            dict:
                the outputs from each Encoder
        """
        encoder_out = {}
        for key in self.encoders:
            encoder_out[key] = self.encoders[key](src_tokens, src_lengths)
        return encoder_out

    def reorder_encoder_out(self, encoder_out, new_order):
        """Reorder encoder output according to new_order."""
        for key in self.encoders:
            encoder_out[key] = self.encoders[key].reorder_encoder_out(encoder_out[key], new_order)
        return encoder_out

    def max_positions(self):
        return min(self.encoders[key].max_positions() for key in self.encoders)

    def upgrade_state_dict(self, state_dict):
        for key in self.encoders:
            self.encoders[key].upgrade_state_dict(state_dict)
        return state_dict


class Fp32GroupNorm(nn.GroupNorm):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, input):
        output = F.group_norm(input.float(), self.num_groups, self.weight.float() if self.weight is not None else None, self.bias.float() if self.bias is not None else None, self.eps)
        return output.type_as(input)


class Fp32LayerNorm(nn.LayerNorm):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def forward(self, input):
        output = F.layer_norm(input.float(), self.normalized_shape, self.weight.float() if self.weight is not None else None, self.bias.float() if self.bias is not None else None, self.eps)
        return output.type_as(input)


class ConvFeatureExtractionModel(nn.Module):

    def __init__(self, conv_layers: List[Tuple[int, int, int]], dropout: float=0.0, mode: str='default', conv_bias: bool=False):
        super().__init__()
        assert mode in {'default', 'layer_norm'}

        def block(n_in, n_out, k, stride, is_layer_norm=False, is_group_norm=False, conv_bias=False):

            def make_conv():
                conv = nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias)
                nn.init.kaiming_normal_(conv.weight)
                return conv
            assert (is_layer_norm and is_group_norm) == False, 'layer norm and group norm are exclusive'
            if is_layer_norm:
                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.Sequential(TransposeLast(), Fp32LayerNorm(dim, elementwise_affine=True), TransposeLast()), nn.GELU())
            elif is_group_norm:
                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), Fp32GroupNorm(dim, dim, affine=True), nn.GELU())
            else:
                return nn.Sequential(make_conv(), nn.Dropout(p=dropout), nn.GELU())
        in_d = 1
        self.conv_layers = nn.ModuleList()
        for i, cl in enumerate(conv_layers):
            assert len(cl) == 3, 'invalid conv definition: ' + str(cl)
            dim, k, stride = cl
            self.conv_layers.append(block(in_d, dim, k, stride, is_layer_norm=mode == 'layer_norm', is_group_norm=mode == 'default' and i == 0, conv_bias=conv_bias))
            in_d = dim

    def forward(self, x):
        x = x.unsqueeze(1)
        for conv in self.conv_layers:
            x = conv(x)
        return x


def load_audio(manifest_path, max_keep, min_keep):
    n_long, n_short = 0, 0
    names, inds, sizes = [], [], []
    with open(manifest_path) as f:
        root = f.readline().strip()
        for ind, line in enumerate(f):
            items = line.strip().split('\t')
            assert len(items) == 2, line
            sz = int(items[1])
            if min_keep is not None and sz < min_keep:
                n_short += 1
            elif max_keep is not None and sz > max_keep:
                n_long += 1
            else:
                names.append(items[0])
                inds.append(ind)
                sizes.append(sz)
    tot = ind + 1
    logger.info(f'max_keep={max_keep}, min_keep={min_keep}, loaded {len(names)}, skipped {n_short} short and {n_long} long, longest-loaded={max(sizes)}, shortest-loaded={min(sizes)}')
    return root, names, inds, tot, sizes


def load_label(label_path, inds, tot):
    with open(label_path) as f:
        labels = [line.rstrip() for line in f]
        assert len(labels) == tot, f'number of labels does not match ({len(labels)} != {tot})'
        labels = [labels[i] for i in inds]
    return labels


def load_label_offset(label_path, inds, tot):
    with open(label_path) as f:
        code_lengths = [len(line.encode('utf-8')) for line in f]
        assert len(code_lengths) == tot, f'number of labels does not match ({len(code_lengths)} != {tot})'
        offsets = list(itertools.accumulate([0] + code_lengths))
        offsets = [(offsets[i], offsets[i + 1]) for i in inds]
    return offsets


FEATURE_OR_SF_AUDIO_FILE_EXTENSIONS = {'.npy', '.wav', '.flac', '.ogg'}


def parse_path(path: str) ->Tuple[str, List[int]]:
    """Parse data path which is either a path to
    1. a .npy/.wav/.flac/.ogg file
    2. a stored ZIP file with slicing info: "[zip_path]:[offset]:[length]"

      Args:
          path (str): the data path to parse

      Returns:
          file_path (str): the file path
          slice_ptr (list of int): empty in case 1;
            byte offset and length for the slice in case 2
    """
    if Path(path).suffix in FEATURE_OR_SF_AUDIO_FILE_EXTENSIONS:
        _path, slice_ptr = path, []
    else:
        _path, *slice_ptr = path.split(':')
        if not Path(_path).is_file():
            raise FileNotFoundError(f'File not found: {_path}')
    assert len(slice_ptr) in {0, 2}, f'Invalid path: {path}'
    slice_ptr = [int(i) for i in slice_ptr]
    return _path, slice_ptr


def mmap_read(path: str, offset: int, length: int) ->bytes:
    with open(path, 'rb') as f:
        with mmap.mmap(f.fileno(), length=0, access=mmap.ACCESS_READ) as mmap_o:
            data = mmap_o[offset:offset + length]
    return data


def read_from_stored_zip(zip_path: str, offset: int, length: int) ->bytes:
    return mmap_read(zip_path, offset, length)


def verify_label_lengths(audio_sizes, audio_rate, label_path, label_rate, inds, tot, tol=0.1):
    if label_rate < 0:
        logger.info(f'{label_path} is sequence label. skipped')
        return
    with open(label_path) as f:
        lengths = [len(line.rstrip().split()) for line in f]
        assert len(lengths) == tot
        lengths = [lengths[i] for i in inds]
    num_invalid = 0
    for i, ind in enumerate(inds):
        dur_from_audio = audio_sizes[i] / audio_rate
        dur_from_label = lengths[i] / label_rate
        if abs(dur_from_audio - dur_from_label) > tol:
            logger.warning(f'audio and label duration differ too much (|{dur_from_audio} - {dur_from_label}| > {tol}) in line {ind + 1} of {label_path}. Check if `label_rate` is correctly set (currently {label_rate}). num. of samples = {audio_sizes[i]}; label length = {lengths[i]}')
            num_invalid += 1
    if num_invalid > 0:
        logger.warning(f'total {num_invalid} (audio, label) pairs with mismatched lengths')


class HubertDataset(FairseqDataset):

    def __init__(self, manifest_path: str, sample_rate: float, label_paths: List[str], label_rates: Union[List[float], float], pad_list: List[str], eos_list: List[str], label_processors: Optional[List[Any]]=None, max_keep_sample_size: Optional[int]=None, min_keep_sample_size: Optional[int]=None, max_sample_size: Optional[int]=None, shuffle: bool=True, pad_audio: bool=False, normalize: bool=False, store_labels: bool=True, random_crop: bool=False, single_target: bool=False):
        self.audio_root, self.audio_names, inds, tot, self.sizes = load_audio(manifest_path, max_keep_sample_size, min_keep_sample_size)
        self.sample_rate = sample_rate
        self.shuffle = shuffle
        self.random_crop = random_crop
        self.num_labels = len(label_paths)
        self.pad_list = pad_list
        self.eos_list = eos_list
        self.label_processors = label_processors
        self.single_target = single_target
        self.label_rates = [label_rates for _ in range(len(label_paths))] if isinstance(label_rates, float) else label_rates
        self.store_labels = store_labels
        if store_labels:
            self.label_list = [load_label(p, inds, tot) for p in label_paths]
        else:
            self.label_paths = label_paths
            self.label_offsets_list = [load_label_offset(p, inds, tot) for p in label_paths]
        assert label_processors is None or len(label_processors) == self.num_labels
        for label_path, label_rate in zip(label_paths, self.label_rates):
            verify_label_lengths(self.sizes, sample_rate, label_path, label_rate, inds, tot)
        self.max_sample_size = max_sample_size if max_sample_size is not None else sys.maxsize
        self.pad_audio = pad_audio
        self.normalize = normalize
        logger.info(f'pad_audio={pad_audio}, random_crop={random_crop}, normalize={normalize}, max_sample_size={self.max_sample_size}')

    def get_audio(self, index):
        wav_path = os.path.join(self.audio_root, self.audio_names[index])
        _path, slice_ptr = parse_path(wav_path)
        if len(slice_ptr) == 0:
            wav, cur_sample_rate = sf.read(_path)
        else:
            assert _path.endswith('.zip')
            data = read_from_stored_zip(_path, slice_ptr[0], slice_ptr[1])
            f = io.BytesIO(data)
            wav, cur_sample_rate = sf.read(f)
        wav = torch.from_numpy(wav).float()
        wav = self.postprocess(wav, cur_sample_rate)
        return wav

    def get_label(self, index, label_idx):
        if self.store_labels:
            label = self.label_list[label_idx][index]
        else:
            with open(self.label_paths[label_idx]) as f:
                offset_s, offset_e = self.label_offsets_list[label_idx][index]
                f.seek(offset_s)
                label = f.read(offset_e - offset_s)
        if self.label_processors is not None:
            label = self.label_processors[label_idx](label)
        return label

    def get_labels(self, index):
        return [self.get_label(index, i) for i in range(self.num_labels)]

    def __getitem__(self, index):
        wav = self.get_audio(index)
        labels = self.get_labels(index)
        return {'id': index, 'source': wav, 'label_list': labels}

    def __len__(self):
        return len(self.sizes)

    def crop_to_max_size(self, wav, target_size):
        size = len(wav)
        diff = size - target_size
        if diff <= 0:
            return wav, 0
        start, end = 0, target_size
        if self.random_crop:
            start = np.random.randint(0, diff + 1)
            end = size - diff + start
        return wav[start:end], start

    def collater(self, samples):
        samples = [s for s in samples if s['source'] is not None]
        if len(samples) == 0:
            return {}
        audios = [s['source'] for s in samples]
        audio_sizes = [len(s) for s in audios]
        if self.pad_audio:
            audio_size = min(max(audio_sizes), self.max_sample_size)
        else:
            audio_size = min(min(audio_sizes), self.max_sample_size)
        collated_audios, padding_mask, audio_starts = self.collater_audio(audios, audio_size)
        targets_by_label = [[s['label_list'][i] for s in samples] for i in range(self.num_labels)]
        targets_list, lengths_list, ntokens_list = self.collater_label(targets_by_label, audio_size, audio_starts)
        net_input = {'source': collated_audios, 'padding_mask': padding_mask}
        batch = {'id': torch.LongTensor([s['id'] for s in samples]), 'net_input': net_input}
        if self.single_target:
            batch['target_lengths'] = lengths_list[0]
            batch['ntokens'] = ntokens_list[0]
            batch['target'] = targets_list[0]
        else:
            batch['target_lengths_list'] = lengths_list
            batch['ntokens_list'] = ntokens_list
            batch['target_list'] = targets_list
        return batch

    def collater_audio(self, audios, audio_size):
        collated_audios = audios[0].new_zeros(len(audios), audio_size)
        padding_mask = torch.BoolTensor(collated_audios.shape).fill_(False)
        audio_starts = [(0) for _ in audios]
        for i, audio in enumerate(audios):
            diff = len(audio) - audio_size
            if diff == 0:
                collated_audios[i] = audio
            elif diff < 0:
                assert self.pad_audio
                collated_audios[i] = torch.cat([audio, audio.new_full((-diff,), 0.0)])
                padding_mask[i, diff:] = True
            else:
                collated_audios[i], audio_starts[i] = self.crop_to_max_size(audio, audio_size)
        return collated_audios, padding_mask, audio_starts

    def collater_frm_label(self, targets, audio_size, audio_starts, label_rate, pad):
        assert label_rate > 0
        s2f = label_rate / self.sample_rate
        frm_starts = [int(round(s * s2f)) for s in audio_starts]
        frm_size = int(round(audio_size * s2f))
        if not self.pad_audio:
            rem_size = [(len(t) - s) for t, s in zip(targets, frm_starts)]
            frm_size = min(frm_size, *rem_size)
        targets = [t[s:s + frm_size] for t, s in zip(targets, frm_starts)]
        logger.debug(f'audio_starts={audio_starts}')
        logger.debug(f'frame_starts={frm_starts}')
        logger.debug(f'frame_size={frm_size}')
        lengths = torch.LongTensor([len(t) for t in targets])
        ntokens = lengths.sum().item()
        targets = data_utils.collate_tokens(targets, pad_idx=pad, left_pad=False)
        return targets, lengths, ntokens

    def collater_seq_label(self, targets, pad):
        lengths = torch.LongTensor([len(t) for t in targets])
        ntokens = lengths.sum().item()
        targets = data_utils.collate_tokens(targets, pad_idx=pad, left_pad=False)
        return targets, lengths, ntokens

    def collater_label(self, targets_by_label, audio_size, audio_starts):
        targets_list, lengths_list, ntokens_list = [], [], []
        itr = zip(targets_by_label, self.label_rates, self.pad_list)
        for targets, label_rate, pad in itr:
            if label_rate == -1.0:
                targets, lengths, ntokens = self.collater_seq_label(targets, pad)
            else:
                targets, lengths, ntokens = self.collater_frm_label(targets, audio_size, audio_starts, label_rate, pad)
            targets_list.append(targets)
            lengths_list.append(lengths)
            ntokens_list.append(ntokens)
        return targets_list, lengths_list, ntokens_list

    def num_tokens(self, index):
        return self.size(index)

    def size(self, index):
        if self.pad_audio:
            return self.sizes[index]
        return min(self.sizes[index], self.max_sample_size)

    def ordered_indices(self):
        if self.shuffle:
            order = [np.random.permutation(len(self))]
        else:
            order = [np.arange(len(self))]
        order.append(self.sizes)
        return np.lexsort(order)[::-1]

    def postprocess(self, wav, cur_sample_rate):
        if wav.dim() == 2:
            wav = wav.mean(-1)
        assert wav.dim() == 1, wav.dim()
        if cur_sample_rate != self.sample_rate:
            raise Exception(f'sr {cur_sample_rate} != {self.sample_rate}')
        if self.normalize:
            with torch.no_grad():
                wav = F.layer_norm(wav, wav.shape)
        return wav


class LabelEncoder(object):

    def __init__(self, dictionary):
        self.dictionary = dictionary

    def __call__(self, label):
        return self.dictionary.encode_line(label, append_eos=False, add_if_not_exist=False)


TASK_CLASS_NAMES = set()


TASK_DATACLASS_REGISTRY = {}


TASK_REGISTRY = {}


def register_task(name, dataclass=None):
    """
    New tasks can be added to fairseq with the
    :func:`~fairseq.tasks.register_task` function decorator.

    For example::

        @register_task('classification')
        class ClassificationTask(FairseqTask):
            (...)

    .. note::

        All Tasks must implement the :class:`~fairseq.tasks.FairseqTask`
        interface.

    Args:
        name (str): the name of the task
    """

    def register_task_cls(cls):
        if name in TASK_REGISTRY:
            return TASK_REGISTRY[name]
        if not issubclass(cls, FairseqTask):
            raise ValueError('Task ({}: {}) must extend FairseqTask'.format(name, cls.__name__))
        if cls.__name__ in TASK_CLASS_NAMES:
            raise ValueError('Cannot register task with duplicate class name ({})'.format(cls.__name__))
        TASK_REGISTRY[name] = cls
        TASK_CLASS_NAMES.add(cls.__name__)
        if dataclass is not None and not issubclass(dataclass, FairseqDataclass):
            raise ValueError('Dataclass {} must extend FairseqDataclass'.format(dataclass))
        cls.__dataclass = dataclass
        if dataclass is not None:
            TASK_DATACLASS_REGISTRY[name] = dataclass
            cs = ConfigStore.instance()
            node = dataclass()
            node._name = name
            cs.store(name=name, group='task', node=node, provider='fairseq')
        return cls
    return register_task_cls


def unfold1d(x, kernel_size: int, padding_l: int, pad_value: float=0):
    """unfold T x B x C to T x B x C x K"""
    if kernel_size > 1:
        T, B, C = x.size()
        x = F.pad(x, (0, 0, 0, 0, padding_l, kernel_size - 1 - padding_l), value=pad_value)
        x = x.as_strided((T, B, C, kernel_size), (B * C, C, 1, B * C))
    else:
        x = x.unsqueeze(3)
    return x


@with_incremental_state
class DynamicConv1dTBC(nn.Module):
    """Dynamic lightweight convolution taking T x B x C inputs
    Args:
        input_size: # of channels of the input
        kernel_size: convolution channels
        padding_l: padding to the left when using "same" padding
        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)
        weight_dropout: the drop rate of the DropConnect to drop the weight
        weight_softmax: normalize the weight with softmax before the convolution
        renorm_padding: re-normalize the filters to ignore the padded part (only the non-padding parts sum up to 1)
        bias: use bias
        conv_bias: bias of the convolution
        query_size: specified when feeding a different input as the query
        in_proj: project the input and generate the filter together

    Shape:
        Input: TxBxC, i.e. (timesteps, batch_size, input_size)
        Output: TxBxC, i.e. (timesteps, batch_size, input_size)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias:   the learnable bias of the module of shape `(input_size)`
    """

    def __init__(self, input_size, kernel_size=1, padding_l=None, num_heads=1, weight_dropout=0.0, weight_softmax=False, renorm_padding=False, bias=False, conv_bias=False, query_size=None, in_proj=False):
        super().__init__()
        self.input_size = input_size
        self.query_size = input_size if query_size is None else query_size
        self.kernel_size = kernel_size
        self.padding_l = padding_l
        self.num_heads = num_heads
        self.weight_dropout_module = FairseqDropout(weight_dropout, module_name=self.__class__.__name__)
        self.weight_softmax = weight_softmax
        self.renorm_padding = renorm_padding
        if in_proj:
            self.weight_linear = Linear(self.input_size, self.input_size + num_heads * kernel_size * 1)
        else:
            self.weight_linear = Linear(self.query_size, num_heads * kernel_size * 1, bias=bias)
        if conv_bias:
            self.conv_bias = nn.Parameter(torch.Tensor(input_size))
        else:
            self.conv_bias = None
        self.reset_parameters()

    @property
    def in_proj(self):
        return self.weight_linear.out_features == self.input_size + self.num_heads * self.kernel_size

    def reset_parameters(self):
        self.weight_linear.reset_parameters()
        if self.conv_bias is not None:
            nn.init.constant_(self.conv_bias, 0.0)

    def forward(self, x, incremental_state=None, query=None, unfold=None):
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
            unfold: unfold the input or not. If not, we use the matrix trick instead
            query: use the specified query to predict the conv filters
        """
        unfold = x.size(0) > 512 if unfold is None else unfold
        unfold = unfold or incremental_state is not None
        assert query is None or not self.in_proj
        if query is None:
            query = x
        if unfold:
            output = self._forward_unfolded(x, incremental_state, query)
        else:
            output = self._forward_expanded(x, incremental_state, query)
        if self.conv_bias is not None:
            output = output + self.conv_bias.view(1, 1, -1)
        return output

    def _forward_unfolded(self, x, incremental_state, query):
        """The conventional implementation of convolutions.
        Unfolding the input by having a window shifting to the right."""
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        if self.in_proj:
            proj = self.weight_linear(x)
            x = proj.narrow(2, 0, self.input_size).contiguous()
            weight = proj.narrow(2, self.input_size, H * K).contiguous().view(T * B * H, -1)
        else:
            weight = self.weight_linear(query).view(T * B * H, -1)
        assert not self.renorm_padding or incremental_state is not None
        if incremental_state is not None:
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is None:
                input_buffer = x.new()
            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)
            if self.kernel_size > 1:
                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size + 1:])
            x_unfold = x_unfold.view(T * B * H, R, -1)
        else:
            padding_l = self.padding_l
            if K > T and padding_l == K - 1:
                weight = weight.narrow(1, K - T, T)
                K, padding_l = T, T - 1
            x_unfold = unfold1d(x, K, padding_l, 0)
            x_unfold = x_unfold.view(T * B * H, R, K)
        if self.weight_softmax and not self.renorm_padding:
            weight = F.softmax(weight, dim=1)
        weight = weight.narrow(1, 0, K)
        if incremental_state is not None:
            weight = weight[:, -x_unfold.size(2):]
            K = weight.size(1)
        if self.weight_softmax and self.renorm_padding:
            weight = F.softmax(weight, dim=1)
        weight = self.weight_dropout_module(weight, inplace=False)
        output = torch.bmm(x_unfold, weight.unsqueeze(2))
        output = output.view(T, B, C)
        return output

    def _forward_expanded(self, x, incremental_stat, query):
        """Turn the convolution filters into band matrices and do matrix multiplication.
        This is faster when the sequence is short, but less memory efficient.
        This is not used in the decoder during inference.
        """
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        if self.in_proj:
            proj = self.weight_linear(x)
            x = proj.narrow(2, 0, self.input_size).contiguous()
            weight = proj.narrow(2, self.input_size, H * K).contiguous().view(T * B * H, -1)
        else:
            weight = self.weight_linear(query).view(T * B * H, -1)
        if not self.renorm_padding:
            if self.weight_softmax:
                weight = F.softmax(weight, dim=1)
            weight = self.weight_dropout_module(weight, inplace=False)
        weight = weight.narrow(1, 0, K).contiguous()
        weight = weight.view(T, B * H, K).transpose(0, 1)
        x = x.view(T, B * H, R).transpose(0, 1)
        if self.weight_softmax and self.renorm_padding:
            weight_expanded = weight.new(B * H, T, T + K - 1).fill_(float('-inf'))
            weight_expanded.as_strided((B * H, T, K), (T * (T + K - 1), T + K, 1)).copy_(weight)
            weight_expanded = weight_expanded.narrow(2, self.padding_l, T)
            weight_expanded = F.softmax(weight_expanded, dim=2)
            weight_expanded = self.weight_dropout_module(weight_expanded, inplace=False)
        else:
            P = self.padding_l
            if K > T and P == K - 1:
                weight = weight.narrow(2, K - T, T)
                K, P = T, T - 1
            weight_expanded = weight.new_zeros(B * H, T, T + K - 1, requires_grad=False)
            weight_expanded.as_strided((B * H, T, K), (T * (T + K - 1), T + K, 1)).copy_(weight)
            weight_expanded = weight_expanded.narrow(2, P, T)
        output = torch.bmm(weight_expanded, x)
        output = output.transpose(0, 1).contiguous().view(T, B, C)
        return output

    def reorder_incremental_state(self, incremental_state, new_order):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state):
        return utils.get_incremental_state(self, incremental_state, 'input_buffer')

    def _set_input_buffer(self, incremental_state, new_buffer):
        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)

    def extra_repr(self):
        s = '{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, conv_bias={}, renorm_padding={}, in_proj={}'.format(self.input_size, self.kernel_size, self.padding_l, self.num_heads, self.weight_softmax, self.conv_bias is not None, self.renorm_padding, self.in_proj)
        if self.query_size != self.input_size:
            s += ', query_size={}'.format(self.query_size)
        if self.weight_dropout_module.p > 0.0:
            s += ', weight_dropout={}'.format(self.weight_dropout_module.p)
        return s


def DynamicConv(input_size, kernel_size=1, padding_l=None, num_heads=1, weight_dropout=0.0, weight_softmax=False, renorm_padding=False, bias=False, conv_bias=False, query_size=None, in_proj=False):
    if torch.cuda.is_available():
        try:
            return DynamicconvLayer(input_size, kernel_size=kernel_size, padding_l=padding_l, num_heads=num_heads, weight_dropout=weight_dropout, weight_softmax=weight_softmax, renorm_padding=renorm_padding, bias=bias, conv_bias=conv_bias, query_size=query_size)
        except ImportError as e:
            None
    return DynamicConv1dTBC(input_size, kernel_size=kernel_size, padding_l=padding_l, num_heads=num_heads, weight_dropout=weight_dropout, weight_softmax=weight_softmax, renorm_padding=renorm_padding, bias=bias, conv_bias=conv_bias, query_size=query_size)


@with_incremental_state
class LightweightConv1dTBC(nn.Module):
    """Lightweight Convolution assuming the input is TxBxC
    Args:
        input_size: # of channels of the input
        kernel_size: convolution channels
        padding_l: padding to the left when using "same" padding
        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)
        weight_dropout: the drop rate of the DropConnect to drop the weight
        weight_softmax: normalize the weight with softmax before the convolution
        bias: use bias

    Shape:
        Input: TxBxC, i.e. (timesteps, batch_size, input_size)
        Output: TxBxC, i.e. (timesteps, batch_size, input_size)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias:   the learnable bias of the module of shape `(input_size)`
    """

    def __init__(self, input_size, kernel_size=1, padding_l=None, num_heads=1, weight_dropout=0.0, weight_softmax=False, bias=False):
        super().__init__()
        self.input_size = input_size
        self.kernel_size = kernel_size
        self.padding_l = padding_l
        self.num_heads = num_heads
        self.weight_dropout_module = FairseqDropout(weight_dropout, module_name=self.__class__.__name__)
        self.weight_softmax = weight_softmax
        self.weight = nn.Parameter(torch.Tensor(num_heads, 1, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(input_size))
        else:
            self.bias = None
        self.reset_parameters()
        self.onnx_trace = False

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.constant_(self.bias, 0.0)

    def forward(self, x, incremental_state=None, unfold=False):
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
            unfold: unfold the input or not. If not, we use the matrix trick instead
        """
        unfold = unfold or incremental_state is not None
        if unfold:
            output = self._forward_unfolded(x, incremental_state)
        else:
            output = self._forward_expanded(x, incremental_state)
        if self.bias is not None:
            output = output + self.bias.view(1, 1, -1)
        return output

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def _forward_unfolded(self, x, incremental_state):
        """The conventional implementation of convolutions.
        Unfolding the input by having a window shifting to the right."""
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        weight = self.weight.view(H, K)
        if incremental_state is not None:
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is None:
                input_buffer = x.new()
            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)
            if self.kernel_size > 1:
                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size + 1:])
            x_unfold = x_unfold.view(T * B * H, R, -1)
        else:
            x_unfold = unfold1d(x, self.kernel_size, self.padding_l, 0)
            x_unfold = x_unfold.view(T * B * H, R, K)
        if self.weight_softmax:
            weight = utils.softmax(weight, dim=1, onnx_trace=self.onnx_trace).type_as(weight)
        if incremental_state is not None:
            weight = weight[:, -x_unfold.size(2):]
            K = weight.size(1)
        weight = weight.view(1, H, K).expand(T * B, H, K).contiguous().view(T * B * H, K, 1)
        weight = self.weight_dropout_module(weight)
        output = torch.bmm(x_unfold, weight)
        output = output.view(T, B, C)
        return output

    def _forward_expanded(self, x, incremental_state):
        """Turn the convolution filters into band matrices and do matrix multiplication.
        This is faster when the sequence is short, but less memory efficient.
        This is not used in the decoder during inference.
        """
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        weight = self.weight.view(H, K)
        if self.weight_softmax:
            weight = utils.softmax(weight, dim=1, onnx_trace=self.onnx_trace).type_as(weight)
        weight = weight.view(1, H, K).expand(T * B, H, K).contiguous()
        weight = weight.view(T, B * H, K).transpose(0, 1)
        x = x.view(T, B * H, R).transpose(0, 1)
        P = self.padding_l
        if K > T and P == K - 1:
            weight = weight.narrow(2, K - T, T)
            K, P = T, T - 1
        weight_expanded = weight.new_zeros(B * H, T, T + K - 1, requires_grad=False)
        weight_expanded.as_strided((B * H, T, K), (T * (T + K - 1), T + K, 1)).copy_(weight)
        weight_expanded = weight_expanded.narrow(2, P, T)
        weight_expanded = self.weight_dropout_module(weight_expanded)
        output = torch.bmm(weight_expanded, x)
        output = output.transpose(0, 1).contiguous().view(T, B, C)
        return output

    def reorder_incremental_state(self, incremental_state, new_order):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state):
        return utils.get_incremental_state(self, incremental_state, 'input_buffer')

    def _set_input_buffer(self, incremental_state, new_buffer):
        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)

    def extra_repr(self):
        s = '{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, bias={}'.format(self.input_size, self.kernel_size, self.padding_l, self.num_heads, self.weight_softmax, self.bias is not None)
        if self.weight_dropout_module.p > 0.0:
            s += ', weight_dropout={}'.format(self.weight_dropout_module.p)
        return s


def LightweightConv(input_size, kernel_size=1, padding_l=None, num_heads=1, weight_dropout=0.0, weight_softmax=False, bias=False):
    if torch.cuda.is_available():
        try:
            return LightconvLayer(input_size, kernel_size=kernel_size, padding_l=padding_l, num_heads=num_heads, weight_dropout=weight_dropout, weight_softmax=weight_softmax, bias=bias)
        except ImportError as e:
            None
    return LightweightConv1dTBC(input_size, kernel_size=kernel_size, padding_l=padding_l, num_heads=num_heads, weight_dropout=weight_dropout, weight_softmax=weight_softmax, bias=bias)


class LightConvDecoderLayer(nn.Module):
    """Decoder layer block.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        no_encoder_attn (bool, optional): whether to attend to encoder outputs.
            Default: ``False``
        kernel_size: kernel size of the convolution
    """

    def __init__(self, args, no_encoder_attn=False, kernel_size=0, dictionary=None):
        super().__init__()
        self.embed_dim = args.decoder_embed_dim
        self.conv_dim = args.decoder_conv_dim
        if args.decoder_glu:
            self.linear1 = Linear(self.embed_dim, 2 * self.conv_dim)
            self.act = nn.GLU()
        else:
            self.linear1 = Linear(self.embed_dim, self.conv_dim)
            self.act = None
        if args.decoder_conv_type == 'lightweight':
            self.conv = LightweightConv(self.conv_dim, kernel_size, padding_l=kernel_size - 1, weight_softmax=args.weight_softmax, num_heads=args.decoder_attention_heads, weight_dropout=args.weight_dropout)
        elif args.decoder_conv_type == 'dynamic':
            self.conv = DynamicConv(self.conv_dim, kernel_size, padding_l=kernel_size - 1, weight_softmax=args.weight_softmax, num_heads=args.decoder_attention_heads, weight_dropout=args.weight_dropout)
        else:
            raise NotImplementedError
        self.linear2 = Linear(self.conv_dim, self.embed_dim)
        self.dropout_module = FairseqDropout(args.dropout, module_name=self.__class__.__name__)
        self.relu_dropout_module = FairseqDropout(args.relu_dropout, module_name=self.__class__.__name__)
        self.input_dropout_module = FairseqDropout(args.input_dropout, module_name=self.__class__.__name__)
        self.normalize_before = args.decoder_normalize_before
        self.conv_layer_norm = LayerNorm(self.embed_dim)
        if no_encoder_attn:
            self.encoder_attn = None
            self.encoder_attn_layer_norm = None
        else:
            self.encoder_attn = MultiheadAttention(self.embed_dim, args.decoder_attention_heads, dropout=args.attention_dropout, encoder_decoder_attention=True, dictionary=dictionary)
            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)
        self.fc1 = Linear(self.embed_dim, args.decoder_ffn_embed_dim)
        self.fc2 = Linear(args.decoder_ffn_embed_dim, self.embed_dim)
        self.final_layer_norm = LayerNorm(self.embed_dim)
        self.need_attn = True

    def forward(self, x: Tensor, encoder_out: Optional[Tensor], encoder_padding_mask: Optional[Tensor], incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], prev_conv_state: Optional[Tensor]=None, prev_attn_state: Optional[Tuple[Tensor, Tensor]]=None, conv_mask: Optional[Tensor]=None, conv_padding_mask: Optional[Tensor]=None):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, src_len)` where padding elements are indicated by ``1``.

        Returns:
            encoded output of shape `(batch, src_len, embed_dim)`
        """
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.conv_layer_norm(x)
        if prev_conv_state is not None:
            self.conv._set_input_buffer(incremental_state, prev_conv_state)
        x = self.input_dropout_module(x)
        x = self.linear1(x)
        if self.act is not None:
            x = self.act(x)
        x = self.conv(x, incremental_state=incremental_state)
        x = self.linear2(x)
        x = self.dropout_module(x)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.conv_layer_norm(x)
        attn: Optional[Tensor] = None
        if self.encoder_attn is not None:
            residual = x
            normalize = self.maybe_layer_norm(before=True)
            if normalize:
                x = self.encoder_attn_layer_norm(x)
            if prev_attn_state is not None:
                saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_attn_state[0], 'prev_value': prev_attn_state[1]}
                self.encoder_attn._set_input_buffer(incremental_state, saved_state)
            x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=not self.training and self.need_attn)
            x = self.dropout_module(x)
            x = residual + x
            normalize = self.maybe_layer_norm(after=True)
            if normalize:
                x = self.encoder_attn_layer_norm(x)
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.final_layer_norm(x)
        x = F.relu(self.fc1(x))
        x = self.relu_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.final_layer_norm(x)
        return x, attn

    def maybe_layer_norm(self, before: bool=False, after: bool=False):
        assert before ^ after, 'Incorrect usage'
        return after ^ self.normalize_before

    def make_generation_fast_(self, need_attn: bool=False, **kwargs):
        self.need_attn = need_attn

    def extra_repr(self):
        return 'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'.format(self.dropout_module.p, self.relu_dropout_module.p, self.input_dropout_module.p, self.normalize_before)


class LightConvDecoder(FairseqIncrementalDecoder):
    """
    LightConv decoder consisting of *args.decoder_layers* layers. Each layer
    is a :class:`LightConvDecoderLayer`.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): decoding dictionary
        embed_tokens (torch.nn.Embedding): output embedding
        no_encoder_attn (bool, optional): whether to attend to encoder outputs.
            Default: ``False``
    """

    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False, final_norm=True):
        super().__init__(dictionary)
        self.dropout_module = FairseqDropout(args.dropout, module_name=self.__class__.__name__)
        self.share_input_output_embed = args.share_decoder_input_output_embed
        input_embed_dim = embed_tokens.embedding_dim
        embed_dim = args.decoder_embed_dim
        output_embed_dim = args.decoder_output_dim
        padding_idx = embed_tokens.padding_idx
        self.max_target_positions = args.max_target_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None
        self.embed_positions = PositionalEmbedding(args.max_target_positions, embed_dim, padding_idx, learned=args.decoder_learned_pos) if not args.no_token_positional_embeddings else None
        self.layers = nn.ModuleList([])
        self.layers.extend([LightConvDecoderLayer(args, no_encoder_attn, kernel_size=args.decoder_kernel_size_list[i], dictionary=dictionary) for i in range(args.decoder_layers)])
        self.adaptive_softmax = None
        self.output_projection = None
        self.project_out_dim = Linear(embed_dim, output_embed_dim, bias=False) if embed_dim != output_embed_dim and not args.tie_adaptive_weights else None
        if args.adaptive_softmax_cutoff is not None:
            self.adaptive_softmax = AdaptiveSoftmax(len(dictionary), output_embed_dim, utils.eval_str_list(args.adaptive_softmax_cutoff, type=int), dropout=args.adaptive_softmax_dropout, adaptive_inputs=embed_tokens if args.tie_adaptive_weights else None, factor=args.adaptive_softmax_factor, tie_proj=args.tie_adaptive_proj)
        elif self.share_input_output_embed:
            self.output_projection = nn.Linear(self.embed_tokens.weight.shape[1], self.embed_tokens.weight.shape[0], bias=False)
            self.output_projection.weight = self.embed_tokens.weight
        else:
            self.output_projection = nn.Linear(output_embed_dim, len(dictionary), bias=False)
            nn.init.normal_(self.output_projection.weight, mean=0, std=output_embed_dim ** -0.5)
        self.register_buffer('version', torch.Tensor([2]))
        self.normalize = args.decoder_normalize_before and final_norm
        if self.normalize:
            self.layer_norm = LayerNorm(embed_dim)
        else:
            self.layer_norm = None

    def forward(self, prev_output_tokens: Tensor, encoder_out: Optional[Dict[str, List[Tensor]]]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, src_lengths: Optional[Any]=None):
        """
        Args:
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (Tensor, optional): output from the encoder, used for
                encoder-side attention
            incremental_state (dict): dictionary used for storing state during
                :ref:`Incremental decoding`

        Returns:
            tuple:
                - the last decoder layer's output of shape `(batch, tgt_len,
                  vocab)`
                - the last decoder layer's attention weights of shape `(batch,
                  tgt_len, src_len)`
        """
        positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state) if self.embed_positions is not None else None
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        x = self.embed_scale * self.embed_tokens(prev_output_tokens.contiguous())
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        attn = None
        inner_states: List[Optional[Tensor]] = [x]
        attn: Optional[Tensor] = None
        for layer in self.layers:
            encoder: Optional[Tensor] = None
            encoder_padding_mask: Optional[Tensor] = None
            if encoder_out is not None:
                if len(encoder_out['encoder_out']) > 0:
                    encoder = encoder_out['encoder_out'][0]
                if 'encoder_padding_mask' in encoder_out and len(encoder_out['encoder_padding_mask']) > 0:
                    encoder_padding_mask = encoder_out['encoder_padding_mask'][0]
            x, attn = layer(x, encoder, encoder_padding_mask, incremental_state)
            inner_states.append(x)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)
        if self.adaptive_softmax is None:
            x = self.output_projection(x)
        return x, {'attn': [attn], 'inner_states': inner_states}

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        if self.embed_positions is None:
            return self.max_target_positions
        return min(self.max_target_positions, self.embed_positions.max_positions)

    def buffered_future_mask(self, tensor):
        dim = tensor.size(0)
        if not hasattr(self, '_future_mask') or self._future_mask is None or self._future_mask.device != tensor.device:
            self._future_mask = torch.triu(utils.fill_with_neg_inf(tensor.new(dim, dim)), 1)
        if self._future_mask.size(0) < dim:
            self._future_mask = torch.triu(utils.fill_with_neg_inf(self._future_mask.resize_(dim, dim)), 1)
        return self._future_mask[:dim, :dim]


class LightConvEncoderLayer(nn.Module):
    """Encoder layer block.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        kernel_size: kernel size of the convolution
    """

    def __init__(self, args, kernel_size=0):
        super().__init__()
        self.embed_dim = args.encoder_embed_dim
        self.conv_dim = args.encoder_conv_dim
        padding_l = kernel_size // 2 if kernel_size % 2 == 1 else ((kernel_size - 1) // 2, kernel_size // 2)
        if args.encoder_glu:
            self.linear1 = Linear(self.embed_dim, 2 * self.conv_dim)
            self.act = nn.GLU()
        else:
            self.linear1 = Linear(self.embed_dim, self.conv_dim)
            self.act = None
        if args.encoder_conv_type == 'lightweight':
            self.conv = LightweightConv(self.conv_dim, kernel_size, padding_l=padding_l, weight_softmax=args.weight_softmax, num_heads=args.encoder_attention_heads, weight_dropout=args.weight_dropout)
        elif args.encoder_conv_type == 'dynamic':
            self.conv = DynamicConv(self.conv_dim, kernel_size, padding_l=padding_l, weight_softmax=args.weight_softmax, num_heads=args.encoder_attention_heads, weight_dropout=args.weight_dropout)
        else:
            raise NotImplementedError
        self.linear2 = Linear(self.conv_dim, self.embed_dim)
        self.dropout_module = FairseqDropout(args.dropout, module_name=self.__class__.__name__)
        self.relu_dropout_module = FairseqDropout(args.relu_dropout, module_name=self.__class__.__name__)
        self.input_dropout_module = FairseqDropout(args.input_dropout, module_name=self.__class__.__name__)
        self.normalize_before = args.encoder_normalize_before
        self.fc1 = Linear(self.embed_dim, args.encoder_ffn_embed_dim)
        self.fc2 = Linear(args.encoder_ffn_embed_dim, self.embed_dim)
        self.layer_norm1 = LayerNorm(self.embed_dim)
        self.layer_norm2 = LayerNorm(self.embed_dim)

    def forward(self, x, encoder_padding_mask: Optional[Tensor]=None) ->Tensor:
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, src_len)` where padding elements are indicated by ``1``.

        Returns:
            encoded output of shape `(batch, src_len, embed_dim)`
        """
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.layer_norm1(x)
        x = self.input_dropout_module(x)
        x = self.linear1(x)
        if self.act is not None:
            x = self.act(x)
        if encoder_padding_mask is not None:
            x = x.masked_fill(encoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)
        x = self.conv(x)
        x = self.linear2(x)
        x = self.dropout_module(x)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.layer_norm1(x)
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.layer_norm2(x)
        x = F.relu(self.fc1(x))
        x = self.relu_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.layer_norm2(x)
        return x

    def maybe_layer_norm(self, before: bool=False, after: bool=False):
        assert before ^ after, 'Incorrect arguments'
        return after ^ self.normalize_before

    def extra_repr(self):
        return 'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'.format(self.dropout_module.p, self.relu_dropout_module.p, self.input_dropout_module.p, self.normalize_before)


class LightConvEncoder(FairseqEncoder):
    """
    LightConv encoder consisting of *args.encoder_layers* layers. Each layer
    is a :class:`LightConvEncoderLayer`.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): encoding dictionary
        embed_tokens (torch.nn.Embedding): input embedding
    """

    def __init__(self, args, dictionary, embed_tokens):
        super().__init__(dictionary)
        self.dropout_module = FairseqDropout(args.dropout, module_name=self.__class__.__name__)
        embed_dim = embed_tokens.embedding_dim
        self.padding_idx = embed_tokens.padding_idx
        self.max_source_positions = args.max_source_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(args.max_source_positions, embed_dim, self.padding_idx, learned=args.encoder_learned_pos) if not args.no_token_positional_embeddings else None
        self.layers = nn.ModuleList([])
        self.layers.extend([LightConvEncoderLayer(args, kernel_size=args.encoder_kernel_size_list[i]) for i in range(args.encoder_layers)])
        self.register_buffer('version', torch.Tensor([2]))
        self.normalize = args.encoder_normalize_before
        if self.normalize:
            self.layer_norm = LayerNorm(embed_dim)
        else:
            self.layer_norm = None

    def forward(self, src_tokens: Tensor, src_lengths: Optional[Tensor]=None) ->Dict[str, List[Tensor]]:
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`

        Returns:
            dict:
                - **encoder_out** (Tensor): the last encoder layer's output of
                  shape `(src_len, batch, embed_dim)`
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
        """
        x = self.embed_scale * self.embed_tokens(src_tokens)
        if self.embed_positions is not None:
            x += self.embed_positions(src_tokens)
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        if not encoder_padding_mask.any():
            encoder_mask = None
        else:
            encoder_mask = encoder_padding_mask
        for layer in self.layers:
            x = layer(x, encoder_mask)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        output_dict: Dict[str, List[Tensor]] = {}
        if src_lengths is not None:
            output_dict['src_lengths'] = [src_lengths]
        output_dict['encoder_out'] = [x]
        if encoder_mask is not None:
            output_dict['encoder_padding_mask'] = [encoder_mask]
        return output_dict

    @torch.jit.export
    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order: Tensor):
        """
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        if len(encoder_out['encoder_out']) == 0:
            encoder = []
        else:
            encoder = [encoder_out['encoder_out'][0].index_select(1, new_order)]
        output_dict = {'encoder_out': encoder}
        if 'encoder_padding_mask' not in encoder_out or len(encoder_out['encoder_padding_mask']) == 0:
            encoder_padding_mask = []
        else:
            encoder_padding_mask = [encoder_out['encoder_padding_mask'][0].index_select(0, new_order)]
        output_dict['encoder_padding_mask'] = encoder_padding_mask
        return output_dict

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        if self.embed_positions is None:
            return self.max_source_positions
        return min(self.max_source_positions, self.embed_positions.max_positions)


class MLPAttention(nn.Module):
    """The original attention from Badhanau et al. (2014)

    https://arxiv.org/abs/1409.0473, based on a Multi-Layer Perceptron.
    The attention score between position i in the encoder and position j in the
    decoder is: alpha_ij = V_a * tanh(W_ae * enc_i + W_ad * dec_j + b_a)
    """

    def __init__(self, decoder_hidden_state_dim, context_dim, attention_dim):
        super().__init__()
        self.context_dim = context_dim
        self.attention_dim = attention_dim
        self.encoder_proj = nn.Linear(context_dim, self.attention_dim, bias=True)
        self.decoder_proj = nn.Linear(decoder_hidden_state_dim, self.attention_dim, bias=False)
        self.to_scores = nn.Linear(self.attention_dim, 1, bias=False)

    def forward(self, decoder_state, source_hids, encoder_padding_mask):
        """The expected input dimensions are:
        decoder_state: bsz x decoder_hidden_state_dim
        source_hids: src_len x bsz x context_dim
        encoder_padding_mask: src_len x bsz
        """
        src_len, bsz, _ = source_hids.size()
        flat_source_hids = source_hids.view(-1, self.context_dim)
        encoder_component = self.encoder_proj(flat_source_hids)
        encoder_component = encoder_component.view(src_len, bsz, self.attention_dim)
        decoder_component = self.decoder_proj(decoder_state).unsqueeze(0)
        hidden_att = torch.tanh((decoder_component + encoder_component).view(-1, self.attention_dim))
        attn_scores = self.to_scores(hidden_att).view(src_len, bsz)
        if encoder_padding_mask is not None:
            attn_scores = attn_scores.float().masked_fill_(encoder_padding_mask, float('-inf')).type_as(attn_scores)
        normalized_masked_attn_scores = F.softmax(attn_scores, dim=0)
        attn_weighted_context = (source_hids * normalized_masked_attn_scores.unsqueeze(2)).sum(dim=0)
        return attn_weighted_context, normalized_masked_attn_scores


class LSTMDecoder(FairseqIncrementalDecoder):

    def __init__(self, dictionary, embed_dim, num_layers, hidden_size, dropout, encoder_output_dim, attention_dim, output_layer_dim):
        """
        Args:
            dictionary: target text dictionary.
            embed_dim: embedding dimension for target tokens.
            num_layers: number of LSTM layers.
            hidden_size: hidden size for LSTM layers.
            dropout: dropout probability. Dropout can be applied to the
                embeddings, the LSTM layers, and the context vector.
            encoder_output_dim: encoder output dimension (hidden size of
                encoder LSTM).
            attention_dim: attention dimension for MLP attention.
            output_layer_dim: size of the linear layer prior to output
                projection.
        """
        super().__init__(dictionary)
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        num_embeddings = len(dictionary)
        padding_idx = dictionary.pad()
        self.embed_tokens = nn.Embedding(num_embeddings, embed_dim, padding_idx)
        if dropout > 0:
            self.dropout = nn.Dropout(p=dropout)
        else:
            self.dropout = None
        self.layers = nn.ModuleList()
        for layer_id in range(num_layers):
            input_size = embed_dim if layer_id == 0 else encoder_output_dim
            self.layers.append(nn.LSTMCell(input_size=input_size, hidden_size=hidden_size))
        self.context_dim = encoder_output_dim
        self.attention = MLPAttention(decoder_hidden_state_dim=hidden_size, context_dim=encoder_output_dim, attention_dim=attention_dim)
        self.deep_output_layer = nn.Linear(hidden_size + encoder_output_dim + embed_dim, output_layer_dim)
        self.output_projection = nn.Linear(output_layer_dim, num_embeddings)

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, **kwargs):
        encoder_padding_mask = encoder_out['encoder_padding_mask']
        encoder_outs = encoder_out['encoder_out']
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
        bsz, seqlen = prev_output_tokens.size()
        srclen = encoder_outs.size(0)
        embeddings = self.embed_tokens(prev_output_tokens)
        x = embeddings
        if self.dropout is not None:
            x = self.dropout(x)
        x = x.transpose(0, 1)
        cached_state = utils.get_incremental_state(self, incremental_state, 'cached_state')
        if cached_state is not None:
            prev_hiddens, prev_cells = cached_state
        else:
            prev_hiddens = [encoder_out['encoder_out'].mean(dim=0)] * self.num_layers
            prev_cells = [x.new_zeros(bsz, self.hidden_size)] * self.num_layers
        attn_scores = x.new_zeros(bsz, srclen)
        attention_outs = []
        outs = []
        for j in range(seqlen):
            input = x[j, :, :]
            attention_out = None
            for i, layer in enumerate(self.layers):
                hidden, cell = layer(input, (prev_hiddens[(i - 1) % self.num_layers], prev_cells[(i - 1) % self.num_layers]))
                if self.dropout is not None:
                    hidden = self.dropout(hidden)
                prev_hiddens[i] = hidden
                prev_cells[i] = cell
                if attention_out is None:
                    attention_out, attn_scores = self.attention(hidden, encoder_outs, encoder_padding_mask)
                    if self.dropout is not None:
                        attention_out = self.dropout(attention_out)
                    attention_outs.append(attention_out)
                input = attention_out
            outs.append(hidden)
        utils.set_incremental_state(self, incremental_state, 'cached_state', (prev_hiddens, prev_cells))
        x = torch.cat(outs, dim=0).view(seqlen, bsz, self.hidden_size)
        attention_outs_concat = torch.cat(attention_outs, dim=0).view(seqlen, bsz, self.context_dim)
        x = x.transpose(0, 1)
        attention_outs_concat = attention_outs_concat.transpose(0, 1)
        x = torch.cat((x, attention_outs_concat, embeddings), dim=2)
        x = self.deep_output_layer(x)
        x = torch.tanh(x)
        if self.dropout is not None:
            x = self.dropout(x)
        x = self.output_projection(x)
        return x, None

    def reorder_incremental_state(self, incremental_state, new_order):
        super().reorder_incremental_state(incremental_state, new_order)
        cached_state = utils.get_incremental_state(self, incremental_state, 'cached_state')
        if cached_state is None:
            return

        def reorder_state(state):
            if isinstance(state, list):
                return [reorder_state(state_i) for state_i in state]
            return state.index_select(0, new_order)
        new_state = tuple(map(reorder_state, cached_state))
        utils.set_incremental_state(self, incremental_state, 'cached_state', new_state)


def LSTM(input_size, hidden_size, **kwargs):
    m = nn.LSTM(input_size, hidden_size, **kwargs)
    for name, param in m.named_parameters():
        if 'weight' in name or 'bias' in name:
            param.data.uniform_(-0.1, 0.1)
    return m


class LSTMEncoder(FairseqEncoder):
    """LSTM encoder."""

    def __init__(self, dictionary, embed_dim=512, hidden_size=512, num_layers=1, dropout_in=0.1, dropout_out=0.1, bidirectional=False, left_pad=True, pretrained_embed=None, padding_idx=None, max_source_positions=DEFAULT_MAX_SOURCE_POSITIONS):
        super().__init__(dictionary)
        self.num_layers = num_layers
        self.dropout_in_module = FairseqDropout(dropout_in * 1.0, module_name=self.__class__.__name__)
        self.dropout_out_module = FairseqDropout(dropout_out * 1.0, module_name=self.__class__.__name__)
        self.bidirectional = bidirectional
        self.hidden_size = hidden_size
        self.max_source_positions = max_source_positions
        num_embeddings = len(dictionary)
        self.padding_idx = padding_idx if padding_idx is not None else dictionary.pad()
        if pretrained_embed is None:
            self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx)
        else:
            self.embed_tokens = pretrained_embed
        self.lstm = LSTM(input_size=embed_dim, hidden_size=hidden_size, num_layers=num_layers, dropout=self.dropout_out_module.p if num_layers > 1 else 0.0, bidirectional=bidirectional)
        self.left_pad = left_pad
        self.output_units = hidden_size
        if bidirectional:
            self.output_units *= 2

    def forward(self, src_tokens: Tensor, src_lengths: Tensor, enforce_sorted: bool=True):
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of
                shape `(batch, src_len)`
            src_lengths (LongTensor): lengths of each source sentence of
                shape `(batch)`
            enforce_sorted (bool, optional): if True, `src_tokens` is
                expected to contain sequences sorted by length in a
                decreasing order. If False, this condition is not
                required. Default: True.
        """
        if self.left_pad:
            src_tokens = utils.convert_padding_direction(src_tokens, torch.zeros_like(src_tokens).fill_(self.padding_idx), left_to_right=True)
        bsz, seqlen = src_tokens.size()
        x = self.embed_tokens(src_tokens)
        x = self.dropout_in_module(x)
        x = x.transpose(0, 1)
        packed_x = nn.utils.rnn.pack_padded_sequence(x, src_lengths.cpu(), enforce_sorted=enforce_sorted)
        if self.bidirectional:
            state_size = 2 * self.num_layers, bsz, self.hidden_size
        else:
            state_size = self.num_layers, bsz, self.hidden_size
        h0 = x.new_zeros(*state_size)
        c0 = x.new_zeros(*state_size)
        packed_outs, (final_hiddens, final_cells) = self.lstm(packed_x, (h0, c0))
        x, _ = nn.utils.rnn.pad_packed_sequence(packed_outs, padding_value=self.padding_idx * 1.0)
        x = self.dropout_out_module(x)
        assert list(x.size()) == [seqlen, bsz, self.output_units]
        if self.bidirectional:
            final_hiddens = self.combine_bidir(final_hiddens, bsz)
            final_cells = self.combine_bidir(final_cells, bsz)
        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()
        return tuple((x, final_hiddens, final_cells, encoder_padding_mask))

    def combine_bidir(self, outs, bsz: int):
        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()
        return out.view(self.num_layers, bsz, -1)

    def reorder_encoder_out(self, encoder_out: Tuple[Tensor, Tensor, Tensor, Tensor], new_order):
        return tuple((encoder_out[0].index_select(1, new_order), encoder_out[1].index_select(1, new_order), encoder_out[2].index_select(1, new_order), encoder_out[3].index_select(1, new_order)))

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        return self.max_source_positions


class MaskedLMEncoder(FairseqEncoder):
    """
    Encoder for Masked Language Modelling.
    """

    def __init__(self, args, dictionary):
        super().__init__(dictionary)
        self.padding_idx = dictionary.pad()
        self.vocab_size = dictionary.__len__()
        self.max_positions = args.max_positions
        self.sentence_encoder = TransformerSentenceEncoder(padding_idx=self.padding_idx, vocab_size=self.vocab_size, num_encoder_layers=args.encoder_layers, embedding_dim=args.encoder_embed_dim, ffn_embedding_dim=args.encoder_ffn_embed_dim, num_attention_heads=args.encoder_attention_heads, dropout=args.dropout, attention_dropout=args.attention_dropout, activation_dropout=args.act_dropout, max_seq_len=self.max_positions, num_segments=args.num_segment, use_position_embeddings=not args.no_token_positional_embeddings, encoder_normalize_before=args.encoder_normalize_before, apply_bert_init=args.apply_bert_init, activation_fn=args.activation_fn, learned_pos_embedding=args.encoder_learned_pos)
        self.share_input_output_embed = args.share_encoder_input_output_embed
        self.embed_out = None
        self.sentence_projection_layer = None
        self.sentence_out_dim = args.sentence_class_num
        self.lm_output_learned_bias = None
        self.load_softmax = not getattr(args, 'remove_head', False)
        self.masked_lm_pooler = nn.Linear(args.encoder_embed_dim, args.encoder_embed_dim)
        self.pooler_activation = utils.get_activation_fn(args.pooler_activation_fn)
        self.lm_head_transform_weight = nn.Linear(args.encoder_embed_dim, args.encoder_embed_dim)
        self.activation_fn = utils.get_activation_fn(args.activation_fn)
        self.layer_norm = LayerNorm(args.encoder_embed_dim)
        self.lm_output_learned_bias = None
        if self.load_softmax:
            self.lm_output_learned_bias = nn.Parameter(torch.zeros(self.vocab_size))
            if not self.share_input_output_embed:
                self.embed_out = nn.Linear(args.encoder_embed_dim, self.vocab_size, bias=False)
            if args.sent_loss:
                self.sentence_projection_layer = nn.Linear(args.encoder_embed_dim, self.sentence_out_dim, bias=False)

    def forward(self, src_tokens, segment_labels=None, masked_tokens=None, **unused):
        """
        Forward pass for Masked LM encoder. This first computes the token
        embedding using the token embedding matrix, position embeddings (if
        specified) and segment embeddings (if specified).

        Here we assume that the sentence representation corresponds to the
        output of the classification_token (see bert_task or cross_lingual_lm
        task for more details).
        Args:
            - src_tokens: B x T matrix representing sentences
            - segment_labels: B x T matrix representing segment label for tokens
        Returns:
            - a tuple of the following:
                - logits for predictions in format B x T x C to be used in
                  softmax afterwards
                - a dictionary of additional data, where 'pooled_output' contains
                  the representation for classification_token and 'inner_states'
                  is a list of internal model states used to compute the
                  predictions (similar in ELMO). 'sentence_logits'
                  is the prediction logit for NSP task and is only computed if
                  this is specified in the input arguments.
        """
        inner_states, sentence_rep = self.sentence_encoder(src_tokens, segment_labels=segment_labels)
        x = inner_states[-1].transpose(0, 1)
        if masked_tokens is not None:
            x = x[masked_tokens, :]
        x = self.layer_norm(self.activation_fn(self.lm_head_transform_weight(x)))
        pooled_output = self.pooler_activation(self.masked_lm_pooler(sentence_rep))
        if self.share_input_output_embed and hasattr(self.sentence_encoder.embed_tokens, 'weight'):
            x = F.linear(x, self.sentence_encoder.embed_tokens.weight)
        elif self.embed_out is not None:
            x = self.embed_out(x)
        if self.lm_output_learned_bias is not None:
            x = x + self.lm_output_learned_bias
        sentence_logits = None
        if self.sentence_projection_layer:
            sentence_logits = self.sentence_projection_layer(pooled_output)
        return x, {'inner_states': inner_states, 'pooled_output': pooled_output, 'sentence_logits': sentence_logits}

    def max_positions(self):
        """Maximum output length supported by the encoder."""
        return self.max_positions

    def upgrade_state_dict_named(self, state_dict, name):
        if isinstance(self.sentence_encoder.embed_positions, SinusoidalPositionalEmbedding):
            state_dict[name + '.sentence_encoder.embed_positions._float_tensor'] = torch.FloatTensor(1)
        if not self.load_softmax:
            for k in list(state_dict.keys()):
                if 'embed_out.weight' in k or 'sentence_projection_layer.weight' in k or 'lm_output_learned_bias' in k:
                    del state_dict[k]
        return state_dict


def ensemble_encoder(func):

    def wrapper(self, *args, **kwargs):
        if self.ensemble_models is None or len(self.ensemble_models) == 1:
            return func(self, *args, **kwargs)
        encoder_outs = [func(model, *args, **kwargs, return_all_hiddens=True) for model in self.ensemble_models]
        _encoder_out = encoder_outs[0].copy()

        def stack(key):
            outs = [e[key][0] for e in encoder_outs]
            return [torch.stack(outs, -1) if outs[0] is not None else None]
        _encoder_out['encoder_out'] = stack('encoder_out')
        _encoder_out['encoder_embedding'] = stack('encoder_embedding')
        num_layers = len(_encoder_out['encoder_states'])
        if num_layers > 0:
            _encoder_out['encoder_states'] = [torch.stack([e['encoder_states'][i] for e in encoder_outs], -1) for i in range(num_layers)]
        return _encoder_out
    return wrapper


def ensemble_decoder(func):

    def wrapper(self, normalize=False, encoder_out=None, *args, **kwargs):
        if self.ensemble_models is None or len(self.ensemble_models) == 1:
            return func(self, *args, normalize=normalize, encoder_out=encoder_out, **kwargs)

        def _replace(encoder_out, new_val):
            new_encoder_out = encoder_out.copy()
            new_encoder_out['encoder_out'] = [new_val]
            return new_encoder_out
        action_outs = [func(model, *args, normalize=normalize, encoder_out=_replace(encoder_out, encoder_out['encoder_out'][0][:, :, :, i]), **kwargs) for i, model in enumerate(self.ensemble_models)]
        if not isinstance(action_outs[0], tuple):
            action_outs = [[a] for a in action_outs]
        else:
            action_outs = [list(a) for a in action_outs]
        ensembled_outs = []
        for i in range(len(action_outs[0])):
            if i == 0 and normalize:
                ensembled_outs += [torch.logsumexp(torch.stack([a[i] for a in action_outs], -1), dim=-1) - math.log(len(self.ensemble_models))]
            elif action_outs[0][i] is not None:
                ensembled_outs += [torch.stack([a[i] for a in action_outs], -1)]
            else:
                ensembled_outs += [None]
        if len(ensembled_outs) == 1:
            return ensembled_outs[0]
        return tuple(ensembled_outs)
    return wrapper


class _EnsembleModelEncoder(object):

    def __init__(self, models):
        self.models = models

    def reorder_encoder_out(self, encoder_outs, new_order):
        encoder_outs = [model.encoder.reorder_encoder_out(encoder_out, new_order) for model, encoder_out in zip(self.models, encoder_outs)]
        return encoder_outs


class BasicEnsembleModel(torch.nn.Module):
    """A wrapper around an ensemble of models."""

    def __init__(self, models):
        super().__init__()
        self.models = torch.nn.ModuleList(models)
        self.bos = self.models[0].decoder.dictionary.bos()
        self.eos = self.models[0].decoder.dictionary.eos()
        self.pad = self.models[0].decoder.dictionary.pad()
        self.unk = self.models[0].decoder.dictionary.unk()
        self.encoder = _EnsembleModelEncoder(self.models)

    def has_encoder(self):
        return hasattr(self.models[0], 'encoder')

    def max_decoder_positions(self):
        return min(m.max_decoder_positions() for m in self.models)

    @torch.no_grad()
    def forward_encoder(self, encoder_input):
        if not self.has_encoder():
            return None
        return [model.forward_encoder(encoder_input) for model in self.models]

    @torch.no_grad()
    def forward_decoder(self, *inputs):
        raise NotImplementedError

    def initialize_output_tokens(self, *inputs):
        raise NotImplementedError


def new_arange(x, *size):
    """
    Return a Tensor of `size` filled with a range function on the device of x.
    If size is empty, using the size of the variable x.
    """
    if len(size) == 0:
        size = x.size()
    return torch.arange(size[-1], device=x.device).expand(*size).contiguous()


def _apply_del_words(in_tokens, in_scores, in_attn, word_del_pred, padding_idx, bos_idx, eos_idx):
    in_masks = in_tokens.ne(padding_idx)
    bos_eos_masks = in_tokens.eq(bos_idx) | in_tokens.eq(eos_idx)
    max_len = in_tokens.size(1)
    word_del_pred.masked_fill_(~in_masks, 1)
    word_del_pred.masked_fill_(bos_eos_masks, 0)
    reordering = new_arange(in_tokens).masked_fill_(word_del_pred, max_len).sort(1)[1]
    out_tokens = in_tokens.masked_fill(word_del_pred, padding_idx).gather(1, reordering)
    out_scores = None
    if in_scores is not None:
        out_scores = in_scores.masked_fill(word_del_pred, 0).gather(1, reordering)
    out_attn = None
    if in_attn is not None:
        _mask = word_del_pred[:, :, None].expand_as(in_attn)
        _reordering = reordering[:, :, None].expand_as(in_attn)
        out_attn = in_attn.masked_fill(_mask, 0.0).gather(1, _reordering)
    return out_tokens, out_scores, out_attn


def _apply_ins_masks(in_tokens, in_scores, mask_ins_pred, padding_idx, unk_idx, eos_idx):
    in_masks = in_tokens.ne(padding_idx)
    in_lengths = in_masks.sum(1)
    in_tokens.masked_fill_(~in_masks, eos_idx)
    mask_ins_pred.masked_fill_(~in_masks[:, 1:], 0)
    out_lengths = in_lengths + mask_ins_pred.sum(1)
    out_max_len = out_lengths.max()
    out_masks = new_arange(out_lengths, out_max_len)[None, :] < out_lengths[:, None]
    reordering = (mask_ins_pred + in_masks[:, 1:].long()).cumsum(1)
    out_tokens = in_tokens.new_zeros(in_tokens.size(0), out_max_len).fill_(padding_idx).masked_fill_(out_masks, unk_idx)
    out_tokens[:, 0] = in_tokens[:, 0]
    out_tokens.scatter_(1, reordering, in_tokens[:, 1:])
    out_scores = None
    if in_scores is not None:
        in_scores.masked_fill_(~in_masks, 0)
        out_scores = in_scores.new_zeros(*out_tokens.size())
        out_scores[:, 0] = in_scores[:, 0]
        out_scores.scatter_(1, reordering, in_scores[:, 1:])
    return out_tokens, out_scores


def _apply_ins_words(in_tokens, in_scores, word_ins_pred, word_ins_scores, unk_idx):
    word_ins_masks = in_tokens.eq(unk_idx)
    out_tokens = in_tokens.masked_scatter(word_ins_masks, word_ins_pred[word_ins_masks])
    if in_scores is not None:
        out_scores = in_scores.masked_scatter(word_ins_masks, word_ins_scores[word_ins_masks])
    else:
        out_scores = None
    return out_tokens, out_scores


def _fill(x, mask, y, padding_idx):
    """
    Filling tensor x with y at masked positions (dim=0).
    """
    if x is None:
        return y
    assert x.dim() == y.dim() and mask.size(0) == x.size(0)
    assert x.dim() == 2 or x.dim() == 3 and x.size(2) == y.size(2)
    n_selected = mask.sum()
    assert n_selected == y.size(0)
    if n_selected == x.size(0):
        return y
    if x.size(1) < y.size(1):
        dims = [x.size(0), y.size(1) - x.size(1)]
        if x.dim() == 3:
            dims.append(x.size(2))
        x = torch.cat([x, x.new_zeros(*dims).fill_(padding_idx)], 1)
        x[mask] = y
    elif x.size(1) > y.size(1):
        x[mask] = padding_idx
        if x.dim() == 2:
            x[mask, :y.size(1)] = y
        else:
            x[mask, :y.size(1), :] = y
    else:
        x[mask] = y
    return x


def _skip(x, mask):
    """
    Getting sliced (dim=0) tensor by mask. Supporting tensor and list/dict of tensors.
    """
    if isinstance(x, int):
        return x
    if x is None:
        return None
    if isinstance(x, torch.Tensor):
        if x.size(0) == mask.size(0):
            return x[mask]
        elif x.size(1) == mask.size(0):
            return x[:, mask]
    if isinstance(x, list):
        return [_skip(x_i, mask) for x_i in x]
    if isinstance(x, dict):
        return {k: _skip(v, mask) for k, v in x.items()}
    raise NotImplementedError


def _skip_encoder_out(encoder, encoder_out, mask):
    if not mask.any():
        return encoder_out
    else:
        return encoder.reorder_encoder_out(encoder_out, mask.nonzero(as_tuple=False).squeeze())


class EnsembleLevT(BasicEnsembleModel):
    """A wrapper around an ensemble of models."""

    def __init__(self, models):
        super().__init__(models)

    @torch.no_grad()
    def forward_decoder(self, decoder_out, encoder_outs, eos_penalty=0.0, max_ratio=None, **kwargs):
        output_tokens = decoder_out.output_tokens
        output_scores = decoder_out.output_scores
        attn = decoder_out.attn
        bsz = output_tokens.size(0)
        if max_ratio is None:
            max_lens = output_tokens.new().fill_(255)
        else:
            if not encoder_outs[0]['encoder_padding_mask']:
                src_lens = encoder_outs[0]['encoder_out'][0].new(bsz).fill_(encoder_outs[0]['encoder_out'][0].size(1))
            else:
                src_lens = (~encoder_outs[0]['encoder_padding_mask'][0]).sum(1)
            max_lens = (src_lens * max_ratio).clamp(min=10).long()
        can_del_word = output_tokens.ne(self.pad).sum(1) > 2
        if can_del_word.sum() != 0:
            output_tokens, output_scores, attn = self.forward_word_del(encoder_outs, output_tokens, output_scores, attn, can_del_word)
        can_ins_mask = output_tokens.ne(self.pad).sum(1) < max_lens
        if can_ins_mask.sum() != 0:
            output_tokens, output_scores = self.forward_mask_ins(encoder_outs, output_tokens, output_scores, can_ins_mask, eos_penalty, max_lens)
        can_ins_word = output_tokens.eq(self.unk).sum(1) > 0
        if can_ins_word.sum() != 0:
            output_tokens, output_scores, attn = self.forward_word_ins(encoder_outs, output_tokens, output_scores, attn, can_ins_word)
        cut_off = output_tokens.ne(self.pad).sum(1).max()
        output_tokens = output_tokens[:, :cut_off]
        output_scores = output_scores[:, :cut_off]
        attn = None if attn is None else attn[:, :cut_off, :]
        return decoder_out._replace(output_tokens=output_tokens, output_scores=output_scores, attn=attn, history=None)

    def forward_word_del(self, encoder_outs, output_tokens, output_scores, attn, can_del_word):
        word_del_score_avg = []
        word_del_attn_avg = []
        for model, encoder_out in zip(self.models, encoder_outs):
            word_del_out, word_del_attn = model.decoder.forward_word_del(_skip(output_tokens, can_del_word), _skip_encoder_out(model.encoder, encoder_out, can_del_word))
            word_del_score = F.log_softmax(word_del_out, 2)
            word_del_score_avg.append(word_del_score)
            word_del_attn_avg.append(word_del_attn)
        word_del_score_avg = torch.logsumexp(torch.stack(word_del_score_avg, dim=0), dim=0) - math.log(len(self.models))
        word_del_pred = word_del_score_avg.max(-1)[1].bool()
        if word_del_attn_avg[0] is not None:
            word_del_attn_avg = torch.stack(word_del_attn_avg, dim=0) / len(self.models)
        else:
            word_del_attn_avg = None
        _tokens, _scores, _attn = _apply_del_words(output_tokens[can_del_word], output_scores[can_del_word], word_del_attn_avg, word_del_pred, self.pad, self.bos, self.eos)
        output_tokens = _fill(output_tokens, can_del_word, _tokens, self.pad)
        output_scores = _fill(output_scores, can_del_word, _scores, 0)
        attn = _fill(attn, can_del_word, _attn, 0.0)
        return output_tokens, output_scores, attn

    def forward_mask_ins(self, encoder_outs, output_tokens, output_scores, can_ins_mask, eos_penalty, max_lens):
        mask_ins_score_avg = []
        for model, encoder_out in zip(self.models, encoder_outs):
            mask_ins_out, _ = model.decoder.forward_mask_ins(_skip(output_tokens, can_ins_mask), _skip_encoder_out(model.encoder, encoder_out, can_ins_mask))
            mask_ins_score = F.log_softmax(mask_ins_out, 2)
            if eos_penalty > 0.0:
                mask_ins_score[:, :, 0] -= eos_penalty
            mask_ins_score_avg.append(mask_ins_score)
        mask_ins_score_avg = torch.logsumexp(torch.stack(mask_ins_score_avg, dim=0), dim=0) - math.log(len(self.models))
        mask_ins_pred = mask_ins_score_avg.max(-1)[1]
        mask_ins_pred = torch.min(mask_ins_pred, max_lens[can_ins_mask, None].expand_as(mask_ins_pred))
        _tokens, _scores = _apply_ins_masks(output_tokens[can_ins_mask], output_scores[can_ins_mask], mask_ins_pred, self.pad, self.unk, self.eos)
        output_tokens = _fill(output_tokens, can_ins_mask, _tokens, self.pad)
        output_scores = _fill(output_scores, can_ins_mask, _scores, 0)
        return output_tokens, output_scores

    def forward_word_ins(self, encoder_outs, output_tokens, output_scores, attn, can_ins_word):
        word_ins_score_avg = []
        word_ins_attn_avg = []
        for model, encoder_out in zip(self.models, encoder_outs):
            word_ins_out, word_ins_attn = model.decoder.forward_word_ins(_skip(output_tokens, can_ins_word), _skip_encoder_out(model.encoder, encoder_out, can_ins_word))
            word_ins_score = F.log_softmax(word_ins_out, 2)
            word_ins_score_avg.append(word_ins_score)
            word_ins_attn_avg.append(word_ins_attn)
        word_ins_score_avg = torch.logsumexp(torch.stack(word_ins_score_avg, dim=0), dim=0) - math.log(len(self.models))
        if word_ins_attn_avg[0] is not None:
            word_ins_attn_avg = torch.stack(word_ins_attn_avg, dim=0) / len(self.models)
        else:
            word_ins_attn_avg = None
        word_ins_score_max, word_ins_pred = word_ins_score_avg.max(-1)
        _tokens, _scores = _apply_ins_words(output_tokens[can_ins_word], output_scores[can_ins_word], word_ins_pred, word_ins_score_max, self.unk)
        output_tokens = _fill(output_tokens, can_ins_word, _tokens, self.pad)
        output_scores = _fill(output_scores, can_ins_word, _scores, 0)
        attn = _fill(attn, can_ins_word, word_ins_attn, 0.0)
        return output_tokens, output_scores, attn

    def initialize_output_tokens(self, encoder_outs, src_tokens):
        return self.models[0].initialize_output_tokens(encoder_outs[0], src_tokens)


def _mean_pooling(enc_feats, src_masks):
    if src_masks is None:
        enc_feats = enc_feats.mean(0)
    else:
        src_masks = (~src_masks).transpose(0, 1).type_as(enc_feats)
        enc_feats = (enc_feats / src_masks.sum(0)[None, :, None] * src_masks[:, :, None]).sum(0)
    return enc_feats


def _uniform_assignment(src_lens, trg_lens):
    max_trg_len = trg_lens.max()
    steps = (src_lens.float() - 1) / (trg_lens.float() - 1)
    index_t = utils.new_arange(trg_lens, max_trg_len).float()
    index_t = steps[:, None] * index_t[None, :]
    index_t = torch.round(index_t).long().detach()
    return index_t


class RobertaHubInterface(nn.Module):
    """A simple PyTorch Hub interface to RoBERTa.

    Usage: https://github.com/pytorch/fairseq/tree/main/examples/roberta
    """

    def __init__(self, cfg, task, model):
        super().__init__()
        self.cfg = cfg
        self.task = task
        self.model = model
        self.bpe = encoders.build_bpe(cfg.bpe)
        self.register_buffer('_float_tensor', torch.tensor([0], dtype=torch.float))

    @property
    def device(self):
        return self._float_tensor.device

    def encode(self, sentence: str, *addl_sentences, no_separator=False) ->torch.LongTensor:
        """
        BPE-encode a sentence (or multiple sentences).

        Every sequence begins with a beginning-of-sentence (`<s>`) symbol.
        Every sentence ends with an end-of-sentence (`</s>`) and we use an
        extra end-of-sentence (`</s>`) as a separator.

        Example (single sentence): `<s> a b c </s>`
        Example (sentence pair): `<s> d e f </s> </s> 1 2 3 </s>`

        The BPE encoding follows GPT-2. One subtle detail is that the GPT-2 BPE
        requires leading spaces. For example::

            >>> roberta.encode('Hello world').tolist()
            [0, 31414, 232, 2]
            >>> roberta.encode(' world').tolist()
            [0, 232, 2]
            >>> roberta.encode('world').tolist()
            [0, 8331, 2]
        """
        bpe_sentence = '<s> ' + self.bpe.encode(sentence) + ' </s>'
        for s in addl_sentences:
            bpe_sentence += ' </s>' if not no_separator else ''
            bpe_sentence += ' ' + self.bpe.encode(s) + ' </s>'
        tokens = self.task.source_dictionary.encode_line(bpe_sentence, append_eos=False, add_if_not_exist=False)
        return tokens.long()

    def decode(self, tokens: torch.LongTensor):
        assert tokens.dim() == 1
        tokens = tokens.numpy()
        if tokens[0] == self.task.source_dictionary.bos():
            tokens = tokens[1:]
        eos_mask = tokens == self.task.source_dictionary.eos()
        doc_mask = eos_mask[1:] & eos_mask[:-1]
        sentences = np.split(tokens, doc_mask.nonzero()[0] + 1)
        sentences = [self.bpe.decode(self.task.source_dictionary.string(s)) for s in sentences]
        if len(sentences) == 1:
            return sentences[0]
        return sentences

    def extract_features(self, tokens: torch.LongTensor, return_all_hiddens: bool=False) ->torch.Tensor:
        if tokens.dim() == 1:
            tokens = tokens.unsqueeze(0)
        if tokens.size(-1) > self.model.max_positions():
            raise ValueError('tokens exceeds maximum length: {} > {}'.format(tokens.size(-1), self.model.max_positions()))
        features, extra = self.model(tokens, features_only=True, return_all_hiddens=return_all_hiddens)
        if return_all_hiddens:
            inner_states = extra['inner_states']
            return [inner_state.transpose(0, 1) for inner_state in inner_states]
        else:
            return features

    def register_classification_head(self, name: str, num_classes: int=None, embedding_size: int=None, **kwargs):
        self.model.register_classification_head(name, num_classes=num_classes, embedding_size=embedding_size, **kwargs)

    def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool=False):
        features = self.extract_features(tokens)
        logits = self.model.classification_heads[head](features)
        if return_logits:
            return logits
        return F.log_softmax(logits, dim=-1)

    def extract_features_aligned_to_words(self, sentence: str, return_all_hiddens: bool=False) ->torch.Tensor:
        """Extract RoBERTa features, aligned to spaCy's word-level tokenizer."""
        nlp = alignment_utils.spacy_nlp()
        tokenizer = alignment_utils.spacy_tokenizer()
        bpe_toks = self.encode(sentence)
        spacy_toks = tokenizer(sentence)
        spacy_toks_ws = [t.text_with_ws for t in tokenizer(sentence)]
        alignment = alignment_utils.align_bpe_to_words(self, bpe_toks, spacy_toks_ws)
        features = self.extract_features(bpe_toks, return_all_hiddens=return_all_hiddens)
        features = features.squeeze(0)
        aligned_feats = alignment_utils.align_features_to_words(self, features, alignment)
        doc = Doc(nlp.vocab, words=['<s>'] + [x.text for x in spacy_toks] + ['</s>'], spaces=[True] + [x.endswith(' ') for x in spacy_toks_ws[:-1]] + [True, False])
        assert len(doc) == aligned_feats.size(0)
        doc.user_token_hooks['vector'] = lambda token: aligned_feats[token.i]
        return doc

    def fill_mask(self, masked_input: str, topk: int=5):
        masked_token = '<mask>'
        assert masked_token in masked_input and masked_input.count(masked_token) == 1, "Please add one {0} token for the input, eg: 'He is a {0} guy'".format(masked_token)
        text_spans = masked_input.split(masked_token)
        text_spans_bpe = ' {0} '.format(masked_token).join([self.bpe.encode(text_span.rstrip()) for text_span in text_spans]).strip()
        tokens = self.task.source_dictionary.encode_line('<s> ' + text_spans_bpe + ' </s>', append_eos=False, add_if_not_exist=False)
        masked_index = (tokens == self.task.mask_idx).nonzero(as_tuple=False)
        if tokens.dim() == 1:
            tokens = tokens.unsqueeze(0)
        with utils.model_eval(self.model):
            features, extra = self.model(tokens.long(), features_only=False, return_all_hiddens=False)
        logits = features[0, masked_index, :].squeeze()
        prob = logits.softmax(dim=0)
        values, index = prob.topk(k=topk, dim=0)
        topk_predicted_token_bpe = self.task.source_dictionary.string(index)
        topk_filled_outputs = []
        for index, predicted_token_bpe in enumerate(topk_predicted_token_bpe.split(' ')):
            predicted_token = self.bpe.decode(predicted_token_bpe)
            if predicted_token_bpe.startswith(''):
                predicted_token = ' ' + predicted_token
            if ' {0}'.format(masked_token) in masked_input:
                topk_filled_outputs.append((masked_input.replace(' {0}'.format(masked_token), predicted_token), values[index].item(), predicted_token))
            else:
                topk_filled_outputs.append((masked_input.replace(masked_token, predicted_token), values[index].item(), predicted_token))
        return topk_filled_outputs

    def disambiguate_pronoun(self, sentence: str) ->bool:
        """
        Usage::

            >>> disambiguate_pronoun('The _trophy_ would not fit in the brown suitcase because [it] was too big.')
            True

            >>> disambiguate_pronoun('The trophy would not fit in the brown suitcase because [it] was too big.')
            'The trophy'
        """
        assert hasattr(self.task, 'disambiguate_pronoun'), 'roberta.disambiguate_pronoun() requires a model trained with the WSC task.'
        with utils.model_eval(self.model):
            return self.task.disambiguate_pronoun(self.model, sentence, use_cuda=self.device.type == 'cuda')


class RobertaLMHead(nn.Module):
    """Head for masked language modeling."""

    def __init__(self, embed_dim, output_dim, activation_fn, weight=None):
        super().__init__()
        self.dense = nn.Linear(embed_dim, embed_dim)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.layer_norm = LayerNorm(embed_dim)
        if weight is None:
            weight = nn.Linear(embed_dim, output_dim, bias=False).weight
        self.weight = weight
        self.bias = nn.Parameter(torch.zeros(output_dim))

    def forward(self, features, masked_tokens=None, **kwargs):
        if masked_tokens is not None:
            features = features[masked_tokens, :]
        x = self.dense(features)
        x = self.activation_fn(x)
        x = self.layer_norm(x)
        x = F.linear(x, self.weight) + self.bias
        return x


class RobertaEncoder(FairseqEncoder):
    """RoBERTa encoder."""

    def __init__(self, args, dictionary):
        super().__init__(dictionary)
        base_architecture(args)
        self.args = args
        if args.encoder_layers_to_keep:
            args.encoder_layers = len(args.encoder_layers_to_keep.split(','))
        embed_tokens = self.build_embedding(len(dictionary), args.encoder_embed_dim, dictionary.pad())
        self.sentence_encoder = self.build_encoder(args, dictionary, embed_tokens)
        self.lm_head = self.build_lm_head(embed_dim=args.encoder_embed_dim, output_dim=len(dictionary), activation_fn=args.activation_fn, weight=self.sentence_encoder.embed_tokens.weight if not args.untie_weights_roberta else None)

    def build_embedding(self, vocab_size, embedding_dim, padding_idx):
        return nn.Embedding(vocab_size, embedding_dim, padding_idx)

    def build_encoder(self, args, dictionary, embed_tokens):
        encoder = TransformerEncoder(args, dictionary, embed_tokens)
        encoder.apply(init_bert_params)
        return encoder

    def build_lm_head(self, embed_dim, output_dim, activation_fn, weight):
        return RobertaLMHead(embed_dim, output_dim, activation_fn, weight)

    def forward(self, src_tokens, features_only=False, return_all_hiddens=False, masked_tokens=None, **unused):
        """
        Args:
            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`
            features_only (bool, optional): skip LM head and just return
                features. If True, the output will be of shape
                `(batch, src_len, embed_dim)`.
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).

        Returns:
            tuple:
                - the LM output of shape `(batch, src_len, vocab)`
                - a dictionary of additional data, where 'inner_states'
                  is a list of hidden states. Note that the hidden
                  states have shape `(src_len, batch, vocab)`.
        """
        x, extra = self.extract_features(src_tokens, return_all_hiddens=return_all_hiddens)
        if not features_only:
            x = self.output_layer(x, masked_tokens=masked_tokens)
        return x, extra

    def extract_features(self, src_tokens, return_all_hiddens=False, **kwargs):
        encoder_out = self.sentence_encoder(src_tokens, return_all_hiddens=return_all_hiddens, token_embeddings=kwargs.get('token_embeddings', None))
        features = encoder_out['encoder_out'][0].transpose(0, 1)
        inner_states = encoder_out['encoder_states'] if return_all_hiddens else None
        return features, {'inner_states': inner_states}

    def output_layer(self, features, masked_tokens=None, **unused):
        return self.lm_head(features, masked_tokens)

    def max_positions(self):
        """Maximum output length supported by the encoder."""
        return self.args.max_positions


class CTCDecoder(FairseqEncoder):

    def __init__(self, dictionary, in_dim):
        super().__init__(dictionary)
        self.proj = nn.Linear(in_dim, len(dictionary))

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        encoder_out = self.proj(src_tokens)
        return {'encoder_out': encoder_out}


class StackedEmbedding(nn.Embedding):
    """Embedding module that supports stacked units -> single embedding"""

    def __init__(self, num_embeddings, embed_dim, padding_idx, num_stacked=1):
        super().__init__(num_embeddings, embed_dim, padding_idx)
        nn.init.normal_(self.weight, mean=0, std=embed_dim ** -0.5)
        nn.init.constant_(self.weight[padding_idx], 0)
        self.offset = 4
        self.vocab_size = num_embeddings - self.offset
        self.num_stacked = num_stacked
        if self.num_stacked > 1:
            self.project_in_dim = Linear(embed_dim * num_stacked, embed_dim, bias=False)

    def forward(self, input):
        if self.num_stacked == 1:
            return super().forward(input)
        mask = input >= self.offset
        stacked_input = []
        cum_input = input.new_zeros(input.shape)
        for i in range(1, self.num_stacked + 1):
            div = pow(self.vocab_size, i)
            next_input = torch.remainder(input - self.offset - cum_input, div)
            cum_input += next_input
            next_input = torch.floor_divide(next_input, div // self.vocab_size)
            stacked_input.append((next_input + self.offset) * mask + input * ~mask)
        stacked_input = torch.stack(stacked_input[::-1], dim=2)
        embed = super().forward(stacked_input).view(input.size(0), input.size(1), -1)
        embed = self.project_in_dim(embed)
        return embed


class TransformerEncoderNoEmb(FairseqEncoder):
    """Transformer encoder without token embeddings."""

    def __init__(self, args):
        super().__init__(None)
        self.layers = nn.ModuleList([TransformerEncoderLayer(args) for _ in range(args.encoder_layers)])
        if args.encoder_normalize_before:
            self.layer_norm = LayerNorm(args.encoder_embed_dim)
        else:
            self.layer_norm = None

    def forward(self, x, encoder_padding_mask, return_all_hiddens=False):
        encoder_states = []
        for layer in self.layers:
            x = layer(x, encoder_padding_mask)
            if return_all_hiddens:
                encoder_states.append(x)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        return {'encoder_out': [x], 'encoder_padding_mask': [encoder_padding_mask] if encoder_padding_mask is not None and encoder_padding_mask.any() else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}

    def reorder_encoder_out(self, encoder_out, new_order):
        new_encoder_out = [] if len(encoder_out['encoder_out']) == 0 else [x.index_select(1, new_order) for x in encoder_out['encoder_out']]
        new_encoder_padding_mask = [] if len(encoder_out['encoder_padding_mask']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_padding_mask']]
        new_encoder_embedding = [] if len(encoder_out['encoder_embedding']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_embedding']]
        encoder_states = encoder_out['encoder_states']
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)
        return {'encoder_out': new_encoder_out, 'encoder_padding_mask': new_encoder_padding_mask, 'encoder_embedding': new_encoder_embedding, 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}


class Conv1dSubsampler(nn.Module):
    """Convolutional subsampler: a stack of 1D convolution (along temporal
    dimension) followed by non-linear activation via gated linear units
    (https://arxiv.org/abs/1911.08460)

    Args:
        in_channels (int): the number of input channels
        mid_channels (int): the number of intermediate channels
        out_channels (int): the number of output channels
        kernel_sizes (List[int]): the kernel size for each convolutional layer
    """

    def __init__(self, in_channels: int, mid_channels: int, out_channels: int, kernel_sizes: List[int]=(3, 3)):
        super(Conv1dSubsampler, self).__init__()
        self.n_layers = len(kernel_sizes)
        self.conv_layers = nn.ModuleList(nn.Conv1d(in_channels if i == 0 else mid_channels // 2, mid_channels if i < self.n_layers - 1 else out_channels * 2, k, stride=2, padding=k // 2) for i, k in enumerate(kernel_sizes))

    def get_out_seq_lens_tensor(self, in_seq_lens_tensor):
        out = in_seq_lens_tensor.clone()
        for _ in range(self.n_layers):
            out = ((out.float() - 1) / 2 + 1).floor().long()
        return out

    def forward(self, src_tokens, src_lengths):
        bsz, in_seq_len, _ = src_tokens.size()
        x = src_tokens.transpose(1, 2).contiguous()
        for conv in self.conv_layers:
            x = conv(x)
            x = nn.functional.glu(x, dim=1)
        _, _, out_seq_len = x.size()
        x = x.transpose(1, 2).transpose(0, 1).contiguous()
        return x, self.get_out_seq_lens_tensor(src_lengths)


def infer_conv_output_dim(conv_op, input_dim, sample_inchannel):
    sample_seq_len = 200
    sample_bsz = 10
    x = torch.randn(sample_bsz, sample_inchannel, sample_seq_len, input_dim)
    x = conv_op(x)
    x = x.transpose(1, 2)
    bsz, seq = x.size()[:2]
    per_channel_dim = x.size()[3]
    return x.contiguous().view(bsz, seq, -1).size(-1), per_channel_dim


class Conv2dSubsampler(nn.Module):
    """Convolutional subsampler: a stack of 2D convolution based on ESPnet implementation
    (https://github.com/espnet/espnet)

    Args:
        input_channels (int): the number of input channels
        input_feat_per_channel (int): encoder input dimension per input channel
        conv_out_channels (int): the number of output channels of conv layer
        encoder_embed_dim (int): encoder dimentions
    """

    def __init__(self, input_channels: int, input_feat_per_channel: int, conv_out_channels: int, encoder_embed_dim: int):
        super().__init__()
        assert input_channels == 1, input_channels
        self.conv = torch.nn.Sequential(torch.nn.Conv2d(input_channels, conv_out_channels, 3, stride=2, padding=3 // 2), torch.nn.ReLU(), torch.nn.Conv2d(conv_out_channels, conv_out_channels, 3, stride=2, padding=3 // 2), torch.nn.ReLU())
        transformer_input_dim = infer_conv_output_dim(input_channels, input_feat_per_channel, conv_out_channels)
        self.out = torch.nn.Linear(transformer_input_dim, encoder_embed_dim)

    def forward(self, src_tokens, src_lengths):
        B, T_i, C = src_tokens.size()
        x = src_tokens.view(B, T_i, 1, C).transpose(1, 2).contiguous()
        x = self.conv(x)
        B, _, T_o, _ = x.size()
        x = x.transpose(1, 2).transpose(0, 1).contiguous().view(T_o, B, -1)
        x = self.out(x)
        subsampling_factor = int(T_i * 1.0 / T_o + 0.5)
        input_len_0 = (src_lengths.float() / subsampling_factor).ceil().long()
        input_len_1 = x.size(0) * torch.ones([src_lengths.size(0)]).long()
        input_lengths = torch.min(input_len_0, input_len_1)
        return x, input_lengths


class S2TTransformerEncoder(FairseqEncoder):
    """Speech-to-text Transformer encoder that consists of input subsampler and
    Transformer encoder."""

    def __init__(self, args):
        super().__init__(None)
        self.encoder_freezing_updates = args.encoder_freezing_updates
        self.num_updates = 0
        self.dropout_module = FairseqDropout(p=args.dropout, module_name=self.__class__.__name__)
        self.embed_scale = math.sqrt(args.encoder_embed_dim)
        if args.no_scale_embedding:
            self.embed_scale = 1.0
        self.padding_idx = 1
        self.conv_version = args.conv_version
        if self.conv_version == 's2t_transformer':
            self.subsample = Conv1dSubsampler(args.input_feat_per_channel * args.input_channels, args.conv_channels, args.encoder_embed_dim, [int(k) for k in args.conv_kernel_sizes.split(',')])
        elif self.conv_version == 'convtransformer':
            self.subsample = Conv2dSubsampler(args.input_channels, args.input_feat_per_channel, args.conv_out_channels, args.encoder_embed_dim)
        self.embed_positions = PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim, self.padding_idx)
        self.transformer_layers = nn.ModuleList([TransformerEncoderLayer(args) for _ in range(args.encoder_layers)])
        if args.encoder_normalize_before:
            self.layer_norm = LayerNorm(args.encoder_embed_dim)
        else:
            self.layer_norm = None
        self.ctc_proj = None
        if getattr(args, 'ctc_weight', 0.0) > 0.0:
            self.ctc_proj = nn.Linear(args.encoder_embed_dim, args.tgt_dict_size)

    def _forward(self, src_tokens, src_lengths, return_all_hiddens=False):
        x, input_lengths = self.subsample(src_tokens, src_lengths)
        x = self.embed_scale * x
        encoder_padding_mask = lengths_to_padding_mask(input_lengths)
        positions = self.embed_positions(encoder_padding_mask).transpose(0, 1)
        x += positions
        x = self.dropout_module(x)
        encoder_states = []
        for layer in self.transformer_layers:
            x = layer(x, encoder_padding_mask)
            if return_all_hiddens:
                encoder_states.append(x)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        return {'encoder_out': [x], 'encoder_padding_mask': [encoder_padding_mask] if encoder_padding_mask.any() else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}

    def forward(self, src_tokens, src_lengths, return_all_hiddens=False):
        if self.num_updates < self.encoder_freezing_updates:
            with torch.no_grad():
                x = self._forward(src_tokens, src_lengths, return_all_hiddens=return_all_hiddens)
        else:
            x = self._forward(src_tokens, src_lengths, return_all_hiddens=return_all_hiddens)
        return x

    def reorder_encoder_out(self, encoder_out, new_order):
        new_encoder_out = [] if len(encoder_out['encoder_out']) == 0 else [x.index_select(1, new_order) for x in encoder_out['encoder_out']]
        new_encoder_padding_mask = [] if len(encoder_out['encoder_padding_mask']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_padding_mask']]
        new_encoder_embedding = [] if len(encoder_out['encoder_embedding']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_embedding']]
        encoder_states = encoder_out['encoder_states']
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)
        return {'encoder_out': new_encoder_out, 'encoder_padding_mask': new_encoder_padding_mask, 'encoder_embedding': new_encoder_embedding, 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}

    def set_num_updates(self, num_updates):
        super().set_num_updates(num_updates)
        self.num_updates = num_updates


class S2STransformerEncoder(S2TTransformerEncoder):
    """Based on S2T transformer encoder, with support
    to incorporate target speaker embedding."""

    def __init__(self, args):
        super().__init__(args)
        self.spk_emb_proj = None
        if args.target_speaker_embed:
            self.spk_emb_proj = Linear(args.encoder_embed_dim + args.speaker_embed_dim, args.encoder_embed_dim)

    def forward(self, src_tokens, src_lengths, tgt_speaker=None, return_all_hiddens=False):
        out = super().forward(src_tokens, src_lengths, return_all_hiddens)
        if self.spk_emb_proj:
            x = out['encoder_out'][0]
            seq_len, bsz, _ = x.size()
            tgt_speaker_emb = tgt_speaker.view(1, bsz, -1).expand(seq_len, bsz, -1)
            x = self.spk_emb_proj(torch.cat([x, tgt_speaker_emb], dim=2))
            out['encoder_out'][0] = x
        return out


class All2All(torch.autograd.Function):

    @staticmethod
    def forward(ctx, xs, input_splits=None, output_splits=None):
        ctx.input_splits = input_splits
        ctx.output_splits = output_splits
        ys = torch.empty_like(xs) if output_splits is None else xs.new_empty(size=[sum(output_splits)] + list(xs.size()[1:]))
        torch.distributed.all_to_all_single(ys, xs, output_split_sizes=output_splits, input_split_sizes=input_splits)
        return ys

    @staticmethod
    def backward(ctx, grad_output):
        result = torch.empty_like(grad_output) if ctx.input_splits is None else grad_output.new_empty(size=[sum(ctx.input_splits)] + list(grad_output.size()[1:]))
        torch.distributed.all_to_all_single(result, grad_output, output_split_sizes=ctx.input_splits, input_split_sizes=ctx.output_splits)
        return result, None, None


class BaseSublayer(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.activation_fn = utils.get_activation_fn(activation=getattr(args, 'activation_fn', 'relu') or 'relu')
        self.norm = LayerNorm(args.decoder_embed_dim, export=False)
        self.ff1 = torch.nn.Linear(args.decoder_embed_dim, args.decoder_ffn_embed_dim)
        self.ff2 = torch.nn.Linear(args.decoder_ffn_embed_dim, args.decoder_embed_dim)
        self.ff2.weight.data.zero_()

    def forward(self, xs):
        return xs + self.ff2(self.activation_fn(self.ff1(self.norm(xs))))


class BaseLayer(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.num_workers = distributed_utils.get_data_parallel_world_size()
        expert_centroids = torch.empty(self.num_workers, args.decoder_embed_dim)
        torch.nn.init.orthogonal_(expert_centroids, gain=0.1)
        self.register_parameter('expert_centroids', torch.nn.Parameter(expert_centroids))
        self.expert_network = nn.Sequential(*[BaseSublayer(args) for _ in range(args.base_sublayers)])
        self.expert_id = distributed_utils.get_data_parallel_rank()
        self.shuffle = args.base_shuffle
        self.cpp = self.load_assignment()
        for param in self.expert_network.parameters():
            param.expert = True

    def forward(self, input_features, *args, **kwargs):
        features = input_features.reshape(-1, input_features.size(-1))
        is_training = input_features.requires_grad
        if self.shuffle and is_training:
            shuffle_sort = torch.randperm(features.size(0), device=features.device)
            features = All2All.apply(features[shuffle_sort])
        with torch.no_grad():
            token_expert_affinities = features.matmul(self.expert_centroids.transpose(0, 1))
        sort_by_expert, input_splits, output_splits = self.balanced_assignment(token_expert_affinities) if is_training else self.greedy_assignment(token_expert_affinities)
        routed_features = All2All.apply(features[sort_by_expert], output_splits, input_splits)
        if routed_features.size(0) > 0:
            alpha = torch.sigmoid(routed_features.mv(self.expert_centroids[self.expert_id])).unsqueeze(1)
            routed_features = alpha * self.expert_network(routed_features) + (1 - alpha) * routed_features
        result = All2All.apply(routed_features, input_splits, output_splits)[self.inverse_sort(sort_by_expert)]
        if self.shuffle and is_training:
            result = All2All.apply(result)[self.inverse_sort(shuffle_sort)]
        return result.view(input_features.size()), None, None

    def inverse_sort(self, order):
        return torch.empty_like(order).scatter_(0, order, torch.arange(0, order.size(0), device=order.device))

    def balanced_assignment(self, scores):
        ok = scores.isfinite()
        if not ok.all():
            scores[~ok] = scores[ok].min()
        return self.cpp.balanced_assignment(scores), None, None

    def greedy_assignment(self, scores, k=1):
        token_to_workers = torch.topk(scores, dim=1, k=k, largest=True).indices.view(-1)
        token_to_workers, sort_ordering = torch.sort(token_to_workers)
        worker2token = sort_ordering // k
        output_splits = torch.zeros((self.num_workers,), dtype=torch.long, device=scores.device)
        workers, counts = torch.unique_consecutive(token_to_workers, return_counts=True)
        output_splits[workers] = counts
        input_splits = All2All.apply(output_splits)
        return worker2token, input_splits.tolist(), output_splits.tolist()

    def load_assignment(self):
        try:
            return libbase
        except ImportError as e:
            sys.stderr.write('ERROR: missing libbase. run `python setup.py build_ext --inplace`\n')
            raise e


def module_name_fordropout(module_name: str) ->str:
    if module_name == 'TransformerEncoderBase':
        return 'TransformerEncoder'
    else:
        return module_name


class TransformerDecoderBase(FairseqIncrementalDecoder):
    """
    Transformer decoder consisting of *cfg.decoder.layers* layers. Each layer
    is a :class:`TransformerDecoderLayer`.

    Args:
        cfg (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): decoding dictionary
        embed_tokens (torch.nn.Embedding): output embedding
        no_encoder_attn (bool, optional): whether to attend to encoder outputs
            (default: False).
    """

    def __init__(self, cfg, dictionary, embed_tokens, no_encoder_attn=False, output_projection=None):
        self.cfg = cfg
        super().__init__(dictionary)
        self.register_buffer('version', torch.Tensor([3]))
        self._future_mask = torch.empty(0)
        self.dropout_module = FairseqDropout(cfg.dropout, module_name=module_name_fordropout(self.__class__.__name__))
        self.decoder_layerdrop = cfg.decoder.layerdrop
        self.share_input_output_embed = cfg.share_decoder_input_output_embed
        input_embed_dim = embed_tokens.embedding_dim
        embed_dim = cfg.decoder.embed_dim
        self.embed_dim = embed_dim
        self.output_embed_dim = cfg.decoder.output_dim
        self.padding_idx = embed_tokens.padding_idx
        self.max_target_positions = cfg.max_target_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = 1.0 if cfg.no_scale_embedding else math.sqrt(embed_dim)
        if not cfg.adaptive_input and cfg.quant_noise.pq > 0:
            self.quant_noise = apply_quant_noise_(nn.Linear(embed_dim, embed_dim, bias=False), cfg.quant_noise.pq, cfg.quant_noise.pq_block_size)
        else:
            self.quant_noise = None
        self.project_in_dim = Linear(input_embed_dim, embed_dim, bias=False) if embed_dim != input_embed_dim else None
        self.embed_positions = PositionalEmbedding(self.max_target_positions, embed_dim, self.padding_idx, learned=cfg.decoder.learned_pos) if not cfg.no_token_positional_embeddings else None
        if cfg.layernorm_embedding:
            self.layernorm_embedding = LayerNorm(embed_dim, export=cfg.export)
        else:
            self.layernorm_embedding = None
        self.cross_self_attention = cfg.cross_self_attention
        if self.decoder_layerdrop > 0.0:
            self.layers = LayerDropModuleList(p=self.decoder_layerdrop)
        else:
            self.layers = nn.ModuleList([])
        self.layers.extend([self.build_decoder_layer(cfg, no_encoder_attn) for _ in range(cfg.decoder.layers)])
        self.num_layers = len(self.layers)
        if cfg.decoder.normalize_before and not cfg.no_decoder_final_norm:
            self.layer_norm = LayerNorm(embed_dim, export=cfg.export)
        else:
            self.layer_norm = None
        self.project_out_dim = Linear(embed_dim, self.output_embed_dim, bias=False) if embed_dim != self.output_embed_dim and not cfg.tie_adaptive_weights else None
        self.adaptive_softmax = None
        self.output_projection = output_projection
        if self.output_projection is None:
            self.build_output_projection(cfg, dictionary, embed_tokens)

    def build_output_projection(self, cfg, dictionary, embed_tokens):
        if cfg.adaptive_softmax_cutoff is not None:
            self.adaptive_softmax = AdaptiveSoftmax(len(dictionary), self.output_embed_dim, utils.eval_str_list(cfg.adaptive_softmax_cutoff, type=int), dropout=cfg.adaptive_softmax_dropout, adaptive_inputs=embed_tokens if cfg.tie_adaptive_weights else None, factor=cfg.adaptive_softmax_factor, tie_proj=cfg.tie_adaptive_proj)
        elif self.share_input_output_embed:
            self.output_projection = nn.Linear(self.embed_tokens.weight.shape[1], self.embed_tokens.weight.shape[0], bias=False)
            self.output_projection.weight = self.embed_tokens.weight
        else:
            self.output_projection = nn.Linear(self.output_embed_dim, len(dictionary), bias=False)
            nn.init.normal_(self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5)
        num_base_layers = cfg.base_layers
        for i in range(num_base_layers):
            self.layers.insert((i + 1) * cfg.decoder.layers // (num_base_layers + 1), BaseLayer(cfg))

    def build_decoder_layer(self, cfg, no_encoder_attn=False):
        layer = transformer_layer.TransformerDecoderLayerBase(cfg, no_encoder_attn)
        checkpoint = cfg.checkpoint_activations
        if checkpoint:
            offload_to_cpu = cfg.offload_activations
            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)
        min_params_to_wrap = cfg.min_params_to_wrap if not checkpoint else 0
        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)
        return layer

    def forward(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, features_only: bool=False, full_context_alignment: bool=False, alignment_layer: Optional[int]=None, alignment_heads: Optional[int]=None, src_lengths: Optional[Any]=None, return_all_hiddens: bool=False):
        """
        Args:
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (optional): output from the encoder, used for
                encoder-side attention, should be of size T x B x C
            incremental_state (dict): dictionary used for storing state during
                :ref:`Incremental decoding`
            features_only (bool, optional): only return features without
                applying output layer (default: False).
            full_context_alignment (bool, optional): don't apply
                auto-regressive mask to self-attention (default: False).

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state, full_context_alignment=full_context_alignment, alignment_layer=alignment_layer, alignment_heads=alignment_heads)
        if not features_only:
            x = self.output_layer(x)
        return x, extra

    def extract_features(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]], incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, full_context_alignment: bool=False, alignment_layer: Optional[int]=None, alignment_heads: Optional[int]=None):
        return self.extract_features_scriptable(prev_output_tokens, encoder_out, incremental_state, full_context_alignment, alignment_layer, alignment_heads)
    """
    A scriptable subclass of this class has an extract_features method and calls
    super().extract_features, but super() is not supported in torchscript. A copy of
    this function is made to be used in the subclass instead.
    """

    def extract_features_scriptable(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]], incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, full_context_alignment: bool=False, alignment_layer: Optional[int]=None, alignment_heads: Optional[int]=None):
        """
        Similar to *forward* but only return features.

        Includes several features from "Jointly Learning to Align and
        Translate with Transformer Models" (Garg et al., EMNLP 2019).

        Args:
            full_context_alignment (bool, optional): don't apply
                auto-regressive mask to self-attention (default: False).
            alignment_layer (int, optional): return mean alignment over
                heads at this layer (default: last layer).
            alignment_heads (int, optional): only average alignment over
                this many heads (default: all heads).

        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        bs, slen = prev_output_tokens.size()
        if alignment_layer is None:
            alignment_layer = self.num_layers - 1
        enc: Optional[Tensor] = None
        padding_mask: Optional[Tensor] = None
        if encoder_out is not None and len(encoder_out['encoder_out']) > 0:
            enc = encoder_out['encoder_out'][0]
        if encoder_out is not None and len(encoder_out['encoder_padding_mask']) > 0:
            padding_mask = encoder_out['encoder_padding_mask'][0]
        positions = None
        if self.embed_positions is not None:
            positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state)
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        prev_output_tokens = prev_output_tokens.contiguous()
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        if self.layernorm_embedding is not None:
            x = self.layernorm_embedding(x)
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        self_attn_padding_mask: Optional[Tensor] = None
        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)
        attn: Optional[Tensor] = None
        inner_states: List[Optional[Tensor]] = [x]
        for idx, layer in enumerate(self.layers):
            if incremental_state is None and not full_context_alignment:
                self_attn_mask = self.buffered_future_mask(x)
            else:
                self_attn_mask = None
            x, layer_attn, _ = layer(x, enc, padding_mask, incremental_state, self_attn_mask=self_attn_mask, self_attn_padding_mask=self_attn_padding_mask, need_attn=bool(idx == alignment_layer), need_head_weights=bool(idx == alignment_layer))
            inner_states.append(x)
            if layer_attn is not None and idx == alignment_layer:
                attn = layer_attn.float()
        if attn is not None:
            if alignment_heads is not None:
                attn = attn[:alignment_heads]
            attn = attn.mean(dim=0)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)
        return x, {'attn': [attn], 'inner_states': inner_states}

    def output_layer(self, features):
        """Project features to the vocabulary size."""
        if self.adaptive_softmax is None:
            return self.output_projection(features)
        else:
            return features

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        if self.embed_positions is None:
            return self.max_target_positions
        return min(self.max_target_positions, self.embed_positions.max_positions)

    def buffered_future_mask(self, tensor):
        dim = tensor.size(0)
        if self._future_mask.size(0) == 0 or not self._future_mask.device == tensor.device or self._future_mask.size(0) < dim:
            self._future_mask = torch.triu(utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1)
        self._future_mask = self._future_mask
        return self._future_mask[:dim, :dim]

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade a (possibly old) state dict for new versions of fairseq."""
        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):
            weights_key = '{}.embed_positions.weights'.format(name)
            if weights_key in state_dict:
                del state_dict[weights_key]
            state_dict['{}.embed_positions._float_tensor'.format(name)] = torch.FloatTensor(1)
        if f'{name}.output_projection.weight' not in state_dict:
            if self.share_input_output_embed:
                embed_out_key = f'{name}.embed_tokens.weight'
            else:
                embed_out_key = f'{name}.embed_out'
            if embed_out_key in state_dict:
                state_dict[f'{name}.output_projection.weight'] = state_dict[embed_out_key]
                if not self.share_input_output_embed:
                    del state_dict[embed_out_key]
        for i in range(self.num_layers):
            layer_norm_map = {'0': 'self_attn_layer_norm', '1': 'encoder_attn_layer_norm', '2': 'final_layer_norm'}
            for old, new in layer_norm_map.items():
                for m in ('weight', 'bias'):
                    k = '{}.layers.{}.layer_norms.{}.{}'.format(name, i, old, m)
                    if k in state_dict:
                        state_dict['{}.layers.{}.{}.{}'.format(name, i, new, m)] = state_dict[k]
                        del state_dict[k]
        version_key = '{}.version'.format(name)
        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) <= 2:
            self.layer_norm = None
            self.normalize = False
            state_dict[version_key] = torch.Tensor([1])
        return state_dict


class TransformerEncoderBase(FairseqEncoder):
    """
    Transformer encoder consisting of *cfg.encoder.layers* layers. Each layer
    is a :class:`TransformerEncoderLayer`.

    Args:
        args (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): encoding dictionary
        embed_tokens (torch.nn.Embedding): input embedding
    """

    def __init__(self, cfg, dictionary, embed_tokens, return_fc=False):
        self.cfg = cfg
        super().__init__(dictionary)
        self.register_buffer('version', torch.Tensor([3]))
        self.dropout_module = FairseqDropout(cfg.dropout, module_name=module_name_fordropout(self.__class__.__name__))
        self.encoder_layerdrop = cfg.encoder.layerdrop
        self.return_fc = return_fc
        embed_dim = embed_tokens.embedding_dim
        self.padding_idx = embed_tokens.padding_idx
        self.max_source_positions = cfg.max_source_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = 1.0 if cfg.no_scale_embedding else math.sqrt(embed_dim)
        self.embed_positions = PositionalEmbedding(cfg.max_source_positions, embed_dim, self.padding_idx, learned=cfg.encoder.learned_pos) if not cfg.no_token_positional_embeddings else None
        if cfg.layernorm_embedding:
            self.layernorm_embedding = LayerNorm(embed_dim, export=cfg.export)
        else:
            self.layernorm_embedding = None
        if not cfg.adaptive_input and cfg.quant_noise.pq > 0:
            self.quant_noise = apply_quant_noise_(nn.Linear(embed_dim, embed_dim, bias=False), cfg.quant_noise.pq, cfg.quant_noise.pq_block_size)
        else:
            self.quant_noise = None
        if self.encoder_layerdrop > 0.0:
            self.layers = LayerDropModuleList(p=self.encoder_layerdrop)
        else:
            self.layers = nn.ModuleList([])
        self.layers.extend([self.build_encoder_layer(cfg) for i in range(cfg.encoder.layers)])
        self.num_layers = len(self.layers)
        if cfg.encoder.normalize_before:
            self.layer_norm = LayerNorm(embed_dim, export=cfg.export)
        else:
            self.layer_norm = None

    def build_encoder_layer(self, cfg):
        layer = transformer_layer.TransformerEncoderLayerBase(cfg, return_fc=self.return_fc)
        checkpoint = cfg.checkpoint_activations
        if checkpoint:
            offload_to_cpu = cfg.offload_activations
            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)
        min_params_to_wrap = cfg.min_params_to_wrap if not checkpoint else 0
        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)
        return layer

    def forward_embedding(self, src_tokens, token_embedding: Optional[torch.Tensor]=None):
        if token_embedding is None:
            token_embedding = self.embed_tokens(src_tokens)
        x = embed = self.embed_scale * token_embedding
        if self.embed_positions is not None:
            x = embed + self.embed_positions(src_tokens)
        if self.layernorm_embedding is not None:
            x = self.layernorm_embedding(x)
        x = self.dropout_module(x)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        return x, embed

    def forward(self, src_tokens, src_lengths: Optional[torch.Tensor]=None, return_all_hiddens: bool=False, token_embeddings: Optional[torch.Tensor]=None):
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (torch.LongTensor): lengths of each source sentence of
                shape `(batch)`
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).
            token_embeddings (torch.Tensor, optional): precomputed embeddings
                default `None` will recompute embeddings

        Returns:
            dict:
                - **encoder_out** (Tensor): the last encoder layer's output of
                  shape `(src_len, batch, embed_dim)`
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
                - **encoder_embedding** (Tensor): the (scaled) embedding lookup
                  of shape `(batch, src_len, embed_dim)`
                - **encoder_states** (List[Tensor]): all intermediate
                  hidden states of shape `(src_len, batch, embed_dim)`.
                  Only populated if *return_all_hiddens* is True.
        """
        return self.forward_scriptable(src_tokens, src_lengths, return_all_hiddens, token_embeddings)

    def forward_scriptable(self, src_tokens, src_lengths: Optional[torch.Tensor]=None, return_all_hiddens: bool=False, token_embeddings: Optional[torch.Tensor]=None):
        """
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (torch.LongTensor): lengths of each source sentence of
                shape `(batch)`
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).
            token_embeddings (torch.Tensor, optional): precomputed embeddings
                default `None` will recompute embeddings

        Returns:
            dict:
                - **encoder_out** (Tensor): the last encoder layer's output of
                  shape `(src_len, batch, embed_dim)`
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
                - **encoder_embedding** (Tensor): the (scaled) embedding lookup
                  of shape `(batch, src_len, embed_dim)`
                - **encoder_states** (List[Tensor]): all intermediate
                  hidden states of shape `(src_len, batch, embed_dim)`.
                  Only populated if *return_all_hiddens* is True.
        """
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        has_pads = torch.tensor(src_tokens.device.type == 'xla') or encoder_padding_mask.any()
        if torch.jit.is_scripting():
            has_pads = torch.tensor(1) if has_pads else torch.tensor(0)
        x, encoder_embedding = self.forward_embedding(src_tokens, token_embeddings)
        x = x * (1 - encoder_padding_mask.unsqueeze(-1).type_as(x) * has_pads.type_as(x))
        x = x.transpose(0, 1)
        encoder_states = []
        fc_results = []
        if return_all_hiddens:
            encoder_states.append(x)
        for layer in self.layers:
            lr = layer(x, encoder_padding_mask=encoder_padding_mask if has_pads else None)
            if isinstance(lr, tuple) and len(lr) == 2:
                x, fc_result = lr
            else:
                x = lr
                fc_result = None
            if return_all_hiddens and not torch.jit.is_scripting():
                assert encoder_states is not None
                encoder_states.append(x)
                fc_results.append(fc_result)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        src_lengths = src_tokens.ne(self.padding_idx).sum(dim=1, dtype=torch.int32).reshape(-1, 1).contiguous()
        return {'encoder_out': [x], 'encoder_padding_mask': [encoder_padding_mask], 'encoder_embedding': [encoder_embedding], 'encoder_states': encoder_states, 'fc_results': fc_results, 'src_tokens': [], 'src_lengths': [src_lengths]}

    @torch.jit.export
    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        if len(encoder_out['encoder_out']) == 0:
            new_encoder_out = []
        else:
            new_encoder_out = [encoder_out['encoder_out'][0].index_select(1, new_order)]
        if len(encoder_out['encoder_padding_mask']) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [encoder_out['encoder_padding_mask'][0].index_select(0, new_order)]
        if len(encoder_out['encoder_embedding']) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [encoder_out['encoder_embedding'][0].index_select(0, new_order)]
        if len(encoder_out['src_tokens']) == 0:
            src_tokens = []
        else:
            src_tokens = [encoder_out['src_tokens'][0].index_select(0, new_order)]
        if len(encoder_out['src_lengths']) == 0:
            src_lengths = []
        else:
            src_lengths = [encoder_out['src_lengths'][0].index_select(0, new_order)]
        encoder_states = encoder_out['encoder_states']
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)
        return {'encoder_out': new_encoder_out, 'encoder_padding_mask': new_encoder_padding_mask, 'encoder_embedding': new_encoder_embedding, 'encoder_states': encoder_states, 'src_tokens': src_tokens, 'src_lengths': src_lengths}

    @torch.jit.export
    def _reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """Dummy re-order function for beamable enc-dec attention"""
        return encoder_out

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        if self.embed_positions is None:
            return self.max_source_positions
        return min(self.max_source_positions, self.embed_positions.max_positions)

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade a (possibly old) state dict for new versions of fairseq."""
        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):
            weights_key = '{}.embed_positions.weights'.format(name)
            if weights_key in state_dict:
                None
                del state_dict[weights_key]
            state_dict['{}.embed_positions._float_tensor'.format(name)] = torch.FloatTensor(1)
        for i in range(self.num_layers):
            self.layers[i].upgrade_state_dict_named(state_dict, '{}.layers.{}'.format(name, i))
        version_key = '{}.version'.format(name)
        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) < 2:
            self.layer_norm = None
            self.normalize = False
            state_dict[version_key] = torch.Tensor([1])
        return state_dict


def base_multitask_text_transformer_decoder_arch(args):
    args.dropout = getattr(args, 'dropout', 0.3)
    args.decoder_layerdrop = getattr(args, 'decoder_layerdrop', 0.0)
    args.share_decoder_input_output_embed = getattr(args, 'share_decoder_input_output_embed', True)
    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
    args.decoder_output_dim = getattr(args, 'decoder_output_dim', args.decoder_embed_dim)
    args.decoder_input_dim = getattr(args, 'decoder_input_dim', args.decoder_embed_dim)
    args.max_target_positions = getattr(args, 'max_target_positions', 1024)
    args.no_scale_embedding = getattr(args, 'no_scale_embedding', False)
    args.adaptive_input = getattr(args, 'adaptive_input', False)
    args.quant_noise_pq = getattr(args, 'quant_noise_pq', 0)
    args.decoder_learned_pos = getattr(args, 'decoder_learned_pos', False)
    args.no_token_positional_embeddings = getattr(args, 'no_token_positional_embeddings', False)
    args.decoder_layers = getattr(args, 'decoder_layers', 2)
    args.adaptive_softmax_cutoff = getattr(args, 'adaptive_softmax_cutoff', None)
    args.activation_dropout = getattr(args, 'activation_dropout', args.dropout)
    args.activation_fn = getattr(args, 'activation_fn', 'relu')
    args.decoder_normalize_before = getattr(args, 'decoder_normalize_before', True)
    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 2048)
    args.attention_dropout = getattr(args, 'attention_dropout', args.dropout)
    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 4)


def decoder_init(m):
    if isinstance(m, torch.nn.Conv1d):
        nn.init.xavier_uniform_(m.weight, torch.nn.init.calculate_gain('tanh'))


class TTSTransformerDecoder(FairseqIncrementalDecoder):

    def __init__(self, args, src_dict, padding_idx=1):
        super().__init__(None)
        self._future_mask = torch.empty(0)
        self.args = args
        self.padding_idx = src_dict.pad() if src_dict else padding_idx
        self.n_frames_per_step = args.n_frames_per_step
        self.out_dim = args.output_frame_dim * args.n_frames_per_step
        self.dropout_module = FairseqDropout(args.dropout, module_name=self.__class__.__name__)
        self.embed_positions = PositionalEmbedding(args.max_target_positions, args.decoder_embed_dim, self.padding_idx)
        self.pos_emb_alpha = nn.Parameter(torch.ones(1))
        self.prenet = nn.Sequential(Prenet(self.out_dim, args.prenet_layers, args.prenet_dim, args.prenet_dropout), nn.Linear(args.prenet_dim, args.decoder_embed_dim))
        self.n_transformer_layers = args.decoder_transformer_layers
        self.transformer_layers = nn.ModuleList(TransformerDecoderLayer(args) for _ in range(self.n_transformer_layers))
        if args.decoder_normalize_before:
            self.layer_norm = LayerNorm(args.decoder_embed_dim)
        else:
            self.layer_norm = None
        self.feat_proj = nn.Linear(args.decoder_embed_dim, self.out_dim)
        self.eos_proj = nn.Linear(args.decoder_embed_dim, 1)
        self.postnet = Postnet(self.out_dim, args.postnet_conv_dim, args.postnet_conv_kernel_size, args.postnet_layers, args.postnet_dropout)
        self.ctc_proj = None
        if getattr(args, 'ctc_weight', 0.0) > 0.0:
            self.ctc_proj = nn.Linear(self.out_dim, len(src_dict))
        self.apply(decoder_init)

    def extract_features(self, prev_outputs, encoder_out=None, incremental_state=None, target_lengths=None, speaker=None, **kwargs):
        alignment_layer = self.n_transformer_layers - 1
        self_attn_padding_mask = lengths_to_padding_mask(target_lengths)
        positions = self.embed_positions(self_attn_padding_mask, incremental_state=incremental_state)
        if incremental_state is not None:
            prev_outputs = prev_outputs[:, -1:, :]
            self_attn_padding_mask = self_attn_padding_mask[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        x = self.prenet(prev_outputs)
        x += self.pos_emb_alpha * positions
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        if not self_attn_padding_mask.any():
            self_attn_padding_mask = None
        attn: Optional[torch.Tensor] = None
        inner_states: List[Optional[torch.Tensor]] = [x]
        for idx, transformer_layer in enumerate(self.transformer_layers):
            if incremental_state is None:
                self_attn_mask = self.buffered_future_mask(x)
            else:
                self_attn_mask = None
            x, layer_attn, _ = transformer_layer(x, encoder_out['encoder_out'][0] if encoder_out is not None and len(encoder_out['encoder_out']) > 0 else None, encoder_out['encoder_padding_mask'][0] if encoder_out is not None and len(encoder_out['encoder_padding_mask']) > 0 else None, incremental_state, self_attn_mask=self_attn_mask, self_attn_padding_mask=self_attn_padding_mask, need_attn=bool(idx == alignment_layer), need_head_weights=bool(idx == alignment_layer))
            inner_states.append(x)
            if layer_attn is not None and idx == alignment_layer:
                attn = layer_attn.float()
        if attn is not None:
            attn = attn.mean(dim=0).transpose(2, 1)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        return x, {'attn': attn, 'inner_states': inner_states}

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, target_lengths=None, speaker=None, **kwargs):
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state, target_lengths=target_lengths, speaker=speaker, **kwargs)
        attn = extra['attn']
        feat_out = self.feat_proj(x)
        bsz, seq_len, _ = x.size()
        eos_out = self.eos_proj(x)
        post_feat_out = feat_out + self.postnet(feat_out)
        return post_feat_out, eos_out, {'attn': attn, 'feature_out': feat_out, 'inner_states': extra['inner_states']}

    def get_normalized_probs(self, net_output, log_probs, sample):
        logits = self.ctc_proj(net_output[2]['feature_out'])
        if log_probs:
            return utils.log_softmax(logits.float(), dim=-1)
        else:
            return utils.softmax(logits.float(), dim=-1)

    def buffered_future_mask(self, tensor):
        dim = tensor.size(0)
        if self._future_mask.size(0) == 0 or not self._future_mask.device == tensor.device or self._future_mask.size(0) < dim:
            self._future_mask = torch.triu(utils.fill_with_neg_inf(torch.zeros([dim, dim])), 1)
        self._future_mask = self._future_mask
        return self._future_mask[:dim, :dim]


class BerardEncoder(FairseqEncoder):

    def __init__(self, input_layers: List[int], conv_layers: List[Tuple[int]], in_channels: int, input_feat_per_channel: int, num_blstm_layers: int, lstm_size: int, dropout: float):
        """
        Args:
            input_layers: list of linear layer dimensions. These layers are
                applied to the input features and are followed by tanh and
                possibly dropout.
            conv_layers: list of conv2d layer configurations. A configuration is
                a tuple (out_channels, conv_kernel_size, stride).
            in_channels: number of input channels.
            input_feat_per_channel: number of input features per channel. These
                are speech features, typically 40 or 80.
            num_blstm_layers: number of bidirectional LSTM layers.
            lstm_size: size of the LSTM hidden (and cell) size.
            dropout: dropout probability. Dropout can be applied after the
                linear layers and LSTM layers but not to the convolutional
                layers.
        """
        super().__init__(None)
        self.input_layers = nn.ModuleList()
        in_features = input_feat_per_channel
        for out_features in input_layers:
            if dropout > 0:
                self.input_layers.append(nn.Sequential(nn.Linear(in_features, out_features), nn.Dropout(p=dropout)))
            else:
                self.input_layers.append(nn.Linear(in_features, out_features))
            in_features = out_features
        self.in_channels = in_channels
        self.input_dim = input_feat_per_channel
        self.conv_kernel_sizes_and_strides = []
        self.conv_layers = nn.ModuleList()
        lstm_input_dim = input_layers[-1]
        for conv_layer in conv_layers:
            out_channels, conv_kernel_size, conv_stride = conv_layer
            self.conv_layers.append(nn.Conv2d(in_channels, out_channels, conv_kernel_size, stride=conv_stride, padding=conv_kernel_size // 2))
            self.conv_kernel_sizes_and_strides.append((conv_kernel_size, conv_stride))
            in_channels = out_channels
            lstm_input_dim //= conv_stride
        lstm_input_dim *= conv_layers[-1][0]
        self.lstm_size = lstm_size
        self.num_blstm_layers = num_blstm_layers
        self.lstm = nn.LSTM(input_size=lstm_input_dim, hidden_size=lstm_size, num_layers=num_blstm_layers, dropout=dropout, bidirectional=True)
        self.output_dim = 2 * lstm_size
        if dropout > 0:
            self.dropout = nn.Dropout(p=dropout)
        else:
            self.dropout = None

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        """
        Args
            src_tokens: padded tensor (B, T, C * feat)
            src_lengths: tensor of original lengths of input utterances (B,)
        """
        bsz, max_seq_len, _ = src_tokens.size()
        x = src_tokens.view(bsz, max_seq_len, self.in_channels, self.input_dim).transpose(1, 2).contiguous()
        for input_layer in self.input_layers:
            x = input_layer(x)
            x = torch.tanh(x)
        for conv_layer in self.conv_layers:
            x = conv_layer(x)
        bsz, _, output_seq_len, _ = x.size()
        x = x.transpose(1, 2).transpose(0, 1).contiguous().view(output_seq_len, bsz, -1)
        input_lengths = src_lengths.clone()
        for k, s in self.conv_kernel_sizes_and_strides:
            p = k // 2
            input_lengths = (input_lengths.float() + 2 * p - k) / s + 1
            input_lengths = input_lengths.floor().long()
        packed_x = nn.utils.rnn.pack_padded_sequence(x, input_lengths)
        h0 = x.new(2 * self.num_blstm_layers, bsz, self.lstm_size).zero_()
        c0 = x.new(2 * self.num_blstm_layers, bsz, self.lstm_size).zero_()
        packed_outs, _ = self.lstm(packed_x, (h0, c0))
        x, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_outs)
        if self.dropout is not None:
            x = self.dropout(x)
        encoder_padding_mask = lengths_to_padding_mask(output_lengths).t()
        return {'encoder_out': x, 'encoder_padding_mask': encoder_padding_mask}

    def reorder_encoder_out(self, encoder_out, new_order):
        encoder_out['encoder_out'] = encoder_out['encoder_out'].index_select(1, new_order)
        encoder_out['encoder_padding_mask'] = encoder_out['encoder_padding_mask'].index_select(1, new_order)
        return encoder_out


class ConvTransformerEncoder(FairseqEncoder):
    """Conv + Transformer encoder"""

    def __init__(self, args):
        """Construct an Encoder object."""
        super().__init__(None)
        self.dropout = args.dropout
        self.embed_scale = 1.0 if args.no_scale_embedding else math.sqrt(args.encoder_embed_dim)
        self.padding_idx = 1
        self.in_channels = 1
        self.input_dim = args.input_feat_per_channel
        self.conv = torch.nn.Sequential(torch.nn.Conv2d(1, args.conv_out_channels, 3, stride=2, padding=3 // 2), torch.nn.ReLU(), torch.nn.Conv2d(args.conv_out_channels, args.conv_out_channels, 3, stride=2, padding=3 // 2), torch.nn.ReLU())
        transformer_input_dim = infer_conv_output_dim(self.in_channels, self.input_dim, args.conv_out_channels)
        self.out = torch.nn.Linear(transformer_input_dim, args.encoder_embed_dim)
        self.embed_positions = PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim, self.padding_idx, learned=False)
        self.transformer_layers = nn.ModuleList([])
        self.transformer_layers.extend([TransformerEncoderLayer(args) for i in range(args.encoder_layers)])
        if args.encoder_normalize_before:
            self.layer_norm = LayerNorm(args.encoder_embed_dim)
        else:
            self.layer_norm = None

    def pooling_ratio(self):
        return 4

    def forward(self, src_tokens, src_lengths):
        """Encode input sequence.
        :param torch.Tensor xs: input tensor
        :param torch.Tensor masks: input mask
        :return: position embedded tensor and mask
        :rtype Tuple[torch.Tensor, torch.Tensor]:
        """
        bsz, max_seq_len, _ = src_tokens.size()
        x = src_tokens.view(bsz, max_seq_len, self.in_channels, self.input_dim).transpose(1, 2).contiguous()
        x = self.conv(x)
        bsz, _, output_seq_len, _ = x.size()
        x = x.transpose(1, 2).transpose(0, 1).contiguous().view(output_seq_len, bsz, -1)
        x = self.out(x)
        x = self.embed_scale * x
        subsampling_factor = int(max_seq_len * 1.0 / output_seq_len + 0.5)
        input_len_0 = (src_lengths.float() / subsampling_factor).ceil().long()
        input_len_1 = x.size(0) * torch.ones([src_lengths.size(0)]).long()
        input_lengths = torch.min(input_len_0, input_len_1)
        encoder_padding_mask = lengths_to_padding_mask(input_lengths)
        positions = self.embed_positions(encoder_padding_mask).transpose(0, 1)
        x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)
        for layer in self.transformer_layers:
            x = layer(x, encoder_padding_mask)
        if not encoder_padding_mask.any():
            maybe_encoder_padding_mask = None
        else:
            maybe_encoder_padding_mask = encoder_padding_mask
        return {'encoder_out': [x], 'encoder_padding_mask': [maybe_encoder_padding_mask] if maybe_encoder_padding_mask is not None else [], 'encoder_embedding': [], 'encoder_states': [], 'src_tokens': [], 'src_lengths': []}

    @torch.jit.export
    def reorder_encoder_out(self, encoder_out: Dict[str, List[Tensor]], new_order):
        """
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        new_encoder_out = [encoder_out['encoder_out'][0].index_select(1, new_order)]
        if len(encoder_out['encoder_padding_mask']) == 0:
            new_encoder_padding_mask = []
        else:
            new_encoder_padding_mask = [encoder_out['encoder_padding_mask'][0].index_select(0, new_order)]
        if len(encoder_out['encoder_embedding']) == 0:
            new_encoder_embedding = []
        else:
            new_encoder_embedding = [encoder_out['encoder_embedding'][0].index_select(0, new_order)]
        encoder_states = encoder_out['encoder_states']
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)
        return {'encoder_out': new_encoder_out, 'encoder_padding_mask': new_encoder_padding_mask, 'encoder_embedding': new_encoder_embedding, 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}


class AudioTransform(ABC):

    @classmethod
    @abstractmethod
    def from_config_dict(cls, config: Optional[Dict]=None):
        pass


class CompositeAudioTransform(AudioTransform):

    def _from_config_dict(cls, transform_type, get_audio_transform, composite_cls, config=None, return_empty=False):
        _config = {} if config is None else config
        _transforms = _config.get(f'{transform_type}_transforms')
        if _transforms is None:
            if return_empty:
                _transforms = []
            else:
                return None
        transforms = [get_audio_transform(_t).from_config_dict(_config.get(_t)) for _t in _transforms]
        return composite_cls(transforms)

    def __init__(self, transforms):
        self.transforms = [t for t in transforms if t is not None]

    def __call__(self, x):
        for t in self.transforms:
            x = t(x)
        return x

    def __repr__(self):
        format_string = [self.__class__.__name__ + '('] + [f'    {t.__repr__()}' for t in self.transforms] + [')']
        return '\n'.join(format_string)


AUDIO_DATASET_TRANSFORM_REGISTRY = {}


def get_audio_dataset_transform(name):
    return AUDIO_DATASET_TRANSFORM_REGISTRY[name]


class CompositeAudioDatasetTransform(CompositeAudioTransform):

    @classmethod
    def from_config_dict(cls, config=None):
        return super()._from_config_dict(cls, 'dataset', get_audio_dataset_transform, CompositeAudioDatasetTransform, config, return_empty=True)

    def get_transform(self, cls):
        for t in self.transforms:
            if isinstance(t, cls):
                return t
        return None

    def has_transform(self, cls):
        return self.get_transform(cls) is not None


AUDIO_FEATURE_TRANSFORM_REGISTRY = {}


def get_audio_feature_transform(name):
    return AUDIO_FEATURE_TRANSFORM_REGISTRY[name]


class CompositeAudioFeatureTransform(CompositeAudioTransform):

    @classmethod
    def from_config_dict(cls, config=None):
        return super()._from_config_dict(cls, 'feature', get_audio_feature_transform, CompositeAudioFeatureTransform, config)


AUDIO_WAVEFORM_TRANSFORM_REGISTRY = {}


def get_audio_waveform_transform(name):
    return AUDIO_WAVEFORM_TRANSFORM_REGISTRY[name]


class CompositeAudioWaveformTransform(CompositeAudioTransform):

    @classmethod
    def from_config_dict(cls, config=None):
        return super()._from_config_dict(cls, 'waveform', get_audio_waveform_transform, CompositeAudioWaveformTransform, config)

    def __call__(self, x, sample_rate):
        for t in self.transforms:
            x, sample_rate = t(x, sample_rate)
        return x, sample_rate


class AudioDatasetTransform(AudioTransform):
    pass


_DEFAULTS = {'rate': 0.25, 'mixing_noise_rate': 0.1, 'noise_path': '', 'noise_snr_min': -5, 'noise_snr_max': 5, 'utterance_snr_min': -5, 'utterance_snr_max': 5}


AUDIO_DATASET_TRANSFORM_CLASS_NAMES = set()


def register_audio_transform(name, cls_type, registry, class_names):

    def register_audio_transform_cls(cls):
        if name in registry:
            raise ValueError(f'Cannot register duplicate transform ({name})')
        if not issubclass(cls, cls_type):
            raise ValueError(f'Transform ({name}: {cls.__name__}) must extend {cls_type.__name__}')
        if cls.__name__ in class_names:
            raise ValueError(f'Cannot register audio transform with duplicate class name ({cls.__name__})')
        registry[name] = cls
        class_names.add(cls.__name__)
        return cls
    return register_audio_transform_cls


def register_audio_dataset_transform(name):
    return register_audio_transform(name, AudioDatasetTransform, AUDIO_DATASET_TRANSFORM_REGISTRY, AUDIO_DATASET_TRANSFORM_CLASS_NAMES)


class AudioWaveformTransform(AudioTransform):
    pass


RATE = 0.25


SNR_MAX = 15.0


SNR_MIN = 5.0


def rand_uniform(a, b):
    return np.random.uniform() * (b - a) + a


class NoiseAugmentTransform(AudioWaveformTransform):

    @classmethod
    def from_config_dict(cls, config=None):
        _config = {} if config is None else config
        return cls(_config.get('samples_path', None), _config.get('snr_min', SNR_MIN), _config.get('snr_max', SNR_MAX), _config.get('rate', RATE))

    def __init__(self, samples_path: str, snr_min: float=SNR_MIN, snr_max: float=SNR_MAX, rate: float=RATE):
        assert samples_path, 'need to provide path to audio samples for noise augmentation'
        assert snr_max >= snr_min, f'empty signal-to-noise range ({snr_min}, {snr_max})'
        assert rate >= 0 and rate <= 1, 'rate should be a float between 0 to 1'
        self.paths = list(Path(samples_path).glob('**/*.wav'))
        self.n_samples = len(self.paths)
        assert self.n_samples > 0, f'no audio files found in {samples_path}'
        self.snr_min = snr_min
        self.snr_max = snr_max
        self.rate = rate

    def __repr__(self):
        return self.__class__.__name__ + '(' + ', '.join([f'n_samples={self.n_samples}', f'snr={self.snr_min}-{self.snr_max}dB', f'rate={self.rate}']) + ')'

    def pick_sample(self, goal_shape, always_2d=False, use_sample_rate=None):
        path = self.paths[np.random.randint(0, self.n_samples)]
        sample = get_waveform(path, always_2d=always_2d, output_sample_rate=use_sample_rate)[0]
        is_2d = len(goal_shape) == 2
        if len(goal_shape) != sample.ndim or is_2d and goal_shape[0] != sample.shape[0]:
            return np.zeros(goal_shape)
        len_dim = len(goal_shape) - 1
        n_repeat = ceil(goal_shape[len_dim] / sample.shape[len_dim])
        repeated = np.tile(sample, [1, n_repeat] if is_2d else n_repeat)
        start = np.random.randint(0, repeated.shape[len_dim] - goal_shape[len_dim] + 1)
        return repeated[:, start:start + goal_shape[len_dim]] if is_2d else repeated[start:start + goal_shape[len_dim]]

    def _mix(self, source, noise, snr):
        get_power = lambda x: np.mean(x ** 2)
        if get_power(noise):
            scl = np.sqrt(get_power(source) / (np.power(10, snr / 10) * get_power(noise)))
        else:
            scl = 0
        return 1 * source + scl * noise

    def _get_noise(self, goal_shape, always_2d=False, use_sample_rate=None):
        return self.pick_sample(goal_shape, always_2d, use_sample_rate)

    def __call__(self, source, sample_rate):
        if np.random.random() > self.rate:
            return source, sample_rate
        noise = self._get_noise(source.shape, always_2d=True, use_sample_rate=sample_rate)
        return self._mix(source, noise, rand_uniform(self.snr_min, self.snr_max)), sample_rate


class NoisyOverlapAugment(AudioDatasetTransform):

    @classmethod
    def from_config_dict(cls, config=None):
        _config = {} if config is None else config
        return NoisyOverlapAugment(_config.get('rate', _DEFAULTS['rate']), _config.get('mixing_noise_rate', _DEFAULTS['mixing_noise_rate']), _config.get('noise_path', _DEFAULTS['noise_path']), _config.get('noise_snr_min', _DEFAULTS['noise_snr_min']), _config.get('noise_snr_max', _DEFAULTS['noise_snr_max']), _config.get('utterance_snr_min', _DEFAULTS['utterance_snr_min']), _config.get('utterance_snr_max', _DEFAULTS['utterance_snr_max']))

    def __init__(self, rate=_DEFAULTS['rate'], mixing_noise_rate=_DEFAULTS['mixing_noise_rate'], noise_path=_DEFAULTS['noise_path'], noise_snr_min=_DEFAULTS['noise_snr_min'], noise_snr_max=_DEFAULTS['noise_snr_max'], utterance_snr_min=_DEFAULTS['utterance_snr_min'], utterance_snr_max=_DEFAULTS['utterance_snr_max']):
        self.rate = rate
        self.mixing_noise_rate = mixing_noise_rate
        self.noise_shaper = NoiseAugmentTransform(noise_path)
        self.noise_snr_min = noise_snr_min
        self.noise_snr_max = noise_snr_max
        self.utterance_snr_min = utterance_snr_min
        self.utterance_snr_max = utterance_snr_max

    def __repr__(self):
        return self.__class__.__name__ + '(' + ', '.join([f'rate={self.rate}', f'mixing_noise_rate={self.mixing_noise_rate}', f'noise_snr_min={self.noise_snr_min}', f'noise_snr_max={self.noise_snr_max}', f'utterance_snr_min={self.utterance_snr_min}', f'utterance_snr_max={self.utterance_snr_max}']) + ')'

    def __call__(self, sources):
        for i, source in enumerate(sources):
            if np.random.random() > self.rate:
                continue
            pri = source.numpy()
            if np.random.random() > self.mixing_noise_rate:
                sec = sources[np.random.randint(0, len(sources))].numpy()
                snr = rand_uniform(self.utterance_snr_min, self.utterance_snr_max)
            else:
                sec = self.noise_shaper.pick_sample(source.shape)
                snr = rand_uniform(self.noise_snr_min, self.noise_snr_max)
            L1 = pri.shape[-1]
            L2 = sec.shape[-1]
            l = np.random.randint(0, min(round(L1 / 2), L2))
            s_source = np.random.randint(0, L1 - l)
            s_sec = np.random.randint(0, L2 - l)
            get_power = lambda x: np.mean(x ** 2)
            if get_power(sec) == 0:
                continue
            scl = np.sqrt(get_power(pri) / (np.power(10, snr / 10) * get_power(sec)))
            pri[s_source:s_source + l] = np.add(pri[s_source:s_source + l], np.multiply(scl, sec[s_sec:s_sec + l]))
            sources[i] = torch.from_numpy(pri).float()
        return sources


def _collate_frames(frames: List[torch.Tensor], is_audio_input: bool=False) ->torch.Tensor:
    """
    Convert a list of 2D frames into a padded 3D tensor
    Args:
        frames (list): list of 2D frames of size L[i]*f_dim. Where L[i] is
            length of i-th frame and f_dim is static dimension of features
    Returns:
        3D tensor of size len(frames)*len_max*f_dim where len_max is max of L[i]
    """
    max_len = max(frame.size(0) for frame in frames)
    if is_audio_input:
        out = frames[0].new_zeros((len(frames), max_len))
    else:
        out = frames[0].new_zeros((len(frames), max_len, frames[0].size(1)))
    for i, v in enumerate(frames):
        out[i, :v.size(0)] = v
    return out


def _is_int_or_np_int(n):
    return isinstance(n, int) or isinstance(n, np.generic) and isinstance(n.item(), int)


def _get_kaldi_fbank(waveform: np.ndarray, sample_rate: int, n_bins=80) ->Optional[np.ndarray]:
    """Get mel-filter bank features via PyKaldi."""
    try:
        mel_opts = MelBanksOptions()
        mel_opts.num_bins = n_bins
        frame_opts = FrameExtractionOptions()
        frame_opts.samp_freq = sample_rate
        opts = FbankOptions()
        opts.mel_opts = mel_opts
        opts.frame_opts = frame_opts
        fbank = Fbank(opts=opts)
        features = fbank.compute(Vector(waveform.squeeze()), 1.0).numpy()
        return features
    except ImportError:
        return None


def _get_torchaudio_fbank(waveform: np.ndarray, sample_rate, n_bins=80) ->Optional[np.ndarray]:
    """Get mel-filter bank features via TorchAudio."""
    try:
        import torchaudio.compliance.kaldi as ta_kaldi
        waveform = torch.from_numpy(waveform)
        features = ta_kaldi.fbank(waveform, num_mel_bins=n_bins, sample_frequency=sample_rate)
        return features.numpy()
    except ImportError:
        return None


SF_AUDIO_FILE_EXTENSIONS = {'.wav', '.flac', '.ogg'}


def convert_waveform(waveform: Union[np.ndarray, torch.Tensor], sample_rate: int, normalize_volume: bool=False, to_mono: bool=False, to_sample_rate: Optional[int]=None) ->Tuple[Union[np.ndarray, torch.Tensor], int]:
    """convert a waveform:
    - to a target sample rate
    - from multi-channel to mono channel
    - volume normalization

    Args:
        waveform (numpy.ndarray or torch.Tensor): 2D original waveform
            (channels x length)
        sample_rate (int): original sample rate
        normalize_volume (bool): perform volume normalization
        to_mono (bool): convert to mono channel if having multiple channels
        to_sample_rate (Optional[int]): target sample rate
    Returns:
        waveform (numpy.ndarray): converted 2D waveform (channels x length)
        sample_rate (float): target sample rate
    """
    try:
        import torchaudio.sox_effects as ta_sox
    except ImportError:
        raise ImportError('Please install torchaudio: pip install torchaudio')
    effects = []
    if normalize_volume:
        effects.append(['gain', '-n'])
    if to_sample_rate is not None and to_sample_rate != sample_rate:
        effects.append(['rate', f'{to_sample_rate}'])
    if to_mono and waveform.shape[0] > 1:
        effects.append(['channels', '1'])
    if len(effects) > 0:
        is_np_input = isinstance(waveform, np.ndarray)
        _waveform = torch.from_numpy(waveform) if is_np_input else waveform
        converted, converted_sample_rate = ta_sox.apply_effects_tensor(_waveform, sample_rate, effects)
        if is_np_input:
            converted = converted.numpy()
        return converted, converted_sample_rate
    return waveform, sample_rate


def get_fbank(path_or_fp: Union[str, BinaryIO], n_bins=80, waveform_transforms=None) ->np.ndarray:
    """Get mel-filter bank features via PyKaldi or TorchAudio. Prefer PyKaldi
    (faster CPP implementation) to TorchAudio (Python implementation). Note that
    Kaldi/TorchAudio requires 16-bit signed integers as inputs and hence the
    waveform should not be normalized."""
    waveform, sample_rate = get_waveform(path_or_fp, normalization=False, waveform_transforms=waveform_transforms)
    features = _get_kaldi_fbank(waveform, sample_rate, n_bins)
    if features is None:
        features = _get_torchaudio_fbank(waveform, sample_rate, n_bins)
    if features is None:
        raise ImportError('Please install pyKaldi or torchaudio to enable online filterbank feature extraction')
    return features


def get_features_from_npy_or_audio(path, waveform_transforms=None):
    ext = Path(path).suffix
    if ext not in FEATURE_OR_SF_AUDIO_FILE_EXTENSIONS:
        raise ValueError(f'Unsupported file format for "{path}"')
    return np.load(path) if ext == '.npy' else get_fbank(path, waveform_transforms=waveform_transforms)


def is_npy_data(data: bytes) ->bool:
    return data[0] == 147 and data[1] == 78


def is_sf_audio_data(data: bytes) ->bool:
    is_wav = data[0] == 82 and data[1] == 73 and data[2] == 70
    is_flac = data[0] == 102 and data[1] == 76 and data[2] == 97
    is_ogg = data[0] == 79 and data[1] == 103 and data[2] == 103
    return is_wav or is_flac or is_ogg


def get_features_or_waveform_from_stored_zip(path, byte_offset, byte_size, need_waveform=False, use_sample_rate=None, waveform_transforms=None):
    assert path.endswith('.zip')
    data = read_from_stored_zip(path, byte_offset, byte_size)
    f = io.BytesIO(data)
    if is_npy_data(data):
        features_or_waveform = np.load(f)
    elif is_sf_audio_data(data):
        features_or_waveform = get_waveform(f, always_2d=False, output_sample_rate=use_sample_rate, waveform_transforms=waveform_transforms)[0] if need_waveform else get_fbank(f, waveform_transforms=waveform_transforms)
    else:
        raise ValueError(f'Unknown file format for "{path}"')
    return features_or_waveform


def get_features_or_waveform(path: str, need_waveform=False, use_sample_rate=None, waveform_transforms=None):
    """Get speech features from .npy file or waveform from .wav/.flac file.
    The file may be inside an uncompressed ZIP file and is accessed via byte
    offset and length.

    Args:
        path (str): File path in the format of "<.npy/.wav/.flac path>" or
        "<zip path>:<byte offset>:<byte length>".
        need_waveform (bool): return waveform instead of features.
        use_sample_rate (int): change sample rate for the input wave file

    Returns:
        features_or_waveform (numpy.ndarray): speech features or waveform.
    """
    _path, slice_ptr = parse_path(path)
    if len(slice_ptr) == 0:
        if need_waveform:
            return get_waveform(_path, always_2d=False, output_sample_rate=use_sample_rate, waveform_transforms=waveform_transforms)[0]
        return get_features_from_npy_or_audio(_path, waveform_transforms=waveform_transforms)
    elif len(slice_ptr) == 2:
        features_or_waveform = get_features_or_waveform_from_stored_zip(_path, slice_ptr[0], slice_ptr[1], need_waveform=need_waveform, use_sample_rate=use_sample_rate, waveform_transforms=waveform_transforms)
    else:
        raise ValueError(f'Invalid path: {path}')
    return features_or_waveform


class S2THubInterface(nn.Module):

    def __init__(self, cfg, task, model):
        super().__init__()
        self.cfg = cfg
        self.task = task
        self.model = model
        self.model.eval()
        self.generator = self.task.build_generator([self.model], self.cfg.generation)

    @classmethod
    def get_model_input(cls, task, audio: Union[str, torch.Tensor]):
        input_type = task.data_cfg.hub.get('input_type', 'fbank80')
        if input_type == 'fbank80_w_utt_cmvn':
            if isinstance(audio, str):
                feat = utt_cmvn.UtteranceCMVN()(get_fbank(audio))
                feat = feat.unsqueeze(0)
            else:
                import torchaudio.compliance.kaldi as kaldi
                feat = kaldi.fbank(audio, num_mel_bins=80).numpy()
        elif input_type in {'waveform', 'standardized_waveform'}:
            if isinstance(audio, str):
                feat, sr = get_wav(audio)
                feat, _ = convert_wav(feat, sr, to_sample_rate=16000, to_mono=True)
            else:
                feat = audio.numpy()
        else:
            raise ValueError(f'Unknown value: input_type = {input_type}')
        src_lengths = torch.Tensor([feat.shape[1]]).long()
        src_tokens = torch.from_numpy(feat)
        if input_type == 'standardized_waveform':
            with torch.no_grad():
                src_tokens = F.layer_norm(src_tokens, src_tokens.shape)
        return {'net_input': {'src_tokens': src_tokens, 'src_lengths': src_lengths, 'prev_output_tokens': None}, 'target_lengths': None, 'speaker': None}

    @classmethod
    def detokenize(cls, task, tokens):
        text = task.tgt_dict.string(tokens)
        tkn_cfg = task.data_cfg.bpe_tokenizer
        tokenizer = encoders.build_bpe(Namespace(**tkn_cfg))
        return text if tokenizer is None else tokenizer.decode(text)

    @classmethod
    def get_prefix_token(cls, task, lang):
        prefix_size = int(task.data_cfg.prepend_tgt_lang_tag)
        prefix_tokens = None
        if prefix_size > 0:
            assert lang is not None
            lang_tag = SpeechToTextDataset.get_lang_tag_idx(lang, task.tgt_dict)
            prefix_tokens = torch.Tensor([lang_tag]).long().unsqueeze(0)
        return prefix_tokens

    @classmethod
    def get_prediction(cls, task, model, generator, sample, tgt_lang=None, synthesize_speech=False) ->Union[str, Tuple[str, Tuple[torch.Tensor, int]]]:
        _tgt_lang = tgt_lang or task.data_cfg.hub.get('tgt_lang', None)
        prefix = cls.get_prefix_token(task, _tgt_lang)
        pred_tokens = generator.generate([model], sample, prefix_tokens=prefix)
        pred = cls.detokenize(task, pred_tokens[0][0]['tokens'])
        eos_token = task.data_cfg.config.get('eos_token', None)
        if eos_token:
            pred = ' '.join(pred.split(' ')[:-1])
        if synthesize_speech:
            pfx = f'{_tgt_lang}_' if task.data_cfg.prepend_tgt_lang_tag else ''
            tts_model_id = task.data_cfg.hub.get(f'{pfx}tts_model_id', None)
            speaker = task.data_cfg.hub.get(f'{pfx}speaker', None)
            if tts_model_id is None:
                logger.warning('TTS model configuration not found')
            else:
                _repo, _id = tts_model_id.split(':')
                tts_model = torch.hub.load(_repo, _id, verbose=False)
                pred = pred, tts_model.predict(pred, speaker=speaker)
        return pred

    def predict(self, audio: Union[str, torch.Tensor], tgt_lang: Optional[str]=None, synthesize_speech: bool=False) ->Union[str, Tuple[str, Tuple[torch.Tensor, int]]]:
        sample = self.get_model_input(self.task, audio)
        return self.get_prediction(self.task, self.model, self.generator, sample, tgt_lang=tgt_lang, synthesize_speech=synthesize_speech)


def attention_suppression(attention_weights: Tensor, scale: float):
    attention_prob = torch.nn.functional.softmax(attention_weights.float(), dim=-1)
    attention_nozeros = attention_prob
    nozeros_sum = torch.sum(attention_nozeros, dim=-1, keepdim=True)
    key_sum = torch.sum(attention_prob, dim=-1, keepdim=True)
    key_mean = key_sum / (nozeros_sum + 1e-08)
    dis = (attention_prob - key_mean) * (attention_prob - key_mean)
    dis_masked = torch.where(attention_nozeros, dis, attention_prob.new_zeros(attention_prob.size()))
    key_var = torch.sum(dis_masked, dim=-1, keepdim=True)
    key_var = key_var / (nozeros_sum - 1.0 + 1e-08)
    key_std = torch.sqrt(key_var)
    key_thread = key_mean - scale * key_std
    inf_tensor = attention_prob.new_zeros(attention_prob.size()).detach()
    inf_tensor[:] = float('-inf')
    attention_weights_float = torch.where(attention_prob < key_thread, inf_tensor, attention_weights.float())
    return attention_weights_float.type_as(attention_weights)


class AugmentedMemoryMultiheadAttention(MultiheadAttention):
    """
    Augmented Memory Attention from
    Streaming Transformer-based Acoustic Models
    Using Self-attention with Augmented Memory
    https://arxiv.org/abs/2005.08042
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, q_noise=0.0, qn_block_size=8, tanh_on_mem=False, memory_dim=None, std_scale=0.5, max_memory_size=-1, disable_mem_on_mem_attn=True):
        super().__init__(embed_dim, num_heads, kdim, vdim, dropout, bias, add_bias_kv, add_zero_attn, self_attention, encoder_decoder_attention, q_noise, qn_block_size)
        self.memory_dim = memory_dim if memory_dim is not None else embed_dim
        self.std_scale = std_scale
        self.disable_mem_on_mem_attn = disable_mem_on_mem_attn
        self.v2e = lambda x: x
        if tanh_on_mem:
            self.squash_mem = torch.tanh
            self.nonlinear_squash_mem = True
        else:
            self.squash_mem = lambda x: x
            self.nonlinear_squash_mem = False
        self.max_memory_size = max_memory_size

    def forward(self, input_and_summary, state):
        """
        input: Encoder states of current segment with left or right context,
            plus one summarization query

        """
        length, batch_size, _ = input_and_summary.shape
        length = length - 1
        memory = state['memory_banks']
        if self.max_memory_size > -1 and len(memory) > self.max_memory_size:
            if self.max_memory_size == 0:
                memory = memory.new_zeros(1, memory.size(1), self.memory_dim)
            else:
                memory = memory[-self.max_memory_size:]
        memory_and_input = torch.cat(memory + [input_and_summary[:-1]], dim=0)
        input_and_sum_query = input_and_summary
        q = self.q_proj(self.v2e(input_and_sum_query))
        k = self.k_proj(self.v2e(memory_and_input))
        v = self.v_proj(self.v2e(memory_and_input))
        q = q.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1) * self.scaling
        k = k.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
        v = v.contiguous().view(-1, batch_size * self.num_heads, self.head_dim).transpose(0, 1)
        attention_weights = torch.bmm(q, k.transpose(1, 2))
        if self.disable_mem_on_mem_attn:
            attention_weights = self.suppress_mem_on_mem_attention(batch_size, self.num_heads, len(memory), attention_weights)
        if self.std_scale is not None:
            attention_weights = attention_suppression(attention_weights, self.std_scale)
        assert list(attention_weights.shape) == [batch_size * self.num_heads, length + 1, length + len(memory)]
        attention_weights = torch.nn.functional.softmax(attention_weights.float(), dim=-1).type_as(attention_weights)
        attention_probs = self.dropout_module(attention_weights)
        attention = torch.bmm(attention_probs, v)
        assert list(attention.shape) == [batch_size * self.num_heads, length + 1, self.head_dim]
        attention = attention.transpose(0, 1).contiguous().view(length + 1, batch_size, self.embed_dim)
        output_and_memory = self.out_proj(attention)
        next_m = output_and_memory[-1:]
        next_m = self.squash_mem(next_m)
        output = output_and_memory[:-1]
        state['memory_banks'].append(next_m)
        return output

    def suppress_mem_on_mem_attention(self, B: int, num_heads: int, mem_size: int, attention_weight: Tensor):
        """
        Arguments:
            - B: batch size
            - num_heads: number of attention heads
            - mem_size: size of memory bank
            - attention_weight: a [B*num_heads, T + 1, T + mem_size] vector

        Return:
            modified attention_weight with [B*num_heads, -1, :mem_size] = -inf
        """
        attention_weight[:, -1, :mem_size] = float('-inf')
        return attention_weight


class AugmentedMemoryTransformerEncoderLayer(TransformerEncoderLayer):

    def __init__(self, args):
        super().__init__(args)
        self.left_context = args.left_context // args.encoder_stride
        self.right_context = args.right_context // args.encoder_stride

    def forward(self, x, state):
        length, batch_size, x_dim = x.size()
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if state.get('memory_banks', None) is None:
            state['memory_banks'] = []
        seg_start = self.left_context
        seg_end = length - self.right_context
        if seg_start < seg_end:
            summarization_query = torch.mean(x[seg_start:seg_end], keepdim=True, dim=0)
        else:
            summarization_query = x.new_zeros(1, batch_size, x_dim)
        x = torch.cat([x, summarization_query], dim=0)
        x = self.self_attn(input_and_summary=x, state=state)
        x = self.dropout_module(x)
        x = residual + x
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = residual + x
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        return x

    def build_self_attention(self, embed_dim, args):
        return AugmentedMemoryMultiheadAttention(embed_dim=embed_dim, num_heads=args.encoder_attention_heads, dropout=args.attention_dropout, self_attention=True, q_noise=self.quant_noise, qn_block_size=self.quant_noise_block_size, tanh_on_mem=True, max_memory_size=args.max_memory_size)


def lengths_to_encoder_padding_mask(lengths, batch_first: bool=False):
    """
    convert lengths (a 1-D Long/Int tensor) to 2-D binary tensor

    Args:
        lengths: a (B, )-shaped tensor
        batch_first: whether to return a (B, T) tensor

    Return:
        max_length: maximum length of B sequences
        encoder_padding_mask: a (max_length, B) binary mask, where
        [t, b] = False for t < lengths[b] and True otherwise

    TODO:
        kernelize this function if benchmarking shows this function is slow
    """
    max_lengths = torch.max(lengths).item()
    bsz = lengths.size(0)
    encoder_padding_mask = torch.arange(max_lengths).view(1, max_lengths).expand(bsz, -1) > lengths.view(bsz, 1).expand(-1, max_lengths)
    if not batch_first:
        return encoder_padding_mask.t(), max_lengths
    else:
        return encoder_padding_mask, max_lengths


class AugmentedMemoryConvTransformerEncoder(ConvTransformerEncoder):

    def __init__(self, args):
        super().__init__(args)
        args.encoder_stride = self.stride()
        self.left_context = args.left_context // args.encoder_stride
        self.right_context = args.right_context // args.encoder_stride
        self.left_context_after_stride = args.left_context // args.encoder_stride
        self.right_context_after_stride = args.right_context // args.encoder_stride
        self.transformer_layers = nn.ModuleList([])
        self.transformer_layers.extend([AugmentedMemoryTransformerEncoderLayer(args) for i in range(args.encoder_layers)])

    def stride(self):
        stride = 4
        return stride

    def forward(self, src_tokens, src_lengths, states=None):
        """Encode input sequence.
        :param torch.Tensor xs: input tensor
        :param torch.Tensor masks: input mask
        :return: position embedded tensor and mask
        :rtype Tuple[torch.Tensor, torch.Tensor]:
        """
        bsz, max_seq_len, _ = src_tokens.size()
        x = src_tokens.view(bsz, max_seq_len, self.in_channels, self.input_dim).transpose(1, 2).contiguous()
        x = self.conv(x)
        bsz, _, output_seq_len, _ = x.size()
        x = x.transpose(1, 2).transpose(0, 1).contiguous().view(output_seq_len, bsz, -1)
        x = self.out(x)
        x = self.embed_scale * x
        subsampling_factor = 1.0 * max_seq_len / output_seq_len
        input_lengths = torch.max((src_lengths.float() / subsampling_factor).ceil().long(), x.size(0) * src_lengths.new_ones([src_lengths.size(0)]).long())
        encoder_padding_mask, _ = lengths_to_encoder_padding_mask(input_lengths, batch_first=True)
        positions = self.embed_positions(encoder_padding_mask).transpose(0, 1)
        x += positions
        x = F.dropout(x, p=self.dropout, training=self.training)
        if states is None:
            states = [{'memory_banks': None, 'encoder_states': None} for i in range(len(self.transformer_layers))]
        for i, layer in enumerate(self.transformer_layers):
            x = layer(x, states[i])
            states[i]['encoder_states'] = x[self.left_context_after_stride:-self.right_context_after_stride]
        lengths = (~encoder_padding_mask[:, self.left_context_after_stride:-self.right_context_after_stride]).sum(dim=1, keepdim=True).long()
        return states[-1]['encoder_states'], lengths, states


@torch.jit.export
def segments_to_sequence(segments: List[Tuple[Tensor, Tensor]], time_axis: int) ->Tuple[Tensor, Tensor]:
    """Concatenate segments into a full sequence."""
    if len(segments) == 1:
        return segments[0]
    tensors_to_concat: List[Tensor] = []
    lengths_to_stack: List[Tensor] = []
    for tensor, lengths in segments:
        tensors_to_concat.append(tensor)
        lengths_to_stack.append(lengths)
    sequence = torch.cat(tensors_to_concat, dim=time_axis)
    lengths = torch.stack(lengths_to_stack, dim=0)
    lengths = torch.sum(lengths, dim=0)
    return sequence, lengths


@torch.jit.export
def pad_sequence(sequence: Tensor, time_axis: int, extra_left_context: int=0, extra_right_context: int=0) ->Tensor:
    """Pad extra left/right contexts to the sequence."""
    if extra_left_context == 0 and extra_right_context == 0:
        return sequence
    tensors_to_concat = []
    if extra_left_context:
        size = extra_left_context,
        fill_value = 0
        indices = torch.full(size=size, fill_value=fill_value, dtype=torch.long, device=sequence.device)
        left_padding = torch.index_select(sequence, time_axis, indices)
        tensors_to_concat.append(left_padding)
    tensors_to_concat.append(sequence)
    if extra_right_context:
        size = list(sequence.shape)
        size[time_axis] = extra_right_context
        right_padding = torch.zeros(size, dtype=sequence.dtype, device=sequence.device)
        tensors_to_concat.append(right_padding)
    padded_sequence = torch.cat(tensors_to_concat, dim=time_axis)
    return padded_sequence


@torch.jit.export
def sequence_to_segments(sequence: Tensor, time_axis: int, lengths: Tensor, segment_size: Optional[int]=None, extra_left_context: int=0, extra_right_context: int=0) ->List[Tuple[Tensor, Tensor]]:
    """Breaks sequence into segments."""
    sequence = pad_sequence(sequence=sequence, time_axis=time_axis, extra_left_context=extra_left_context, extra_right_context=extra_right_context)
    lengths = lengths + extra_left_context + extra_right_context
    segments: List[Tuple[Tensor, Tensor]] = []
    if segment_size is None:
        segments.append((sequence, lengths))
        return segments
    offset = 0
    end = sequence.shape[time_axis]
    step = segment_size
    size = extra_left_context + segment_size + extra_right_context
    while offset + extra_left_context + extra_right_context < end:
        clamped_size = min(size, end - offset)
        segment_lengths = torch.clamp(lengths - offset, min=0, max=clamped_size)
        indices = torch.arange(start=offset, end=offset + clamped_size, step=1, dtype=torch.long, device=sequence.device)
        segment_tensor = torch.index_select(sequence, time_axis, indices)
        segments.append((segment_tensor, segment_lengths))
        offset = offset + step
    return segments


class SequenceEncoder(FairseqEncoder):
    """
    SequenceEncoder encodes sequences.

    More specifically, `src_tokens` and `src_lengths` in `forward()` should
    describe a batch of "complete" sequences rather than segments.

    Segment-by-segment inference can be triggered by `segment_size`:
    1) `segment_size` is None:
        SequenceEncoder treats the input sequence as one single segment.
    2) `segment_size` is not None (some int instead):
        SequenceEncoder does the following:
            1. breaks the input sequence into several segments
            2. inference on each segment and collect the outputs
            3. concatanete segment outputs into the output sequence.
    Note that `segment_size` here shouldn't include additional left/right
    contexts needed, for example if we wish to infer with LC-BLSTM where the
    middle chunk size is 100 and right context is 20, `segment_size` should be
    100.
    """

    def __init__(self, args, module):
        super().__init__(None)
        self.module = module
        self.input_time_axis = 1
        self.output_time_axis = 0
        self.segment_size = args.segment_size
        self.left_context = args.left_context
        self.right_context = args.right_context

    def forward(self, src_tokens: Tensor, src_lengths: Tensor, states=None):
        seg_src_tokens_lengths = sequence_to_segments(sequence=src_tokens, time_axis=self.input_time_axis, lengths=src_lengths, segment_size=self.segment_size, extra_left_context=self.left_context, extra_right_context=self.right_context)
        seg_encoder_states_lengths: List[Tuple[Tensor, Tensor]] = []
        for seg_src_tokens, seg_src_lengths in seg_src_tokens_lengths:
            seg_encoder_states, seg_enc_lengths, states = self.module(seg_src_tokens, seg_src_lengths, states=states)
            seg_encoder_states_lengths.append((seg_encoder_states, seg_enc_lengths))
        encoder_out, enc_lengths = segments_to_sequence(segments=seg_encoder_states_lengths, time_axis=self.output_time_axis)
        encoder_padding_mask, _ = lengths_to_encoder_padding_mask(enc_lengths, batch_first=True)
        if not encoder_padding_mask.any():
            encoder_padding_mask = None
        return {'encoder_out': [encoder_out], 'encoder_padding_mask': [encoder_padding_mask], 'encoder_embedding': [], 'encoder_states': [states], 'src_tokens': [], 'src_lengths': []}

    def incremental_encode(self, seg_src_tokens: Tensor, seg_src_lengths: Tensor, states=None):
        """
        Different from forward function, this function takes segmented speech
        as input, and append encoder states to previous states
        """
        seg_encoder_states, seg_enc_lengths, states = self.module(seg_src_tokens, seg_src_lengths, states=states)
        return seg_encoder_states, seg_enc_lengths, states


class RelativePositionEmbedding(nn.Module):
    """
    Implementation according to https://arxiv.org/abs/1803.02155
    """

    def __init__(self, head_dim, max_position, norm_init=True):
        super().__init__()
        self.head_dim = head_dim
        self.max_position = max_position
        self.embeddings = nn.Parameter(torch.Tensor(max_position * 2 + 1, head_dim))
        if norm_init:
            nn.init.xavier_normal_(self.embeddings)
        else:
            nn.init.xavier_uniform_(self.embeddings)

    def forward(self, input: Tensor):
        output = nn.functional.embedding(input.long(), self.embeddings)
        return output


class PositionwiseFF(nn.Module):
    """
    FFN layer in transformer.

    Args:
        input_dim: input embedding dimension
        ffn_dim: FFN layer inner dimension
        dropout_on_fc1: dropout for first linear layer
        dropout_on_fc2: dropout fr second linear layer
        activation_fn: activation function used after first linear layer.                 Only relu or gelu is supported.

    """

    def __init__(self, input_dim, ffn_dim, dropout_on_fc1, dropout_on_fc2, activation_fn):
        super(PositionwiseFF, self).__init__()
        self.input_dim = input_dim
        self.ffn_dim = ffn_dim
        if activation_fn == 'relu':
            ac = nn.ReLU()
        elif activation_fn == 'gelu':
            ac = nn.GELU()
        else:
            raise ValueError('Unsupported activation_fn = ({})'.format(activation_fn))
        self.module = nn.Sequential(nn.Linear(input_dim, ffn_dim), ac, nn.Dropout(dropout_on_fc1), nn.Linear(ffn_dim, input_dim), nn.Dropout(dropout_on_fc2))
        self.layer_norm = Fp32LayerNorm(input_dim)

    def forward(self, input):
        module_out = self.module(self.layer_norm(input))
        output = module_out + input
        return output

    def quantize_(self, params=None):
        if params and 'per_channel' in params and params['per_channel']:
            qconfig = per_channel_dynamic_qconfig
        else:
            qconfig = default_dynamic_qconfig
        quantization.quantize_dynamic(self, {torch.nn.Linear: qconfig}, dtype=torch.qint8, inplace=True)
        return self


class SummarizationLayer(nn.Module):

    def __init__(self, method, segment_size, embedding_dim):
        super(SummarizationLayer, self).__init__()
        self.segment_size = segment_size
        self.embedding_dim = embedding_dim
        nonlin_match = re.match('nonlinear\\((?P<act>[a-z]+),(?P<dim>[0-9]+)\\)', method)
        self.method = method
        if method == 'mean':
            self.module = nn.AvgPool1d(kernel_size=segment_size, stride=segment_size, ceil_mode=True)
        elif method == 'max':
            self.module = nn.MaxPool1d(kernel_size=segment_size, stride=segment_size, ceil_mode=True)
        elif method == 'linear':
            self.module = nn.Linear(segment_size, 1)
        elif nonlin_match:
            nonlin_args = nonlin_match.groupdict()
            act_type = nonlin_args['act']
            hid_dim = int(nonlin_args['dim'])
            if act_type == 'relu':
                act = nn.ReLU()
            elif act_type == 'gelu':
                act = nn.GELU()
            else:
                raise ValueError('Unsupported activation_fn = ({})'.format(act_type))
            self.module = nn.Sequential(nn.Linear(segment_size, hid_dim), act, nn.Linear(hid_dim, 1))
        else:
            raise ValueError('Unsupported summarization method = ({})'.format(method))

    def forward(self, input):
        input = input.permute(1, 2, 0)
        if self.method == 'mean' or self.method == 'max':
            output = self.module(input)
            output = output.permute(2, 0, 1)
            return output
        full_seg_length = input.size(2) // self.segment_size * self.segment_size
        if full_seg_length > 0:
            B = input.size(0)
            D = input.size(1)
            input_todo = input[:, :, :full_seg_length].contiguous().view(B, -1, self.segment_size)
            output = self.module(input_todo)
            output = output.view(B, D, -1)
        else:
            output = input.new_zeros(input.size(0), input.size(1), 0)
        left = input.size(2) - full_seg_length
        if left > 0:
            zeros = input.new_zeros(input.size(0), input.size(1), 1)
            output = torch.cat([output, zeros], dim=2)
        output = output.permute(2, 0, 1)
        return output


class NoOp(torch.nn.Module):
    """
    NoOp simply passes the input as the output.
    """

    def __init__(self):
        super().__init__()

    def forward(self, input: Tensor) ->Tensor:
        return input


class NoSegAugmentedMemoryMultiheadAttentionBmm(nn.Module):
    """
    Whole utterance augmented memory multihead attention using BMM.

    Different with previous augmented memory multihead attention where
    the utterance is chunked into segments. Here we use attention mask
    achieve so. The input embedding [right_context, utterance, summary]
    is a concatenation of right context, utterance and summary.

    Right context block is the concatenation of all the right context for
    each segments. [right_context_0, right_context_1, ..., right_context_n]
    For example, if we have utterance = [v0, v1, v2, ...., v20]. segment
    size 8, right_context size 4. Then the right context blocks =
    [v8, v9, v10, v11, v16, v17, v18, v19, 0, 0, 0, 0], where v8, v9, v10,
    and v11 are the right context for first segment. v16, v17, v18 and v19
    are the right context for second segment. 0, 0, 0 and 0 are right context
    for the last segment.

    utterance is corresponding to input embedding sequence

    summary is concatenation of average of each segments. [summary_0,
    summary_1, ..., ].

    In augmented memory multihead attention, the query is [right_context,
    utterance, summary], key is [memory, right_context, utterance]. Different
    with AugmentedMemoryMultiheadAttentionBmm, memory here is passed from
    previous attention layer. For the first attention layer, memory is average
    of each segment.

    Memory is a concatenation of memory from each segments in previous attention
    layer. For example, current layer is i, then memory is [m_0, m_1, ..., m_n].
    Each m_k is the output from seg_k in layer i-1.

    args:
        input_dim: input embedding dimension
        num_heads: number of heads in multihead self-attention
        dropout: attention dropout
        std_scale: if std_scale is not None. The weak attention suppression is
            turned on. For std_scale = 0.5, all the attention smaller than
            mean + 0.5 * std will be suppressed.
        scaled_init: whether to use scaled init for linear weight
        tanh_on_mem: whether to use tanh on memory output
        use_mem: whether to use memory or not. When max_memory_size is 0, then
            we don't have memory anymore.
        layer_index: current self-attention layer index that is used in depth
            initialization
        max_relative_position: max relative position used in relative position
            embedding
        rpe_old_option: To be compatible with previous model. The previous model
            was trained with attention += attention + rpe. The correct equation
            should be attention = attention + rpe

    """

    def __init__(self, input_dim, num_heads, dropout=0.0, std_scale=None, scaled_init=False, tanh_on_mem=False, use_mem=True, mini_batches=False, negative_inf='-inf', layer_index=-1, max_relative_position=0, rpe_old_option=True):
        if input_dim % num_heads:
            raise ValueError('input_dim ({}) must be divisible by num_heads ({})'.format(input_dim, num_heads))
        super().__init__()
        embed_dim = input_dim
        self.e2h_kv = torch.nn.Linear(input_dim, 2 * input_dim, bias=True)
        self.e2h_q = torch.nn.Linear(input_dim, input_dim, bias=True)
        self.rpe_old_option = rpe_old_option
        if max_relative_position > 0:
            self.use_rpe = True
            self.rpe_k = RelativePositionEmbedding(head_dim=input_dim // num_heads, max_position=max_relative_position)
            self.rpe_v = RelativePositionEmbedding(head_dim=input_dim // num_heads, max_position=max_relative_position)
        else:
            self.use_rpe = False
            self.rpe_k = None
            self.rpe_v = None
        if scaled_init:
            if layer_index == -1:
                gain = 1.0 / math.sqrt(2)
            else:
                gain = 1.0 / math.sqrt(layer_index + 1)
            torch.nn.init.xavier_uniform_(self.e2h_kv.weight, gain=gain)
            torch.nn.init.xavier_uniform_(self.e2h_q.weight, gain=gain)
        self.out_proj = torch.nn.Linear(embed_dim, embed_dim, bias=True)
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        self.scaling = self.head_dim ** -0.5
        self.std_scale = std_scale
        self.use_mem = use_mem
        self.mini_batches = mini_batches
        self.negative_inf = negative_inf
        if tanh_on_mem:
            self.squash_mem = torch.tanh
            self.nonlinear_squash_mem = True
        else:
            self.squash_mem = NoOp()
            self.nonlinear_squash_mem = False

    def prepare_qkv(self, input: Tensor, mems: Tensor, lengths: Tensor, summary_length: int, lc_length: int):
        T, B, D = input.shape
        mem_length = mems.size(0)
        utterance_length = torch.max(lengths)
        right_context_blocks_length = T - utterance_length - summary_length
        rc_block = input[:right_context_blocks_length, :, :]
        utterance_block = input[right_context_blocks_length:T - summary_length, :, :]
        if B == 1:
            padding_mask = None
        else:
            klengths = lengths + mem_length + right_context_blocks_length + lc_length
            padding_mask = lengths_to_padding_mask(lengths=klengths)
        mem_rc_input = torch.cat([mems, rc_block, utterance_block], dim=0)
        key_length = mem_rc_input.size(0) + lc_length
        rc_input_sum = input
        q = self.e2h_q(rc_input_sum)
        kv = self.e2h_kv(mem_rc_input)
        k, v = kv.chunk(chunks=2, dim=2)
        result_qkv = q, k, v
        input_shape = T, B, D
        result_lengths_info = mem_length, utterance_length, right_context_blocks_length, key_length
        if padding_mask is not None:
            assert padding_mask.size(0) == B
            assert padding_mask.size(1) == key_length
        return result_qkv, input_shape, result_lengths_info, padding_mask

    def prepare_attention_weights(self, q: Tensor, new_k: Tensor, new_v: Tensor, input_shape: Tuple[int, int, int], rpe: Optional[Tensor]) ->Tuple[Tensor, Tensor, Tensor]:
        T, B, D = input_shape
        q = q.contiguous().view(-1, B * self.num_heads, self.head_dim).transpose(0, 1) * self.scaling
        k = new_k.contiguous().view(-1, B * self.num_heads, self.head_dim).transpose(0, 1)
        v = new_v.contiguous().view(-1, B * self.num_heads, self.head_dim).transpose(0, 1)
        attention_weights = torch.bmm(q, k.transpose(1, 2))
        if self.use_rpe and rpe is not None and self.rpe_v is not None:
            r_k = self.rpe_k(rpe)
            attention_weights_rpe = torch.matmul(q.transpose(0, 1), r_k.transpose(1, 2)).transpose(0, 1)
            attention_weights = attention_weights + attention_weights_rpe
        attention_weights_float = attention_weights.float()
        return attention_weights, attention_weights_float, v

    def prepare_attention_output(self, attention_weights: Tensor, attention_weights_float: Tensor, v: Tensor, input_shape: Tuple[int, int, int], key_length: int, padding_mask: Optional[Tensor], rpe: Optional[Tensor]) ->Tensor:
        T, B, D = input_shape
        if padding_mask is not None:
            attention_weights_float = attention_weights_float.view(B, self.num_heads, T, key_length)
            attention_weights_float = attention_weights_float.masked_fill(padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
            attention_weights_float = attention_weights_float.view(B * self.num_heads, T, key_length)
        if self.std_scale is not None:
            attention_weights_float = attention_suppression(attention_weights_float, self.std_scale)
        attention_weights_float = torch.nn.functional.softmax(attention_weights_float, dim=-1)
        attention_weights = attention_weights_float.type_as(attention_weights)
        attention_probs = torch.nn.functional.dropout(attention_weights, p=self.dropout, training=self.training)
        attention = torch.bmm(attention_probs, v)
        if self.use_rpe and rpe is not None and self.rpe_v is not None:
            r_v = self.rpe_v(rpe)
            attention_rpe = torch.matmul(attention_probs.transpose(0, 1), r_v).transpose(0, 1)
            if self.rpe_old_option:
                attention += attention + attention_rpe
            else:
                attention = attention + attention_rpe
        assert list(attention.shape) == [B * self.num_heads, T, self.head_dim]
        attention = attention.transpose(0, 1).contiguous().view(T, B, self.embed_dim)
        rc_output_memory = self.out_proj(attention)
        return rc_output_memory

    @torch.jit.unused
    def forward(self, input: Tensor, lengths: Tensor, mems: Tensor, attention_mask: Tensor, pre_mems: Optional[Tensor]=None, left_context_key: Optional[Tensor]=None, left_context_val: Optional[Tensor]=None, rpe: Optional[Tensor]=None) ->Tuple[Tensor, Tensor, Tensor, Tensor]:
        """
        forward function for NoSegAugmentedMemoryMultiheadAttentionBmm in training.

        args:
            input: formed in the following way
                [right_context_0, right_contex_1, ..., seg_0, seg_1,
                ..., summary_0, summary_1,..]
            lengths: the length of query which is [seg_0, seg_1, ....]
            mems: [mem_0, mem_1, ...].
            attention_mask: attention mask for query = [right_context, query, summary]
                key = [mem, right_context, query]. This is only used for traing.

        """
        if self.use_mem:
            mem_length = mems.size(0)
            summary_length = mem_length + 1
            if pre_mems is not None:
                mems = torch.cat([pre_mems, mems], dim=0)
        else:
            mem_length = 0
            summary_length = 0
        if left_context_key is not None:
            lc_length = left_context_key.size(0)
        else:
            lc_length = 0
        results = self.prepare_qkv(input=input, mems=mems, lengths=lengths, summary_length=summary_length, lc_length=lc_length)
        result_qkv, input_shape, result_lengths_info, padding_mask = results
        q, k, v = result_qkv
        mem_length, utterance_length, right_context_blocks_length, key_length = result_lengths_info
        if left_context_key is not None:
            new_k = torch.cat([k[:mem_length + right_context_blocks_length, :, :], left_context_key, k[-utterance_length:, :, :]], dim=0)
            new_v = torch.cat([v[:mem_length + right_context_blocks_length, :, :], left_context_val, v[-utterance_length:, :, :]], dim=0)
            next_k = new_k[mem_length + right_context_blocks_length:, :, :]
            next_v = new_v[mem_length + right_context_blocks_length:, :, :]
        else:
            new_k = k
            new_v = v
            next_k = None
            next_v = None
        attention_weights, attention_weights_float, v = self.prepare_attention_weights(q=q, new_k=new_k, new_v=new_v, input_shape=input_shape, rpe=rpe)
        attention_mask = attention_mask.unsqueeze(0)
        attention_weights_float = attention_weights_float.masked_fill(attention_mask, float(self.negative_inf))
        rc_output_memory = self.prepare_attention_output(attention_weights=attention_weights, attention_weights_float=attention_weights_float, v=v, input_shape=input_shape, key_length=key_length, padding_mask=padding_mask, rpe=rpe)
        if self.use_mem:
            if self.mini_batches:
                next_m = rc_output_memory[-summary_length:]
            else:
                next_m = rc_output_memory[-summary_length:-1]
            next_m = self.squash_mem(next_m)
            rc_output = rc_output_memory[:-summary_length]
            if not self.nonlinear_squash_mem:
                next_m = torch.clamp(next_m, min=-10, max=10)
        else:
            next_m = mems
            rc_output = rc_output_memory
        return rc_output, next_m, next_k, next_v

    @torch.jit.export
    def forward_jit(self, input: Tensor, lengths: Tensor, mems: Tensor, left_context_key: Tensor, left_context_val: Tensor, rpe: Optional[Tensor]) ->Tuple[Tensor, Tensor, Tensor, Tensor]:
        """
        forward function for NoSegAugmentedMemoryMultiheadAttentionBmm in decoding.

        args:
            input: formed in the following way
                [right_context_0, right_contex_1, ..., seg_0, seg_1,
                ..., summary_0, summary_1,..]
            lengths: the length of query which is [seg_0, seg_1, ....]
            mems: [mem_0, mem_1, ...].
            left_context_key: left_context for key part. This is only used for online
                decoding. In training, this is empty tensor
            left_context_val: left_context for value part. This is only used for online
                decoding. In training, this is empty tensor

        """
        lc_length = left_context_key.size(0)
        if self.use_mem:
            summary_length = 1
        else:
            summary_length = 0
        results = self.prepare_qkv(input=input, mems=mems, lengths=lengths, summary_length=summary_length, lc_length=lc_length)
        result_qkv, input_shape, result_lengths_info, padding_mask = results
        q, k, v = result_qkv
        mem_length, utterance_length, right_context_blocks_length, key_length = result_lengths_info
        new_k = torch.cat([k[:mem_length + right_context_blocks_length, :, :], left_context_key, k[-utterance_length:, :, :]], dim=0)
        new_v = torch.cat([v[:mem_length + right_context_blocks_length, :, :], left_context_val, v[-utterance_length:, :, :]], dim=0)
        next_k = new_k[mem_length + right_context_blocks_length:, :, :]
        next_v = new_v[mem_length + right_context_blocks_length:, :, :]
        attention_weights, attention_weights_float, v = self.prepare_attention_weights(q=q, new_k=new_k, new_v=new_v, input_shape=input_shape, rpe=rpe)
        attention_weights_float[:, -1, :mem_length] = float(self.negative_inf)
        rc_output_memory = self.prepare_attention_output(attention_weights=attention_weights, attention_weights_float=attention_weights_float, v=v, input_shape=input_shape, key_length=key_length, padding_mask=padding_mask, rpe=rpe)
        if self.use_mem:
            next_m = rc_output_memory[-1:]
            next_m = self.squash_mem(next_m)
            rc_output = rc_output_memory[:-1]
            if not self.nonlinear_squash_mem:
                next_m = torch.clamp(next_m, min=-10, max=10)
        else:
            rc_output = rc_output_memory
            next_m = mems
        return rc_output, next_m, next_k, next_v

    def quantize_(self, params=None):
        if params and 'per_channel' in params and params['per_channel']:
            qconfig = per_channel_dynamic_qconfig
        else:
            qconfig = default_dynamic_qconfig
        quantization.quantize_dynamic(self, {torch.nn.Linear: qconfig}, dtype=torch.qint8, inplace=True)
        return self


class NoSegAugmentedMemoryTransformer(nn.Module):
    """
    Whole utterance augmented memory transformer.

    This is not pyspeech nn layer. It is used as a module in a master layer where
    multiple transformers is used.
    """

    def __init__(self, input_dim, num_heads, ffn_dim, dropout_in_attn=0.0, dropout_on_attn=None, dropout_on_fc1=None, dropout_on_fc2=None, activation_fn='relu', tanh_on_mem=False, std_scale=None, scaled_init=False, segment_size=128, use_mem=True, mini_batches=False, negative_inf='-inf', layer_index=-1, summarization_method='mean', max_relative_position=0, rpe_old_option=True):
        super(NoSegAugmentedMemoryTransformer, self).__init__()
        self.attention = NoSegAugmentedMemoryMultiheadAttentionBmm(input_dim=input_dim, num_heads=num_heads, dropout=dropout_in_attn, scaled_init=scaled_init, tanh_on_mem=tanh_on_mem, std_scale=std_scale, use_mem=use_mem, mini_batches=mini_batches, negative_inf=negative_inf, layer_index=layer_index, max_relative_position=max_relative_position)
        self.dropout = nn.Dropout(dropout_on_attn)
        self.pos_ff = PositionwiseFF(input_dim=input_dim, ffn_dim=ffn_dim, dropout_on_fc1=dropout_on_fc1, dropout_on_fc2=dropout_on_fc2, activation_fn=activation_fn)
        self.layer_norm_pre = Fp32LayerNorm(input_dim)
        self.layer_norm = Fp32LayerNorm(input_dim)
        self.segment_size = segment_size
        self.use_mem = use_mem
        self.memory_op = SummarizationLayer(summarization_method, segment_size, input_dim)

    def set_mini_batches(self, mini_batches):
        self.attention.mini_batches = mini_batches

    def gen_summary_queries(self, input):
        sum_input = self.memory_op(input)
        return sum_input

    def pre_attention_ops(self, input, right_context_blocks):
        rc_length = right_context_blocks.size(0)
        input_length = input.size(0)
        rc_and_input = torch.cat([right_context_blocks, input], dim=0)
        residual_input = rc_and_input
        rc_and_input = self.layer_norm_pre(rc_and_input)
        query_input = rc_and_input[-input_length:, :, :]
        return rc_length, input_length, residual_input, query_input, rc_and_input

    def after_attention_ops(self, attention_output, residual_input):
        output = self.dropout(attention_output)
        output = output + residual_input
        output = self.pos_ff(output)
        output = self.layer_norm(output)
        return output

    @torch.jit.export
    def forward_jit(self, input: Tensor, lengths: Tensor, mems: Tensor, left_context_key: Tensor, left_context_val: Tensor, right_context_blocks: Tensor, rpe: Optional[Tensor]) ->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
        results = self.pre_attention_ops(input, right_context_blocks)
        rc_length, input_length, residual_input, query_input, rc_and_input = results
        if self.use_mem:
            summary_query = self.gen_summary_queries(query_input)
            summary_query = summary_query[0:1, :, :]
            rc_qu_su = torch.cat([rc_and_input, summary_query], dim=0)
        else:
            rc_qu_su = rc_and_input
        rc_output, next_m, next_k, next_v = self.attention.forward_jit(input=rc_qu_su, lengths=lengths, mems=mems, left_context_key=left_context_key, left_context_val=left_context_val, rpe=rpe)
        rc_output = self.after_attention_ops(rc_output, residual_input)
        results = rc_output[-input_length:, :, :], next_m, rc_output[0:rc_length, :, :], next_k, next_v
        return results

    @torch.jit.unused
    def forward(self, input, lengths, mems, right_context_blocks, attention_mask, pre_mems, left_context_key, left_context_val, rpe):
        results = self.pre_attention_ops(input, right_context_blocks)
        rc_length, input_length, residual_input, query_input, rc_and_input = results
        if self.use_mem:
            summary_query = self.gen_summary_queries(query_input)
            rc_qu_su = torch.cat([rc_and_input, summary_query], dim=0)
        else:
            rc_qu_su = rc_and_input
        rc_output, next_m, next_k, next_v = self.attention(input=rc_qu_su, lengths=lengths, mems=mems, attention_mask=attention_mask, pre_mems=pre_mems, left_context_key=left_context_key, left_context_val=left_context_val, rpe=rpe)
        rc_output = self.after_attention_ops(rc_output, residual_input)
        results = rc_output[-input_length:, :, :], next_m, rc_output[0:rc_length, :, :], next_k, next_v
        return results


class NoSegAugmentedMemoryTransformerEncoderLayer(FairseqEncoder):
    """
    Whole utterance augmented memory transformer encoder layer. This is a master layer
    where we can define multiple augmented memory transformers. There are two reasons
    to setup the master layer.
    1. We only need to define once about the attention mask. All the layers in the master
       layer share the same mask.
    2. pyspeech nn layer has special input and output format. Defining one master layer is
       easier to passing memory between different layes inside the master layer

    args:
        input_dim: input embedding dimension
        num_heads: number of heads in multihead self-attention
        ffn_dim: ffn dimension in FFN layer
        num_layers: number of augmented memory transformer layers
        dropout_in_attn: dropout used in multi-head self-attention
        dropout_on_attn: dropout used for output from te multihead self-attention
        dropout_on_fc1: dropout used in FFN layer for the first linear layer
        dropout_on_fc2: dropout used in FFN layer for the second linear layer
        segment_size: segment size for each segment
        context_config: (left_context_size, right_context_size) defines the surround context size
            for each segment
        max_memory_size: maximum memory size used for each segment
        scaled_init: whether use scaled init for weight initialization in attention layer
        std_scale: if std_scale is not None. The weak attention suppression is
            turned on. For std_scale = 0.5, all the attention smaller than
            mean + 0.5 * std will be suppressed.
        activation_fn: activation function used in FFN layer. [ReLU, GELU] supported
        tanh_on_mem: whether use tanh on memory
        mini_batches: use mini-btach training
        negative_inf: the negative infinity value used in attention masking. default is "-inf".
            For some situation, e.g. LM. it is better to use "-1e8" to avoid nan issue.
        summarization_method: method to generate segment summrization embedding
        max_relative_position: max relatie position for relative position embedding
        rpe_old_option: To be compatible with previous model. The previous model
            was trained with attention += attention + rpe. The correct equation
            should be attention = attention + rpe
        [TODO]: remove the rpe_old_option by the end of 2021 Q1.

    """

    def __init__(self, input_dim, num_heads, ffn_dim, num_layers=1, dropout_in_attn=0.0, dropout_on_attn=0.0, dropout_on_fc1=0.0, dropout_on_fc2=0.0, segment_size=128, context_config=(0, 0), max_memory_size=0, scaled_init=True, std_scale=None, activation_fn='relu', tanh_on_mem=False, mini_batches=False, negative_inf='-inf', deep_init=True, summarization_method='mean', max_relative_position=0, rpe_old_option=True):
        super().__init__(None)
        if input_dim % num_heads:
            raise ValueError('input_dim ({}) must be divisible by num_heads ({})'.format(input_dim, num_heads))
        if max_memory_size < 0:
            raise ValueError('max_memory_size must be >= 0')
        self.left_context, self.right_context = context_config
        self.segment_size = segment_size
        self.memory_dim = input_dim
        self.max_memory_size = max_memory_size
        self.mini_batches = mini_batches
        if self.max_memory_size != 0:
            self.use_mem = True
        else:
            self.use_mem = False
        self.memory_op = SummarizationLayer(summarization_method, segment_size, input_dim)
        self.layers = torch.nn.ModuleList()
        self.num_layers = num_layers
        self.max_relative_position = max_relative_position
        if self.max_relative_position > 0:
            self.use_rpe = True
        else:
            self.use_rpe = False
        for i in range(self.num_layers):
            if deep_init:
                layer_index = i
            else:
                layer_index = -1
            self.layers.append(NoSegAugmentedMemoryTransformer(num_heads=num_heads, input_dim=input_dim, ffn_dim=ffn_dim, dropout_in_attn=dropout_in_attn, dropout_on_attn=dropout_on_attn, dropout_on_fc1=dropout_on_fc1, dropout_on_fc2=dropout_on_fc2, segment_size=segment_size, std_scale=std_scale, activation_fn=activation_fn, tanh_on_mem=tanh_on_mem, scaled_init=scaled_init, use_mem=self.use_mem, mini_batches=mini_batches, negative_inf=negative_inf, layer_index=layer_index, summarization_method=summarization_method, max_relative_position=max_relative_position, rpe_old_option=rpe_old_option))

    def set_mini_batches(self, mini_batches):
        self.mini_batches = mini_batches
        for layer in self.layers:
            layer.set_mini_batches(mini_batches)

    def _get_relative_position(self, input: Tensor, max_relative_position: int, left_context_length: int, past_length: int, is_decoding: bool):
        T, B, D = input.shape
        num_segs = math.ceil((T - self.right_context) / self.segment_size)
        u_st = past_length * self.segment_size
        u_ed = u_st + T
        utterance_ranges = torch.arange(u_st, u_ed - self.right_context)
        left_context_ranges = torch.arange(u_st - left_context_length, u_st)
        right_context_blocks = []
        for i in range(0, num_segs - 1):
            st = (i + 1) * self.segment_size + u_st
            ed = st + self.right_context
            assert ed < u_ed
            temp = torch.arange(st, ed)
            right_context_blocks.append(temp)
        right_context_blocks.append(torch.arange(u_ed - self.right_context, u_ed))
        right_context_ranges = torch.cat(right_context_blocks)
        if self.use_mem:
            if is_decoding:
                memory_size = min(past_length, self.max_memory_size)
            else:
                memory_size = num_segs + past_length - 1
            memory_bank_ranges = torch.arange(-max_relative_position - 1, -max_relative_position - 1 - memory_size, -1)
            summary_pos_st = u_ed + max_relative_position + 1
            summary_vector_ranges = torch.arange(summary_pos_st, summary_pos_st + num_segs)
            key_ranges = torch.cat([memory_bank_ranges, right_context_ranges, left_context_ranges, utterance_ranges])
            query_ranges = torch.cat([right_context_ranges, utterance_ranges, summary_vector_ranges])
        else:
            key_ranges = torch.cat([right_context_ranges, left_context_ranges, utterance_ranges])
            query_ranges = torch.cat([right_context_ranges, utterance_ranges])
        distance = key_ranges[None, :] - query_ranges[:, None]
        distance_clamp = torch.clamp(distance, -max_relative_position, max_relative_position) + max_relative_position
        distance_clamp = distance_clamp.long().detach()
        return distance_clamp

    def _get_attention_mask(self, input, past_length=0, left_context_cache=0):
        utterance_length, batch_size, _ = input.shape
        summary_length = math.ceil(utterance_length / self.segment_size)
        num_segs = summary_length
        rc_length = self.right_context * num_segs
        rc = self.right_context
        lc = self.left_context
        lcc = left_context_cache
        if self.use_mem:
            mem_length = num_segs - 1 + past_length
        else:
            mem_length = 0
        rc_mask = []
        query_mask = []
        summary_mask = []
        for j in range(0, num_segs):
            ssize = min(self.segment_size, utterance_length - j * self.segment_size)
            rc_size = rc
            rc_mat = []
            q_mat = []
            s_mat = []
            m_start = max(j + past_length - self.max_memory_size, 0)
            if self.use_mem:
                rc_mat.append(input.new_zeros(rc_size, m_start))
                q_mat.append(input.new_zeros(ssize, m_start))
                s_mat.append(input.new_zeros(1, m_start))
                col_1 = j + past_length - m_start
                rc_mat.append(torch.ones(rc_size, col_1, device=input.device))
                q_mat.append(torch.ones(ssize, col_1, device=input.device))
                s_mat.append(input.new_zeros(1, col_1))
                col_2 = mem_length - (j + past_length)
                rc_mat.append(input.new_zeros(rc_size, col_2))
                q_mat.append(input.new_zeros(ssize, col_2))
                s_mat.append(input.new_zeros(1, col_2))
            rc_start = j * rc
            rc_mat.append(input.new_zeros(rc_size, rc_start))
            q_mat.append(input.new_zeros(ssize, rc_start))
            s_mat.append(input.new_zeros(1, rc_start))
            rc_end = rc_start + rc
            col_4 = rc
            rc_mat.append(torch.ones(rc_size, col_4, device=input.device))
            q_mat.append(torch.ones(ssize, col_4, device=input.device))
            s_mat.append(torch.ones(1, col_4, device=input.device))
            col_5 = rc_length - rc_end
            rc_mat.append(input.new_zeros(rc_size, col_5))
            q_mat.append(input.new_zeros(ssize, col_5))
            s_mat.append(input.new_zeros(1, col_5))
            seg_start = max(j * self.segment_size + lcc - lc, 0)
            rc_mat.append(input.new_zeros(rc_size, seg_start))
            q_mat.append(input.new_zeros(ssize, seg_start))
            s_mat.append(input.new_zeros(1, seg_start))
            seg_end = min((j + 1) * self.segment_size + lcc, utterance_length + lcc)
            col_7 = seg_end - seg_start
            rc_mat.append(torch.ones(rc_size, col_7, device=input.device))
            q_mat.append(torch.ones(ssize, col_7, device=input.device))
            s_mat.append(torch.ones(1, col_7, device=input.device))
            col_8 = utterance_length + lcc - seg_end
            rc_mat.append(input.new_zeros(rc_size, col_8))
            q_mat.append(input.new_zeros(ssize, col_8))
            s_mat.append(input.new_zeros(1, col_8))
            rc_mask.append(torch.cat(rc_mat, dim=1))
            query_mask.append(torch.cat(q_mat, dim=1))
            summary_mask.append(torch.cat(s_mat, dim=1))
        if self.use_mem:
            attention_mask = 1 - torch.cat([torch.cat(rc_mask, dim=0), torch.cat(query_mask, dim=0), torch.cat(summary_mask, dim=0)], dim=0)
        else:
            attention_mask = 1 - torch.cat([torch.cat(rc_mask, dim=0), torch.cat(query_mask, dim=0)], dim=0)
        return attention_mask

    @torch.jit.export
    def init_state(self, batch_size: int, device: Optional[Device]=None) ->List[Tensor]:
        empty_memory = torch.zeros(self.num_layers, self.max_memory_size, batch_size, self.memory_dim, device=device)
        left_context_key = torch.zeros(self.num_layers, self.left_context, batch_size, self.memory_dim, device=device)
        left_context_val = torch.zeros(self.num_layers, self.left_context, batch_size, self.memory_dim, device=device)
        past_length = torch.zeros(1, batch_size, dtype=torch.int32, device=device)
        return [empty_memory, left_context_key, left_context_val, past_length]

    @torch.jit.export
    def batch_state(self, states: List[List[Tensor]]) ->List[Tensor]:
        if len(states) == 0:
            return []
        batched_m = []
        batched_lc_key = []
        batched_lc_val = []
        batched_past_length = []
        for state in states:
            if len(state) == 0:
                continue
            m, lc_key, lc_val, past_length = state
            batched_m.append(m)
            batched_lc_key.append(lc_key)
            batched_lc_val.append(lc_val)
            batched_past_length.append(past_length)
        if len(batched_m) == 0 or len(batched_lc_key) == 0 or len(batched_lc_val) == 0 or len(batched_past_length) == 0:
            return [torch.tensor([]), torch.tensor([]), torch.tensor([]), torch.tensor([])]
        batched_m = torch.cat(batched_m, dim=2)
        batched_lc_key = torch.cat(batched_lc_key, dim=2)
        batched_lc_val = torch.cat(batched_lc_val, dim=2)
        batched_past_length = torch.cat(batched_past_length, dim=1)
        return [batched_m, batched_lc_key, batched_lc_val, batched_past_length]

    @torch.jit.export
    def reorder_state(self, state: List[Tensor], indices: Tensor) ->List[Tensor]:
        if len(state) == 0:
            return []
        m, lc_key, lc_val, past_length = state
        indices = indices
        reord_m = torch.index_select(m, 2, indices)
        reord_lc_key = torch.index_select(lc_key, 2, indices)
        reord_lc_val = torch.index_select(lc_val, 2, indices)
        reord_past_length = torch.index_select(past_length, 1, indices)
        return [reord_m, reord_lc_key, reord_lc_val, reord_past_length]

    @torch.jit.export
    def reset_state(self, state: List[Tensor], indices: Tensor) ->List[Tensor]:
        m, lc_key, lc_val, past_length = state
        m = m.index_fill(dim=2, index=indices, value=0.0)
        lc_key = lc_key.index_fill(dim=2, index=indices, value=0.0)
        lc_val = lc_val.index_fill(dim=2, index=indices, value=0.0)
        past_length = past_length.index_fill(dim=1, index=indices, value=0)
        return [m, lc_key, lc_val, past_length]

    @torch.jit.export
    def state_size(self) ->int:
        return 4

    @torch.jit.export
    def batch_size_in_state(self, state: Optional[List[Tensor]], sloppy: bool=True) ->Optional[int]:
        if state is None:
            return None
        return state[0].size(2)

    def gen_summary_queries(self, input):
        sum_input = self.memory_op(input)
        return sum_input

    def _gen_right_context_padded_input(self, input):
        right_context_blocks = []
        T, B, D = input.shape
        num_segs = math.ceil((T - self.right_context) / self.segment_size)
        for i in range(0, num_segs - 1):
            st = (i + 1) * self.segment_size
            ed = st + self.right_context
            assert ed < T
            temp = input[st:ed, :, :]
            right_context_blocks.append(temp)
        right_context_blocks.append(input[T - self.right_context:, :, :])
        return torch.cat(right_context_blocks, dim=0)

    def _gen_segs_right_context(self, input, lengths):
        segments = []
        T, B, D = input.size()
        nT = T - self.right_context
        num_segs = math.ceil(nT / self.segment_size)
        for i in range(0, num_segs - 1):
            st = i * self.segment_size
            ed = min(T, st + self.segment_size + self.right_context)
            temp = input[st:ed, :, :]
            rest_lengths = torch.clamp(lengths - self.segment_size, min=0, max=nT - (i + 1) * self.segment_size)
            segments.append((temp, lengths - rest_lengths + self.right_context))
            lengths = rest_lengths
        last_seg = input[st + self.segment_size:, :, :]
        segments.append((last_seg, rest_lengths + self.right_context))
        return segments

    @torch.jit.unused
    def forward(self, input: Tensor, padding_masks: Tensor, state: Optional[List[Tensor]]=None) ->Tuple[Tensor, Tensor, List[Tensor], List[Tensor]]:
        lengths = (~padding_masks).sum(dim=1).long()
        if self.mini_batches:
            return self.forward_mini_batches(input, lengths, state)
        T, B, D = input.size()
        right_context_blocks = self._gen_right_context_padded_input(input)
        if self.use_rpe:
            rpe = self._get_relative_position(input=input, max_relative_position=self.max_relative_position, left_context_length=0, past_length=0, is_decoding=False)
        else:
            rpe = None
        input = input[:T - self.right_context, :, :]
        attention_mask = self._get_attention_mask(input)
        if self.use_mem:
            mems = self.gen_summary_queries(input)[:-1, :, :]
        else:
            mems = torch.zeros(0, input.size(1), input.size(2), device=input.device)
            mems = mems.type_as(input)
        output = input
        all_outputs = []
        for layer in self.layers:
            output, mems, right_context_blocks, _, _ = layer(input=output, lengths=lengths, attention_mask=attention_mask, mems=mems, right_context_blocks=right_context_blocks, pre_mems=None, left_context_key=None, left_context_val=None, rpe=rpe)
            all_outputs.append(output)
        return output, padding_masks, [], all_outputs

    def forward_jit_mini_batch_init(self, seg: Tensor, state: Optional[List[Tensor]]=None, is_decoding: bool=False):
        if state is None:
            state = self.init_state(batch_size=seg.size(1), device=seg.device)
            if seg.dtype == torch.half:
                state = [state[0].half(), state[1].half(), state[2].half(), state[3]]
        if self.use_mem:
            full_mems = self.gen_summary_queries(seg)
            if is_decoding:
                mems = full_mems[0:1, :, :]
                state_mems = torch.cat([state[0][0], mems], dim=0)
            else:
                mems = full_mems[:-1, :, :]
                state_mems = torch.cat([state[0][0], full_mems], dim=0)
        else:
            mems = state[0][0]
            state_mems = mems
        past_length = state[3][0][0].item()
        past_left_context = min(past_length * self.segment_size, self.left_context)
        past_length = min(self.max_memory_size, past_length)
        return state, mems, state_mems, past_length, past_left_context

    def state_update_before(self, layer: int, state: List[Tensor], past_length: int, past_left_context: int):
        pre_mems = state[0][layer][self.max_memory_size - past_length:, :, :]
        lc_key = state[1][layer][self.left_context - past_left_context:, :, :]
        lc_val = state[2][layer][self.left_context - past_left_context:, :, :]
        return pre_mems, lc_key, lc_val

    def state_update_after(self, layer: int, state: List[Tensor], mems: Tensor, next_key: Tensor, next_val: Tensor, mems_list: List[Tensor], lc_key_list: List[Tensor], lc_val_list: List[Tensor]):
        if layer < self.num_layers - 1:
            state_mems = torch.cat([state[0][layer + 1], mems], dim=0)
            mems_list.append(state_mems[-self.max_memory_size:, :, :])
        mems = mems[:-1, :, :]
        new_k = torch.cat([state[1][layer], next_key], dim=0)
        new_v = torch.cat([state[2][layer], next_val], dim=0)
        lc_key_list.append(new_k[-self.left_context:, :, :])
        lc_val_list.append(new_v[-self.left_context:, :, :])
        return mems_list, lc_key_list, lc_val_list, mems

    def state_update_after_loop(self, state: List[Tensor], mems_list: List[Tensor], lc_key_list: List[Tensor], lc_val_list: List[Tensor], update_length: int):
        state[0] = torch.stack(mems_list, dim=0)
        state[1] = torch.stack(lc_key_list, dim=0)
        state[2] = torch.stack(lc_val_list, dim=0)
        state[3] = state[3] + update_length
        return state

    @torch.jit.unused
    def forward_mini_batches(self, input: Tensor, lengths: Tensor, state: Optional[List[Tensor]]=None) ->Tuple[Tensor, Tensor, List[Tensor], List[Tensor]]:
        T, B, D = input.size()
        seg = input[:T - self.right_context, :, :]
        right_context_blocks = self._gen_right_context_padded_input(input)
        mems_list = []
        lc_key_list = []
        lc_val_list = []
        results = self.forward_jit_mini_batch_init(seg, state, False)
        state, mems, state_mems, past_length, past_left_context = results
        if self.use_rpe:
            rpe = self._get_relative_position(input=input, max_relative_position=self.max_relative_position, left_context_length=past_left_context, past_length=past_length, is_decoding=False)
        else:
            rpe = None
        attention_mask = self._get_attention_mask(seg, past_length, past_left_context)
        mems_list.append(state_mems[-self.max_memory_size:, :, :])
        output = seg
        i = 0
        all_outputs = []
        for layer in self.layers:
            pre_mems, lc_key, lc_val = self.state_update_before(i, state, past_length, past_left_context)
            output, mems, right_context_blocks, next_key, next_val = layer.forward(input=output, lengths=lengths, attention_mask=attention_mask, mems=mems, right_context_blocks=right_context_blocks, pre_mems=pre_mems, left_context_key=lc_key, left_context_val=lc_val, rpe=rpe)
            all_outputs.append(output)
            mems_list, lc_key_list, lc_val_list, mems = self.state_update_after(layer=i, state=state, mems=mems, next_key=next_key, next_val=next_val, mems_list=mems_list, lc_key_list=lc_key_list, lc_val_list=lc_val_list)
            i += 1
        update_length = math.ceil((T - self.right_context) / self.segment_size)
        state = self.state_update_after_loop(state=state, mems_list=mems_list, lc_key_list=lc_key_list, lc_val_list=lc_val_list, update_length=update_length)
        return output, lengths, state, all_outputs

    def forward_jit_test(self, input: Tensor, lengths: Tensor, state: Optional[List[Tensor]]=None) ->Tuple[Tensor, Tensor, List[Tensor]]:
        """
        This one simulate sequence encoder forward jit. This is for unit test purpose.
        It is not used in training or decoding. Note, extra_right_context is set in
        the model. In unit test, input = [utterance, right_context], lengths =
        [utterance_length].
        args:
            input: input utterance
            lengths: utterance input length
            state: None here. input is whole utterance
        """
        seg_src_tokens_lengths = self._gen_segs_right_context(input, lengths)
        seg_enc_tokens_lengths: List[Tuple[Tensor, Tensor]] = []
        state: Optional[List[Tensor]] = None
        for seg_src_tokens, seg_src_lengths in seg_src_tokens_lengths:
            seg_enc_tokens, seg_enc_lengths, state = self.forward_jit(input=seg_src_tokens, lengths=seg_src_lengths, state=state)
            seg_enc_tokens_lengths.append((seg_enc_tokens, seg_enc_lengths))
        enc_tokens, enc_lengths = segments_to_sequence(segments=seg_enc_tokens_lengths, time_axis=0)
        state = []
        return enc_tokens, enc_lengths, state

    @torch.jit.export
    def forward_jit(self, input: Tensor, lengths: Tensor, state: Optional[List[Tensor]]=None) ->Tuple[Tensor, Tensor, List[Tensor]]:
        """
        Forward helper for online decoding.

        args:
            input: [seg, right_context]. We assume in online we
                always padding the right context to the preset right context size.
                For the last segment, we may have short segment size, but right
                context size is the same as other segments
            lengths: utterance input length is the utterance segment length and
                     right context size
            state: [memory, left_context_key, left_context_val]. To improve throughput,
                in addition to memory, we also cache key and value for left_context in
                multihead self-attention
        """
        T, B, D = input.size()
        rc_str = T - self.right_context
        rc_end = T
        right_context_blocks = input[rc_str:rc_end, :, :]
        seg = input[:rc_str, :, :]
        lengths = torch.clamp(lengths - self.right_context, min=0)
        mems_list = []
        lc_key_list = []
        lc_val_list = []
        results = self.forward_jit_mini_batch_init(seg, state, True)
        state, mems, state_mems, past_length, past_left_context = results
        if self.use_rpe:
            rpe = self._get_relative_position(input=input, max_relative_position=self.max_relative_position, left_context_length=past_left_context, past_length=past_length, is_decoding=True)
        else:
            rpe = None
        mems_list.append(state_mems[-self.max_memory_size:, :, :])
        output = seg
        i = 0
        for layer in self.layers:
            true_mems, lc_key, lc_val = self.state_update_before(layer=i, state=state, past_length=past_length, past_left_context=past_left_context)
            output, mems, right_context_blocks, next_key, next_val = layer.forward_jit(input=output, lengths=lengths, mems=true_mems, right_context_blocks=right_context_blocks, left_context_key=lc_key, left_context_val=lc_val, rpe=rpe)
            mems_list, lc_key_list, lc_val_list, _ = self.state_update_after(layer=i, state=state, mems_list=mems_list, mems=mems, next_key=next_key, next_val=next_val, lc_key_list=lc_key_list, lc_val_list=lc_val_list)
            i += 1
        state = self.state_update_after_loop(state=state, mems_list=mems_list, lc_key_list=lc_key_list, lc_val_list=lc_val_list, update_length=1)
        return output, lengths, state

    def quantize_(self, params=None):
        if params and 'per_channel' in params and params['per_channel']:
            qconfig = per_channel_dynamic_qconfig
        else:
            qconfig = default_dynamic_qconfig
        quantization.quantize_dynamic(self, {torch.nn.Linear: qconfig}, dtype=torch.qint8, inplace=True)
        return self


class RelPositionalEncoding(nn.Module):
    """Relative positional encoding module (new implementation).

    Args:
        d_model: Embedding dimension.
        dropout_rate: Dropout rate.
        max_len: Maximum input length.
    """

    def __init__(self, max_len, d_model):
        """Construct an PositionalEncoding object."""
        super(RelPositionalEncoding, self).__init__()
        self.d_model = d_model
        self.pe = None
        self.extend_pe(torch.tensor(0.0).expand(1, max_len))

    def extend_pe(self, x):
        """Reset the positional encodings."""
        if self.pe is not None:
            if self.pe.size(1) >= x.size(1) * 2 - 1:
                if self.pe.dtype != x.dtype or self.pe.device != x.device:
                    self.pe = self.pe
                return
        pe_positive = torch.zeros(x.size(1), self.d_model)
        pe_negative = torch.zeros(x.size(1), self.d_model)
        position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / self.d_model))
        pe_positive[:, 0::2] = torch.sin(position * div_term)
        pe_positive[:, 1::2] = torch.cos(position * div_term)
        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)
        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)
        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)
        pe_negative = pe_negative[1:].unsqueeze(0)
        pe = torch.cat([pe_positive, pe_negative], dim=1)
        self.pe = pe

    def forward(self, x: torch.Tensor):
        """Add positional encoding.
        Args:
            x : Input tensor T X B X C.
        Returns:
            torch.Tensor: Encoded tensor T X B X C.

        """
        x = x.transpose(0, 1)
        self.extend_pe(x)
        pos_emb = self.pe[:, self.pe.size(1) // 2 - x.size(1) + 1:self.pe.size(1) // 2 + x.size(1)]
        pos_emb = pos_emb.transpose(0, 1)
        return pos_emb


class S2TConformerEncoder(FairseqEncoder):
    """Conformer Encoder for speech translation based on https://arxiv.org/abs/2005.08100"""

    def __init__(self, args):
        super().__init__(None)
        self.encoder_freezing_updates = args.encoder_freezing_updates
        self.num_updates = 0
        self.embed_scale = math.sqrt(args.encoder_embed_dim)
        if args.no_scale_embedding:
            self.embed_scale = 1.0
        self.padding_idx = 1
        self.conv_version = args.conv_version
        if self.conv_version == 's2t_transformer':
            self.subsample = Conv1dSubsampler(args.input_feat_per_channel * args.input_channels, args.conv_channels, args.encoder_embed_dim, [int(k) for k in args.conv_kernel_sizes.split(',')])
        elif self.conv_version == 'convtransformer':
            self.subsample = Conv2dSubsampler(args.input_channels, args.input_feat_per_channel, args.conv_out_channels, args.encoder_embed_dim)
        self.pos_enc_type = args.pos_enc_type
        if self.pos_enc_type == 'rel_pos':
            self.embed_positions = RelPositionalEncoding(args.max_source_positions, args.encoder_embed_dim)
        elif self.pos_enc_type == 'rope':
            self.embed_positions = None
        else:
            self.pos_enc_type = 'abs'
            self.embed_positions = PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim, self.padding_idx)
        self.linear = torch.nn.Linear(args.encoder_embed_dim, args.encoder_embed_dim)
        self.dropout = torch.nn.Dropout(args.dropout)
        self.conformer_layers = torch.nn.ModuleList([ConformerEncoderLayer(embed_dim=args.encoder_embed_dim, ffn_embed_dim=args.encoder_ffn_embed_dim, attention_heads=args.encoder_attention_heads, dropout=args.dropout, depthwise_conv_kernel_size=args.depthwise_conv_kernel_size, attn_type=args.attn_type, pos_enc_type=self.pos_enc_type, use_fp16=args.fp16) for _ in range(args.encoder_layers)])

    def _forward(self, src_tokens, src_lengths, return_all_hiddens=False):
        """
        Args:
            src_tokens: Input source tokens Tensor of shape B X T X C
            src_lengths: Lengths Tensor corresponding to input source tokens
            return_all_hiddens: If true will append the self attention states to the encoder states
        Returns:
            encoder_out: Tensor of shape B X T X C
            encoder_padding_mask: Optional Tensor with mask
            encoder_embedding: Optional Tensor. Always empty here
            encoder_states: List of Optional Tensors wih self attention states
            src_tokens: Optional Tensor. Always empty here
            src_lengths: Optional Tensor. Always empty here
        """
        x, input_lengths = self.subsample(src_tokens, src_lengths)
        encoder_padding_mask = lengths_to_padding_mask(input_lengths)
        x = self.embed_scale * x
        if self.pos_enc_type == 'rel_pos':
            positions = self.embed_positions(x)
        elif self.pos_enc_type == 'rope':
            positions = None
        else:
            positions = self.embed_positions(encoder_padding_mask).transpose(0, 1)
            x += positions
            positions = None
        x = self.linear(x)
        x = self.dropout(x)
        encoder_states = []
        for layer in self.conformer_layers:
            x, _ = layer(x, encoder_padding_mask, positions)
            if return_all_hiddens:
                encoder_states.append(x)
        return {'encoder_out': [x], 'encoder_padding_mask': [encoder_padding_mask] if encoder_padding_mask.any() else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}

    def forward(self, src_tokens, src_lengths, return_all_hiddens=False):
        if self.num_updates < self.encoder_freezing_updates:
            with torch.no_grad():
                x = self._forward(src_tokens, src_lengths, return_all_hiddens=return_all_hiddens)
        else:
            x = self._forward(src_tokens, src_lengths, return_all_hiddens=return_all_hiddens)
        return x

    def reorder_encoder_out(self, encoder_out, new_order):
        """Required method for a FairseqEncoder. Calls the method from the parent class"""
        return S2TTransformerEncoder.reorder_encoder_out(self, encoder_out, new_order)

    def set_num_updates(self, num_updates):
        super().set_num_updates(num_updates)
        self.num_updates = num_updates


class SpeechWavTransformerEncoder(FairseqEncoder):

    @staticmethod
    def add_args(parser):
        parser.add_argument('--dropout-input', type=float, metavar='D', help='dropout to apply to the input (after feat extr)')
        parser.add_argument('--dropout-features', type=float, metavar='D', help='dropout to apply to the unmasked features (after feat extr)')
        parser.add_argument('--speech-extractor-mode', type=str, default='layer_norm', choices=['default', 'layer_norm'], help='feature extractor norm')
        parser.add_argument('--speech-conv-bias', action='store_true', help='include bias in speech conv encoder')
        parser.add_argument('--conv-feature-layers', default='[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', help='string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]')
        parser.add_argument('--speech-mask-length', type=int, help='repeat the mask indices multiple times')
        parser.add_argument('--speech-mask-prob', type=float, help='probability of replacing a token with mask')
        parser.add_argument('--speech-mask-selection', type=str, choices=['static', 'uniform', 'normal', 'poisson'], help='how to choose masks')
        parser.add_argument('--speech-mask-other', type=float, help="stdev of the mask length in case of 'normal' selection strategy")
        parser.add_argument('--speech-no-mask-overlap', action='store_true', help='whether to allow masks to overlap')
        parser.add_argument('--speech-mask-min-space', type=int, help='min space between spans (if no overlap is enabled)')
        parser.add_argument('--speech-mask-channel-length', type=int, help='repeat the mask indices multiple times')
        parser.add_argument('--speech-mask-channel-prob', type=float, help='probability of replacing a token with mask')
        parser.add_argument('--speech-mask-channel-selection', type=str, choices=['static', 'uniform', 'normal', 'poisson'], help='how to choose masks')
        parser.add_argument('--speech-mask-channel-other', type=float, help="stdev of the mask length in case of 'normal' selection strategy")
        parser.add_argument('--speech-no-mask-channel-overlap', action='store_true', help='whether to allow masks to overlap')
        parser.add_argument('--no-scale-feature', action='store_true', help='no scale for the calculated features')
        parser.add_argument('--speech-mask-channel-min-space', type=int, help='min space between spans (if no overlap is enabled)')
        parser.add_argument('--feature-grad-mult', type=float, help='reset feature grad mult in wav2vec 2.0 to this')
        parser.add_argument('--conv-pos', type=int, default=128, help='number of filters for convolutional positional embeddings')
        parser.add_argument('--conv-pos-groups', type=int, default=16, help='number of groups for convolutional positional embedding')
        parser.add_argument('--speech-encoder-layers', type=int, help='number of speech encoder layers')
        parser.add_argument('--text-encoder-layers', type=int, help='number of text encoder layers')

    def __init__(self, args, alway_mask=False):
        super().__init__(args)
        self.args = args
        self.dropout = args.dropout
        self.embedding_dim = args.encoder_embed_dim
        self.feat_scale = math.sqrt(args.encoder_embed_dim)
        if args.no_scale_feature:
            self.feat_scale = 1.0
        subsample = ConvFeatureExtractionModel(conv_layers=eval(args.conv_feature_layers), dropout=0.0, mode=args.speech_extractor_mode, conv_bias=args.speech_conv_bias)
        self.feature_enc_layers = eval(args.conv_feature_layers)
        self.subsample = subsample
        self.feat_proj = nn.Linear(self.feature_enc_layers[-1][0], self.embedding_dim) if self.feature_enc_layers[-1][0] != self.embedding_dim else None
        self.feat_layer_norm = LayerNorm(self.feature_enc_layers[-1][0])
        self.embed_positions = nn.Conv1d(self.embedding_dim, self.embedding_dim, kernel_size=args.conv_pos, padding=args.conv_pos // 2, groups=args.conv_pos_groups)
        std = math.sqrt(4 / (args.conv_pos * self.embedding_dim))
        nn.init.normal_(self.embed_positions.weight, mean=0, std=std)
        nn.init.constant_(self.embed_positions.bias, 0)
        self.embed_positions = nn.utils.weight_norm(self.embed_positions, name='weight', dim=2)
        self.embed_positions = nn.Sequential(self.embed_positions, SamePad(args.conv_pos), nn.GELU())
        self.mask_prob = args.speech_mask_prob
        self.mask_selection = args.speech_mask_selection
        self.mask_other = args.speech_mask_other
        self.mask_length = args.speech_mask_length
        self.no_mask_overlap = args.speech_no_mask_overlap
        self.mask_min_space = args.speech_mask_min_space
        self.mask_channel_prob = args.speech_mask_channel_prob
        self.mask_channel_selection = args.speech_mask_channel_selection
        self.mask_channel_other = args.speech_mask_channel_other
        self.mask_channel_length = args.speech_mask_channel_length
        self.no_mask_channel_overlap = args.speech_no_mask_channel_overlap
        self.mask_channel_min_space = args.speech_mask_channel_min_space
        self.dropout_input = nn.Dropout(args.dropout_input)
        self.dropout_features = nn.Dropout(args.dropout_features)
        self.feature_grad_mult = args.feature_grad_mult
        self.mask_emb = nn.Parameter(torch.FloatTensor(args.encoder_embed_dim).uniform_())
        self.layers = nn.ModuleList([TransformerEncoderLayer(args) for _ in range(args.encoder_layers)])
        self.layer_norm = LayerNorm(args.encoder_embed_dim)
        self.normalize_before = args.encoder_normalize_before
        self.alway_mask = alway_mask

    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):
        """
        Computes the output length of the convolutional layers
        """

        def _conv_out_length(input_length, kernel_size, stride):
            return torch.floor((input_length - kernel_size) / stride + 1)
        for i in range(len(self.feature_enc_layers)):
            input_lengths = _conv_out_length(input_lengths, self.feature_enc_layers[i][1], self.feature_enc_layers[i][2])
        return input_lengths

    def apply_mask(self, x, padding_mask):
        B, T, C = x.shape
        if self.mask_prob > 0:
            mask_indices = compute_mask_indices((B, T), padding_mask, self.mask_prob, self.mask_length, self.mask_selection, self.mask_other, min_masks=2, no_overlap=self.no_mask_overlap, min_space=self.mask_min_space)
            mask_indices = torch.from_numpy(mask_indices)
            x[mask_indices] = self.mask_emb
        else:
            mask_indices = None
        if self.mask_channel_prob > 0:
            mask_channel_indices = compute_mask_indices((B, C), None, self.mask_channel_prob, self.mask_channel_length, self.mask_channel_selection, self.mask_channel_other, no_overlap=self.no_mask_channel_overlap, min_space=self.mask_channel_min_space)
            mask_channel_indices = torch.from_numpy(mask_channel_indices).unsqueeze(1).expand(-1, T, -1)
            x[mask_channel_indices] = 0
        return x, mask_indices

    def forward(self, src_tokens, src_lengths, return_all_hiddens=False, padding_mask=None, features_only=True):
        mask = self.training or self.alway_mask
        if self.feature_grad_mult > 0 and self.training:
            features = self.subsample(src_tokens)
            if self.feature_grad_mult != 1.0:
                features = GradMultiply.apply(features, self.feature_grad_mult)
        else:
            with torch.no_grad():
                features = self.subsample(src_tokens)
        features = features.transpose(1, 2)
        features = self.feat_layer_norm(features)
        if self.feat_proj is not None:
            features = self.feat_proj(features)
        if padding_mask is not None:
            input_lengths = (1 - padding_mask.long()).sum(-1)
        else:
            input_lengths = src_lengths
        output_lengths = self._get_feat_extract_output_lengths(input_lengths)
        padding_mask = torch.zeros(features.shape[:2], dtype=features.dtype, device=features.device)
        padding_mask[torch.arange(padding_mask.shape[0], device=padding_mask.device), output_lengths - 1] = 1
        padding_mask = (1 - padding_mask.flip([-1]).cumsum(-1).flip([-1])).bool()
        features = self.feat_scale * features if self.feat_scale != 1.0 else features
        unmasked_features = features.clone()
        features = self.dropout_input(features)
        unmasked_features = self.dropout_features(unmasked_features)
        if mask:
            x, mask_indices = self.apply_mask(features, padding_mask)
        else:
            x = features
            mask_indices = None

        def cal_transformer_layers(x, encoder_padding_mask, return_all_hiddens=False):
            positions = self.embed_positions(x.transpose(1, 2)).transpose(1, 2)
            x = x + positions
            if not self.normalize_before:
                x = self.layer_norm(x)
            x = x.transpose(0, 1)
            encoder_states = []
            for layer in self.layers:
                x = layer(x, encoder_padding_mask)
                if return_all_hiddens:
                    encoder_states.append(x)
            if self.normalize_before:
                x = self.layer_norm(x)
            return x, encoder_states
        x, encoder_states = cal_transformer_layers(x, padding_mask, return_all_hiddens)
        if features_only:
            return {'encoder_out': [x], 'encoder_padding_mask': [padding_mask] if padding_mask is not None else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': [], 'mask_indices': [mask_indices]}
        x_unmasked = x
        if self.mask_prob > 0 or self.mask_channel_prob > 0:
            x_unmasked, _ = cal_transformer_layers(unmasked_features, padding_mask)
        return {'encoder_out': [x], 'encoder_unmasked_out': [x_unmasked], 'encoder_padding_mask': [padding_mask] if padding_mask is not None else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': [], 'mask_indices': [mask_indices] if mask_indices is not None else []}

    def reorder_encoder_out(self, encoder_out, new_order):
        new_encoder_out = [] if len(encoder_out['encoder_out']) == 0 else [x.index_select(1, new_order) for x in encoder_out['encoder_out']]
        new_encoder_padding_mask = [] if len(encoder_out['encoder_padding_mask']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_padding_mask']]
        new_encoder_embedding = [] if len(encoder_out['encoder_embedding']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_embedding']]
        encoder_states = encoder_out['encoder_states']
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)
        return {'encoder_out': new_encoder_out, 'encoder_padding_mask': new_encoder_padding_mask, 'encoder_embedding': new_encoder_embedding, 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}


class StackedSpeechWavTransformerEncoder(FairseqEncoder):

    def __init__(self, speech_enc, text_enc_layers, text_layer_norm):
        super().__init__(None)
        self.speech_encoder = speech_enc
        self.text_encoder_layers = text_enc_layers
        self.final_layer_norm = text_layer_norm

    def forward(self, src_tokens, src_lengths=None, return_all_hiddens=False, padding_mask=None, features_only=True):
        out = self.speech_encoder.forward(src_tokens, src_lengths, return_all_hiddens, padding_mask=padding_mask, features_only=features_only)
        x = out['encoder_out'][0]
        encoder_padding_mask = None
        if len(out['encoder_padding_mask']) > 0:
            encoder_padding_mask = out['encoder_padding_mask'][0]

        def cal_text_layers(x, padding_mask, return_all_hiddens=False):
            encoder_states = []
            for layer in self.text_encoder_layers:
                x = layer(x, padding_mask)
                if return_all_hiddens:
                    encoder_states.append(x)
            if self.final_layer_norm is not None:
                x = self.final_layer_norm(x)
            return x, encoder_states
        x, encoder_states = cal_text_layers(x, encoder_padding_mask, return_all_hiddens)
        if features_only:
            return {'encoder_out': [x], 'encoder_padding_mask': [encoder_padding_mask] if encoder_padding_mask is not None else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}
        x_u = out['encoder_unmasked_out'][0]
        x_u, _ = cal_text_layers(x_u, encoder_padding_mask)
        return {'encoder_out': [x], 'encoder_unmasked_out': [x_u], 'encoder_padding_mask': [encoder_padding_mask] if encoder_padding_mask is not None else [], 'encoder_embedding': [], 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': [], 'mask_indices': out['mask_indices']}

    def reorder_encoder_out(self, encoder_out, new_order):
        return self.speech_encoder.reorder_encoder_out(encoder_out, new_order)


class Permute(torch.nn.Module):

    def __init__(self, dims):
        super().__init__()
        self.dims = dims

    def forward(self, input: Tensor) ->Tensor:
        return input.permute(self.dims).contiguous()


class Conv1dAdaptor(nn.Module):

    def __init__(self, in_dim, out_dim, n_layers=3, kernel_size=3, stride=2, layerdrop=0.0, layernorm=False, proj=False):
        super().__init__()
        self.proj, self.proj_ln = None, None
        self.post_proj, self.post_proj_ln = None, None
        if proj:
            self.proj = nn.Sequential(nn.Linear(in_dim, in_dim * 4), nn.ReLU(), nn.Linear(in_dim * 4, in_dim))
            self.proj_ln = LayerNorm(in_dim)
            self.post_proj = nn.Sequential(nn.Linear(out_dim, out_dim * 4), nn.ReLU(), nn.Linear(out_dim * 4, out_dim))
            self.post_proj_ln = LayerNorm(out_dim)
        self.layers = nn.ModuleList(nn.Conv1d(in_dim if i == 0 else out_dim, out_dim * 2, kernel_size, stride=stride, padding=kernel_size // 2) for i in range(n_layers))
        self.stride = stride
        self.layerdrop = layerdrop
        self.layernorm = LayerNorm(in_dim) if layernorm else None

    @classmethod
    def add_args(cls, parser):
        parser.add_argument('--adaptor-n-layers', type=int)
        parser.add_argument('--adaptor-kernel-size', type=int)
        parser.add_argument('--adaptor-stride', type=int)
        parser.add_argument('--adaptor-layerdrop', type=float)
        parser.add_argument('--adaptor-layernorm', action='store_true')
        parser.add_argument('--adaptor-proj', action='store_true')

    def forward(self, x, padding_mask: Optional[torch.Tensor]):
        if self.layernorm is not None:
            x = self.layernorm(x)
        if self.proj is not None:
            x = x + 0.5 * self.proj(x)
            x = self.proj_ln(x)
        if padding_mask is not None:
            x = utils.index_put(x, padding_mask.T, 0)
        x = x.transpose(0, 1).transpose(1, 2)
        out_lens = None
        if padding_mask is not None:
            out_lens = (~padding_mask).sum(1).float()
        for layer in self.layers:
            layerdrop_prob = np.random.random()
            if not self.training or layerdrop_prob > self.layerdrop:
                x = nn.functional.glu(layer(x), dim=1)
                if padding_mask is not None:
                    out_lens = ((out_lens - 1) / self.stride + 1).floor()
        x = x.transpose(1, 2).transpose(0, 1)
        if self.post_proj is not None:
            x = x + 0.5 * self.post_proj(x)
            x = self.post_proj_ln(x)
        out_padding_mask = None
        if padding_mask is not None:
            out_padding_mask = lengths_to_padding_mask(out_lens.long())
            x = utils.index_put(x, out_padding_mask.T, 0)
        return x, out_padding_mask


def add_wav2vec_asr_args(parser):
    parser.add_argument('--w2v-path', help='path to wav2vec 2.0 model')
    parser.add_argument('--no-pretrained-weights', action='store_true', help='if true, does not load pretrained weights')
    parser.add_argument('--dropout-input', type=float, metavar='D', help='dropout to apply to the input (after feat extr)')
    parser.add_argument('--final-dropout', type=float, metavar='D', help='dropout after transformer and before final projection')
    parser.add_argument('--apply-mask', action='store_true', help='apply masking during fine-tuning')
    parser.add_argument('--dropout', type=float, metavar='D', help='dropout probability inside wav2vec 2.0 model')
    parser.add_argument('--attention-dropout', type=float, metavar='D', help='dropout probability for attention weights inside wav2vec 2.0 model')
    parser.add_argument('--activation-dropout', '--relu-dropout', type=float, metavar='D', help='dropout probability after activation in FFN inside wav2vec 2.0 model')
    parser.add_argument('--mask-length', type=int, help='repeat the mask indices multiple times')
    parser.add_argument('--mask-prob', type=float, help='probability of replacing a token with mask')
    parser.add_argument('--mask-selection', type=str, choices=['static', 'uniform', 'normal', 'poisson'], help='how to choose masks')
    parser.add_argument('--mask-other', type=float, help="stdev of the mask length in case of 'normal' selection strategy")
    parser.add_argument('--no-mask-overlap', action='store_true', help='whether to allow masks to overlap')
    parser.add_argument('--mask-channel-length', type=int, help='repeat the mask indices multiple times')
    parser.add_argument('--mask-channel-prob', type=float, help='probability of replacing a token with mask')
    parser.add_argument('--mask-channel-selection', type=str, choices=['static', 'uniform', 'normal', 'poisson'], help='how to choose masks')
    parser.add_argument('--mask-channel-other', type=float, help="stdev of the mask length in case of 'normal' selection strategy")
    parser.add_argument('--no-mask-channel-overlap', action='store_true', help='whether to allow masks to overlap')
    parser.add_argument('--freeze-finetune-updates', type=int, metavar='N', help='dont finetune wav2vec for this many updates')
    parser.add_argument('--feature-grad-mult', type=float, metavar='D', help='reset feature grad mult in wav2vec 2.0 to this')
    parser.add_argument('--layerdrop', type=float, metavar='D', help='probability of dropping a layer in wav2vec 2.0')
    parser.add_argument('--max-positions', type=int, metavar='N', help='Max input positions to be used in the conformer encoder in wav2vec 2.0')
    parser.add_argument('--encoder-proj', action='store_true')
    parser.add_argument('--w2v-args', default=None)
    parser.add_argument('--remove-weight-norm', action='store_true', help='if set, then the weight-norm (in one pos_conv layer) is removed from the model')
    parser.add_argument('--encoder-embed-dim', type=int, metavar='N', help='encoder embedding dimension to be used when w2v_path is None and no encoder_proj is set')


def need_finetuning(ft_params, param_name):
    if ft_params == 'all':
        return True
    ft_params_list = ft_params.split(',')
    for ft_param in ft_params_list:
        if ft_param in param_name:
            return True
    return False


class Wav2VecEncoderWithAdaptor(FairseqEncoder):

    def build_adaptor(self, args):
        adaptor = None
        if args.adaptor_n_layers > 0:
            adaptor = Conv1dAdaptor(args.decoder_embed_dim, args.decoder_embed_dim, n_layers=args.adaptor_n_layers, kernel_size=args.adaptor_kernel_size, stride=args.adaptor_stride, layerdrop=args.adaptor_layerdrop, layernorm=args.adaptor_layernorm, proj=args.adaptor_proj)
        return adaptor

    def __init__(self, args):
        super().__init__(None)
        self.w2v_encoder = Wav2VecEncoder(args)
        self.is_v0_arch = not args.adaptor_proj
        self.w2v_proj_ln = None
        if not self.is_v0_arch and self.w2v_encoder.proj is not None:
            self.w2v_proj_ln = LayerNorm(args.decoder_embed_dim)
        self.adaptor = self.build_adaptor(args)
        self.num_updates = 0
        self.freezing_updates = args.w2v_freezing_updates
        self.finetuning_params = args.finetune_w2v_params
        for k, p in self.w2v_encoder.w2v_model.named_parameters():
            p.requires_grad = need_finetuning(self.finetuning_params, k)

    @classmethod
    def add_args(cls, parser):
        """Add model-specific arguments to the parser."""
        add_wav2vec_asr_args(parser)
        parser.add_argument('--normalize', action='store_true', help='if set, normalizes input to have 0 mean and unit variance')
        parser.add_argument('--finetune-w2v-params', type=str, metavar='STR', help='comma-separated param strings to finetune.')
        parser.add_argument('--w2v-freezing-updates', type=int)
        parser.add_argument('--load-pretrained-encoder-from', type=str, metavar='STR')
        Conv1dAdaptor.add_args(parser)

    def set_num_updates(self, num_updates):
        super().set_num_updates(num_updates)
        self.num_updates = num_updates

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        if self.freezing_updates is not None and self.num_updates > self.freezing_updates:
            for p in self.w2v_encoder.w2v_model.parameters():
                p.requires_grad = True
        padding_mask = lengths_to_padding_mask(src_lengths)
        out = self.w2v_encoder.forward(src_tokens, padding_mask, tbc=True)
        x, padding_mask = out['encoder_out'], out['padding_mask']
        if self.w2v_proj_ln is not None:
            x = self.w2v_proj_ln(x)
        if self.adaptor is not None:
            x, padding_mask = self.adaptor(x, padding_mask)
        return {'encoder_out': [x], 'encoder_padding_mask': [] if padding_mask is None else [padding_mask], 'encoder_embedding': [], 'encoder_states': [], 'src_tokens': [], 'src_lengths': []}

    def reorder_encoder_out(self, encoder_out, new_order):
        new_encoder_out = [] if len(encoder_out['encoder_out']) == 0 else [x.index_select(1, new_order) for x in encoder_out['encoder_out']]
        new_encoder_padding_mask = [] if len(encoder_out['encoder_padding_mask']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_padding_mask']]
        new_encoder_embedding = [] if len(encoder_out['encoder_embedding']) == 0 else [x.index_select(0, new_order) for x in encoder_out['encoder_embedding']]
        encoder_states = encoder_out['encoder_states']
        if len(encoder_states) > 0:
            for idx, state in enumerate(encoder_states):
                encoder_states[idx] = state.index_select(1, new_order)
        return {'encoder_out': new_encoder_out, 'encoder_padding_mask': new_encoder_padding_mask, 'encoder_embedding': new_encoder_embedding, 'encoder_states': encoder_states, 'src_tokens': [], 'src_lengths': []}


def add_decoder_args(parser):
    parser.add_argument('--activation-fn', type=str, default='relu', choices=utils.get_available_activation_fns(), help='activation function to use')
    parser.add_argument('--decoder-dropout', type=float, metavar='D', help='dropout probability')
    parser.add_argument('--decoder-attention-dropout', type=float, metavar='D', help='dropout probability for attention weights')
    parser.add_argument('--decoder-activation-dropout', type=float, metavar='D', help='dropout probability after activation in FFN.')
    parser.add_argument('--decoder-embed-dim', type=int, metavar='N', help='decoder embedding dimension')
    parser.add_argument('--decoder-ffn-embed-dim', type=int, metavar='N', help='decoder embedding dimension for FFN')
    parser.add_argument('--decoder-layers', type=int, metavar='N', help='num decoder layers')
    parser.add_argument('--decoder-attention-heads', type=int, metavar='N', help='num decoder attention heads')
    parser.add_argument('--decoder-normalize-before', action='store_true', help='apply layernorm before each decoder block')
    parser.add_argument('--layernorm-embedding', action='store_true', help='add layernorm to embedding')
    parser.add_argument('--decoder-layerdrop', type=float, metavar='D', help='layerdrop probability for decoder')
    parser.add_argument('--decoder-learned-pos', action='store_true', help='learn positional embedding in decoder')
    parser.add_argument('--share-decoder-input-output-embed', action='store_true', help='share decoder input and output embeddings')
    parser.add_argument('--no-scale-embedding', action='store_true', help='if True, dont scale embeddings')
    parser.add_argument('--load-pretrained-decoder-from', type=str, metavar='STR', help='model to take decoder weights from (for initialization)')
    parser.add_argument('--finetune-decoder-params', type=str, metavar='STR', help='comma-separated param strings to finetune.')


def build_embedding(dictionary, embed_dim):
    num_embeddings = len(dictionary)
    padding_idx = dictionary.pad()
    return Embedding(num_embeddings, embed_dim, padding_idx)


def remove_weight_norm_from_model(model):
    from functools import reduce
    layers_with_wn = []
    for param_name, _ in model.named_parameters():
        if param_name.endswith('_g'):
            module_names = param_name.split('.')[:-1]
            wn_module = reduce(getattr, module_names, model)
            layers_with_wn.append(wn_module)
    for wn_module in layers_with_wn:
        torch.nn.utils.remove_weight_norm(wn_module)
        logger.warning(f'Weight norm removed from module with {wn_module}\n')


class VariancePredictor(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.conv1 = nn.Sequential(nn.Conv1d(args.encoder_embed_dim, args.var_pred_hidden_dim, kernel_size=args.var_pred_kernel_size, padding=(args.var_pred_kernel_size - 1) // 2), nn.ReLU())
        self.ln1 = nn.LayerNorm(args.var_pred_hidden_dim)
        self.dropout_module = FairseqDropout(p=args.var_pred_dropout, module_name=self.__class__.__name__)
        self.conv2 = nn.Sequential(nn.Conv1d(args.var_pred_hidden_dim, args.var_pred_hidden_dim, kernel_size=args.var_pred_kernel_size, padding=1), nn.ReLU())
        self.ln2 = nn.LayerNorm(args.var_pred_hidden_dim)
        self.proj = nn.Linear(args.var_pred_hidden_dim, 1)

    def forward(self, x):
        x = self.conv1(x.transpose(1, 2)).transpose(1, 2)
        x = self.dropout_module(self.ln1(x))
        x = self.conv2(x.transpose(1, 2)).transpose(1, 2)
        x = self.dropout_module(self.ln2(x))
        return self.proj(x).squeeze(dim=2)


class CodeGenerator(Generator):

    def __init__(self, cfg):
        super().__init__(cfg)
        self.dict = nn.Embedding(cfg['num_embeddings'], cfg['embedding_dim'])
        self.multispkr = cfg.get('multispkr', None)
        self.embedder = cfg.get('embedder_params', None)
        if self.multispkr and not self.embedder:
            self.spkr = nn.Embedding(cfg.get('num_speakers', 200), cfg['embedding_dim'])
        elif self.embedder:
            self.spkr = nn.Linear(cfg.get('embedder_dim', 256), cfg['embedding_dim'])
        self.dur_predictor = None
        if cfg.get('dur_predictor_params', None):
            self.dur_predictor = VariancePredictor(Namespace(**cfg['dur_predictor_params']))
        self.f0 = cfg.get('f0', None)
        n_f0_bin = cfg.get('f0_quant_num_bin', 0)
        self.f0_quant_embed = None if n_f0_bin <= 0 else nn.Embedding(n_f0_bin, cfg['embedding_dim'])

    @staticmethod
    def _upsample(signal, max_frames):
        if signal.dim() == 3:
            bsz, channels, cond_length = signal.size()
        elif signal.dim() == 2:
            signal = signal.unsqueeze(2)
            bsz, channels, cond_length = signal.size()
        else:
            signal = signal.view(-1, 1, 1)
            bsz, channels, cond_length = signal.size()
        signal = signal.unsqueeze(3).repeat(1, 1, 1, max_frames // cond_length)
        reminder = (max_frames - signal.shape[2] * signal.shape[3]) // signal.shape[3]
        if reminder > 0:
            raise NotImplementedError('Padding condition signal - misalignment between condition features.')
        signal = signal.view(bsz, channels, max_frames)
        return signal

    def forward(self, **kwargs):
        x = self.dict(kwargs['code']).transpose(1, 2)
        if self.dur_predictor and kwargs.get('dur_prediction', False):
            assert x.size(0) == 1, 'only support single sample'
            log_dur_pred = self.dur_predictor(x.transpose(1, 2))
            dur_out = torch.clamp(torch.round(torch.exp(log_dur_pred) - 1).long(), min=1)
            x = torch.repeat_interleave(x, dur_out.view(-1), dim=2)
        if self.f0:
            if self.f0_quant_embed:
                kwargs['f0'] = self.f0_quant_embed(kwargs['f0'].long()).transpose(1, 2)
            else:
                kwargs['f0'] = kwargs['f0'].unsqueeze(1)
            if x.shape[-1] < kwargs['f0'].shape[-1]:
                x = self._upsample(x, kwargs['f0'].shape[-1])
            elif x.shape[-1] > kwargs['f0'].shape[-1]:
                kwargs['f0'] = self._upsample(kwargs['f0'], x.shape[-1])
            x = torch.cat([x, kwargs['f0']], dim=1)
        if self.multispkr:
            assert 'spkr' in kwargs, 'require "spkr" input for multispeaker CodeHiFiGAN vocoder'
            spkr = self.spkr(kwargs['spkr']).transpose(1, 2)
            spkr = self._upsample(spkr, x.shape[-1])
            x = torch.cat([x, spkr], dim=1)
        for k, feat in kwargs.items():
            if k in ['spkr', 'code', 'f0', 'dur_prediction']:
                continue
            feat = self._upsample(feat, x.shape[-1])
            x = torch.cat([x, feat], dim=1)
        return super().forward(x)


class PositionwiseFeedForward(nn.Module):

    def __init__(self, in_dim, hidden_dim, kernel_size, dropout):
        super().__init__()
        self.ffn = nn.Sequential(nn.Conv1d(in_dim, hidden_dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2), nn.ReLU(), nn.Conv1d(hidden_dim, in_dim, kernel_size=kernel_size, padding=(kernel_size - 1) // 2))
        self.layer_norm = LayerNorm(in_dim)
        self.dropout = self.dropout_module = FairseqDropout(p=dropout, module_name=self.__class__.__name__)

    def forward(self, x):
        residual = x
        x = self.ffn(x.transpose(1, 2)).transpose(1, 2)
        x = self.dropout(x)
        return self.layer_norm(x + residual)


class FFTLayer(torch.nn.Module):

    def __init__(self, embed_dim, n_heads, hidden_dim, kernel_size, dropout, attention_dropout):
        super().__init__()
        self.self_attn = MultiheadAttention(embed_dim, n_heads, dropout=attention_dropout, self_attention=True)
        self.layer_norm = LayerNorm(embed_dim)
        self.ffn = PositionwiseFeedForward(embed_dim, hidden_dim, kernel_size, dropout=dropout)

    def forward(self, x, padding_mask=None):
        residual = x
        x = x.transpose(0, 1)
        x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=padding_mask, need_weights=False)
        x = x.transpose(0, 1)
        x = self.layer_norm(x + residual)
        return self.ffn(x)


class LengthRegulator(nn.Module):

    def forward(self, x, durations):
        out_lens = durations.sum(dim=1)
        max_len = out_lens.max()
        bsz, seq_len, dim = x.size()
        out = x.new_zeros((bsz, max_len, dim))
        for b in range(bsz):
            indices = []
            for t in range(seq_len):
                indices.extend([t] * utils.item(durations[b, t]))
            indices = torch.tensor(indices, dtype=torch.long)
            out_len = utils.item(out_lens[b])
            out[b, :out_len] = x[b].index_select(0, indices)
        return out, out_lens


class VarianceAdaptor(nn.Module):

    def __init__(self, args):
        super().__init__()
        self.args = args
        self.length_regulator = LengthRegulator()
        self.duration_predictor = VariancePredictor(args)
        self.pitch_predictor = VariancePredictor(args)
        self.energy_predictor = VariancePredictor(args)
        n_bins, steps = self.args.var_pred_n_bins, self.args.var_pred_n_bins - 1
        self.pitch_bins = torch.linspace(args.pitch_min, args.pitch_max, steps)
        self.embed_pitch = Embedding(n_bins, args.encoder_embed_dim)
        self.energy_bins = torch.linspace(args.energy_min, args.energy_max, steps)
        self.embed_energy = Embedding(n_bins, args.encoder_embed_dim)

    def get_pitch_emb(self, x, tgt=None, factor=1.0):
        out = self.pitch_predictor(x)
        bins = self.pitch_bins
        if tgt is None:
            out = out * factor
            emb = self.embed_pitch(torch.bucketize(out, bins))
        else:
            emb = self.embed_pitch(torch.bucketize(tgt, bins))
        return out, emb

    def get_energy_emb(self, x, tgt=None, factor=1.0):
        out = self.energy_predictor(x)
        bins = self.energy_bins
        if tgt is None:
            out = out * factor
            emb = self.embed_energy(torch.bucketize(out, bins))
        else:
            emb = self.embed_energy(torch.bucketize(tgt, bins))
        return out, emb

    def forward(self, x, padding_mask, durations=None, pitches=None, energies=None, d_factor=1.0, p_factor=1.0, e_factor=1.0):
        log_dur_out = self.duration_predictor(x)
        dur_out = torch.clamp(torch.round((torch.exp(log_dur_out) - 1) * d_factor).long(), min=0)
        dur_out.masked_fill_(padding_mask, 0)
        pitch_out, pitch_emb = self.get_pitch_emb(x, pitches, p_factor)
        x = x + pitch_emb
        energy_out, energy_emb = self.get_energy_emb(x, energies, e_factor)
        x = x + energy_emb
        x, out_lens = self.length_regulator(x, dur_out if durations is None else durations)
        return x, out_lens, log_dur_out, pitch_out, energy_out


def model_init(m):
    if isinstance(m, nn.Conv1d):
        nn.init.xavier_uniform_(m.weight, torch.nn.init.calculate_gain('relu'))


class FastSpeech2Encoder(FairseqEncoder):

    def __init__(self, args, src_dict, embed_speaker):
        super().__init__(src_dict)
        self.args = args
        self.padding_idx = src_dict.pad()
        self.n_frames_per_step = args.n_frames_per_step
        self.out_dim = args.output_frame_dim * args.n_frames_per_step
        self.embed_speaker = embed_speaker
        self.spk_emb_proj = None
        if embed_speaker is not None:
            self.spk_emb_proj = nn.Linear(args.encoder_embed_dim + args.speaker_embed_dim, args.encoder_embed_dim)
        self.dropout_module = FairseqDropout(p=args.dropout, module_name=self.__class__.__name__)
        self.embed_tokens = Embedding(len(src_dict), args.encoder_embed_dim, padding_idx=self.padding_idx)
        self.embed_positions = PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim, self.padding_idx)
        self.pos_emb_alpha = nn.Parameter(torch.ones(1))
        self.dec_pos_emb_alpha = nn.Parameter(torch.ones(1))
        self.encoder_fft_layers = nn.ModuleList(FFTLayer(args.encoder_embed_dim, args.encoder_attention_heads, args.fft_hidden_dim, args.fft_kernel_size, dropout=args.dropout, attention_dropout=args.attention_dropout) for _ in range(args.encoder_layers))
        self.var_adaptor = VarianceAdaptor(args)
        self.decoder_fft_layers = nn.ModuleList(FFTLayer(args.decoder_embed_dim, args.decoder_attention_heads, args.fft_hidden_dim, args.fft_kernel_size, dropout=args.dropout, attention_dropout=args.attention_dropout) for _ in range(args.decoder_layers))
        self.out_proj = nn.Linear(args.decoder_embed_dim, self.out_dim)
        self.postnet = None
        if args.add_postnet:
            self.postnet = Postnet(self.out_dim, args.postnet_conv_dim, args.postnet_conv_kernel_size, args.postnet_layers, args.postnet_dropout)
        self.apply(model_init)

    def forward(self, src_tokens, src_lengths=None, speaker=None, durations=None, pitches=None, energies=None, **kwargs):
        x = self.embed_tokens(src_tokens)
        enc_padding_mask = src_tokens.eq(self.padding_idx)
        x += self.pos_emb_alpha * self.embed_positions(enc_padding_mask)
        x = self.dropout_module(x)
        for layer in self.encoder_fft_layers:
            x = layer(x, enc_padding_mask)
        if self.embed_speaker is not None:
            bsz, seq_len, _ = x.size()
            emb = self.embed_speaker(speaker).expand(bsz, seq_len, -1)
            x = self.spk_emb_proj(torch.cat([x, emb], dim=2))
        x, out_lens, log_dur_out, pitch_out, energy_out = self.var_adaptor(x, enc_padding_mask, durations, pitches, energies)
        dec_padding_mask = lengths_to_padding_mask(out_lens)
        x += self.dec_pos_emb_alpha * self.embed_positions(dec_padding_mask)
        for layer in self.decoder_fft_layers:
            x = layer(x, dec_padding_mask)
        x = self.out_proj(x)
        x_post = None
        if self.postnet is not None:
            x_post = x + self.postnet(x)
        return x, x_post, out_lens, log_dur_out, pitch_out, energy_out


class TTSHubInterface(nn.Module):

    def __init__(self, cfg, task, model):
        super().__init__()
        self.cfg = cfg
        self.task = task
        self.model = model
        self.model.eval()
        self.update_cfg_with_data_cfg(self.cfg, self.task.data_cfg)
        self.generator = self.task.build_generator([self.model], self.cfg)

    @classmethod
    def phonemize(cls, text: str, lang: Optional[str], phonemizer: Optional[str]=None, preserve_punct: bool=False, to_simplified_zh: bool=False):
        if to_simplified_zh:
            text = hanziconv.HanziConv.toSimplified(text)
        if phonemizer == 'g2p':
            g2p = g2p_en.G2p()
            if preserve_punct:
                return ' '.join('|' if p == ' ' else p for p in g2p(text))
            else:
                res = [{',': 'sp', ';': 'sp'}.get(p, p) for p in g2p(text)]
                return ' '.join(p for p in res if p.isalnum())
        if phonemizer == 'g2pc':
            g2p = g2pc.G2pC()
            return ' '.join([w[3] for w in g2p(text)])
        elif phonemizer == 'ipa':
            assert lang is not None
            lang_map = {'en': 'en-us', 'fr': 'fr-fr'}
            return phonemizer.phonemize(text, backend='espeak', language=lang_map.get(lang, lang), separator=Separator(word='| ', phone=' '))
        else:
            return text

    @classmethod
    def tokenize(cls, text: str, tkn_cfg: Dict[str, str]):
        sentencepiece_model = tkn_cfg.get('sentencepiece_model', None)
        if sentencepiece_model is not None:
            assert Path(sentencepiece_model).exists()
            spm = sp.SentencePieceProcessor()
            spm.Load(sentencepiece_model)
            return ' '.join(spm.Encode(text, out_type=str))
        else:
            return text

    @classmethod
    def update_cfg_with_data_cfg(cls, cfg, data_cfg):
        cfg['task'].vocoder = data_cfg.vocoder.get('type', 'griffin_lim')

    @classmethod
    def get_model_input(cls, task, text: str, speaker: Optional[int]=None, verbose: bool=False):
        phonemized = cls.phonemize(text, task.data_cfg.hub.get('lang', None), task.data_cfg.hub.get('phonemizer', None), task.data_cfg.hub.get('preserve_punct', False), task.data_cfg.hub.get('to_simplified_zh', False))
        tkn_cfg = task.data_cfg.bpe_tokenizer
        tokenized = cls.tokenize(phonemized, tkn_cfg)
        if verbose:
            logger.info(f'text: {text}')
            logger.info(f'phonemized: {phonemized}')
            logger.info(f'tokenized: {tokenized}')
        spk = task.data_cfg.hub.get('speaker', speaker)
        n_speakers = len(task.speaker_to_id or {})
        if spk is None and n_speakers > 0:
            spk = random.randint(0, n_speakers - 1)
        if spk is not None:
            spk = max(0, min(spk, n_speakers - 1))
        if verbose:
            logger.info(f'speaker: {spk}')
        spk = None if spk is None else torch.Tensor([[spk]]).long()
        src_tokens = task.src_dict.encode_line(tokenized, add_if_not_exist=False).view(1, -1)
        src_lengths = torch.Tensor([len(tokenized.split())]).long()
        return {'net_input': {'src_tokens': src_tokens, 'src_lengths': src_lengths, 'prev_output_tokens': None}, 'target_lengths': None, 'speaker': spk}

    @classmethod
    def get_prediction(cls, task, model, generator, sample) ->Tuple[torch.Tensor, int]:
        prediction = generator.generate(model, sample)
        return prediction[0]['waveform'], task.sr

    def predict(self, text: str, speaker: Optional[int]=None, verbose: bool=False) ->Tuple[torch.Tensor, int]:
        sample = self.get_model_input(self.task, text, speaker, verbose=verbose)
        return self.get_prediction(self.task, self.model, self.generator, sample)


def encoder_init(m):
    if isinstance(m, nn.Conv1d):
        nn.init.xavier_uniform_(m.weight, torch.nn.init.calculate_gain('relu'))


class Tacotron2Encoder(FairseqEncoder):

    def __init__(self, args, src_dict, embed_speaker):
        super().__init__(src_dict)
        self.padding_idx = src_dict.pad()
        self.embed_speaker = embed_speaker
        self.spk_emb_proj = None
        if embed_speaker is not None:
            self.spk_emb_proj = nn.Linear(args.encoder_embed_dim + args.speaker_embed_dim, args.encoder_embed_dim)
        self.embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, padding_idx=self.padding_idx)
        assert args.encoder_conv_kernel_size % 2 == 1
        self.convolutions = nn.ModuleList(nn.Sequential(nn.Conv1d(args.encoder_embed_dim, args.encoder_embed_dim, kernel_size=args.encoder_conv_kernel_size, padding=(args.encoder_conv_kernel_size - 1) // 2), nn.BatchNorm1d(args.encoder_embed_dim), nn.ReLU(), nn.Dropout(args.encoder_dropout)) for _ in range(args.encoder_conv_layers))
        self.lstm = nn.LSTM(args.encoder_embed_dim, args.encoder_embed_dim // 2, num_layers=args.encoder_lstm_layers, batch_first=True, bidirectional=True)
        self.apply(encoder_init)

    def forward(self, src_tokens, src_lengths=None, speaker=None, **kwargs):
        x = self.embed_tokens(src_tokens)
        x = x.transpose(1, 2).contiguous()
        for conv in self.convolutions:
            x = conv(x)
        x = x.transpose(1, 2).contiguous()
        src_lengths = src_lengths.cpu().long()
        x = nn.utils.rnn.pack_padded_sequence(x, src_lengths, batch_first=True)
        x = self.lstm(x)[0]
        x = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)[0]
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        if self.embed_speaker is not None:
            seq_len, bsz, _ = x.size()
            emb = self.embed_speaker(speaker).expand(seq_len, bsz, -1)
            x = self.spk_emb_proj(torch.cat([x, emb], dim=2))
        return {'encoder_out': [x], 'encoder_padding_mask': encoder_padding_mask}


class LSTMCellWithZoneOut(nn.Module):
    """
    Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations
    https://arxiv.org/abs/1606.01305
    """

    def __init__(self, prob: float, input_size: int, hidden_size: int, bias: bool=True):
        super(LSTMCellWithZoneOut, self).__init__()
        self.lstm_cell = nn.LSTMCell(input_size, hidden_size, bias=bias)
        self.prob = prob
        if prob > 1.0 or prob < 0.0:
            raise ValueError('zoneout probability must be in the range from 0.0 to 1.0.')

    def zoneout(self, h, next_h, prob):
        if isinstance(h, tuple):
            return tuple([self.zoneout(h[i], next_h[i], prob) for i in range(len(h))])
        if self.training:
            mask = h.new_zeros(*h.size()).bernoulli_(prob)
            return mask * h + (1 - mask) * next_h
        return prob * h + (1 - prob) * next_h

    def forward(self, x, h):
        return self.zoneout(h, self.lstm_cell(x, h), self.prob)


class LocationAttention(nn.Module):
    """
    Attention-Based Models for Speech Recognition
    https://arxiv.org/pdf/1506.07503.pdf

    :param int encoder_dim: # projection-units of encoder
    :param int decoder_dim: # units of decoder
    :param int attn_dim: attention dimension
    :param int conv_dim: # channels of attention convolution
    :param int conv_kernel_size: filter size of attention convolution
    """

    def __init__(self, attn_dim, encoder_dim, decoder_dim, attn_state_kernel_size, conv_dim, conv_kernel_size, scaling=2.0):
        super(LocationAttention, self).__init__()
        self.attn_dim = attn_dim
        self.decoder_dim = decoder_dim
        self.scaling = scaling
        self.proj_enc = nn.Linear(encoder_dim, attn_dim)
        self.proj_dec = nn.Linear(decoder_dim, attn_dim, bias=False)
        self.proj_attn = nn.Linear(conv_dim, attn_dim, bias=False)
        self.conv = nn.Conv1d(attn_state_kernel_size, conv_dim, 2 * conv_kernel_size + 1, padding=conv_kernel_size, bias=False)
        self.proj_out = nn.Sequential(nn.Tanh(), nn.Linear(attn_dim, 1))
        self.proj_enc_out = None

    def clear_cache(self):
        self.proj_enc_out = None

    def forward(self, encoder_out, encoder_padding_mask, decoder_h, attn_state):
        """
        :param torch.Tensor encoder_out: padded encoder hidden state B x T x D
        :param torch.Tensor encoder_padding_mask: encoder padding mask
        :param torch.Tensor decoder_h: decoder hidden state B x D
        :param torch.Tensor attn_prev: previous attention weight B x K x T
        :return: attention weighted encoder state (B, D)
        :rtype: torch.Tensor
        :return: previous attention weights (B x T)
        :rtype: torch.Tensor
        """
        bsz, seq_len, _ = encoder_out.size()
        if self.proj_enc_out is None:
            self.proj_enc_out = self.proj_enc(encoder_out)
        attn = self.conv(attn_state)
        attn = self.proj_attn(attn.transpose(1, 2))
        if decoder_h is None:
            decoder_h = encoder_out.new_zeros(bsz, self.decoder_dim)
        dec_h = self.proj_dec(decoder_h).view(bsz, 1, self.attn_dim)
        out = self.proj_out(attn + self.proj_enc_out + dec_h).squeeze(2)
        out.masked_fill_(encoder_padding_mask, -float('inf'))
        w = F.softmax(self.scaling * out, dim=1)
        c = torch.sum(encoder_out * w.view(bsz, seq_len, 1), dim=1)
        return c, w


class Tacotron2Decoder(FairseqIncrementalDecoder):

    def __init__(self, args, src_dict):
        super().__init__(None)
        self.args = args
        self.n_frames_per_step = args.n_frames_per_step
        self.out_dim = args.output_frame_dim * args.n_frames_per_step
        self.prenet = Prenet(self.out_dim, args.prenet_layers, args.prenet_dim, args.prenet_dropout)
        self.attention_lstm = LSTMCellWithZoneOut(args.zoneout, args.prenet_dim + args.encoder_embed_dim, args.decoder_lstm_dim)
        self.attention = LocationAttention(args.attention_dim, args.encoder_embed_dim, args.decoder_lstm_dim, 1 + int(args.attention_use_cumprob), args.attention_conv_dim, args.attention_conv_kernel_size)
        self.lstm = nn.ModuleList(LSTMCellWithZoneOut(args.zoneout, args.encoder_embed_dim + args.decoder_lstm_dim, args.decoder_lstm_dim) for i in range(args.decoder_lstm_layers))
        proj_in_dim = args.encoder_embed_dim + args.decoder_lstm_dim
        self.feat_proj = nn.Linear(proj_in_dim, self.out_dim)
        self.eos_proj = nn.Linear(proj_in_dim, 1)
        self.postnet = Postnet(self.out_dim, args.postnet_conv_dim, args.postnet_conv_kernel_size, args.postnet_layers, args.postnet_dropout)
        self.ctc_proj = None
        if getattr(args, 'ctc_weight', 0.0) > 0.0:
            self.ctc_proj = nn.Linear(self.out_dim, len(src_dict))
        self.apply(decoder_init)

    def _get_states(self, incremental_state, enc_out):
        bsz, in_len, _ = enc_out.size()
        alstm_h = self.get_incremental_state(incremental_state, 'alstm_h')
        if alstm_h is None:
            alstm_h = enc_out.new_zeros(bsz, self.args.decoder_lstm_dim)
        alstm_c = self.get_incremental_state(incremental_state, 'alstm_c')
        if alstm_c is None:
            alstm_c = enc_out.new_zeros(bsz, self.args.decoder_lstm_dim)
        lstm_h = self.get_incremental_state(incremental_state, 'lstm_h')
        if lstm_h is None:
            lstm_h = [enc_out.new_zeros(bsz, self.args.decoder_lstm_dim) for _ in range(self.args.decoder_lstm_layers)]
        lstm_c = self.get_incremental_state(incremental_state, 'lstm_c')
        if lstm_c is None:
            lstm_c = [enc_out.new_zeros(bsz, self.args.decoder_lstm_dim) for _ in range(self.args.decoder_lstm_layers)]
        attn_w = self.get_incremental_state(incremental_state, 'attn_w')
        if attn_w is None:
            attn_w = enc_out.new_zeros(bsz, in_len)
        attn_w_cum = self.get_incremental_state(incremental_state, 'attn_w_cum')
        if attn_w_cum is None:
            attn_w_cum = enc_out.new_zeros(bsz, in_len)
        return alstm_h, alstm_c, lstm_h, lstm_c, attn_w, attn_w_cum

    def _get_init_attn_c(self, enc_out, enc_mask):
        bsz = enc_out.size(0)
        if self.args.init_attn_c == 'zero':
            return enc_out.new_zeros(bsz, self.args.encoder_embed_dim)
        elif self.args.init_attn_c == 'avg':
            enc_w = (~enc_mask).type(enc_out.type())
            enc_w = enc_w / enc_w.sum(dim=1, keepdim=True)
            return torch.sum(enc_out * enc_w.unsqueeze(2), dim=1)
        else:
            raise ValueError(f'{self.args.init_attn_c} not supported')

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None, target_lengths=None, **kwargs):
        enc_mask = encoder_out['encoder_padding_mask']
        enc_out = encoder_out['encoder_out'][0]
        in_len = enc_out.size(1)
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:, :]
        bsz, out_len, _ = prev_output_tokens.size()
        prenet_out = self.prenet(prev_output_tokens)
        alstm_h, alstm_c, lstm_h, lstm_c, attn_w, attn_w_cum = self._get_states(incremental_state, enc_out)
        attn_ctx = self._get_init_attn_c(enc_out, enc_mask)
        attn_out = enc_out.new_zeros(bsz, in_len, out_len)
        feat_out = enc_out.new_zeros(bsz, out_len, self.out_dim)
        eos_out = enc_out.new_zeros(bsz, out_len)
        for t in range(out_len):
            alstm_in = torch.cat((attn_ctx, prenet_out[:, t, :]), dim=1)
            alstm_h, alstm_c = self.attention_lstm(alstm_in, (alstm_h, alstm_c))
            attn_state = attn_w.unsqueeze(1)
            if self.args.attention_use_cumprob:
                attn_state = torch.stack((attn_w, attn_w_cum), dim=1)
            attn_ctx, attn_w = self.attention(enc_out, enc_mask, alstm_h, attn_state)
            attn_w_cum = attn_w_cum + attn_w
            attn_out[:, :, t] = attn_w
            for i, cur_lstm in enumerate(self.lstm):
                if i == 0:
                    lstm_in = torch.cat((attn_ctx, alstm_h), dim=1)
                else:
                    lstm_in = torch.cat((attn_ctx, lstm_h[i - 1]), dim=1)
                lstm_h[i], lstm_c[i] = cur_lstm(lstm_in, (lstm_h[i], lstm_c[i]))
            proj_in = torch.cat((attn_ctx, lstm_h[-1]), dim=1)
            feat_out[:, t, :] = self.feat_proj(proj_in)
            eos_out[:, t] = self.eos_proj(proj_in).squeeze(1)
        self.attention.clear_cache()
        self.set_incremental_state(incremental_state, 'alstm_h', alstm_h)
        self.set_incremental_state(incremental_state, 'alstm_c', alstm_c)
        self.set_incremental_state(incremental_state, 'lstm_h', lstm_h)
        self.set_incremental_state(incremental_state, 'lstm_c', lstm_c)
        self.set_incremental_state(incremental_state, 'attn_w', attn_w)
        self.set_incremental_state(incremental_state, 'attn_w_cum', attn_w_cum)
        post_feat_out = feat_out + self.postnet(feat_out)
        eos_out = eos_out.view(bsz, out_len, 1)
        return post_feat_out, eos_out, {'attn': attn_out, 'feature_out': feat_out}


class TTSTransformerEncoder(FairseqEncoder):

    def __init__(self, args, src_dict, embed_speaker):
        super().__init__(src_dict)
        self.padding_idx = src_dict.pad()
        self.embed_speaker = embed_speaker
        self.spk_emb_proj = None
        if embed_speaker is not None:
            self.spk_emb_proj = nn.Linear(args.encoder_embed_dim + args.speaker_embed_dim, args.encoder_embed_dim)
        self.dropout_module = FairseqDropout(p=args.dropout, module_name=self.__class__.__name__)
        self.embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, padding_idx=self.padding_idx)
        assert args.encoder_conv_kernel_size % 2 == 1
        self.prenet = nn.ModuleList(nn.Sequential(nn.Conv1d(args.encoder_embed_dim, args.encoder_embed_dim, kernel_size=args.encoder_conv_kernel_size, padding=(args.encoder_conv_kernel_size - 1) // 2), nn.BatchNorm1d(args.encoder_embed_dim), nn.ReLU(), nn.Dropout(args.encoder_dropout)) for _ in range(args.encoder_conv_layers))
        self.prenet_proj = nn.Linear(args.encoder_embed_dim, args.encoder_embed_dim)
        self.embed_positions = PositionalEmbedding(args.max_source_positions, args.encoder_embed_dim, self.padding_idx)
        self.pos_emb_alpha = nn.Parameter(torch.ones(1))
        self.transformer_layers = nn.ModuleList(TransformerEncoderLayer(args) for _ in range(args.encoder_transformer_layers))
        if args.encoder_normalize_before:
            self.layer_norm = LayerNorm(args.encoder_embed_dim)
        else:
            self.layer_norm = None
        self.apply(encoder_init)

    def forward(self, src_tokens, src_lengths=None, speaker=None, **kwargs):
        x = self.embed_tokens(src_tokens)
        x = x.transpose(1, 2).contiguous()
        for conv in self.prenet:
            x = conv(x)
        x = x.transpose(1, 2).contiguous()
        x = self.prenet_proj(x)
        padding_mask = src_tokens.eq(self.padding_idx)
        positions = self.embed_positions(padding_mask)
        x += self.pos_emb_alpha * positions
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        for layer in self.transformer_layers:
            x = layer(x, padding_mask)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        if self.embed_speaker is not None:
            seq_len, bsz, _ = x.size()
            emb = self.embed_speaker(speaker).transpose(0, 1)
            emb = emb.expand(seq_len, bsz, -1)
            x = self.spk_emb_proj(torch.cat([x, emb], dim=2))
        return {'encoder_out': [x], 'encoder_padding_mask': [padding_mask] if padding_mask.any() else [], 'encoder_embedding': [], 'encoder_states': [], 'src_tokens': [], 'src_lengths': []}


class PseudoInverseMelScale(torch.nn.Module):

    def __init__(self, n_stft, n_mels, sample_rate, f_min, f_max) ->None:
        super(PseudoInverseMelScale, self).__init__()
        self.n_mels = n_mels
        basis = get_mel_filters(sample_rate, (n_stft - 1) * 2, n_mels, f_min, f_max)
        basis = torch.pinverse(basis)
        self.register_buffer('basis', basis)

    def forward(self, melspec: torch.Tensor) ->torch.Tensor:
        shape = melspec.shape
        n_mels, time = shape[-2], shape[-1]
        melspec = melspec.view(-1, n_mels, time)
        freq, _ = self.basis.size()
        assert self.n_mels == n_mels, (self.n_mels, n_mels)
        specgram = self.basis.matmul(melspec).clamp(min=0)
        specgram = specgram.view(shape[:-2] + (freq, time))
        return specgram


class GriffinLim(torch.nn.Module):

    def __init__(self, n_fft: int, win_length: int, hop_length: int, n_iter: int, window_fn=torch.hann_window):
        super(GriffinLim, self).__init__()
        self.transform = TTSSpectrogram(n_fft, win_length, hop_length, return_phase=True)
        basis = get_fourier_basis(n_fft)
        basis = torch.pinverse(n_fft / hop_length * basis).T[:, None, :]
        basis *= get_window(window_fn, n_fft, win_length)
        self.register_buffer('basis', basis)
        self.n_fft = n_fft
        self.win_length = win_length
        self.hop_length = hop_length
        self.n_iter = n_iter
        self.tiny = 1.1754944e-38

    @classmethod
    def get_window_sum_square(cls, n_frames, hop_length, win_length, n_fft, window_fn=torch.hann_window) ->torch.Tensor:
        w_sq = get_window(window_fn, n_fft, win_length) ** 2
        n = n_fft + hop_length * (n_frames - 1)
        x = torch.zeros(n, dtype=torch.float32)
        for i in range(n_frames):
            ofst = i * hop_length
            x[ofst:min(n, ofst + n_fft)] += w_sq[:max(0, min(n_fft, n - ofst))]
        return x

    def inverse(self, magnitude: torch.Tensor, phase) ->torch.Tensor:
        x = torch.cat([magnitude * torch.cos(phase), magnitude * torch.sin(phase)], dim=1)
        x = F.conv_transpose1d(x, self.basis, stride=self.hop_length)
        win_sum_sq = self.get_window_sum_square(magnitude.shape[-1], hop_length=self.hop_length, win_length=self.win_length, n_fft=self.n_fft)
        approx_nonzero_indices = win_sum_sq > self.tiny
        x[:, :, approx_nonzero_indices] /= win_sum_sq[approx_nonzero_indices]
        x *= self.n_fft / self.hop_length
        x = x[:, :, self.n_fft // 2:]
        x = x[:, :, :-self.n_fft // 2]
        return x

    def forward(self, specgram: torch.Tensor) ->torch.Tensor:
        angles = np.angle(np.exp(2.0j * np.pi * np.random.rand(*specgram.shape)))
        angles = torch.from_numpy(angles)
        _specgram = specgram.view(-1, specgram.shape[-2], specgram.shape[-1])
        waveform = self.inverse(_specgram, angles).squeeze(1)
        for _ in range(self.n_iter):
            _, angles = self.transform(waveform)
            waveform = self.inverse(_specgram, angles).squeeze(1)
        return waveform.squeeze(0)


class AugTransformerDecoderBase(TransformerDecoderBase):
    """
    Transformer decoder augmented with an additional cross-attention. Each layer
    is a :class:`AugTransformerDecoderLayerBase`.

    Args:
        cfg (argparse.Namespace): parsed command-line arguments
        dictionary (~fairseq.data.Dictionary): decoding dictionary
        embed_tokens (torch.nn.Embedding): output embedding
        encoder_attn_merge_type (str, optional): the way to combine outputs from
            two cross-attention modules. If "sequential" is set, two cross-attention
            modules are stacked sequentially. If "parallel" is set, they are processed
            in parallel and combined before feeding it to FFN (default: sequential).
        dropnet_ratio (float, optional): a probability to drop each cross-attention
            module during training (default: 0.0).
    """

    def __init__(self, cfg, dictionary, embed_tokens, output_projection=None, encoder_attn_merge_type='sequential', dropnet_ratio=0.0):
        super().__init__(cfg, dictionary, embed_tokens, no_encoder_attn=False, output_projection=output_projection)
        self.cross_self_attention = cfg.cross_self_attention
        if self.decoder_layerdrop > 0.0:
            self.layers = LayerDropModuleList(p=self.decoder_layerdrop)
        else:
            self.layers = nn.ModuleList([])
        self.layers.extend([self.build_decoder_layer(cfg, encoder_attn_merge_type, dropnet_ratio) for _ in range(cfg.decoder.layers)])

    def build_decoder_layer(self, cfg, encoder_attn_merge_type='sequential', dropnet_ratio=0):
        layer = transformer_layer_aug.AugTransformerDecoderLayerBase(cfg, no_encoder_attn=False, encoder_attn_merge_type=encoder_attn_merge_type, dropnet_ratio=dropnet_ratio)
        checkpoint = cfg.checkpoint_activations
        if checkpoint:
            offload_to_cpu = cfg.offload_activations
            layer = checkpoint_wrapper(layer, offload_to_cpu=offload_to_cpu)
        min_params_to_wrap = cfg.min_params_to_wrap if not checkpoint else 0
        layer = fsdp_wrap(layer, min_num_params=min_params_to_wrap)
        return layer

    def forward(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]]=None, encoder_out_aug: Optional[Dict[str, List[Tensor]]]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, features_only: bool=False, full_context_alignment: bool=False, alignment_layer: Optional[int]=None, alignment_heads: Optional[int]=None, src_lengths: Optional[Any]=None, return_all_hiddens: bool=False):
        """
        Args:
            prev_output_tokens (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_out (optional): output from the encoder, used for
                encoder-side attention, should be of size T x B x C
            incremental_state (dict): dictionary used for storing state during
                :ref:`Incremental decoding`
            features_only (bool, optional): only return features without
                applying output layer (default: False).
            full_context_alignment (bool, optional): don't apply
                auto-regressive mask to self-attention (default: False).

        Returns:
            tuple:
                - the decoder's output of shape `(batch, tgt_len, vocab)`
                - a dictionary with any model-specific outputs
        """
        x, extra = self.extract_features(prev_output_tokens, encoder_out=encoder_out, encoder_out_aug=encoder_out_aug, incremental_state=incremental_state, full_context_alignment=full_context_alignment, alignment_layer=alignment_layer, alignment_heads=alignment_heads)
        if not features_only:
            x = self.output_layer(x)
        return x, extra

    def extract_features(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]], encoder_out_aug: Optional[Dict[str, List[Tensor]]], incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, full_context_alignment: bool=False, alignment_layer: Optional[int]=None, alignment_heads: Optional[int]=None):
        return self.extract_features_scriptable(prev_output_tokens, encoder_out, encoder_out_aug, incremental_state, full_context_alignment, alignment_layer, alignment_heads)
    """
    A scriptable subclass of this class has an extract_features method and calls
    super().extract_features, but super() is not supported in torchscript. A copy of
    this function is made to be used in the subclass instead.
    """

    def extract_features_scriptable(self, prev_output_tokens, encoder_out: Optional[Dict[str, List[Tensor]]], encoder_out_aug: Optional[Dict[str, List[Tensor]]], incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, full_context_alignment: bool=False, alignment_layer: Optional[int]=None, alignment_heads: Optional[int]=None):
        """
        Similar to *forward* but only return features.

        Includes several features from "Jointly Learning to Align and
        Translate with Transformer Models" (Garg et al., EMNLP 2019).

        Args:
            full_context_alignment (bool, optional): don't apply
                auto-regressive mask to self-attention (default: False).
            alignment_layer (int, optional): return mean alignment over
                heads at this layer (default: last layer).
            alignment_heads (int, optional): only average alignment over
                this many heads (default: all heads).

        Returns:
            tuple:
                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - a dictionary with any model-specific outputs
        """
        bs, slen = prev_output_tokens.size()
        if alignment_layer is None:
            alignment_layer = self.num_layers - 1
        enc: Optional[Tensor] = None
        padding_mask: Optional[Tensor] = None
        if encoder_out is not None and len(encoder_out['encoder_out']) > 0:
            enc = encoder_out['encoder_out'][0]
        if encoder_out is not None and len(encoder_out['encoder_padding_mask']) > 0:
            padding_mask = encoder_out['encoder_padding_mask'][0]
        enc_aug: Optional[Tensor] = None
        padding_mask_aug: Optional[Tensor] = None
        if encoder_out_aug is not None and len(encoder_out_aug['encoder_out']) > 0:
            enc_aug = encoder_out_aug['encoder_out'][0]
        if encoder_out_aug is not None and len(encoder_out_aug['encoder_padding_mask']) > 0:
            padding_mask_aug = encoder_out_aug['encoder_padding_mask'][0]
        positions = None
        if self.embed_positions is not None:
            positions = self.embed_positions(prev_output_tokens, incremental_state=incremental_state)
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
            if positions is not None:
                positions = positions[:, -1:]
        prev_output_tokens = prev_output_tokens.contiguous()
        x = self.embed_scale * self.embed_tokens(prev_output_tokens)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        if self.project_in_dim is not None:
            x = self.project_in_dim(x)
        if positions is not None:
            x += positions
        if self.layernorm_embedding is not None:
            x = self.layernorm_embedding(x)
        x = self.dropout_module(x)
        x = x.transpose(0, 1)
        self_attn_padding_mask: Optional[Tensor] = None
        if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():
            self_attn_padding_mask = prev_output_tokens.eq(self.padding_idx)
        attn: Optional[Tensor] = None
        attn_aug: Optional[Tensor] = None
        inner_states: List[Optional[Tensor]] = [x]
        for idx, layer in enumerate(self.layers):
            if incremental_state is None and not full_context_alignment:
                self_attn_mask = self.buffered_future_mask(x)
            else:
                self_attn_mask = None
            x, layer_attn, layer_attn_aug, _ = layer(x, enc, padding_mask, enc_aug, padding_mask_aug, incremental_state, self_attn_mask=self_attn_mask, self_attn_padding_mask=self_attn_padding_mask, need_attn=bool(idx == alignment_layer), need_head_weights=bool(idx == alignment_layer))
            inner_states.append(x)
            if layer_attn is not None and idx == alignment_layer:
                attn = layer_attn.float()
            if layer_attn_aug is not None and idx == alignment_layer:
                attn_aug = layer_attn_aug.float()
        if attn is not None:
            if alignment_heads is not None:
                attn = attn[:alignment_heads]
            attn = attn.mean(dim=0)
        if attn_aug is not None:
            if alignment_heads is not None:
                attn_aug = attn_aug[:alignment_heads]
            attn_aug = attn_aug.mean(dim=0)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        if self.project_out_dim is not None:
            x = self.project_out_dim(x)
        return x, {'attn': [attn], 'attn_aug': [attn_aug], 'inner_states': inner_states}

    def upgrade_state_dict_named(self, state_dict, name):
        """Upgrade a (possibly old) state dict for new versions of fairseq."""
        if isinstance(self.embed_positions, SinusoidalPositionalEmbedding):
            weights_key = '{}.embed_positions.weights'.format(name)
            if weights_key in state_dict:
                del state_dict[weights_key]
            state_dict['{}.embed_positions._float_tensor'.format(name)] = torch.FloatTensor(1)
        if f'{name}.output_projection.weight' not in state_dict:
            if self.share_input_output_embed:
                embed_out_key = f'{name}.embed_tokens.weight'
            else:
                embed_out_key = f'{name}.embed_out'
            if embed_out_key in state_dict:
                state_dict[f'{name}.output_projection.weight'] = state_dict[embed_out_key]
                if not self.share_input_output_embed:
                    del state_dict[embed_out_key]
        for i in range(self.num_layers):
            layer_norm_map = {'0': 'self_attn_layer_norm', '1': 'encoder_attn_layer_norm', '2': 'encoder_attn_layer_norm2', '3': 'final_layer_norm'}
            for old, new in layer_norm_map.items():
                for m in ('weight', 'bias'):
                    k = '{}.layers.{}.layer_norms.{}.{}'.format(name, i, old, m)
                    if k in state_dict:
                        state_dict['{}.layers.{}.{}.{}'.format(name, i, new, m)] = state_dict[k]
                        del state_dict[k]
        version_key = '{}.version'.format(name)
        if utils.item(state_dict.get(version_key, torch.Tensor([1]))[0]) <= 2:
            self.layer_norm = None
            self.normalize = False
            state_dict[version_key] = torch.Tensor([1])
        return state_dict


class AugTransformerDecoder(AugTransformerDecoderBase):

    def __init__(self, args, dictionary, embed_tokens, output_projection=None):
        self.args = args
        super().__init__(TransformerConfig.from_namespace(args), dictionary, embed_tokens, no_encoder_attn=False, output_projection=output_projection, encoder_attn_merge_type=getattr(args, 'synthesizer_augmented_cross_attention_merge_type', 'sequential'), dropnet_ratio=getattr(args, 'dropnet_ratio', 0))

    def build_output_projection(self, args, dictionary, embed_tokens):
        super().build_output_projection(TransformerConfig.from_namespace(args), dictionary, embed_tokens)

    def build_decoder_layer(self, args, encoder_attn_merge_type='sequential', dropnet_ratio=0):
        return super().build_decoder_layer(TransformerConfig.from_namespace(args), no_encoder_attn=False, encoder_attn_merge_type=encoder_attn_merge_type, dropnet_ratio=dropnet_ratio)


class AdaptiveInput(nn.Module):

    def __init__(self, vocab_size: int, padding_idx: int, initial_dim: int, factor: float, output_dim: int, cutoff: List[int], q_noise: float=0, qn_block_size: int=8):
        super().__init__()
        if vocab_size > cutoff[-1]:
            cutoff = cutoff + [vocab_size]
        else:
            assert vocab_size == cutoff[-1], 'cannot specify cutoff larger than vocab size'
        self.cutoff = cutoff
        self.embedding_dim = output_dim
        self.padding_idx = padding_idx
        self.embeddings = nn.ModuleList()
        for i in range(len(self.cutoff)):
            prev = self.cutoff[i - 1] if i > 0 else 0
            size = self.cutoff[i] - prev
            dim = int(initial_dim // factor ** i)
            seq = nn.Sequential(nn.Embedding(size, dim, self.padding_idx), quant_noise(nn.Linear(dim, output_dim, bias=False), q_noise, qn_block_size))
            self.embeddings.append(seq)
            self.padding_idx = None
        self.padding_idx = padding_idx

        def init_weights(m):
            if isinstance(m, nn.Embedding):
                nn.init.normal_(m.weight, mean=0, std=m.weight.shape[1] ** -0.5)
                nn.init.constant_(m.weight[padding_idx], 0)
            elif hasattr(m, 'weight'):
                nn.init.xavier_uniform_(m.weight)
        self.apply(init_weights)
        self.register_buffer('_float_tensor', torch.FloatTensor(1))

    def weights_for_band(self, band: int):
        return self.embeddings[band][0].weight, self.embeddings[band][1].weight

    def forward(self, input: torch.Tensor):
        result = self._float_tensor.new(input.shape + (self.embedding_dim,))
        for i in range(len(self.cutoff)):
            mask = input.lt(self.cutoff[i])
            if i > 0:
                mask.mul_(input.ge(self.cutoff[i - 1]))
                chunk_input = input[mask] - self.cutoff[i - 1]
            else:
                chunk_input = input[mask]
            if mask.any():
                result[mask] = self.embeddings[i](chunk_input)
        return result


CHAR_EOS_IDX = 257


CHAR_PAD_IDX = 0


class Highway(torch.nn.Module):
    """
    A `Highway layer <https://arxiv.org/abs/1505.00387>`_.
    Adopted from the AllenNLP implementation.
    """

    def __init__(self, input_dim: int, num_layers: int=1):
        super(Highway, self).__init__()
        self.input_dim = input_dim
        self.layers = nn.ModuleList([nn.Linear(input_dim, input_dim * 2) for _ in range(num_layers)])
        self.activation = nn.ReLU()
        self.reset_parameters()

    def reset_parameters(self):
        for layer in self.layers:
            nn.init.constant_(layer.bias[self.input_dim:], 1)
            nn.init.constant_(layer.bias[:self.input_dim], 0)
            nn.init.xavier_normal_(layer.weight)

    def forward(self, x: torch.Tensor):
        for layer in self.layers:
            projection = layer(x)
            proj_x, gate = projection.chunk(2, dim=-1)
            proj_x = self.activation(proj_x)
            gate = torch.sigmoid(gate)
            x = gate * x + (gate.new_tensor([1]) - gate) * proj_x
        return x


class CharacterTokenEmbedder(torch.nn.Module):

    def __init__(self, vocab: Dictionary, filters: List[Tuple[int, int]], char_embed_dim: int, word_embed_dim: int, highway_layers: int, max_char_len: int=50, char_inputs: bool=False):
        super(CharacterTokenEmbedder, self).__init__()
        self.onnx_trace = False
        self.embedding_dim = word_embed_dim
        self.max_char_len = max_char_len
        self.char_embeddings = nn.Embedding(257, char_embed_dim, padding_idx=0)
        self.symbol_embeddings = nn.Parameter(torch.FloatTensor(2, word_embed_dim))
        self.eos_idx, self.unk_idx = 0, 1
        self.char_inputs = char_inputs
        self.convolutions = nn.ModuleList()
        for width, out_c in filters:
            self.convolutions.append(nn.Conv1d(char_embed_dim, out_c, kernel_size=width))
        last_dim = sum(f[1] for f in filters)
        self.highway = Highway(last_dim, highway_layers) if highway_layers > 0 else None
        self.projection = nn.Linear(last_dim, word_embed_dim)
        assert vocab is not None or char_inputs, 'vocab must be set if not using char inputs'
        self.vocab = None
        if vocab is not None:
            self.set_vocab(vocab, max_char_len)
        self.reset_parameters()

    def prepare_for_onnx_export_(self):
        self.onnx_trace = True

    def set_vocab(self, vocab, max_char_len):
        word_to_char = torch.LongTensor(len(vocab), max_char_len)
        truncated = 0
        for i in range(len(vocab)):
            if i < vocab.nspecial:
                char_idxs = [0] * max_char_len
            else:
                chars = vocab[i].encode()
                char_idxs = [(c + 1) for c in chars] + [0] * (max_char_len - len(chars))
            if len(char_idxs) > max_char_len:
                truncated += 1
                char_idxs = char_idxs[:max_char_len]
            word_to_char[i] = torch.LongTensor(char_idxs)
        if truncated > 0:
            logger.info('truncated {} words longer than {} characters'.format(truncated, max_char_len))
        self.vocab = vocab
        self.word_to_char = word_to_char

    @property
    def padding_idx(self):
        return Dictionary().pad() if self.vocab is None else self.vocab.pad()

    def reset_parameters(self):
        nn.init.xavier_normal_(self.char_embeddings.weight)
        nn.init.xavier_normal_(self.symbol_embeddings)
        nn.init.xavier_uniform_(self.projection.weight)
        nn.init.constant_(self.char_embeddings.weight[self.char_embeddings.padding_idx], 0.0)
        nn.init.constant_(self.projection.bias, 0.0)

    def forward(self, input: torch.Tensor):
        if self.char_inputs:
            chars = input.view(-1, self.max_char_len)
            pads = chars[:, 0].eq(CHAR_PAD_IDX)
            eos = chars[:, 0].eq(CHAR_EOS_IDX)
            if eos.any():
                if self.onnx_trace:
                    chars = torch.where(eos.unsqueeze(1), chars.new_zeros(1), chars)
                else:
                    chars[eos] = 0
            unk = None
        else:
            flat_words = input.view(-1)
            chars = self.word_to_char[flat_words.type_as(self.word_to_char)].type_as(input)
            pads = flat_words.eq(self.vocab.pad())
            eos = flat_words.eq(self.vocab.eos())
            unk = flat_words.eq(self.vocab.unk())
        word_embs = self._convolve(chars)
        if self.onnx_trace:
            if pads.any():
                word_embs = torch.where(pads.unsqueeze(1), word_embs.new_zeros(1), word_embs)
            if eos.any():
                word_embs = torch.where(eos.unsqueeze(1), self.symbol_embeddings[self.eos_idx], word_embs)
            if unk is not None and unk.any():
                word_embs = torch.where(unk.unsqueeze(1), self.symbol_embeddings[self.unk_idx], word_embs)
        else:
            if pads.any():
                word_embs[pads] = 0
            if eos.any():
                word_embs[eos] = self.symbol_embeddings[self.eos_idx]
            if unk is not None and unk.any():
                word_embs[unk] = self.symbol_embeddings[self.unk_idx]
        return word_embs.view(input.size()[:2] + (-1,))

    def _convolve(self, char_idxs: torch.Tensor):
        char_embs = self.char_embeddings(char_idxs)
        char_embs = char_embs.transpose(1, 2)
        conv_result = []
        for conv in self.convolutions:
            x = conv(char_embs)
            x, _ = torch.max(x, -1)
            x = F.relu(x)
            conv_result.append(x)
        x = torch.cat(conv_result, dim=-1)
        if self.highway is not None:
            x = self.highway(x)
        x = self.projection(x)
        return x


F0_FRAME_SPACE = 0.005


class Paddings(object):

    def __init__(self, code_val, dur_val=0, f0_val=-2.0):
        self.code = code_val
        self.dur = dur_val
        self.f0 = f0_val


class Shifts(object):

    def __init__(self, shifts_str, pads):
        self._shifts = list(map(int, shifts_str.split(',')))
        assert len(self._shifts) == 2, self._shifts
        assert all(s >= 0 for s in self._shifts)
        self.extra_length = max(s for s in self._shifts)
        self.pads = pads

    @property
    def dur(self):
        return self._shifts[0]

    @property
    def f0(self):
        return self._shifts[1]

    @staticmethod
    def shift_one(seq, left_pad_num, right_pad_num, pad):
        assert seq.ndim == 1
        bos = seq.new_full((left_pad_num,), pad)
        eos = seq.new_full((right_pad_num,), pad)
        seq = torch.cat([bos, seq, eos])
        mask = torch.ones_like(seq).bool()
        mask[left_pad_num:len(seq) - right_pad_num] = 0
        return seq, mask

    def __call__(self, code, dur, f0):
        if self.extra_length == 0:
            code_mask = torch.zeros_like(code).bool()
            dur_mask = torch.zeros_like(dur).bool()
            f0_mask = torch.zeros_like(f0).bool()
            return code, code_mask, dur, dur_mask, f0, f0_mask
        code, code_mask = self.shift_one(code, 0, self.extra_length, self.pads.code)
        dur, dur_mask = self.shift_one(dur, self.dur, self.extra_length - self.dur, self.pads.dur)
        f0, f0_mask = self.shift_one(f0, self.f0, self.extra_length - self.f0, self.pads.f0)
        return code, code_mask, dur, dur_mask, f0, f0_mask


def interpolate_f0(f0):
    try:
        from scipy.interpolate import interp1d
    except ImportError:
        raise 'Please install scipy (`pip install scipy`)'
    orig_t = np.arange(f0.shape[0])
    f0_interp = f0[:]
    ii = f0_interp != 0
    if ii.sum() > 1:
        f0_interp = interp1d(orig_t[ii], f0_interp[ii], bounds_error=False, kind='linear', fill_value=0)(orig_t)
        f0_interp = torch.Tensor(f0_interp).type_as(f0)
    return f0_interp


def naive_quantize(x, edges):
    bin_idx = (x.view(-1, 1) > edges.view(1, -1)).long().sum(dim=1)
    return bin_idx


class CodeDataset(FairseqDataset):

    def __init__(self, manifest, dictionary, dur_dictionary, f0_dictionary, config, discrete_dur, discrete_f0, log_f0, normalize_f0_mean, normalize_f0_std, interpolate_f0, return_filename=False, strip_filename=True, shifts='0,0', return_continuous_f0=False):
        random.seed(1234)
        self.dictionary = dictionary
        self.dur_dictionary = dur_dictionary
        self.f0_dictionary = f0_dictionary
        self.config = config
        self.discrete_dur = discrete_dur
        self.discrete_f0 = discrete_f0
        self.log_f0 = log_f0
        self.normalize_f0_mean = normalize_f0_mean
        self.normalize_f0_std = normalize_f0_std
        self.interpolate_f0 = interpolate_f0
        self.return_filename = return_filename
        self.strip_filename = strip_filename
        self.f0_code_ratio = config.code_hop_size / (config.sampling_rate * F0_FRAME_SPACE)
        self.manifest = manifest
        self._codes = None
        self._durs = None
        self._f0s = None
        with open(f'{manifest}.leng.txt', 'r') as f:
            lengs = [int(line.rstrip()) for line in f]
            edges = np.cumsum([0] + lengs)
            self.starts, self.ends = edges[:-1], edges[1:]
        with open(f'{manifest}.path.txt', 'r') as f:
            self.file_names = [line.rstrip() for line in f]
        logger.info(f'num entries: {len(self.starts)}')
        if os.path.exists(f'{manifest}.f0_stat.pt'):
            self.f0_stats = torch.load(f'{manifest}.f0_stat.pt')
        elif config.f0_stats:
            self.f0_stats = torch.load(config.f0_stats)
        self.multispkr = config.multispkr
        if config.multispkr:
            with open(f'{manifest}.speaker.txt', 'r') as f:
                self.spkrs = [line.rstrip() for line in f]
            self.id_to_spkr = sorted(self.spkrs)
            self.spkr_to_id = {k: v for v, k in enumerate(self.id_to_spkr)}
        self.pads = Paddings(dictionary.pad(), 0, f0_dictionary.pad() if discrete_f0 else -5.0)
        self.shifts = Shifts(shifts, pads=self.pads)
        self.return_continuous_f0 = return_continuous_f0

    def get_data_handlers(self):
        logging.info(f'loading data for {self.manifest}')
        self._codes = np.load(f'{self.manifest}.code.npy', mmap_mode='r')
        self._durs = np.load(f'{self.manifest}.dur.npy', mmap_mode='r')
        if self.discrete_f0:
            if self.config.f0_vq_type == 'precomp':
                self._f0s = np.load(f'{self.manifest}.{self.config.f0_vq_name}.npy', mmap_mode='r')
            elif self.config.f0_vq_type == 'naive':
                self._f0s = np.load(f'{self.manifest}.f0.npy', mmap_mode='r')
                quantizers_path = self.config.get_f0_vq_naive_quantizer(self.log_f0, self.normalize_f0_mean, self.normalize_f0_std)
                quantizers = torch.load(quantizers_path)
                n_units = self.config.f0_vq_n_units
                self._f0_quantizer = torch.from_numpy(quantizers[n_units])
            else:
                raise ValueError(f'f0_vq_type {self.config.f0_vq_type} not supported')
        else:
            self._f0s = np.load(f'{self.manifest}.f0.npy', mmap_mode='r')

    def preprocess_f0(self, f0, stats):
        """
        1. interpolate
        2. log transform (keep unvoiced frame 0)
        """
        f0 = f0.clone()
        if self.interpolate_f0:
            f0 = interpolate_f0(f0)
        mask = f0 != 0
        if self.log_f0:
            f0[mask] = f0[mask].log()
        if self.normalize_f0_mean:
            mean = stats['logf0_mean'] if self.log_f0 else stats['f0_mean']
            f0[mask] = f0[mask] - mean
        if self.normalize_f0_std:
            std = stats['logf0_std'] if self.log_f0 else stats['f0_std']
            f0[mask] = f0[mask] / std
        return f0

    def _get_raw_item(self, index):
        start, end = self.starts[index], self.ends[index]
        if self._codes is None:
            self.get_data_handlers()
        code = torch.from_numpy(np.array(self._codes[start:end])).long()
        dur = torch.from_numpy(np.array(self._durs[start:end]))
        f0 = torch.from_numpy(np.array(self._f0s[start:end]))
        return code, dur, f0

    def __getitem__(self, index):
        code, dur, f0 = self._get_raw_item(index)
        code = torch.cat([code.new([self.dictionary.bos()]), code])
        dur = torch.cat([dur.new([0]), dur])
        if self.discrete_dur:
            dur = self.dur_dictionary.encode_line(' '.join(map(str, dur.tolist())), append_eos=False).long()
        else:
            dur = dur.float()
        raw_f0 = None
        if self.discrete_f0:
            if self.config.f0_vq_type == 'precomp':
                f0 = self.f0_dictionary.encode_line(' '.join(map(str, f0.tolist())), append_eos=False).long()
            else:
                f0 = f0.float()
                f0 = self.preprocess_f0(f0, self.f0_stats[self.spkrs[index]])
                if self.return_continuous_f0:
                    raw_f0 = f0
                    raw_f0 = torch.cat([raw_f0.new([self.f0_dictionary.bos()]), raw_f0])
                f0 = naive_quantize(f0, self._f0_quantizer)
            f0 = torch.cat([f0.new([self.f0_dictionary.bos()]), f0])
        else:
            f0 = f0.float()
            if self.multispkr:
                f0 = self.preprocess_f0(f0, self.f0_stats[self.spkrs[index]])
            else:
                f0 = self.preprocess_f0(f0, self.f0_stats)
            f0 = torch.cat([f0.new([0]), f0])
        if raw_f0 is not None:
            *_, raw_f0, raw_f0_mask = self.shifts(code, dur, raw_f0)
        else:
            raw_f0_mask = None
        code, code_mask, dur, dur_mask, f0, f0_mask = self.shifts(code, dur, f0)
        if raw_f0_mask is not None:
            assert (raw_f0_mask == f0_mask).all()
        feats = {'source': code[:-1], 'target': code[1:], 'mask': code_mask[1:].logical_or(code_mask[:-1]), 'dur_source': dur[:-1], 'dur_target': dur[1:], 'dur_mask': dur_mask[1:].logical_or(dur_mask[:-1]), 'f0_source': f0[:-1], 'f0_target': f0[1:], 'f0_mask': f0_mask[1:].logical_or(f0_mask[:-1])}
        if raw_f0 is not None:
            feats['raw_f0'] = raw_f0[1:]
        if self.return_filename:
            fname = self.file_names[index]
            feats['filename'] = fname if not self.strip_filename else Path(fname).with_suffix('').name
        return feats

    def __len__(self):
        return len(self.starts)

    def size(self, index):
        return self.ends[index] - self.starts[index] + self.shifts.extra_length

    def num_tokens(self, index):
        return self.size(index)

    def collater(self, samples):
        pad_idx, eos_idx = self.dictionary.pad(), self.dictionary.eos()
        if len(samples) == 0:
            return {}
        src_tokens = data_utils.collate_tokens([s['source'] for s in samples], pad_idx, eos_idx, left_pad=False)
        tgt_tokens = data_utils.collate_tokens([s['target'] for s in samples], pad_idx=pad_idx, eos_idx=pad_idx, left_pad=False)
        src_durs, tgt_durs = [data_utils.collate_tokens([s[k] for s in samples], pad_idx=self.pads.dur, eos_idx=self.pads.dur, left_pad=False) for k in ['dur_source', 'dur_target']]
        src_f0s, tgt_f0s = [data_utils.collate_tokens([s[k] for s in samples], pad_idx=self.pads.f0, eos_idx=self.pads.f0, left_pad=False) for k in ['f0_source', 'f0_target']]
        mask, dur_mask, f0_mask = [data_utils.collate_tokens([s[k] for s in samples], pad_idx=1, eos_idx=1, left_pad=False) for k in ['mask', 'dur_mask', 'f0_mask']]
        src_lengths = torch.LongTensor([s['source'].numel() for s in samples])
        n_tokens = sum(len(s['source']) for s in samples)
        result = {'nsentences': len(samples), 'ntokens': n_tokens, 'net_input': {'src_tokens': src_tokens, 'src_lengths': src_lengths, 'dur_src': src_durs, 'f0_src': src_f0s}, 'target': tgt_tokens, 'dur_target': tgt_durs, 'f0_target': tgt_f0s, 'mask': mask, 'dur_mask': dur_mask, 'f0_mask': f0_mask}
        if 'filename' in samples[0]:
            result['filename'] = [s['filename'] for s in samples]
        if 'prefix' in samples[0]:
            result['prefix'] = [s['prefix'] for s in samples]
        if 'raw_f0' in samples[0]:
            raw_f0s = data_utils.collate_tokens([s['raw_f0'] for s in samples], pad_idx=self.pads.f0, eos_idx=self.pads.f0, left_pad=False)
            result['raw_f0'] = raw_f0s
        return result


class ExpressiveCodeDataConfig(object):

    def __init__(self, json_path):
        with open(json_path, 'r') as f:
            self.config = json.load(f)
        self._manifests = self.config['manifests']

    @property
    def manifests(self):
        return self._manifests

    @property
    def n_units(self):
        return self.config['n_units']

    @property
    def sampling_rate(self):
        return self.config['sampling_rate']

    @property
    def code_hop_size(self):
        return self.config['code_hop_size']

    @property
    def f0_stats(self):
        """pre-computed f0 statistics path"""
        return self.config.get('f0_stats', None)

    @property
    def f0_vq_type(self):
        """naive or precomp"""
        return self.config['f0_vq_type']

    @property
    def f0_vq_name(self):
        return self.config['f0_vq_name']

    def get_f0_vq_naive_quantizer(self, log, norm_mean, norm_std):
        key = 'log' if log else 'linear'
        if norm_mean and norm_std:
            key += '_mean_std_norm'
        elif norm_mean:
            key += '_mean_norm'
        else:
            key += '_none_norm'
        return self.config['f0_vq_naive_quantizer'][key]

    @property
    def f0_vq_n_units(self):
        return self.config['f0_vq_n_units']

    @property
    def multispkr(self):
        """how to parse speaker label from audio path"""
        return self.config.get('multispkr', None)


class UnitDictionary(Dictionary):
    """
    A fixed-sized Dictionary that operates on integer-valued tokens
    wth a trivial (identity) token <-> id mapping.
    Special symbols (bos, eos, ...) have ids above n_units.
    """

    def __init__(self, *, n_units, bos='<s>', pad='<pad>', eos='</s>', unk='<unk>', extra_special_symbols=None, clip=False):
        self.n_units = n_units
        self.bos_word, self.unk_word, self.pad_word, self.eos_word = bos, unk, pad, eos
        self.clip = clip
        self.symbols = []
        self.count = []
        self.indices = {}
        for i in range(n_units):
            self.add_symbol(str(i))
        self.bos_index = self.add_symbol(bos)
        self.pad_index = self.add_symbol(pad)
        self.eos_index = self.add_symbol(eos)
        self.unk_index = self.add_symbol(unk)
        if extra_special_symbols:
            for s in extra_special_symbols:
                self.add_symbol(s)
        self.nspecial = len(self.symbols)

    def encode_line(self, line, append_eos=True, prepend_bos=False) ->torch.IntTensor:
        words = [int(x) for x in line.split()]
        if self.clip:
            words = [min(self.n_units - 1, word) for word in words]
        if prepend_bos:
            words = [self.bos_index] + words
        if append_eos:
            words.append(self.eos_index)
        ids = torch.IntTensor(words)
        return ids


def base_ulm_architecture(args):
    base_lm_architecture(args)


class ZeroPad1d(nn.Module):

    def __init__(self, pad_left, pad_right):
        super().__init__()
        self.pad_left = pad_left
        self.pad_right = pad_right

    def forward(self, x):
        return F.pad(x, (self.pad_left, self.pad_right))


def norm_block(is_layer_norm, dim, affine=True):
    if is_layer_norm:
        mod = nn.Sequential(TransposeLast(), Fp32LayerNorm(dim, elementwise_affine=affine), TransposeLast())
    else:
        mod = Fp32GroupNorm(1, dim, affine=affine)
    return mod


class ConvAggegator(nn.Module):

    def __init__(self, conv_layers, embed, dropout, skip_connections, residual_scale, non_affine_group_norm, conv_bias, zero_pad, activation):
        super().__init__()

        def block(n_in, n_out, k, stride):
            ka = k // 2
            kb = ka - 1 if k % 2 == 0 else ka
            pad = ZeroPad1d(ka + kb, 0) if zero_pad else nn.ReplicationPad1d((ka + kb, 0))
            return nn.Sequential(pad, nn.Conv1d(n_in, n_out, k, stride=stride, bias=conv_bias), nn.Dropout(p=dropout), norm_block(False, n_out, affine=not non_affine_group_norm), activation)
        in_d = embed
        self.conv_layers = nn.ModuleList()
        self.residual_proj = nn.ModuleList()
        for dim, k, stride in conv_layers:
            if in_d != dim and skip_connections:
                self.residual_proj.append(nn.Conv1d(in_d, dim, 1, bias=False))
            else:
                self.residual_proj.append(None)
            self.conv_layers.append(block(in_d, dim, k, stride))
            in_d = dim
        self.conv_layers = nn.Sequential(*self.conv_layers)
        self.skip_connections = skip_connections
        self.residual_scale = math.sqrt(residual_scale)

    def forward(self, x):
        for rproj, conv in zip(self.residual_proj, self.conv_layers):
            residual = x
            x = conv(x)
            if self.skip_connections:
                if rproj is not None:
                    residual = rproj(residual)
                x = (x + residual) * self.residual_scale
        return x


class GumbelVectorQuantizer(nn.Module):

    def __init__(self, dim, num_vars, temp, groups, combine_groups, vq_dim, time_first, activation=nn.GELU(), weight_proj_depth=1, weight_proj_factor=1, hard=True, std=0):
        """Vector quantization using gumbel softmax

        Args:
            dim: input dimension (channels)
            num_vars: number of quantized vectors per group
            temp: temperature for training. this should be a tuple of 3 elements: (start, stop, decay factor)
            groups: number of groups for vector quantization
            combine_groups: whether to use the vectors for all groups
            vq_dim: dimensionality of the resulting quantized vector
            time_first: if true, expect input in BxTxC format, otherwise in BxCxT
            activation: what activation to use (should be a module). this is only used if weight_proj_depth is > 1
            weight_proj_depth: number of layers (with activation in between) to project input before computing logits
            weight_proj_factor: this is used only if weight_proj_depth is > 1. scales the inner dimensionality of
                                projections by this factor
        """
        super().__init__()
        self.groups = groups
        self.combine_groups = combine_groups
        self.input_dim = dim
        self.num_vars = num_vars
        self.time_first = time_first
        self.hard = hard
        assert vq_dim % groups == 0, f'dim {vq_dim} must be divisible by groups {groups} for concatenation'
        var_dim = vq_dim // groups
        num_groups = groups if not combine_groups else 1
        self.vars = nn.Parameter(torch.FloatTensor(1, num_groups * num_vars, var_dim))
        if std == 0:
            nn.init.uniform_(self.vars)
        else:
            nn.init.normal_(self.vars, mean=0, std=std)
        if weight_proj_depth > 1:

            def block(input_dim, output_dim):
                return nn.Sequential(nn.Linear(input_dim, output_dim), activation)
            inner_dim = self.input_dim * weight_proj_factor
            self.weight_proj = nn.Sequential(*[block(self.input_dim if i == 0 else inner_dim, inner_dim) for i in range(weight_proj_depth - 1)], nn.Linear(inner_dim, groups * num_vars))
        else:
            self.weight_proj = nn.Linear(self.input_dim, groups * num_vars)
            nn.init.normal_(self.weight_proj.weight, mean=0, std=1)
            nn.init.zeros_(self.weight_proj.bias)
        if isinstance(temp, str):
            temp = ast.literal_eval(temp)
        assert len(temp) == 3, f'{temp}, {len(temp)}'
        self.max_temp, self.min_temp, self.temp_decay = temp
        self.curr_temp = self.max_temp
        self.codebook_indices = None

    def set_num_updates(self, num_updates):
        self.curr_temp = max(self.max_temp * self.temp_decay ** num_updates, self.min_temp)

    def get_codebook_indices(self):
        if self.codebook_indices is None:
            from itertools import product
            p = [range(self.num_vars)] * self.groups
            inds = list(product(*p))
            self.codebook_indices = torch.tensor(inds, dtype=torch.long, device=self.vars.device).flatten()
            if not self.combine_groups:
                self.codebook_indices = self.codebook_indices.view(self.num_vars ** self.groups, -1)
                for b in range(1, self.groups):
                    self.codebook_indices[:, b] += self.num_vars * b
                self.codebook_indices = self.codebook_indices.flatten()
        return self.codebook_indices

    def codebook(self):
        indices = self.get_codebook_indices()
        return self.vars.squeeze(0).index_select(0, indices).view(self.num_vars ** self.groups, -1)

    def sample_from_codebook(self, b, n):
        indices = self.get_codebook_indices()
        indices = indices.view(-1, self.groups)
        cb_size = indices.size(0)
        assert n < cb_size, f'sample size {n} is greater than size of codebook {cb_size}'
        sample_idx = torch.randint(low=0, high=cb_size, size=(b * n,))
        indices = indices[sample_idx]
        z = self.vars.squeeze(0).index_select(0, indices.flatten()).view(b, n, -1)
        return z

    def to_codebook_index(self, indices):
        res = indices.new_full(indices.shape[:-1], 0)
        for i in range(self.groups):
            exponent = self.groups - i - 1
            res += indices[..., i] * self.num_vars ** exponent
        return res

    def forward_idx(self, x):
        res = self.forward(x, produce_targets=True)
        return res['x'], res['targets']

    def forward(self, x, produce_targets=False):
        result = {'num_vars': self.num_vars * self.groups}
        if not self.time_first:
            x = x.transpose(1, 2)
        bsz, tsz, fsz = x.shape
        x = x.reshape(-1, fsz)
        x = self.weight_proj(x)
        x = x.view(bsz * tsz * self.groups, -1)
        with torch.no_grad():
            _, k = x.max(-1)
            hard_x = x.new_zeros(*x.shape).scatter_(-1, k.view(-1, 1), 1.0).view(bsz * tsz, self.groups, -1)
            hard_probs = torch.mean(hard_x.float(), dim=0)
            result['code_perplexity'] = torch.exp(-torch.sum(hard_probs * torch.log(hard_probs + 1e-07), dim=-1)).sum()
        avg_probs = torch.softmax(x.view(bsz * tsz, self.groups, -1).float(), dim=-1).mean(dim=0)
        result['prob_perplexity'] = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-07), dim=-1)).sum()
        result['temp'] = self.curr_temp
        if self.training:
            x = F.gumbel_softmax(x.float(), tau=self.curr_temp, hard=self.hard).type_as(x)
        else:
            x = hard_x
        x = x.view(bsz * tsz, -1)
        vars = self.vars
        if self.combine_groups:
            vars = vars.repeat(1, self.groups, 1)
        if produce_targets:
            result['targets'] = x.view(bsz * tsz * self.groups, -1).argmax(dim=-1).view(bsz, tsz, self.groups).detach()
        x = x.unsqueeze(-1) * vars
        x = x.view(bsz * tsz, self.groups, self.num_vars, -1)
        x = x.sum(-2)
        x = x.view(bsz, tsz, -1)
        if not self.time_first:
            x = x.transpose(1, 2)
        result['x'] = x
        return result


class KmeansVectorQuantizer(nn.Module):

    def __init__(self, dim, num_vars, groups, combine_groups, vq_dim, time_first, gamma=0.25):
        """Vector quantization using straight pass-through estimator (i.e. kmeans)

        Args:
            dim: input dimension (channels)
            num_vars: number of quantized vectors per group
            groups: number of groups for vector quantization
            combine_groups: whether to use the vectors for all groups
            vq_dim: dimensionality of the resulting quantized vector
            time_first: if true, expect input in BxTxC format, otherwise in BxCxT
            gamma: commitment loss coefficient
        """
        super().__init__()
        self.groups = groups
        self.combine_groups = combine_groups
        self.input_dim = dim
        self.num_vars = num_vars
        self.vq_dim = vq_dim
        self.time_first = time_first
        assert vq_dim % groups == 0, f'dim {vq_dim} must be divisible by groups {groups} for concatenation'
        self.var_dim = vq_dim // groups
        num_groups = groups if not combine_groups else 1
        self.embedding = nn.Parameter(0.01 * torch.randn(num_vars, num_groups, self.var_dim))
        self.projection = nn.Sequential(nn.Conv1d(dim, dim, kernel_size=1, groups=groups, bias=False), Fp32GroupNorm(groups, dim))
        self.gamma = gamma
        self.mse_mean = nn.MSELoss(reduction='mean')

    def _pass_grad(self, x, y):
        """Manually set gradient for backward pass.
        for y = f(x), ensure that during the backward pass,
        dL/dy = dL/dx regardless of f(x).
        Returns:
            y, with the gradient forced to be dL/dy = dL/dx.
        """
        return y.detach() + (x - x.detach())

    @property
    def expand_embedding(self):
        if self.combine_groups:
            return self.embedding.expand(self.num_vars, self.groups, self.var_dim)
        return self.embedding

    def forward_idx(self, x):
        res = self.forward(x, produce_targets=True)
        return res['x'], res['targets']

    def forward(self, x, produce_targets=False):
        result = {'num_vars': self.num_vars}
        if self.time_first:
            x = x.transpose(1, 2)
        bsz, fsz, tsz = x.shape
        ze = self.projection(x)
        ze_ = ze.view(bsz, self.groups, self.var_dim, tsz).permute(0, 3, 1, 2)
        d = (ze_.unsqueeze(0) - self.expand_embedding.unsqueeze(1).unsqueeze(1)).view(self.num_vars, bsz, tsz, self.groups, -1).norm(dim=-1, p=2)
        idx = d.argmin(dim=0)
        zq = torch.stack([self.expand_embedding[idx[..., group], group] for group in range(self.groups)], dim=-2).view(bsz, tsz, self.groups * self.var_dim).permute(0, 2, 1)
        assert ze.shape == zq.shape, (ze.shape, zq.shape)
        x = self._pass_grad(ze, zq)
        with torch.no_grad():
            hard_x = idx.new_zeros(bsz * tsz * self.groups, self.num_vars).scatter_(-1, idx.view(-1, 1), 1.0).view(bsz * tsz, self.groups, -1)
            hard_probs = torch.mean(hard_x.float(), dim=0)
            result['code_perplexity'] = torch.exp(-torch.sum(hard_probs * torch.log(hard_probs + 1e-07), dim=-1)).sum()
        if produce_targets:
            result['targets'] = idx
        if self.time_first:
            x = x.transpose(1, 2)
        result['x'] = x
        ze = ze.float()
        zq = zq.float()
        latent_loss = self.mse_mean(zq, ze.detach())
        commitment_loss = self.mse_mean(ze, zq.detach())
        result['kmeans_loss'] = latent_loss + self.gamma * commitment_loss
        return result


def buffered_arange(max, device='cpu'):
    if not hasattr(buffered_arange, 'buf'):
        buffered_arange.buf = torch.LongTensor()
    if max > buffered_arange.buf.numel():
        buffered_arange.buf.resize_(max)
        torch.arange(max, out=buffered_arange.buf)
    return buffered_arange.buf[:max]


class Wav2VecPredictionsModel(nn.Module):

    def __init__(self, in_dim, out_dim, prediction_steps, n_negatives, cross_sample_negatives, sample_distance, dropout, offset, balanced_classes, infonce):
        super().__init__()
        self.n_negatives = n_negatives
        self.cross_sample_negatives = cross_sample_negatives
        self.sample_distance = sample_distance
        self.project_to_steps = nn.ConvTranspose2d(in_dim, out_dim, (1, prediction_steps))
        self.dropout = nn.Dropout(p=dropout)
        self.offset = offset
        self.balanced_classes = balanced_classes
        self.infonce = infonce

    def sample_negatives(self, y):
        bsz, fsz, tsz = y.shape
        y = y.transpose(0, 1)
        y = y.contiguous().view(fsz, -1)
        cross_high = tsz * bsz
        high = tsz if self.sample_distance is None else min(tsz, self.sample_distance)
        assert high > 1
        neg_idxs = torch.randint(low=0, high=high, size=(bsz, self.n_negatives * tsz))
        with torch.no_grad():
            if self.n_negatives > 0:
                tszs = buffered_arange(tsz).unsqueeze(-1).expand(-1, self.n_negatives).flatten()
                neg_idxs = torch.randint(low=0, high=high - 1, size=(bsz, self.n_negatives * tsz))
                neg_idxs[neg_idxs >= tszs] += 1
            if self.cross_sample_negatives > 0:
                tszs = buffered_arange(tsz).unsqueeze(-1).expand(-1, self.cross_sample_negatives).flatten()
                cross_neg_idxs = torch.randint(low=0, high=cross_high - 1, size=(bsz, self.cross_sample_negatives * tsz))
                cross_neg_idxs[cross_neg_idxs >= tszs] += 1
        if self.n_negatives > 0:
            for i in range(1, bsz):
                neg_idxs[i] += i * high
        else:
            neg_idxs = cross_neg_idxs
        if self.cross_sample_negatives > 0 and self.n_negatives > 0:
            neg_idxs = torch.cat([neg_idxs, cross_neg_idxs], dim=1)
        negs = y[..., neg_idxs.view(-1)]
        negs = negs.view(fsz, bsz, self.n_negatives + self.cross_sample_negatives, tsz).permute(2, 1, 0, 3)
        return negs

    def forward(self, x, y):
        x = x.unsqueeze(-1)
        x = self.project_to_steps(x)
        x = self.dropout(x)
        negatives = self.sample_negatives(y)
        y = y.unsqueeze(0)
        targets = torch.cat([y, negatives], dim=0)
        copies = targets.size(0)
        bsz, dim, tsz, steps = x.shape
        steps = min(steps, tsz - self.offset)
        predictions = x.new(bsz * copies * (tsz - self.offset + 1) * steps - (steps + 1) * steps // 2 * copies * bsz)
        if self.infonce:
            labels = predictions.new_full((predictions.shape[0] // copies,), 0, dtype=torch.long)
        else:
            labels = torch.zeros_like(predictions)
        weights = torch.full_like(labels, 1 / self.n_negatives) if self.balanced_classes and not self.infonce else None
        start = end = 0
        for i in range(steps):
            offset = i + self.offset
            end = start + (tsz - offset) * bsz * copies
            if self.infonce:
                predictions[start:end] = torch.einsum('bct,nbct->tbn', x[..., :-offset, i], targets[..., offset:]).flatten()
            else:
                pos_num = (end - start) // copies
                predictions[start:end] = torch.einsum('bct,nbct->nbt', x[..., :-offset, i], targets[..., offset:]).flatten()
                labels[start:start + pos_num] = 1.0
                if weights is not None:
                    weights[start:start + pos_num] = 1.0
            start = end
        assert end == predictions.numel(), '{} != {}'.format(end, predictions.numel())
        if self.infonce:
            predictions = predictions.view(-1, copies)
        elif weights is not None:
            labels = labels, weights
        return predictions, labels


class XMODHubInterface(RobertaHubInterface):

    def extract_features(self, tokens: torch.LongTensor, return_all_hiddens: bool=False, lang_id=None) ->torch.Tensor:
        if tokens.dim() == 1:
            tokens = tokens.unsqueeze(0)
        if tokens.size(-1) > self.model.max_positions():
            raise ValueError('tokens exceeds maximum length: {} > {}'.format(tokens.size(-1), self.model.max_positions()))
        features, extra = self.model(tokens, features_only=True, return_all_hiddens=return_all_hiddens, lang_id=lang_id)
        if return_all_hiddens:
            inner_states = extra['inner_states']
            return [inner_state.transpose(0, 1) for inner_state in inner_states]
        else:
            return features

    def predict(self, head: str, tokens: torch.LongTensor, return_logits: bool=False, lang_id=None):
        features = self.extract_features(tokens, lang_id=lang_id)
        logits = self.model.classification_heads[head](features)
        if return_logits:
            return logits
        return F.log_softmax(logits, dim=-1)


class Adapter(nn.Module):

    def __init__(self, cfg, red_fac=2):
        super(Adapter, self).__init__()
        self.cfg = cfg
        self.embed_dim = cfg.encoder_embed_dim
        self.quant_noise = getattr(cfg, 'quant_noise_pq', 0)
        self.quant_noise_block_size = getattr(cfg, 'quant_noise_pq_block_size', 8) or 8
        self.activation_fn = utils.get_activation_fn(activation=getattr(cfg, 'activation_fn', 'relu') or 'relu')
        self.fc1 = quant_noise(nn.Linear(self.embed_dim, self.embed_dim // red_fac), p=self.quant_noise, block_size=self.quant_noise_block_size)
        self.fc2 = quant_noise(nn.Linear(self.embed_dim // red_fac, self.embed_dim), p=self.quant_noise, block_size=self.quant_noise_block_size)
        activation_dropout_p = getattr(cfg, 'activation_dropout', 0) or 0
        if activation_dropout_p == 0:
            activation_dropout_p = getattr(cfg, 'relu_dropout', 0) or 0
        self.activation_dropout_module = FairseqDropout(float(activation_dropout_p), module_name=self.__class__.__name__)

    def forward(self, x):
        x = self.activation_fn(self.fc1(x))
        if not hasattr(self.cfg, 'adapter_dropout') or self.cfg.adapter_dropout:
            x = self.activation_dropout_module(x)
        x = self.fc2(x)
        return x


class XMODTransformerEncoderLayerBase(TransformerEncoderLayer):
    """Encoder layer block.

    In the original paper each operation (multi-head attention or FFN) is
    postprocessed with: `dropout -> add residual -> layernorm`. In the
    tensor2tensor code they suggest that learning is more robust when
    preprocessing each layer with layernorm and postprocessing with:
    `dropout -> add residual`. We default to the approach in the paper, but the
    tensor2tensor approach can be enabled by setting
    *cfg.encoder.normalize_before* to ``True``.

    Args:
        args (argparse.Namespace): parsed command-line arguments
    """

    def __init__(self, cfg):
        super().__init__(cfg)
        if hasattr(cfg, 'adapter_modules') and cfg.adapter_modules:
            export = getattr(cfg, 'export', False)
            if cfg.adapter_layer_norm:
                self.adapter_layer_norm = LayerNorm(self.embed_dim, export=export)
            self.adapter_modules = nn.ModuleDict(dict())
            if hasattr(self.cfg, 'bottleneck'):
                bottleneck = self.cfg.bottleneck
            else:
                bottleneck = 2
            for language in cfg.languages:
                self.adapter_modules[str(language)] = Adapter(cfg, red_fac=bottleneck)

    def lang_adapter(self, lang_id, x):
        if hasattr(self.cfg, 'adapter_modules') and self.cfg.adapter_modules:
            if lang_id is None:
                lang_id = ['en_XX'] * x.shape[1]
            d_langs = [lang_id[0]]
            lang_lengths = [1]
            for lang in lang_id[1:]:
                if lang == d_langs[-1]:
                    lang_lengths[-1] += 1
                else:
                    d_langs.append(lang)
                    lang_lengths.append(1)
            if not hasattr(self.cfg, 'ln_before_adapter') or not self.cfg.ln_before_adapter:
                residual = x
            if self.cfg.adapter_layer_norm:
                x = self.adapter_layer_norm(x)
            elif self.cfg.adapter_reuse_layer_norm:
                x = self.final_layer_norm(x)
            if hasattr(self.cfg, 'ln_before_adapter') and self.cfg.ln_before_adapter:
                residual = x
            split_x = torch.split(x, lang_lengths, 1)
            x_ = []
            for i, (lang, s_x) in enumerate(zip(d_langs, split_x)):
                lang = lang.replace('_rom', '').replace('_zaw', '')
                x_.append(self.adapter_modules[str(lang)](s_x))
            x = torch.cat(x_, 1)
            x = self.dropout_module(x)
            x = self.residual_connection(x, residual)
        return x

    def forward(self, x, encoder_padding_mask: Optional[Tensor], attn_mask: Optional[Tensor]=None, lang_id: Optional[list]=None):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, seq_len)` where padding elements are indicated by ``1``.
            attn_mask (ByteTensor): binary tensor of shape `(tgt_len, src_len)`,
                where `tgt_len` is the length of output and `src_len` is the
                length of input, though here both are equal to `seq_len`.
                `attn_mask[tgt_i, src_j] = 1` means that when calculating the
                embedding for `tgt_i`, we exclude (mask out) `src_j`. This is
                useful for strided self-attention.

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if attn_mask is not None:
            attn_mask = attn_mask.masked_fill(attn_mask, -100000000.0)
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, need_weights=False, attn_mask=attn_mask)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        x = self.lang_adapter(lang_id, x)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        return x


class XMODEncoder(RobertaEncoder):
    """XMOD encoder."""

    def build_encoder(self, args, dictionary, embed_tokens):
        encoder = XMODTransformerEncoder(args, dictionary, embed_tokens)
        encoder.apply(init_bert_params)
        return encoder

    def forward(self, src_tokens, features_only=False, return_all_hiddens=False, masked_tokens=None, lang_id=None, **unused):
        """
        Args:
            src_tokens (LongTensor): input tokens of shape `(batch, src_len)`
            features_only (bool, optional): skip LM head and just return
                features. If True, the output will be of shape
                `(batch, src_len, embed_dim)`.
            return_all_hiddens (bool, optional): also return all of the
                intermediate hidden states (default: False).

        Returns:
            tuple:
                - the LM output of shape `(batch, src_len, vocab)`
                - a dictionary of additional data, where 'inner_states'
                  is a list of hidden states. Note that the hidden
                  states have shape `(src_len, batch, vocab)`.
        """
        x, extra = self.extract_features(src_tokens, return_all_hiddens=return_all_hiddens, lang_id=lang_id)
        if not features_only:
            x = self.output_layer(x, masked_tokens=masked_tokens)
        return x, extra

    def extract_features(self, src_tokens, return_all_hiddens=False, lang_id=None, **kwargs):
        encoder_out = self.sentence_encoder(src_tokens, return_all_hiddens=return_all_hiddens, lang_id=lang_id, token_embeddings=kwargs.get('token_embeddings', None))
        features = encoder_out['encoder_out'][0].transpose(0, 1)
        inner_states = encoder_out['encoder_states'] if return_all_hiddens else None
        return features, {'inner_states': inner_states}


class BeamableMM(nn.Module):
    """This module provides an optimized MM for beam decoding with attention.

    It leverage the fact that the source-side of the input is replicated beam
    times and the target-side of the input is of width one. This layer speeds up
    inference by replacing the inputs {(bsz x 1 x nhu), (bsz x sz2 x nhu)}
    with smaller inputs {(bsz/beam x beam x nhu), (bsz/beam x sz2 x nhu)}.
    """

    def __init__(self, beam_size=None):
        super(BeamableMM, self).__init__()
        self.beam_size = beam_size

    def forward(self, input1, input2):
        if not self.training and self.beam_size is not None and input1.dim() == 3 and input1.size(1) == 1:
            bsz, beam = input1.size(0), self.beam_size
            input1 = input1[:, 0, :].unfold(0, beam, beam).transpose(2, 1)
            input2 = input2.unfold(0, beam, beam)[:, :, :, 0]
            if input1.size(0) == 1:
                output = torch.mm(input1[0, :, :], input2[0, :, :])
            else:
                output = input1.bmm(input2)
            return output.view(bsz, 1, -1)
        else:
            return input1.bmm(input2)

    def set_beam_size(self, beam_size):
        self.beam_size = beam_size


class DynamicConv_scripatable(nn.Module, FairseqIncrementalState):
    """Dynamic lightweight convolution taking T x B x C inputs
    Args:
        input_size: # of channels of the input
        kernel_size: convolution channels
        padding_l: padding to the left when using "same" padding
        num_heads: number of heads used. The weight is of shape (num_heads, 1, kernel_size)
        weight_dropout: the drop rate of the DropConnect to drop the weight
        weight_softmax: normalize the weight with softmax before the convolution
        renorm_padding: re-normalize the filters to ignore the padded part (only the non-padding parts sum up to 1)
        bias: use bias
        conv_bias: bias of the convolution
        query_size: specified when feeding a different input as the query
        in_proj: project the input and generate the filter together

    Shape:
        Input: TxBxC, i.e. (timesteps, batch_size, input_size)
        Output: TxBxC, i.e. (timesteps, batch_size, input_size)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias:   the learnable bias of the module of shape `(input_size)`
    """

    def __init__(self, input_size, kernel_size=1, padding_l=None, num_heads=1, weight_dropout=0.0, weight_softmax=False, renorm_padding=False, bias=False, conv_bias=False, query_size=None, in_proj=False):
        super().__init__()
        self.input_size = input_size
        self.query_size = input_size if query_size is None else query_size
        self.kernel_size = kernel_size
        self.padding_l = padding_l
        self.num_heads = num_heads
        self.weight_dropout_module = FairseqDropout(weight_dropout, module_name=self.__class__.__name__)
        self.weight_softmax = weight_softmax
        self.renorm_padding = renorm_padding
        if in_proj:
            self.weight_linear = Linear(self.input_size, self.input_size + num_heads * kernel_size * 1)
        else:
            self.weight_linear = Linear(self.query_size, num_heads * kernel_size * 1, bias=bias)
        self.in_proj = self.weight_linear.out_features == self.input_size + self.num_heads * self.kernel_size
        self.has_conv_bias = conv_bias
        self.conv_bias = nn.Parameter(torch.Tensor(input_size).view(1, 1, -1))
        self.init_incremental_state()
        self.reset_parameters()

    def reset_parameters(self):
        self.weight_linear.reset_parameters()
        if self.has_conv_bias:
            nn.init.constant_(self.conv_bias, 0.0)

    def forward(self, x, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, query: Optional[Tensor]=None):
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
            unfold: unfold the input or not. If not, we use the matrix trick instead
            query: use the specified query to predict the conv filters
        """
        assert query is None or not self.in_proj
        if query is None:
            query = x
        output = self._forward_unfolded(x, incremental_state, query)
        if self.has_conv_bias:
            output = output + self.conv_bias
        return output

    def _forward_unfolded(self, x, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], query):
        """The conventional implementation of convolutions.
        Unfolding the input by having a window shifting to the right."""
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        TxBxH = T * B * H
        if self.in_proj:
            proj = self.weight_linear(x)
            x = proj.narrow(2, 0, self.input_size).contiguous()
            weight = proj.narrow(2, self.input_size, H * K).contiguous().view(TxBxH, -1)
        else:
            weight = self.weight_linear(query).view(TxBxH, -1)
        assert not self.renorm_padding or incremental_state is not None
        if incremental_state is not None:
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is not None:
                x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)
            else:
                x_unfold = x.unsqueeze(3).clone()
            if self.kernel_size > 1:
                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size + 1:])
            x_unfold = x_unfold.view(TxBxH, R, -1)
        else:
            padding_l = self.padding_l
            if K > T and padding_l == K - 1:
                weight = weight.narrow(1, K - T, T)
                K, padding_l = T, T - 1
            x_unfold = unfold1d(x, K, padding_l, 0.0)
            x_unfold = x_unfold.view(TxBxH, R, K)
        if self.weight_softmax and not self.renorm_padding:
            weight = F.softmax(weight, dim=1)
        weight = weight.narrow(1, 0, K)
        if incremental_state is not None:
            weight = weight[:, -x_unfold.size(2):]
            K = weight.size(1)
        if self.weight_softmax and self.renorm_padding:
            weight = F.softmax(weight, dim=1)
        weight = self.weight_dropout_module(weight, inplace=False)
        output = torch.bmm(x_unfold, weight.unsqueeze(2))
        output = output.view(T, B, C)
        return output

    def reorder_incremental_state(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_order: Tensor):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]):
        result = self.get_incremental_state(incremental_state, 'input_buffer')
        if result is not None and 'input_buffer' in result:
            return result['input_buffer']
        else:
            return None

    def _set_input_buffer(self, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]], new_buffer: Optional[Tensor]):
        result = self.set_incremental_state(incremental_state, 'input_buffer', {'input_buffer': new_buffer})
        if result is not None:
            incremental_state = result
        return incremental_state

    def extra_repr(self):
        s = '{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, conv_bias={}, renorm_padding={}, in_proj={}'.format(self.input_size, self.kernel_size, self.padding_l, self.num_heads, self.weight_softmax, self.conv_bias is not None, self.renorm_padding, self.in_proj)
        if self.query_size != self.input_size:
            s += ', query_size={}'.format(self.query_size)
        if self.weight_dropout_module.p > 0.0:
            s += ', weight_dropout={}'.format(self.weight_dropout_module.p)
        return s


def logsumexp(x, dim=1):
    return torch.logsumexp(x.float(), dim=dim).type_as(x)


class DynamicCRF(nn.Module):
    """Dynamic CRF layer is used to approximate the traditional
    Conditional Random Fields (CRF)
    $P(y | x) = 1/Z(x) exp(sum_i s(y_i, x) + sum_i t(y_{i-1}, y_i, x))$

    where in this function, we assume the emition scores (s) are given,
    and the transition score is a |V| x |V| matrix $M$

    in the following two aspects:
     (1) it used a low-rank approximation for the transition matrix:
         $M = E_1 E_2^T$
     (2) it used a beam to estimate the normalizing factor Z(x)
    """

    def __init__(self, num_embedding, low_rank=32, beam_size=64):
        super().__init__()
        self.E1 = nn.Embedding(num_embedding, low_rank)
        self.E2 = nn.Embedding(num_embedding, low_rank)
        self.vocb = num_embedding
        self.rank = low_rank
        self.beam = beam_size

    def extra_repr(self):
        return 'vocab_size={}, low_rank={}, beam_size={}'.format(self.vocb, self.rank, self.beam)

    def forward(self, emissions, targets, masks, beam=None):
        """
        Compute the conditional log-likelihood of a sequence of target tokens given emission scores

        Args:
            emissions (`~torch.Tensor`): Emission score are usually the unnormalized decoder output
                ``(batch_size, seq_len, vocab_size)``. We assume batch-first
            targets (`~torch.LongTensor`): Sequence of target token indices
                ``(batch_size, seq_len)
            masks (`~torch.ByteTensor`): Mask tensor with the same size as targets

        Returns:
            `~torch.Tensor`: approximated log-likelihood
        """
        numerator = self._compute_score(emissions, targets, masks)
        denominator = self._compute_normalizer(emissions, targets, masks, beam)
        return numerator - denominator

    def forward_decoder(self, emissions, masks=None, beam=None):
        """
        Find the most likely output sequence using Viterbi algorithm.

        Args:
            emissions (`~torch.Tensor`): Emission score are usually the unnormalized decoder output
                ``(batch_size, seq_len, vocab_size)``. We assume batch-first
            masks (`~torch.ByteTensor`): Mask tensor with the same size as targets

        Returns:
            `~torch.LongTensor`: decoded sequence from the CRF model
        """
        return self._viterbi_decode(emissions, masks, beam)

    def _compute_score(self, emissions, targets, masks=None):
        batch_size, seq_len = targets.size()
        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]
        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)
        scores = emission_scores
        scores[:, 1:] += transition_scores
        if masks is not None:
            scores = scores * masks.type_as(scores)
        return scores.sum(-1)

    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):
        beam = beam if beam is not None else self.beam
        batch_size, seq_len = emissions.size()[:2]
        if targets is not None:
            _emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))
            beam_targets = _emissions.topk(beam, 2)[1]
            beam_emission_scores = emissions.gather(2, beam_targets)
        else:
            beam_emission_scores, beam_targets = emissions.topk(beam, 2)
        beam_transition_score1 = self.E1(beam_targets[:, :-1])
        beam_transition_score2 = self.E2(beam_targets[:, 1:])
        beam_transition_matrix = torch.bmm(beam_transition_score1.view(-1, beam, self.rank), beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))
        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)
        score = beam_emission_scores[:, 0]
        for i in range(1, seq_len):
            next_score = score[:, :, None] + beam_transition_matrix[:, i - 1]
            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]
            if masks is not None:
                score = torch.where(masks[:, i:i + 1], next_score, score)
            else:
                score = next_score
        return logsumexp(score, dim=1)

    def _viterbi_decode(self, emissions, masks=None, beam=None):
        beam = beam if beam is not None else self.beam
        batch_size, seq_len = emissions.size()[:2]
        beam_emission_scores, beam_targets = emissions.topk(beam, 2)
        beam_transition_score1 = self.E1(beam_targets[:, :-1])
        beam_transition_score2 = self.E2(beam_targets[:, 1:])
        beam_transition_matrix = torch.bmm(beam_transition_score1.view(-1, beam, self.rank), beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))
        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)
        traj_tokens, traj_scores = [], []
        finalized_tokens, finalized_scores = [], []
        score = beam_emission_scores[:, 0]
        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()
        for i in range(1, seq_len):
            traj_scores.append(score)
            _score = score[:, :, None] + beam_transition_matrix[:, i - 1]
            _score, _index = _score.max(dim=1)
            _score = _score + beam_emission_scores[:, i]
            if masks is not None:
                score = torch.where(masks[:, i:i + 1], _score, score)
                index = torch.where(masks[:, i:i + 1], _index, dummy)
            else:
                score, index = _score, _index
            traj_tokens.append(index)
        best_score, best_index = score.max(dim=1)
        finalized_tokens.append(best_index[:, None])
        finalized_scores.append(best_score[:, None])
        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):
            previous_index = finalized_tokens[-1]
            finalized_tokens.append(idx.gather(1, previous_index))
            finalized_scores.append(scs.gather(1, previous_index))
        finalized_tokens.reverse()
        finalized_tokens = torch.cat(finalized_tokens, 1)
        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]
        finalized_scores.reverse()
        finalized_scores = torch.cat(finalized_scores, 1)
        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]
        return finalized_scores, finalized_tokens


class dynamicconvFunction(Function):

    @staticmethod
    def forward(ctx, x, weights, padding_l):
        ctx.padding_l = padding_l
        outputs = dynamicconv_cuda.forward(x, weights, padding_l)
        variables = [x, weights]
        ctx.save_for_backward(*variables)
        return outputs[0]

    @staticmethod
    def backward(ctx, grad_output):
        outputs = dynamicconv_cuda.backward(grad_output.contiguous(), ctx.padding_l, *ctx.saved_tensors)
        grad_input, grad_weights = outputs
        return grad_input, grad_weights, None


@with_incremental_state
class DynamicconvLayer(nn.Module):

    def __init__(self, input_size, kernel_size=1, padding_l=None, weight_softmax=False, num_heads=1, weight_dropout=0.0, bias=False, renorm_padding=False, conv_bias=False, query_size=None):
        super(DynamicconvLayer, self).__init__()
        self.input_size = input_size
        self.query_size = input_size if query_size is None else query_size
        self.kernel_size = kernel_size
        self.padding_l = padding_l
        self.num_heads = num_heads
        self.weight_softmax = weight_softmax
        self.weight_dropout_module = FairseqDropout(weight_dropout, module_name=self.__class__.__name__)
        self.renorm_padding = renorm_padding
        self.bias = bias
        self.weight_linear = nn.Linear(input_size, num_heads * kernel_size, bias)
        if conv_bias:
            self.conv_bias = nn.Parameter(torch.Tensor(input_size))
        else:
            self.conv_bias = None
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight_linear.weight)
        if self.conv_bias is not None:
            nn.init.constant_(self.conv_bias, 0.0)
            nn.init.constant_(self.weight_linaer.bias, 0.0)

    def forward(self, x, incremental_state=None, query=None, unfold=None):
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        if incremental_state is not None:
            unfold = x.size(0) > 512 if unfold is None else unfold
            unfold = unfold or incremental_state is not None
            assert query is None
            if query is None:
                query = x
            if unfold:
                output = self._forward_unfolded(x, incremental_state, query)
            else:
                output = self._forward_expanded(x, incremental_state, query)
            if self.conv_bias is not None:
                output = output + self.conv_bias.view(1, 1, -1)
            return output
        else:
            weight = self.weight_linear(x).view(T, B, H, K)
            if self.weight_softmax:
                weight = F.softmax(weight, dim=-1)
            if self.weight_dropout_module.p:
                weight = self.weight_dropout_module(weight)
            weight = weight.permute(1, 2, 3, 0).contiguous()
            self.filters = weight
            x = x.permute(1, 2, 0).contiguous()
            output = dynamicconvFunction.apply(x, weight, self.padding_l).permute(2, 0, 1)
            if self.conv_bias is not None:
                output = output + self.conv_bias.view(1, 1, -1)
            return output

    def reorder_incremental_state(self, incremental_state, new_order):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state):
        return utils.get_incremental_state(self, incremental_state, 'input_buffer')

    def _set_input_buffer(self, incremental_state, new_buffer):
        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)

    def _forward_unfolded(self, x, incremental_state, query):
        """The conventional implementation of convolutions.
        Unfolding the input by having a window shifting to the right."""
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        weight = self.weight_linear(query).view(T * B * H, -1)
        assert not self.renorm_padding or incremental_state is not None
        if incremental_state is not None:
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is None:
                input_buffer = x.new()
            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)
            if self.kernel_size > 1:
                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size + 1:])
            x_unfold = x_unfold.view(T * B * H, R, -1)
        else:
            padding_l = self.padding_l
            if K > T and padding_l == K - 1:
                weight = weight.narrow(1, K - T, T)
                K, padding_l = T, T - 1
            x_unfold = unfold1d(x, K, padding_l, 0)
            x_unfold = x_unfold.view(T * B * H, R, K)
        if self.weight_softmax and not self.renorm_padding:
            weight = F.softmax(weight, dim=1)
        weight = weight.narrow(1, 0, K)
        if incremental_state is not None:
            weight = weight[:, -x_unfold.size(2):]
            K = weight.size(1)
        if self.weight_softmax and self.renorm_padding:
            weight = F.softmax(weight, dim=1)
        weight = self.weight_dropout_module(weight, inplace=False)
        output = torch.bmm(x_unfold, weight.unsqueeze(2))
        output = output.view(T, B, C)
        return output

    def _forward_expanded(self, x, incremental_stat, query):
        """Turn the convolution filters into band matrices and do matrix multiplication.
        This is faster when the sequence is short, but less memory efficient.
        This is not used in the decoder during inference.
        """
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        weight = self.weight_linear(query).view(T * B * H, -1)
        if not self.renorm_padding:
            if self.weight_softmax:
                weight = F.softmax(weight, dim=1)
            weight = self.weight_dropout_module(weight, inplace=False)
        weight = weight.narrow(1, 0, K).contiguous()
        weight = weight.view(T, B * H, K).transpose(0, 1)
        x = x.view(T, B * H, R).transpose(0, 1)
        if self.weight_softmax and self.renorm_padding:
            weight_expanded = weight.new(B * H, T, T + K - 1).fill_(float('-inf'))
            weight_expanded.as_strided((B * H, T, K), (T * (T + K - 1), T + K, 1)).copy_(weight)
            weight_expanded = weight_expanded.narrow(2, self.padding_l, T)
            weight_expanded = F.softmax(weight_expanded, dim=2)
            weight_expanded = self.weight_dropout_module(weight_expanded, inplace=False)
        else:
            P = self.padding_l
            if K > T and P == K - 1:
                weight = weight.narrow(2, K - T, T)
                K, P = T, T - 1
            weight_expanded = weight.new_zeros(B * H, T, T + K - 1, requires_grad=False)
            weight_expanded.as_strided((B * H, T, K), (T * (T + K - 1), T + K, 1)).copy_(weight)
            weight_expanded = weight_expanded.narrow(2, P, T)
        output = torch.bmm(weight_expanded, x)
        output = output.transpose(0, 1).contiguous().view(T, B, C)
        return output


class Fp32BatchNorm(nn.Module):

    def __init__(self, sync=False, *args, **kwargs):
        super().__init__()
        if sync:
            if utils.get_global_world_size() == 1:
                sync = False
        if sync:
            self.bn = nn.SyncBatchNorm(*args, **kwargs)
        else:
            self.bn = nn.BatchNorm1d(*args, **kwargs)
        self.sync = sync

    def forward(self, input):
        if self.bn.running_mean.dtype != torch.float:
            if self.sync:
                self.bn.running_mean = self.bn.running_mean.float()
                self.bn.running_var = self.bn.running_var.float()
                if self.bn.affine:
                    try:
                        self.bn.weight = self.bn.weight.float()
                        self.bn.bias = self.bn.bias.float()
                    except:
                        self.bn.float()
            else:
                self.bn.float()
        output = self.bn(input.float())
        return output.type_as(input)


class Fp32InstanceNorm(nn.InstanceNorm1d):

    def __init__(self, *args, **kwargs):
        self.transpose_last = 'transpose_last' in kwargs and kwargs['transpose_last']
        if 'transpose_last' in kwargs:
            del kwargs['transpose_last']
        super().__init__(*args, **kwargs)

    def forward(self, input):
        if self.transpose_last:
            input = input.transpose(1, 2)
        output = F.instance_norm(input.float(), running_mean=self.running_mean, running_var=self.running_var, weight=self.weight.float() if self.weight is not None else None, bias=self.bias.float() if self.bias is not None else None, use_input_stats=self.training or not self.track_running_stats, momentum=self.momentum, eps=self.eps)
        if self.transpose_last:
            output = output.transpose(1, 2)
        return output.type_as(input)


class Chunk(nn.Module):

    def __init__(self, chunks, fn, along_dim=-1):
        super().__init__()
        self.dim = along_dim
        self.chunks = chunks
        self.fn = fn

    def forward(self, x, **kwargs):
        if self.chunks <= 1:
            return self.fn(x, **kwargs)
        chunks = x.chunk(self.chunks, dim=self.dim)
        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim=self.dim)


class PreNorm(nn.ModuleList):

    def __init__(self, norm_class, dim, fn):
        super().__init__()
        self.norm = norm_class(dim)
        self.fn = fn

    def forward(self, x, **kwargs):
        x = self.norm(x)
        return self.fn(x, **kwargs)


def map_first_tuple_or_el(x, fn):
    if isinstance(x, tuple):
        return (fn(x[0]),) + x[1:]
    return fn(x)


class ReZero(nn.Module):

    def __init__(self, fn):
        super().__init__()
        self.residual_weight = nn.Parameter(torch.zeros(1))
        self.fn = fn

    def forward(self, x, **kwargs):
        x = self.fn(x, **kwargs)
        return map_first_tuple_or_el(x, lambda t: t * self.residual_weight)


class ScaleNorm(nn.Module):

    def __init__(self, dim, eps=1e-05):
        super().__init__()
        self.g = nn.Parameter(torch.ones(1))
        self.eps = eps

    def forward(self, x):

        def norm(t):
            n = torch.norm(t, dim=-1, keepdim=True).clamp(min=self.eps)
            return t / n * self.g
        return map_first_tuple_or_el(x, norm)


def identity(x, *args, **kwargs):
    return x


class ProjectInOut(nn.Module):

    def __init__(self, fn, dim_in, dim_out, project_out=True):
        super().__init__()
        self.fn = fn
        self.project_in = nn.Linear(dim_in, dim_out)
        self.project_out = nn.Linear(dim_out, dim_in) if project_out else identity

    def forward(self, x, **kwargs):
        x = self.project_in(x)
        x, loss = self.fn(x, **kwargs)
        x = self.project_out(x)
        return x, loss


class MatrixMultiply(nn.Module):

    def __init__(self, tensor, transpose=False):
        super().__init__()
        self.tensor = tensor
        self.transpose = transpose

    def forward(self, x):
        tensor = self.tensor
        if self.transpose:
            tensor = tensor.t()
        return x @ tensor


class FixedPositionalEmbedding(nn.Module):

    def __init__(self, dim, max_seq_len):
        super().__init__()
        inv_freq = 1.0 / 10000 ** (torch.arange(0, dim, 2).float() / dim)
        position = torch.arange(0, max_seq_len, dtype=torch.float)
        sinusoid_inp = torch.einsum('i,j->ij', position, inv_freq)
        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)
        self.register_buffer('emb', emb)

    def forward(self, x):
        return self.emb[None, :x.shape[1], :]


class GELU_(nn.Module):

    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))


GELU = nn.GELU if hasattr(nn, 'GELU') else GELU_


class FeedForward(nn.Module):

    def __init__(self, dim, mult=4, dropout=0.0, activation=None, glu=False):
        super().__init__()
        activation = default(activation, GELU)
        self.glu = glu
        self.w1 = nn.Linear(dim, dim * mult * (2 if glu else 1))
        self.act = activation()
        self.dropout = nn.Dropout(dropout)
        self.w2 = nn.Linear(dim * mult, dim)

    def forward(self, x, **kwargs):
        if not self.glu:
            x = self.w1(x)
            x = self.act(x)
        else:
            x, v = self.w1(x).chunk(2, dim=-1)
            x = self.act(x) * v
        x = self.dropout(x)
        x = self.w2(x)
        return x


class lightconvFunction(Function):

    @staticmethod
    def forward(ctx, x, weights, padding_l):
        ctx.padding_l = padding_l
        outputs = lightconv_cuda.forward(x, weights, padding_l)
        variables = [x, weights]
        ctx.save_for_backward(*variables)
        return outputs[0]

    @staticmethod
    def backward(ctx, grad_output):
        outputs = lightconv_cuda.backward(grad_output.contiguous(), ctx.padding_l, *ctx.saved_tensors)
        grad_input, grad_weights = outputs
        return grad_input, grad_weights, None


@with_incremental_state
class LightconvLayer(nn.Module):

    def __init__(self, input_size, kernel_size=1, padding_l=None, weight_softmax=False, num_heads=1, weight_dropout=0.0, bias=False):
        super(LightconvLayer, self).__init__()
        self.input_size = input_size
        self.kernel_size = kernel_size
        self.padding_l = padding_l
        self.num_heads = num_heads
        self.weight_softmax = weight_softmax
        self.weight_dropout_module = FairseqDropout(weight_dropout, module_name=self.__class__.__name__)
        self.weight = nn.Parameter(torch.Tensor(num_heads, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(input_size))
        else:
            self.bias = None
        self.reset_parameters()

    def upgrade_state_dict_named(self, state_dict, name):
        prefix = name + '.' if name != '' else ''
        for k, v in state_dict.items():
            if k.endswith(prefix + 'weight'):
                if v.dim() == 3 and v.size(1) == 1:
                    state_dict[k] = v.squeeze(1)

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.constant_(self.bias, 0.0)

    def forward(self, x, incremental_state=None):
        if incremental_state is not None:
            T, B, C = x.size()
            K, H = self.kernel_size, self.num_heads
            R = C // H
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is None:
                input_buffer = x.new()
            x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)
            if self.kernel_size > 1:
                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size + 1:])
            x_unfold = x_unfold.view(T * B * H, R, -1)
            weight = self.weight
            if self.weight_softmax:
                weight = F.softmax(weight.float(), dim=1).type_as(weight)
            weight = weight[:, -x_unfold.size(2):]
            K = weight.size(1)
            weight = weight.view(1, H, K).expand(T * B, H, K).contiguous().view(T * B * H, K, 1)
            weight = self.weight_dropout_module(weight)
            output = torch.bmm(x_unfold, weight)
            output = output.view(T, B, C)
            return output
        else:
            x = x.permute(1, 2, 0).contiguous()
            weight = self.weight
            if self.weight_softmax:
                weight = F.softmax(self.weight, -1)
            if self.weight_dropout_module.p:
                weight = self.weight_dropout_module(weight)
            return lightconvFunction.apply(x, weight, self.padding_l).permute(2, 0, 1)

    def reorder_incremental_state(self, incremental_state, new_order):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state):
        return utils.get_incremental_state(self, incremental_state, 'input_buffer')

    def _set_input_buffer(self, incremental_state, new_buffer):
        return utils.set_incremental_state(self, incremental_state, 'input_buffer', new_buffer)

    def half(self):
        return self._apply(lambda t: t.half() if t.is_floating_point() else t)


class LightweightConv1d(nn.Module):
    """Lightweight Convolution assuming the input is BxCxT
    This is just an example that explains LightConv clearer than the TBC version.
    We don't use this module in the model.

    Args:
        input_size: # of channels of the input and output
        kernel_size: convolution channels
        padding: padding
        num_heads: number of heads used. The weight is of shape
            `(num_heads, 1, kernel_size)`
        weight_softmax: normalize the weight with softmax before the convolution

    Shape:
        Input: BxCxT, i.e. (batch_size, input_size, timesteps)
        Output: BxCxT, i.e. (batch_size, input_size, timesteps)

    Attributes:
        weight: the learnable weights of the module of shape
            `(num_heads, 1, kernel_size)`
        bias: the learnable bias of the module of shape `(input_size)`
    """

    def __init__(self, input_size, kernel_size=1, padding=0, num_heads=1, weight_softmax=False, bias=False, weight_dropout=0.0):
        super().__init__()
        self.input_size = input_size
        self.kernel_size = kernel_size
        self.num_heads = num_heads
        self.padding = padding
        self.weight_softmax = weight_softmax
        self.weight = nn.Parameter(torch.Tensor(num_heads, 1, kernel_size))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(input_size))
        else:
            self.bias = None
        self.weight_dropout_module = FairseqDropout(weight_dropout, module_name=self.__class__.__name__)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.constant_(self.bias, 0.0)

    def forward(self, input):
        """
        input size: B x C x T
        output size: B x C x T
        """
        B, C, T = input.size()
        H = self.num_heads
        weight = self.weight
        if self.weight_softmax:
            weight = F.softmax(weight, dim=-1)
        weight = self.weight_dropout_module(weight)
        input = input.view(-1, H, T)
        output = F.conv1d(input, weight, padding=self.padding, groups=self.num_heads)
        output = output.view(B, C, T)
        if self.bias is not None:
            output = output + self.bias.view(1, -1, 1)
        return output


class PositionalEncoding(nn.Module):
    """Positional encoding.

    Args:
        d_model: Embedding dimension.
        dropout_rate: Dropout rate.
        max_len: Maximum input length.
        reverse: Whether to reverse the input position.
    """

    def __init__(self, d_model, dropout_rate, max_len=5000, reverse=False):
        """Construct an PositionalEncoding object."""
        super(PositionalEncoding, self).__init__()
        self.d_model = d_model
        self.reverse = reverse
        self.xscale = math.sqrt(self.d_model)
        self.dropout = nn.Dropout(p=dropout_rate)
        self.pe = None
        self.extend_pe(torch.tensor(0.0).expand(1, max_len))

    def extend_pe(self, x):
        """Reset the positional encodings."""
        if self.pe is not None:
            if self.pe.size(1) >= x.size(1):
                if self.pe.dtype != x.dtype or self.pe.device != x.device:
                    self.pe = self.pe
                return
        pe = torch.zeros(x.size(1), self.d_model)
        if self.reverse:
            position = torch.arange(x.size(1) - 1, -1, -1.0, dtype=torch.float32).unsqueeze(1)
        else:
            position = torch.arange(0, x.size(1), dtype=torch.float32).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float32) * -(math.log(10000.0) / self.d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.pe = pe

    def forward(self, x: torch.Tensor):
        """Add positional encoding.
        Args:
            x (torch.Tensor): Input tensor B X T X C
        Returns:
            torch.Tensor: Encoded tensor B X T X C
        """
        self.extend_pe(x)
        x = x * self.xscale + self.pe[:, :x.size(1)]
        return self.dropout(x)


class PQConv2d(nn.Module):
    """
    Quantized counterpart of nn.Conv2d module. Stores the centroid, the assignments
    and the non-quantized biases. The full weight is re-instantiated at each forward
    pass and autograd automatically computes the gradients with respect to the
    centroids.

    Args:
        - centroids: centroids of size n_centroids x block_size
        - assignments: assignments of the centroids to the subvectors
          of size self.out_channels x n_blocks
        - bias: the non-quantized bias, must be either torch.Tensor or None

    Remarks:
        - We refer the reader to the official documentation of the nn.Conv2d module
          for the other arguments and the behavior of the module.
        - Performance tests on GPU show that this implementation is 10% slower than
          the non-quantized nn.Conv2d module for a standard training loop.
        - During the backward, the gradients are averaged by cluster and not summed.
          This explains the hook registered to the centroids.
    """

    def __init__(self, centroids, assignments, bias, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, padding_mode='zeros'):
        super(PQConv2d, self).__init__()
        self.block_size = centroids.size(1)
        self.n_centroids = centroids.size(0)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.kernel_size = _pair(kernel_size)
        self.stride = _pair(stride)
        self.padding = _pair(padding)
        self.dilation = _pair(dilation)
        self.groups = groups
        self.padding_mode = padding_mode
        if in_channels // groups * np.prod(self.kernel_size) % self.block_size != 0:
            raise ValueError('Wrong PQ sizes')
        if len(assignments) % out_channels != 0:
            raise ValueError('Wrong PQ sizes')
        if in_channels % groups != 0:
            raise ValueError('in_channels must be divisible by groups')
        if out_channels % groups != 0:
            raise ValueError('out_channels must be divisible by groups')
        self.centroids = nn.Parameter(centroids, requires_grad=True)
        self.register_buffer('assignments', assignments)
        self.register_buffer('counts', torch.bincount(assignments).type_as(centroids))
        if bias is not None:
            self.bias = nn.Parameter(bias)
        else:
            self.register_parameter('bias', None)
        self.centroids.register_hook(lambda x: x / self.counts[:, None])

    @property
    def weight(self):
        return self.centroids[self.assignments].reshape(-1, self.out_channels, self.block_size).permute(1, 0, 2).reshape(self.out_channels, self.in_channels // self.groups, *self.kernel_size)

    def forward(self, x):
        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)

    def extra_repr(self):
        s = '{in_channels}, {out_channels}, kernel_size={kernel_size}, stride={stride}'
        if self.padding != (0,) * len(self.padding):
            s += ', padding={padding}'
        if self.dilation != (1,) * len(self.dilation):
            s += ', dilation={dilation}'
        if self.groups != 1:
            s += ', groups={groups}'
        if self.bias is None:
            s += ', bias=False'
        if self.padding_mode != 'zeros':
            s += ', padding_mode={padding_mode}'
        s += ', n_centroids={n_centroids}, block_size={block_size}'
        return s.format(**self.__dict__)


class PQEmbedding(nn.Module):
    """
    Quantized counterpart of nn.Embedding module. Stores the centroids and
    the assignments. The full weight is re-instantiated at each forward
    pass.

    Args:
        - centroids: centroids of size n_centroids x block_size
        - assignments: assignments of the centroids to the subvectors
          of size self.out_features x n_blocks
        - bias: the non-quantized bias

    Remarks:
        - We refer the reader to the official documentation of the nn.Embedding module
          for the other arguments and the behavior of the module
        - Performance tests on GPU show that this implementation is 10% slower than
          the non-quantized nn.Embedding module for a standard training loop.
    """

    def __init__(self, centroids, assignments, num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None):
        super(PQEmbedding, self).__init__()
        self.block_size = centroids.size(1)
        self.n_centroids = centroids.size(0)
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'
            elif padding_idx < 0:
                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'
                padding_idx = self.num_embeddings + padding_idx
        self.padding_idx = padding_idx
        self.max_norm = max_norm
        self.norm_type = norm_type
        self.scale_grad_by_freq = scale_grad_by_freq
        self.sparse = sparse
        if self.embedding_dim % self.block_size != 0:
            raise ValueError('Wrong PQ sizes')
        if len(assignments) % self.num_embeddings != 0:
            raise ValueError('Wrong PQ sizes')
        self.centroids = nn.Parameter(centroids, requires_grad=True)
        self.register_buffer('assignments', assignments)
        self.register_buffer('counts', torch.bincount(assignments).type_as(centroids))

    @property
    def weight(self):
        return self.centroids[self.assignments].reshape(-1, self.num_embeddings, self.block_size).permute(1, 0, 2).flatten(1, 2)

    def forward(self, input):
        return F.embedding(input, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)

    def extra_repr(self):
        s = '{num_embeddings}, {embedding_dim}'
        if self.padding_idx is not None:
            s += ', padding_idx={padding_idx}'
        if self.max_norm is not None:
            s += ', max_norm={max_norm}'
        if self.norm_type != 2:
            s += ', norm_type={norm_type}'
        if self.scale_grad_by_freq is not False:
            s += ', scale_grad_by_freq={scale_grad_by_freq}'
        if self.sparse is not False:
            s += ', sparse=True'
        s += ', n_centroids={n_centroids}, block_size={block_size}'
        return s.format(**self.__dict__)


class PQLinear(nn.Module):
    """
    Quantized counterpart of nn.Linear module. Stores the centroid, the assignments
    and the non-quantized biases. The full weight is re-instantiated at each forward
    pass.

    Args:
        - centroids: centroids of size n_centroids x block_size
        - assignments: assignments of the centroids to the subvectors
          of size self.out_features x n_blocks
        - bias: the non-quantized bias

    Remarks:
        - We refer the reader to the official documentation of the nn.Linear module
          for the other arguments and the behavior of the module
        - Performance tests on GPU show that this implementation is 15% slower than
          the non-quantized nn.Linear module for a standard training loop.
    """

    def __init__(self, centroids, assignments, bias, in_features, out_features):
        super(PQLinear, self).__init__()
        self.block_size = centroids.size(1)
        self.n_centroids = centroids.size(0)
        self.in_features = in_features
        self.out_features = out_features
        if self.in_features % self.block_size != 0:
            raise ValueError('Wrong PQ sizes')
        if len(assignments) % self.out_features != 0:
            raise ValueError('Wrong PQ sizes')
        self.centroids = nn.Parameter(centroids, requires_grad=True)
        self.register_buffer('assignments', assignments)
        self.register_buffer('counts', torch.bincount(assignments).type_as(centroids))
        if bias is not None:
            self.bias = nn.Parameter(bias)
        else:
            self.register_parameter('bias', None)

    @property
    def weight(self):
        return self.centroids[self.assignments].reshape(-1, self.out_features, self.block_size).permute(1, 0, 2).flatten(1, 2)

    def forward(self, x):
        return F.linear(x, self.weight, self.bias)

    def extra_repr(self):
        return f'in_features={self.in_features},                 out_features={self.out_features},                 n_centroids={self.n_centroids},                 block_size={self.block_size},                 bias={self.bias is not None}'


def emulate_int(w, bits, method, scale=None, zero_point=None):
    q = globals()[f'emulate_int8_{method}']
    return q(w, scale=scale, zero_point=zero_point, bits=bits)


class IntConv2d(_ConvNd):
    """
    Quantized counterpart of the nn.Conv2d module that applies QuantNoise during training.

    Args:
        - standard nn.Conv2d parameters
        - p: amount of noise to inject (0 = no quantization, 1 = quantize all the weights)
        - bits: number of bits
        - method: choose among {"tensor", "histogram", "channel"}
        - update_step: recompute scale and zero_point every update_steps iterations

    Remarks:
        - We use the straight-thgourh estimator so that the gradients
          back-propagate nicely in the network, this is implemented with
          the detach() trick
        - Parameters scale and zero_point are recomputed every update_step
          forward pass to reduce the overhead
        - At test time, the weights are fully quantized
    """

    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', p=0, bits=8, method='histogram', update_step=1000):
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        super(IntConv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, False, _pair(0), groups, bias, padding_mode)
        self.p = p
        self.bits = bits
        self.method = method
        self.update_step = update_step
        self.counter = 0

    def _conv_forward(self, input, weight):
        if self.padding_mode != 'zeros':
            return F.conv2d(F.pad(input, self._padding_repeated_twice, mode=self.padding_mode), weight, self.bias, self.stride, _pair(0), self.dilation, self.groups)
        return F.conv2d(input, weight, self.bias, self.stride, self.padding, self.dilation, self.groups)

    def forward(self, input):
        p = self.p if self.training else 1
        if self.counter % self.update_step == 0:
            self.scale = None
            self.zero_point = None
        self.counter += 1
        weight_quantized, self.scale, self.zero_point = emulate_int(self.weight.detach(), bits=self.bits, method=self.method, scale=self.scale, zero_point=self.zero_point)
        mask = torch.zeros_like(self.weight)
        mask.bernoulli_(1 - p)
        noise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)
        clamp_low = -self.scale * self.zero_point
        clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)
        weight = torch.clamp(self.weight, clamp_low.item(), clamp_high.item()) + noise.detach()
        output = self._conv_forward(input, weight)
        return output

    def extra_repr(self):
        return 'in_channels={}, out_channels={}, kernel_size={}, stride={}, padding={}, dilation={}, groups={}, bias={}, quant_noise={}, bits={}, method={}'.format(self.in_channels, self.out_channels, self.kernel_size, self.stride, self.padding, self.dilation, self.groups, self.bias is not None, self.p, self.bits, self.method)


class IntEmbedding(nn.Module):
    """
    Quantized counterpart of the nn.Embedding module that applies QuantNoise during training.

    Args:
        - num_embeddings: number of tokens
        - embedding_dim: embedding dimension
        - p: amount of noise to inject (0 = no quantization, 1 = quantize all the weights)
        - bits: number of bits
        - method: choose among {"tensor", "histogram", "channel"}
        - update_step: recompute scale and zero_point every update_steps iterations

    Remarks:
        - We use the straight-through estimator so that the gradients
          back-propagate nicely in the network, this is implemented with
          the detach() trick
        - Parameters scale and zero_point are recomputed every update_step
          forward pass to reduce the overhead
        - At test time, the weights are fully quantized
    """

    def __init__(self, num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None, p=0, update_step=1000, bits=8, method='histogram'):
        super(IntEmbedding, self).__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        if padding_idx is not None:
            if padding_idx > 0:
                assert padding_idx < self.num_embeddings, 'Padding_idx must be within num_embeddings'
            elif padding_idx < 0:
                assert padding_idx >= -self.num_embeddings, 'Padding_idx must be within num_embeddings'
                padding_idx = self.num_embeddings + padding_idx
        self.padding_idx = padding_idx
        self.max_norm = max_norm
        self.norm_type = norm_type
        self.scale_grad_by_freq = scale_grad_by_freq
        if _weight is None:
            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))
            self.reset_parameters()
        else:
            assert list(_weight.shape) == [num_embeddings, embedding_dim], 'Shape of weight does not match num_embeddings and embedding_dim'
            self.weight = nn.Parameter(_weight)
        self.sparse = sparse
        self.p = p
        self.bits = bits
        self.method = method
        self.update_step = update_step
        self.counter = 0

    def reset_parameters(self):
        nn.init.normal_(self.weight)
        if self.padding_idx is not None:
            with torch.no_grad():
                self.weight[self.padding_idx].fill_(0)

    def forward(self, input):
        p = self.p if self.training else 1
        if self.counter % self.update_step == 0:
            self.scale = None
            self.zero_point = None
        self.counter += 1
        weight_quantized, self.scale, self.zero_point = emulate_int(self.weight.detach(), bits=self.bits, method=self.method, scale=self.scale, zero_point=self.zero_point)
        mask = torch.zeros_like(self.weight)
        mask.bernoulli_(1 - p)
        noise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)
        clamp_low = -self.scale * self.zero_point
        clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)
        weight = torch.clamp(self.weight, clamp_low.item(), clamp_high.item()) + noise.detach()
        output = F.embedding(input, weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)
        return output

    def extra_repr(self):
        s = '{num_embeddings}, {embedding_dim}'
        if self.padding_idx is not None:
            s += ', padding_idx={padding_idx}'
        if self.max_norm is not None:
            s += ', max_norm={max_norm}'
        if self.norm_type != 2:
            s += ', norm_type={norm_type}'
        if self.scale_grad_by_freq is not False:
            s += ', scale_grad_by_freq={scale_grad_by_freq}'
        if self.sparse is not False:
            s += ', sparse=True'
        s += 'quant_noise={p}, bits={bits}, method={method}'
        return s.format(**self.__dict__)


class IntLinear(nn.Module):
    """
    Quantized counterpart of the nn.Linear module that applies QuantNoise during training.

    Args:
        - in_features: input features
        - out_features: output features
        - bias: bias or not
        - p: amount of noise to inject (0 = no quantization, 1 = quantize all the weights)
        - bits: number of bits
        - method: choose among {"tensor", "histogram", "channel"}
        - update_step: recompute scale and zero_point every update_steps iterations

    Remarks:
        - We use the straight-through estimator so that the gradients
          back-propagate nicely in the network, this is implemented with
          the detach() trick.
        - Parameters scale and zero_point are recomputed every update_step
          forward pass to reduce the overhead
        - At test time, the weights are fully quantized
    """

    def __init__(self, in_features, out_features, bias=True, p=0, update_step=3000, bits=8, method='histogram'):
        super(IntLinear, self).__init__()
        self.in_features = int(in_features)
        self.out_features = int(out_features)
        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))
        self.chosen_bias = bias
        if self.chosen_bias:
            self.bias = torch.nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()
        self.p = p
        self.bits = bits
        self.method = method
        self.update_step = update_step
        self.counter = 0

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        if self.chosen_bias:
            nn.init.constant_(self.bias, 0.0)
        return

    def forward(self, input):
        p = self.p if self.training else 1
        if self.counter % self.update_step == 0:
            self.scale = None
            self.zero_point = None
        self.counter += 1
        weight_quantized, self.scale, self.zero_point = emulate_int(self.weight.detach(), bits=self.bits, method=self.method, scale=self.scale, zero_point=self.zero_point)
        mask = torch.zeros_like(self.weight)
        mask.bernoulli_(1 - p)
        noise = (weight_quantized - self.weight).masked_fill(mask.bool(), 0)
        clamp_low = -self.scale * self.zero_point
        clamp_high = self.scale * (2 ** self.bits - 1 - self.zero_point)
        weight = torch.clamp(self.weight, clamp_low.item(), clamp_high.item()) + noise.detach()
        output = F.linear(input, weight, self.bias)
        return output

    def extra_repr(self):
        return 'in_features={}, out_features={}, bias={}, quant_noise={}, bits={}, method={}'.format(self.in_features, self.out_features, self.bias is not None, self.p, self.bits, self.method)


class SparseMultiheadAttention(MultiheadAttention):
    """Sparse Multi-Headed Attention.

    "Generating Long Sequences with Sparse Transformers". Implements
    fixed factorized self attention, where l=stride and c=expressivity.
    A(1) includes all words in the stride window and A(2) takes a summary of c
    words from the end of each stride window.
    If is_bidirectional=False, we do not include any words past the current word,
    as in the paper.
    """

    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, self_attention=False, encoder_decoder_attention=False, stride=32, expressivity=8, is_bidirectional=True):
        super().__init__(embed_dim, num_heads, kdim, vdim, dropout, bias, add_bias_kv, add_zero_attn, self_attention, encoder_decoder_attention)
        self.is_bidirectional = is_bidirectional
        self.stride = stride
        self.expressivity = expressivity
        assert self.stride > 0 and self.stride >= self.expressivity

    def compute_checkpoint(self, word_index):
        if word_index % self.stride == 0 and word_index != 0:
            checkpoint_index = word_index - self.expressivity
        else:
            checkpoint_index = math.floor(word_index / self.stride) * self.stride + self.stride - self.expressivity
        return checkpoint_index

    def compute_subset_summaries(self, absolute_max):
        checkpoint_index = self.compute_checkpoint(0)
        subset_two = set()
        while checkpoint_index <= absolute_max - 1:
            summary = set(range(checkpoint_index, min(checkpoint_index + self.expressivity + 1, absolute_max)))
            subset_two = subset_two.union(summary)
            checkpoint_index = self.compute_checkpoint(checkpoint_index + self.stride)
        return subset_two

    def compute_fixed_attention_subset(self, word_index, tgt_len):
        if not self.is_bidirectional:
            absolute_max = word_index + 1
        else:
            absolute_max = tgt_len
        rounded_index = math.floor((word_index + self.stride) / self.stride) * self.stride
        if word_index % self.stride == 0 and word_index != 0:
            subset_one = set(range(word_index - self.stride, min(absolute_max, word_index + 1)))
        else:
            subset_one = set(range(max(0, rounded_index - self.stride), min(absolute_max, rounded_index + 1)))
        subset_two = set()
        if not self.is_bidirectional:
            subset_two = self.compute_subset_summaries(absolute_max)
        return subset_one.union(subset_two)

    def buffered_sparse_mask(self, tensor, tgt_len, src_len):
        assert tgt_len > self.stride
        sparse_mask = torch.empty((tgt_len, src_len)).float().fill_(float('-inf'))
        subset_summaries = set()
        if self.is_bidirectional:
            subset_summaries = self.compute_subset_summaries(tgt_len)
        for i in range(tgt_len):
            fixed_attention_subset = self.compute_fixed_attention_subset(i, tgt_len)
            fixed_attention_subset = fixed_attention_subset.union(subset_summaries)
            included_word_indices = torch.LongTensor(list(fixed_attention_subset))
            sparse_mask[i].index_fill_(0, included_word_indices, 0)
        return sparse_mask.type_as(tensor)

    def apply_sparse_mask(self, attn_weights, tgt_len, src_len, bsz):
        sparse_mask = self.buffered_sparse_mask(attn_weights, tgt_len, src_len)
        sparse_mask = sparse_mask.unsqueeze(0).expand(bsz * self.num_heads, tgt_len, src_len)
        attn_weights += sparse_mask


class AugTransformerDecoderLayerBase(TransformerDecoderLayerBase):
    """Decoder layer block augmented with an additional cross-attention.

    This decoder block is processed with the sequence of the following sub-modules.
        self-attention -> cross-attention (first) -> cross-attention (second) -> FFN

    Args:
        cfg (argparse.Namespace): parsed command-line arguments
        encoder_attn_merge_type (str, optional): the way to combine outputs from
            two cross-attention modules. If "sequential" is set, two cross-attention
            modules are stacked sequentially. If "parallel" is set, they are processed
            in parallel and combined before feeding it to FFN (default: sequential).
        dropnet_ratio (float, optional): a probability to drop each cross-attention
            module during training (default: 0.0).
    """

    def __init__(self, cfg, add_bias_kv=False, add_zero_attn=False, encoder_attn_merge_type='sequential', dropnet_ratio=0.0):
        super().__init__(cfg, no_encoder_attn=False, add_bias_kv=add_bias_kv, add_zero_attn=False)
        self.encoder_attn = self.build_encoder_attention(self.embed_dim, cfg)
        self.encoder_attn_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
        self.encoder_attn2 = self.build_encoder_attention(self.embed_dim, cfg)
        if encoder_attn_merge_type == 'sequential':
            self.encoder_attn_layer_norm2 = LayerNorm(self.embed_dim, export=cfg.export)
        else:
            self.encoder_attn_layer_norm2 = None
        self.encoder_attn_merge_type = encoder_attn_merge_type
        self.dropnet_ratio = dropnet_ratio

    def forward(self, x, encoder_out: Optional[torch.Tensor]=None, encoder_padding_mask: Optional[torch.Tensor]=None, encoder_out_aug: Optional[torch.Tensor]=None, encoder_padding_mask2: Optional[torch.Tensor]=None, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, prev_self_attn_state: Optional[List[torch.Tensor]]=None, prev_attn_state: Optional[List[torch.Tensor]]=None, self_attn_mask: Optional[torch.Tensor]=None, self_attn_padding_mask: Optional[torch.Tensor]=None, need_attn: bool=False, need_head_weights: bool=False):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor, optional): binary
                ByteTensor of shape `(batch, src_len)` where padding
                elements are indicated by ``1``.
            need_attn (bool, optional): return attention weights
            need_head_weights (bool, optional): return attention weights
                for each head (default: return average over heads).

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        if need_head_weights:
            need_attn = True
        residual = x
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
        if prev_self_attn_state is not None:
            prev_key, prev_value = prev_self_attn_state[:2]
            saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
            if len(prev_self_attn_state) >= 3:
                saved_state['prev_key_padding_mask'] = prev_self_attn_state[2]
            assert incremental_state is not None
            self.self_attn._set_input_buffer(incremental_state, saved_state)
        _self_attn_input_buffer = self.self_attn._get_input_buffer(incremental_state)
        if self.cross_self_attention and not (incremental_state is not None and _self_attn_input_buffer is not None and 'prev_key' in _self_attn_input_buffer):
            if self_attn_mask is not None:
                assert encoder_out is not None
                self_attn_mask = torch.cat((x.new_zeros(x.size(0), encoder_out.size(0)), self_attn_mask), dim=1)
            if self_attn_padding_mask is not None:
                if encoder_padding_mask is None:
                    assert encoder_out is not None
                    encoder_padding_mask = self_attn_padding_mask.new_zeros(encoder_out.size(1), encoder_out.size(0))
                self_attn_padding_mask = torch.cat((encoder_padding_mask, self_attn_padding_mask), dim=1)
            assert encoder_out is not None
            y = torch.cat((encoder_out, x), dim=0)
        else:
            y = x
        x, attn = self.self_attn(query=x, key=y, value=y, key_padding_mask=self_attn_padding_mask, incremental_state=incremental_state, need_weights=False, attn_mask=self_attn_mask)
        if self.c_attn is not None:
            tgt_len, bsz = x.size(0), x.size(1)
            x = x.view(tgt_len, bsz, self.nh, self.head_dim)
            x = torch.einsum('tbhd,h->tbhd', x, self.c_attn)
            x = x.reshape(tgt_len, bsz, self.embed_dim)
        if self.attn_ln is not None:
            x = self.attn_ln(x)
        x = self.dropout_module(x)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
        assert encoder_out is not None
        assert encoder_out_aug is not None
        if self.encoder_attn_merge_type == 'sequential':
            ratios = self.get_dropnet_ratio()
            if ratios[0] > 0:
                residual = x
                if self.normalize_before:
                    x = self.encoder_attn_layer_norm(x)
                if prev_attn_state is not None:
                    prev_key, prev_value = prev_attn_state[:2]
                    saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
                    if len(prev_attn_state) >= 3:
                        saved_state['prev_key_padding_mask'] = prev_attn_state[2]
                    assert incremental_state is not None
                    self.encoder_attn._set_input_buffer(incremental_state, saved_state)
                x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
                x = self.dropout_module(x)
                x = self.residual_connection(x, residual)
                if not self.normalize_before:
                    x = self.encoder_attn_layer_norm(x)
                x = ratios[0] * x
            if ratios[1] > 0:
                residual = x
                if self.normalize_before:
                    x = self.encoder_attn_layer_norm2(x)
                if prev_attn_state is not None:
                    prev_key, prev_value = prev_attn_state[:2]
                    saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
                    if len(prev_attn_state) >= 3:
                        saved_state['prev_key_padding_mask'] = prev_attn_state[2]
                    assert incremental_state is not None
                    self.encoder_attn2._set_input_buffer(incremental_state, saved_state)
                x, attn2 = self.encoder_attn2(query=x, key=encoder_out_aug, value=encoder_out_aug, key_padding_mask=encoder_padding_mask2, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
                x = self.dropout_module(x)
                x = self.residual_connection(x, residual)
                if not self.normalize_before:
                    x = self.encoder_attn_layer_norm2(x)
                x = ratios[1] * x
        elif self.encoder_attn_merge_type == 'parallel':
            residual = x
            if self.normalize_before:
                x = self.encoder_attn_layer_norm(x)
            if prev_attn_state is not None:
                prev_key, prev_value = prev_attn_state[:2]
                saved_state: Dict[str, Optional[Tensor]] = {'prev_key': prev_key, 'prev_value': prev_value}
                if len(prev_attn_state) >= 3:
                    saved_state['prev_key_padding_mask'] = prev_attn_state[2]
                assert incremental_state is not None
                self.encoder_attn._set_input_buffer(incremental_state, saved_state)
            x1, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
            x2, attn2 = self.encoder_attn2(query=x, key=encoder_out_aug, value=encoder_out_aug, key_padding_mask=encoder_padding_mask2, incremental_state=incremental_state, static_kv=True, need_weights=need_attn or not self.training and self.need_attn, need_head_weights=need_head_weights)
            x1 = self.dropout_module(x1)
            x2 = self.dropout_module(x2)
            ratios = self.get_dropnet_ratio()
            x = ratios[0] * x1 + ratios[1] * x2
            x = self.residual_connection(x, residual)
            if not self.normalize_before:
                x = self.encoder_attn_layer_norm(x)
        else:
            raise NotImplementedError(self.encoder_attn_merge_type)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        if self.ffn_layernorm is not None:
            x = self.ffn_layernorm(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        if self.w_resid is not None:
            residual = torch.mul(self.w_resid, residual)
        x = self.residual_connection(x, residual)
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        if self.onnx_trace and incremental_state is not None:
            saved_state = self.self_attn._get_input_buffer(incremental_state)
            assert saved_state is not None
            if self_attn_padding_mask is not None:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value'], saved_state['prev_key_padding_mask']]
            else:
                self_attn_state = [saved_state['prev_key'], saved_state['prev_value']]
            return x, attn, attn2, self_attn_state
        return x, attn, attn2, None

    def get_dropnet_ratio(self):
        if self.encoder_attn_merge_type == 'sequential':
            if self.dropnet_ratio > 0:
                frand = float(uniform(0, 1))
                if frand < self.dropnet_ratio and self.training:
                    return [2, 0]
                elif frand > 1 - self.dropnet_ratio and self.training:
                    return [0, 2]
                else:
                    return [1, 1]
            else:
                return [1, 1]
        elif self.encoder_attn_merge_type == 'parallel':
            if self.dropnet_ratio > 0:
                frand = float(uniform(0, 1))
                if frand < self.dropnet_ratio and self.training:
                    return [1, 0]
                elif frand > 1 - self.dropnet_ratio and self.training:
                    return [0, 1]
                else:
                    return [0.5, 0.5]
            else:
                return [0.5, 0.5]


class VGGBlock(torch.nn.Module):
    """
    VGG motibated cnn module https://arxiv.org/pdf/1409.1556.pdf

    Args:
        in_channels: (int) number of input channels (typically 1)
        out_channels: (int) number of output channels
        conv_kernel_size: convolution channels
        pooling_kernel_size: the size of the pooling window to take a max over
        num_conv_layers: (int) number of convolution layers
        input_dim: (int) input dimension
        conv_stride: the stride of the convolving kernel.
            Can be a single number or a tuple (sH, sW)  Default: 1
        padding: implicit paddings on both sides of the input.
            Can be a single number or a tuple (padH, padW). Default: None
        layer_norm: (bool) if layer norm is going to be applied. Default: False

    Shape:
        Input: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)
        Output: BxCxTxfeat, i.e. (batch_size, input_size, timesteps, features)
    """

    def __init__(self, in_channels, out_channels, conv_kernel_size, pooling_kernel_size, num_conv_layers, input_dim, conv_stride=1, padding=None, layer_norm=False):
        assert input_dim is not None, 'Need input_dim for LayerNorm and infer_conv_output_dim'
        super(VGGBlock, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.conv_kernel_size = _pair(conv_kernel_size)
        self.pooling_kernel_size = _pair(pooling_kernel_size)
        self.num_conv_layers = num_conv_layers
        self.padding = tuple(e // 2 for e in self.conv_kernel_size) if padding is None else _pair(padding)
        self.conv_stride = _pair(conv_stride)
        self.layers = nn.ModuleList()
        for layer in range(num_conv_layers):
            conv_op = nn.Conv2d(in_channels if layer == 0 else out_channels, out_channels, self.conv_kernel_size, stride=self.conv_stride, padding=self.padding)
            self.layers.append(conv_op)
            if layer_norm:
                conv_output_dim, per_channel_dim = infer_conv_output_dim(conv_op, input_dim, in_channels if layer == 0 else out_channels)
                self.layers.append(nn.LayerNorm(per_channel_dim))
                input_dim = per_channel_dim
            self.layers.append(nn.ReLU())
        if self.pooling_kernel_size is not None:
            pool_op = nn.MaxPool2d(kernel_size=self.pooling_kernel_size, ceil_mode=True)
            self.layers.append(pool_op)
            self.total_output_dim, self.output_dim = infer_conv_output_dim(pool_op, input_dim, out_channels)

    def forward(self, x):
        for i, _ in enumerate(self.layers):
            x = self.layers[i](x)
        return x


def is_cuda_extension_usable() ->bool:
    """Check whether ngram_repeat_block_cuda is built properly"""
    if not EXTENSION_BUILT or not torch.cuda.is_available():
        return False
    bsz = 2
    tokens = torch.tensor([[4, 4, 3, 2], [1, 2, 3, 4]], dtype=torch.long, device='cuda')
    lprobs = torch.rand((8, 12), device='cuda')
    try:
        outputs = ngram_repeat_block_cuda.forward(tokens, lprobs, bsz, 3, 4, 3)
        outputs = outputs + 4
        return True
    except RuntimeError:
        warnings.warn('NGramRepeatBlock extension must be rebuilt.Run TORCH_CUDA_ARCH_LIST="6.0;6.1;7.0" python setup.py build_ext --inplace')
        return False


class NGramRepeatBlock(nn.Module):
    """Wrapper class for calling ngram_repeat_block cuda extension"""

    def __init__(self, no_repeat_ngram_size: int, use_extension: bool=True):
        super().__init__()
        self.use_extension = is_cuda_extension_usable() if use_extension else False
        self.no_repeat_ngram_size = no_repeat_ngram_size

    def reset_parameters(self):
        pass

    @torch.jit.unused
    def call_cuda_extension(self, tokens, lprobs, bsz: int, beam_size: int, step: int):
        return ngram_repeat_block_cuda.forward(tokens, lprobs, bsz, step, beam_size, self.no_repeat_ngram_size)

    def forward(self, tokens, lprobs, bsz: int, beam_size: int, step: int):
        """
        Args:
            tokens(Tensor): Input tokens(Bsz*beam, seq_len)
            lprobs(Tensor): likelihood probability,
            Expected to be updated in place.(Bsz*beam, vocab_size)
            bsz(int): batch size
            step(int): current step
            beam_size(int): beam size
            no_repeat_ngram_size(int): Ngram size
        """
        msg = f'expected {bsz * beam_size} got'
        assert tokens.size(0) == bsz * beam_size, f'{msg} {tokens.size(0)}'
        assert lprobs.size(0) == bsz * beam_size, f'{msg} {lprobs.size(0)}'
        if self.use_extension:
            return self.call_cuda_extension(tokens, lprobs, bsz, beam_size, step)
        else:
            return self._no_repeat_ngram(tokens, lprobs, bsz, beam_size, step)

    def _no_repeat_ngram(self, tokens, lprobs, bsz: int, beam_size: int, step: int):
        """For each hypothesis generate a list of previous ngrams and set associated lprobs to -inf"""
        banned_tokens = [torch.jit.annotate(List[int], []) for bbsz_idx in range(bsz * beam_size)]
        if step + 2 - self.no_repeat_ngram_size >= 0:
            cpu_tokens: List[List[int]] = tokens.cpu().tolist()
            check_start_pos = step + 2 - self.no_repeat_ngram_size
            for bbsz_idx in range(bsz * beam_size):
                ngram_to_check = cpu_tokens[bbsz_idx][-(self.no_repeat_ngram_size - 1):]
                for i in range(check_start_pos):
                    if ngram_to_check == cpu_tokens[bbsz_idx][i:i + self.no_repeat_ngram_size - 1]:
                        banned_tokens[bbsz_idx].append(cpu_tokens[bbsz_idx][i + self.no_repeat_ngram_size - 1])
        for bbsz_idx in range(bsz * beam_size):
            lprobs[bbsz_idx][torch.tensor(banned_tokens[bbsz_idx], dtype=torch.int64)] = torch.tensor(-math.inf)
        return lprobs


class Search(nn.Module):

    def __init__(self, tgt_dict):
        super().__init__()
        self.pad = tgt_dict.pad()
        self.unk = tgt_dict.unk()
        self.eos = tgt_dict.eos()
        self.vocab_size = len(tgt_dict)
        self.src_lengths = torch.tensor(-1)
        self.supports_constraints = False
        self.stop_on_max_len = False

    def step(self, step, lprobs, scores, prev_output_tokens=None, original_batch_idxs=None):
        """Take a single search step.

        Args:
            step: the current search step, starting at 0
            lprobs: (bsz x input_beam_size x vocab_size)
                the model's log-probabilities over the vocabulary at the current step
            scores: (bsz x input_beam_size x step)
                the historical model scores of each hypothesis up to this point
            prev_output_tokens: (bsz x step)
                the previously generated oputput tokens
            original_batch_idxs: (bsz)
                the tensor with the batch indices, in the range [0, bsz)
                this is useful in case there has been applied a re-ordering
                and we need to know the orignal indices

        Return: A tuple of (scores, indices, beams) where:
            scores: (bsz x output_beam_size)
                the scores of the chosen elements; output_beam_size can be
                larger than input_beam_size, e.g., we may return
                2*input_beam_size to account for EOS
            indices: (bsz x output_beam_size)
                the indices of the chosen elements
            beams: (bsz x output_beam_size)
                the hypothesis ids of the chosen elements, in the range [0, input_beam_size)
        """
        raise NotImplementedError

    @torch.jit.export
    def set_src_lengths(self, src_lengths):
        self.src_lengths = src_lengths

    @torch.jit.export
    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):
        """Initialize constraint states for constrained decoding (if supported).

        Args:
            batch_constraints: (torch.Tensor, optional)
                the list of constraints, in packed form
            beam_size: (int)
                the beam size
        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        pass

    def prune_sentences(self, batch_idxs: Tensor):
        """
        Removes constraint states for completed sentences (if supported).
        This is called from sequence_generator._generate() when sentences are
        deleted from the batch.

        Args:
            batch_idxs: Indices of *sentences* whose constraint state should be *kept*.
        """
        pass

    def update_constraints(self, active_hypos: Tensor):
        """
        Updates the constraint states by selecting the beam items that are retained.
        This is called at each time step of sequence_generator._generate() when
        the set of 2 * {beam_size} candidate hypotheses are reduced to the beam size.

        Args:
            active_hypos: (batch size, beam size)
              list of integers denoting, for each sentence, which beam candidate items
              should be kept.
        """
        pass


class BeamSearch(Search):

    def __init__(self, tgt_dict):
        super().__init__(tgt_dict)
        self.constraint_states = None

    @torch.jit.export
    def step(self, step: int, lprobs, scores: Optional[Tensor], prev_output_tokens: Optional[Tensor]=None, original_batch_idxs: Optional[Tensor]=None):
        bsz, beam_size, vocab_size = lprobs.size()
        if step == 0:
            lprobs = lprobs[:, ::beam_size, :].contiguous()
        else:
            assert scores is not None
            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)
        top_prediction = torch.topk(lprobs.view(bsz, -1), k=min(beam_size * 2, lprobs.view(bsz, -1).size(1) - 1))
        scores_buf = top_prediction[0]
        indices_buf = top_prediction[1]
        beams_buf = torch.div(indices_buf, vocab_size, rounding_mode='trunc')
        indices_buf = indices_buf.fmod(vocab_size)
        return scores_buf, indices_buf, beams_buf


class PrefixConstrainedBeamSearch(Search):

    def __init__(self, tgt_dict, prefix_allowed_tokens_fn):
        super().__init__(tgt_dict)
        self.prefix_allowed_tokens_fn = prefix_allowed_tokens_fn
        self.stop_on_max_len = True

    @torch.jit.export
    def apply_mask(self, x, prev_output_tokens, original_batch_idxs):
        beam_size = x.shape[0] // original_batch_idxs.shape[0]
        original_batch_idxs = original_batch_idxs.unsqueeze(-1).repeat((1, beam_size)).flatten().tolist()
        mask = torch.full_like(x, -math.inf)
        for sent_i, (sent, batch_i) in enumerate(zip(prev_output_tokens, original_batch_idxs)):
            mask[sent_i, :, self.prefix_allowed_tokens_fn(batch_i, sent)] = 0
        return mask

    @torch.jit.export
    def step(self, step: int, lprobs: Tensor, scores: Tensor, prev_output_tokens: Tensor, original_batch_idxs: Tensor):
        bsz, beam_size, vocab_size = lprobs.size()
        lprobs += self.apply_mask(lprobs.view(bsz * beam_size, 1, vocab_size), prev_output_tokens, original_batch_idxs).view(bsz, beam_size, vocab_size)
        if step == 0:
            lprobs = lprobs[:, ::beam_size, :].contiguous()
        else:
            assert scores is not None
            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)
        top_prediction = torch.topk(lprobs.view(bsz, -1), k=min(beam_size, lprobs.view(bsz, -1).size(1) - 1))
        scores_buf = top_prediction[0]
        indices_buf = top_prediction[1]
        beams_buf = indices_buf // vocab_size
        indices_buf = indices_buf.fmod(vocab_size)
        return scores_buf, indices_buf, beams_buf


class ConstraintState:

    def __init__(self):
        pass


class ConstraintSequence:

    def __init__(self, sequences: List[List[int]]):
        """Represents a set of possibly multitoken constraints by
        concatenating them and internally recording the end points.
        """
        self.sequences = []
        self.endpoints = []
        self.num_tokens = 0
        self.tokens = set()
        for sequence in sequences:
            for token in sequence:
                self.tokens.add(token)
            self.num_tokens += len(sequence)
            self.endpoints += [(False) for x in range(len(sequence) - 1)] + [True]
            self.sequences += sequence

    def __getitem__(self, key: int):
        return self.sequences[key]

    def __len__(self):
        return len(self.sequences)

    def __str__(self):
        return str(self.sequences)


def unpack_constraints(constraint_tensor: torch.Tensor) ->List[torch.Tensor]:
    """
    Transforms *one row* of a packed constraint tensor (e.g., for one
    sentence in the batch) into a list of constraint tensors.
    """
    constraint_list = []
    num_constraints = constraint_tensor[0]
    constraints = constraint_tensor.tolist()
    offset = 1
    for i in range(num_constraints):
        where = constraints.index(0, offset)
        constraint_list.append(constraint_tensor[offset:where])
        offset = where + 1
    return constraint_list


class OrderedConstraintState(ConstraintState):
    """
    Records progress through the set of linear nonbranching constraints with gaps.
    """

    def __init__(self, sequence: ConstraintSequence, state: int=-1):
        self.sequence = sequence
        self.state = state

    @staticmethod
    def create(constraint_tensor: torch.Tensor):
        constraint_list = unpack_constraints(constraint_tensor)
        return OrderedConstraintState(ConstraintSequence(constraint_list), -1)

    def __str__(self):
        return f'{self.state}/{self.bank}x{self.num_completed}'

    def __copy__(self):
        return OrderedConstraintState(self.sequence, self.state)

    def copy(self):
        return self.__copy__()

    @property
    def num_completed(self):
        if self.state == -1:
            return 0
        count = len(list(filter(lambda x: x, self.sequence.endpoints[0:self.state + 1])))
        return count

    @property
    def is_root(self):
        return self.state == -1

    @property
    def name(self):
        if self.state == -1:
            return 'ROOT'
        else:
            return str(self.sequence[self.state])

    @property
    def bank(self) ->int:
        return self.state + 1

    @property
    def finished(self):
        return self.state + 1 == len(self.sequence)

    @property
    def token_counts(self):
        return self.sequence.token_counts()

    @property
    def tokens(self):
        return self.sequence.tokens

    @property
    def num_constraint_tokens(self):
        return sum(self.token_counts.values())

    def next_tokens(self) ->Set[int]:
        """Returns the list of tokens that could come next.
        These are (a) all tokens extending the root state and, for
        non-root states, additionally all tokens extending the current
        state."""
        tokens = set()
        if self.state > 0:
            tokens.add(self.sequence[0])
        if not self.finished:
            tokens.add(self.sequence[self.state + 1])
        return tokens

    def advance(self, token: int):
        """Reads in a token and advances the state. Here's how it works.

        We can advance to the next state if:
        - there is a matching child
        - its path isn't blocked

        A path is blocked when all constraints that are descendants of
        that node have already been generated, in the current state.

        If we are not able to advance from the current state, we "fall
        off the graph" and return to the root state. There, we again
        try to advance, checking the same criteria.

        In any case, when falling off the graph, we need to do some
        bookkeeping. We:
        - check whether any constraints were met (all prefixes of
          current state)
        - if one is found, mark it as completed
        - adjust visited nodes accordingly
        """
        token = int(token)
        if self.finished:
            next_state = self.copy()
        elif self.sequence[self.state + 1] == token:
            next_state = OrderedConstraintState(self.sequence, self.state + 1)
        elif self.sequence.endpoints[self.state]:
            next_state = self.copy()
        elif token == self.sequence[0]:
            next_state = OrderedConstraintState(self.sequence, 0)
        else:
            next_state = OrderedConstraintState(self.sequence, -1)
        return next_state


class ConstraintNode:
    """
    Represents a node in a trie managing unordered constraints.
    """

    def __init__(self, token: int=None, parent=None):
        self.token = int(token) if token is not None else None
        self.parent = parent
        self.terminal = 0
        self.children = {}
        self.num_constraints = 0

    @property
    def id(self):
        return self.token

    def __str__(self):
        term = self.terminal != 0
        return f'[{self.token}].{term}#{self.num_constraints}'

    def __getitem__(self, key: int):
        return self.children.get(key, None)

    def next_tokens(self) ->Set[int]:
        """The set of child labels."""
        return set(self.children.keys())

    @staticmethod
    def create(constraints: List[List[int]]):
        root = ConstraintNode()
        for sequence in constraints:
            root.add_sequence(sequence)
        return root

    @staticmethod
    def print_graph(node: 'ConstraintNode'):
        if len(node.children) == 0:
            return str(node)
        else:
            s = f'({node}'
            for child in node.children.values():
                s += ' ' + ConstraintNode.print_graph(child)
            s += ')'
            return s

    def token_counts(self) ->Counter:
        """Returns a counter of the number of times each token is used
        in a constraint.
        """
        token_counts = Counter()
        kids = list(self.children.values())
        while len(kids) > 0:
            kid = kids.pop()
            token_counts[kid.id] += kid.num_constraints
            kids += list(kid.children.values())
        return token_counts

    def tokens(self) ->Set[int]:
        """Returns the set of tokens in constraints."""
        return set(self.token_counts().keys())

    def add_sequence(self, sequence: List[int]):
        """Adds a constraint, represented as a list of integers, to
        the trie."""
        assert len(sequence) > 0
        token = int(sequence[0])
        if token not in self.children:
            self.children[token] = ConstraintNode(token, parent=self)
        node = self.children[token]
        if len(sequence) == 1:
            node.terminal += 1
            node.num_constraints += 1
            parent = node.parent
            while parent is not None:
                parent.num_constraints += 1
                parent = parent.parent
        else:
            node.add_sequence(sequence[1:])


class UnorderedConstraintState(ConstraintState):
    """
    Records progress through the set of constraints for each item in the beam
    using a trie.
    """

    def __init__(self, node: ConstraintNode, copy_from: 'ConstraintState'=None):
        self.node = node
        if copy_from is None:
            self.root = node
            self.completed = Counter()
            self.generated = Counter()
            self.needed_tokens = self.root.tokens()
        else:
            self.completed = Counter(copy_from.completed)
            self.generated = Counter(copy_from.generated)
            self.root = copy_from.root
        if self.node != self.root:
            self.generated[node] += 1

    @staticmethod
    def create(constraint_tensor: torch.Tensor):
        constraint_list = unpack_constraints(constraint_tensor)
        constraint_trie_root = ConstraintNode.create(constraint_list)
        return UnorderedConstraintState(constraint_trie_root)

    def __str__(self):
        gen_str = ','.join([str(node) for node in self.generated])
        return f'{self.name}/{self.bank}({gen_str})x{self.num_completed}'

    def __copy__(self):
        copied_state = UnorderedConstraintState(self.node, copy_from=self)
        return copied_state

    def copy(self):
        return self.__copy__()

    @property
    def name(self):
        if self.node.id is None:
            return 'ROOT'
        else:
            return str(self.node.id)

    @property
    def is_root(self):
        return self.node == self.root

    @property
    def bank(self):
        return sum(self.generated.values())

    @property
    def num_completed(self):
        """The number of constraints (not constraint tokens) that are completed.
        In addition to the already-completed states, we need to account for the
        current state, which might get marked as completed when another token
        is generated.
        """
        in_final = self.node.terminal and self.completed[self.node] < self.node.terminal
        return sum(self.completed.values()) + in_final

    @property
    def finished(self):
        return self.root.num_constraints - self.num_completed == 0

    @property
    def token_counts(self):
        return self.root.token_counts()

    @property
    def tokens(self):
        return self.root.tokens()

    @property
    def num_constraint_tokens(self):
        return sum(self.token_counts.values())

    def next_tokens(self) ->Set[int]:
        """Returns the list of tokens that could come next.
        These are (a) all tokens extending the root state and, for
        non-root states, additionally all tokens extending the current
        state."""
        if self.node != self.root:
            return self.root.next_tokens().union(self.node.next_tokens())
        else:
            return self.root.next_tokens()

    def advance(self, token: int):
        """Reads in a token and advances the state. Here's how it works.

        We can advance to the next state if:
        - there is a matching child
        - its path isn't blocked

        A path is blocked when all constraints that are descendants of
        that node have already been generated, in the current state.

        If we are not able to advance from the current state, we "fall
        off the graph" and return to the root state. There, we again
        try to advance, checking the same criteria.

        In any case, when falling off the graph, we need to do some
        bookkeeping. We:
        - check whether any constraints were met (all prefixes of
          current state)
        - if one is found, mark it as completed
        - adjust visited nodes accordingly
        """
        token = int(token)
        next_state = None
        child = self.node[token]
        if child is not None and self.generated[child] < child.num_constraints:
            next_state = UnorderedConstraintState(child, copy_from=self)

        def rewind():
            """If we're mid-trie and an "illegal" token is chosen next, we need
            to reset our state to the root state. However, along the way, we need
            to check whether a prefix of the current trie state represents a state
            we could mark as completed.
            """
            node = self.node
            while node != self.root:
                if node.terminal and self.completed[node] < node.terminal:
                    next_state.completed[node] += 1
                    return
                next_state.generated[node] -= 1
                node = node.parent
        if next_state is None and token in self.root.next_tokens():
            child = self.root[token]
            if self.generated[child] < child.num_constraints:
                next_state = UnorderedConstraintState(child, copy_from=self)
            else:
                next_state = UnorderedConstraintState(self.root, copy_from=self)
            rewind()
        elif next_state is None:
            next_state = UnorderedConstraintState(self.root, copy_from=self)
            rewind()
        return next_state


class LexicallyConstrainedBeamSearch(Search):
    """Implements lexically constrained beam search as described in

        Fast Lexically Constrained Decoding with Dynamic Beam
        Allocation for Neural Machine Translation.  Post & Vilar,
        NAACL 2018.  https://www.aclweb.org/anthology/N18-1119/

    and

        Improved Lexically Constrained Decoding for Translation and
        Monolingual Rewriting. Hu et al, NAACL
        2019. https://www.aclweb.org/anthology/N19-1090/

    This is accomplished by maintaining, for each beam hypothesis, a
    ConstraintState object (see constraints.py) that tracks which
    constraints have been generated and using this information to
    shape the beam for each input sentence.
    """

    def __init__(self, tgt_dict, representation):
        super().__init__(tgt_dict)
        self.representation = representation
        self.vocab_size = len(tgt_dict)
        self.num_cands = 0
        self.supports_constraints = True

    @torch.jit.export
    def init_constraints(self, batch_constraints: Optional[Tensor], beam_size: int):
        self.constraint_states = []
        for constraint_tensor in batch_constraints:
            if self.representation == 'ordered':
                constraint_state = OrderedConstraintState.create(constraint_tensor)
            elif self.representation == 'unordered':
                constraint_state = UnorderedConstraintState.create(constraint_tensor)
            self.constraint_states.append([constraint_state for i in range(beam_size)])

    @torch.jit.export
    def prune_sentences(self, batch_idxs: Tensor):
        self.constraint_states = [self.constraint_states[i] for i in batch_idxs.tolist()]

    @torch.jit.export
    def update_constraints(self, active_hypos: Tensor):
        if self.constraint_states:
            batch_size = active_hypos.size(0)
            for sentid in range(batch_size):
                self.constraint_states[sentid] = [self.constraint_states[sentid][i] for i in active_hypos[sentid]]

    @torch.jit.export
    def step(self, step: int, lprobs: Tensor, scores: Optional[Tensor], prev_output_tokens: Optional[Tensor]=None, original_batch_idxs: Optional[Tensor]=None):
        """
        A constrained step builds a large candidates list from the following:
        - the top 2 * {beam_size} items over the whole beam
        - for each item in the beam
          - the top {each_k} (default 1)
          - all next constraints
        We then compute the constrained state of each beam item, and assign
        stripe codes: 0 to the best in each bank, 1 to the 2nd-best, and so
        on. We then sort by (stripe, score), and truncate the list at
        2 * beam size.

        Args:
            step: the decoder step
            lprobs: (batch size, beam size, target vocab)
                the target-vocab distributions for each item in the beam.
        Retrun: A tuple of (scores, indices, beams, constraints) where:
            scores: (batch, output beam size)
                the scores of the chosen elements
            indices: (batch, output beam size)
                the target vocab indices of the chosen elements
            beams: (batch, output beam size)
                the 0-indexed hypothesis ids of the chosen elements
            constraints: (batch, output beam size)
                the new constraint states
        """
        each_k = 1
        device = lprobs.device
        batch_size, beam_size, vocab_size = lprobs.size()
        self.num_cands = min(beam_size * 2, lprobs.view(batch_size, -1).size(1) - 1)
        constraint_states = self.constraint_states
        if constraint_states and step > 0:
            not_finished_indices = []
            for sentno, sent_constraints in enumerate(constraint_states):
                for beamno, state in enumerate(sent_constraints):
                    index = sentno * beam_size + beamno
                    if not state.finished:
                        not_finished_indices.append(index)
            not_finished_indices = torch.tensor(not_finished_indices)
            if not_finished_indices.numel() > 0:
                lprobs.view(batch_size * beam_size, -1)[not_finished_indices, self.eos] = -math.inf
        if step == 0:
            lprobs = lprobs[:, ::beam_size, :].contiguous()
        else:
            assert scores is not None
            lprobs = lprobs + scores[:, :, step - 1].unsqueeze(-1)
        top_prediction = torch.topk(lprobs.view(batch_size, -1), self.num_cands)
        scores_buf, indices_buf = top_prediction
        beams_buf = indices_buf // vocab_size
        indices_buf = indices_buf.fmod(vocab_size)
        if not constraint_states:
            return scores_buf, indices_buf, beams_buf
        if step > 0:
            top_scores, top_indices = torch.topk(lprobs.view(batch_size * beam_size, -1), k=each_k, dim=1)
            top_scores = top_scores.view(batch_size, -1)
            top_indices = top_indices.view(batch_size, -1)
            scores_buf = torch.cat((scores_buf, top_scores), dim=1)
            indices_buf = torch.cat((indices_buf, top_indices), dim=1)
            new_beams = torch.arange(0, beam_size, device=device).repeat(batch_size, 1)
            beams_buf = torch.cat((beams_buf, new_beams), dim=1)
        new_scores_buf = torch.zeros((batch_size, 2 * beam_size), device=device)
        new_indices_buf = torch.zeros((batch_size, 2 * beam_size), device=device).long()
        new_beams_buf = torch.zeros((batch_size, 2 * beam_size), device=device).long()
        for sentno, states in enumerate(constraint_states):
            scores, indices, beams, new_states = self.step_sentence(step, sentno, lprobs[sentno], constraint_states[sentno], beams_buf[sentno].clone(), indices_buf[sentno].clone(), scores_buf[sentno].clone())
            new_scores_buf[sentno] = scores
            new_indices_buf[sentno] = indices
            new_beams_buf[sentno] = beams
            self.constraint_states[sentno] = new_states
        return new_scores_buf, new_indices_buf, new_beams_buf

    @torch.jit.export
    def step_sentence(self, step: int, sentno: int, lprobs: Tensor, constraint_states: List[List[ConstraintState]], beams_buf: Tensor, indices_buf: Tensor, scores_buf: Tensor):
        """Does per-sentence processing. Adds all constraints for each
        hypothesis to the list of candidates; then removes duplicates,
        sorts, and dynamically stripes across the banks. All tensor inputs
        are collapsed to those pertaining to a single input sentence.
        """
        device = lprobs.device
        for beamno, state in enumerate(constraint_states):
            next_tokens = torch.tensor(list(state.next_tokens()), device=device).long()
            if next_tokens.numel() != 0:
                indices_buf = torch.cat((indices_buf, next_tokens))
                next_beams = torch.tensor(beamno, device=device).repeat(next_tokens.size(0)).long()
                beams_buf = torch.cat((beams_buf, next_beams))
                next_values = lprobs[beamno].take(next_tokens.view(-1))
                scores_buf = torch.cat((scores_buf, next_values))
            if step == 0:
                break
        cands_size = indices_buf.size(0)
        constraint_states = [constraint_states[beams_buf[i]].advance(indices_buf[i]) for i in range(cands_size)]
        banks = torch.tensor([state.bank for state in constraint_states], device=device)
        num_constraint_tokens = len(state.tokens)
        MAX_SCORE = -100
        sort_key = (num_constraint_tokens - banks) * MAX_SCORE + scores_buf
        sort_values, sort_indices = sort_key.sort(dim=0, descending=True)
        scores_buf = scores_buf[sort_indices]
        indices_buf = indices_buf[sort_indices]
        beams_buf = beams_buf[sort_indices]
        banks = banks[sort_indices]
        constraint_states = [constraint_states[i] for i in sort_indices]

        def roll(t):
            """Rolls a 1d tensor left by 1.

            [0, 1, 2, 3, 4] becomes [4, 0, 1, 2, 3]
            """
            return torch.cat((t[-1].unsqueeze(0), t[0:-1]), dim=0)
        uniques_mask = beams_buf * (self.vocab_size + 1) + indices_buf
        uniques_mask = roll(uniques_mask) != uniques_mask
        scores_buf = torch.masked_select(scores_buf, uniques_mask)
        indices_buf = torch.masked_select(indices_buf, uniques_mask)
        beams_buf = torch.masked_select(beams_buf, uniques_mask)
        banks = torch.masked_select(banks, uniques_mask)
        i = 1
        for mask in uniques_mask[1:]:
            if not mask:
                constraint_states.pop(i)
            i += mask
        stripe_offsets = [(offset * (len(banks) + 1)) for offset in range(len(banks) + 1)]
        stripes = torch.zeros_like(banks)
        cur_bank_count = -1
        cur_bank = banks[0]
        for i, bank in enumerate(banks):
            if bank != cur_bank:
                cur_bank_count = 0
                cur_bank = bank
            else:
                cur_bank_count += 1
            stripes[i] = num_constraint_tokens - bank + stripe_offsets[cur_bank_count]
        sort_values, sort_indices = stripes.sort(dim=0)
        scores_buf = scores_buf[sort_indices]
        indices_buf = indices_buf[sort_indices]
        beams_buf = beams_buf[sort_indices]
        constraint_states = [constraint_states[i] for i in sort_indices]
        scores_buf = scores_buf[:self.num_cands]
        indices_buf = indices_buf[:self.num_cands]
        beams_buf = beams_buf[:self.num_cands]
        return scores_buf, indices_buf, beams_buf, constraint_states


class LengthConstrainedBeamSearch(Search):

    def __init__(self, tgt_dict, min_len_a, min_len_b, max_len_a, max_len_b):
        super().__init__(tgt_dict)
        self.min_len_a = min_len_a
        self.min_len_b = min_len_b
        self.max_len_a = max_len_a
        self.max_len_b = max_len_b
        self.beam = BeamSearch(tgt_dict)
        self.needs_src_lengths = True

    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor]=None, original_batch_idxs: Optional[Tensor]=None):
        min_lens = self.min_len_a * self.src_lengths + self.min_len_b
        max_lens = self.max_len_a * self.src_lengths + self.max_len_b
        lprobs[step < min_lens, :, self.eos] = -math.inf
        lprobs[step >= max_lens, :, self.eos] = 0
        return self.beam.step(step, lprobs, scores)


class DiverseBeamSearch(Search):
    """Diverse Beam Search.

    See "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence
    Models" for details.

    We only implement the Hamming Diversity penalty here, which performed best
    in the original paper.
    """

    def __init__(self, tgt_dict, num_groups, diversity_strength):
        super().__init__(tgt_dict)
        self.num_groups = num_groups
        self.diversity_strength = -diversity_strength
        self.beam = BeamSearch(tgt_dict)

    @torch.jit.export
    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor]=None, original_batch_idxs: Optional[Tensor]=None):
        bsz, beam_size, vocab_size = lprobs.size()
        if beam_size % self.num_groups != 0:
            raise ValueError('DiverseBeamSearch requires --beam to be divisible by the number of groups')
        diversity_buf = torch.zeros(lprobs[:, 0, :].size())
        scores_G, indices_G, beams_G = [], [], []
        for g in range(self.num_groups):
            lprobs_g = lprobs[:, g::self.num_groups, :]
            scores_g = scores[:, g::self.num_groups, :] if step > 0 else None
            if g > 0:
                lprobs_g = torch.add(lprobs_g, other=diversity_buf.unsqueeze(1), alpha=self.diversity_strength)
            else:
                lprobs_g = lprobs_g.contiguous()
            scores_buf, indices_buf, beams_buf = self.beam.step(step, lprobs_g, scores_g)
            beams_buf.mul_(self.num_groups).add_(g)
            scores_G.append(scores_buf.clone())
            indices_G.append(indices_buf.clone())
            beams_G.append(beams_buf.clone())
            diversity_buf.scatter_add_(1, indices_buf, torch.ones(indices_buf.size()))
        scores_buf = torch.stack(scores_G, dim=2).view(bsz, -1)
        indices_buf = torch.stack(indices_G, dim=2).view(bsz, -1)
        beams_buf = torch.stack(beams_G, dim=2).view(bsz, -1)
        return scores_buf, indices_buf, beams_buf


class Sampling(Search):
    sampling_topk: int
    sampling_topp: float

    def __init__(self, tgt_dict, sampling_topk=-1, sampling_topp=-1.0):
        super().__init__(tgt_dict)
        self.sampling_topk = sampling_topk
        self.sampling_topp = sampling_topp

    def _sample_topp(self, lprobs):
        """Sample among the smallest set of elements whose cumulative probability mass exceeds p.

        See `"The Curious Case of Neural Text Degeneration"
        (Holtzman et al., 2019) <https://arxiv.org/abs/1904.09751>`_.

        Args:
            lprobs: (bsz x input_beam_size x vocab_size)
                the model's log-probabilities over the vocabulary at the current step

        Return: A tuple of (trimed_probs, truncated_indices) where:
            trimed_probs: (bsz x input_beam_size x ?)
                the model's probabilities over the elements selected to sample from. The
                width of the third dimension is determined by top-P.
            truncated_indices: (bsz x input_beam_size x ?)
                the indices of the chosen elements.
        """
        probs = lprobs.exp_()
        sorted_probs, sorted_indices = probs.sort(descending=True)
        cumsum_probs = sorted_probs.cumsum(dim=2)
        mask = cumsum_probs.lt(self.sampling_topp)
        cumsum_mask = mask.cumsum(dim=2)
        last_included = cumsum_mask[:, :, -1:]
        last_included.clamp_(0, mask.size()[2] - 1)
        mask = mask.scatter_(2, last_included, 1)
        max_dim = last_included.max()
        truncated_mask = mask[:, :, :max_dim + 1]
        truncated_probs = sorted_probs[:, :, :max_dim + 1]
        truncated_indices = sorted_indices[:, :, :max_dim + 1]
        trim_mask = ~truncated_mask
        trimed_probs = truncated_probs.masked_fill_(trim_mask, 0)
        return trimed_probs, truncated_indices

    @torch.jit.export
    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor]=None, original_batch_idxs: Optional[Tensor]=None):
        bsz, beam_size, vocab_size = lprobs.size()
        if step == 0:
            lprobs = lprobs[:, ::beam_size, :].contiguous()
        if self.sampling_topp > 0:
            probs, top_indices = self._sample_topp(lprobs)
        elif self.sampling_topk > 0:
            lprobs, top_indices = lprobs.topk(self.sampling_topk)
            probs = lprobs.exp_()
        else:
            probs = lprobs.exp_()
            top_indices = torch.empty(0)
        if step == 0:
            indices_buf = torch.multinomial(probs.view(bsz, -1), beam_size, replacement=True).view(bsz, beam_size)
        else:
            indices_buf = torch.multinomial(probs.view(bsz * beam_size, -1), 1, replacement=True).view(bsz, beam_size)
        if step == 0:
            probs = probs.expand(bsz, beam_size, -1)
        scores_buf = torch.gather(probs, dim=2, index=indices_buf.unsqueeze(-1))
        scores_buf = scores_buf.log_().view(bsz, -1)
        if self.sampling_topk > 0 or self.sampling_topp > 0:
            indices_buf = torch.gather(top_indices.expand(bsz, beam_size, -1), dim=2, index=indices_buf.unsqueeze(-1)).squeeze(2)
        if step == 0:
            beams_buf = indices_buf.new_zeros(bsz, beam_size)
        else:
            beams_buf = torch.arange(0, beam_size).repeat(bsz, 1)
            scores_buf.add_(torch.gather(scores[:, :, step - 1], dim=1, index=beams_buf))
        return scores_buf, indices_buf, beams_buf


class DiverseSiblingsSearch(Search):
    """
    Beam search with diverse siblings.

    See "A Simple, Fast Diverse Decoding Algorithm for Neural Generation" for details.
    https://arxiv.org/abs/1611.08562

    1/ Calculate hypotheses for each beam
    2/ Intra-sibling ordering
    3/ Rewrite scores
    4/ Choose top K hypotheses

    if diversity_rate == 0 is equivalent to BeamSearch
    """

    def __init__(self, tgt_dict, diversity_rate):
        super().__init__(tgt_dict)
        self.diversity_rate = diversity_rate
        self.beam = BeamSearch(tgt_dict)

    def step(self, step: int, lprobs, scores, prev_output_tokens: Optional[Tensor]=None, original_batch_idxs: Optional[Tensor]=None):
        bsz, beam_size, vocab_size = lprobs.size()
        k = min(beam_size * 2, lprobs.view(bsz, -1).size(1) - 1)
        s_list: List[Tensor]
        i_list: List[Tensor]
        s_list = [torch.empty(0) for i in range(beam_size)]
        i_list = [torch.LongTensor() for i in range(beam_size)]
        sibling_score = torch.arange(1, k + 1) * self.diversity_rate
        if step == 0:
            return self.beam.step(step, lprobs, scores)
        lprobs.add_(scores[:, :, step - 1].unsqueeze(-1))
        for i in range(beam_size):
            torch.topk(lprobs[:, i, :].view(bsz, -1), k, out=(s_list[i], i_list[i]))
            i_list[i].fmod_(vocab_size)
            s_list[i].sub_(sibling_score)
        indices = torch.stack(i_list, dim=1).view(bsz, -1)
        final_scores = torch.empty(0)
        final_indices = torch.LongTensor()
        final_beams = torch.LongTensor()
        final_scores, final_indices = torch.topk(torch.stack(s_list, dim=1).view(bsz, -1), k)
        final_beams = final_indices // k
        for i in range(bsz):
            final_indices[i] = indices[i][final_indices[i]]
        return final_scores, final_indices, final_beams


class EnsembleModel(nn.Module):
    """A wrapper around an ensemble of models."""

    def __init__(self, models):
        super().__init__()
        self.models_size = len(models)
        self.single_model = models[0]
        self.models = nn.ModuleList(models)
        self.has_incremental: bool = False
        if all(hasattr(m, 'decoder') and isinstance(m.decoder, FairseqIncrementalDecoder) for m in models):
            self.has_incremental = True

    def forward(self):
        pass

    def has_encoder(self):
        return hasattr(self.single_model, 'encoder')

    def has_incremental_states(self):
        return self.has_incremental

    def max_decoder_positions(self):
        return min([m.max_decoder_positions() for m in self.models if hasattr(m, 'max_decoder_positions')] + [sys.maxsize])

    def set_decoder_beam_size(self, beam_size):
        """Set beam size for efficient beamable enc-dec attention."""
        if beam_size > 1:
            for model in self.models:
                if hasattr(model, 'set_beam_size'):
                    model.set_beam_size(beam_size)

    @torch.jit.export
    def forward_encoder(self, net_input: Dict[str, Tensor]):
        if not self.has_encoder():
            return None
        return [model.encoder.forward_torchscript(net_input) for model in self.models]

    @torch.jit.export
    def forward_decoder(self, tokens, encoder_outs: List[Dict[str, List[Tensor]]], incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]], temperature: float=1.0):
        log_probs = []
        avg_attn: Optional[Tensor] = None
        encoder_out: Optional[Dict[str, List[Tensor]]] = None
        for i, model in enumerate(self.models):
            if self.has_encoder():
                encoder_out = encoder_outs[i]
            if self.has_incremental_states():
                decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out, incremental_state=incremental_states[i])
            elif hasattr(model, 'decoder'):
                decoder_out = model.decoder.forward(tokens, encoder_out=encoder_out)
            else:
                decoder_out = model.forward(tokens)
            attn: Optional[Tensor] = None
            decoder_len = len(decoder_out)
            if decoder_len > 1 and decoder_out[1] is not None:
                if isinstance(decoder_out[1], Tensor):
                    attn = decoder_out[1]
                else:
                    attn_holder = decoder_out[1]['attn']
                    if isinstance(attn_holder, Tensor):
                        attn = attn_holder
                    elif attn_holder is not None:
                        attn = attn_holder[0]
                if attn is not None:
                    attn = attn[:, -1, :]
            decoder_out_tuple = decoder_out[0][:, -1:, :].div_(temperature), None if decoder_len <= 1 else decoder_out[1]
            probs = model.get_normalized_probs(decoder_out_tuple, log_probs=True, sample=None)
            probs = probs[:, -1, :]
            if self.models_size == 1:
                return probs, attn
            log_probs.append(probs)
            if attn is not None:
                if avg_attn is None:
                    avg_attn = attn
                else:
                    avg_attn.add_(attn)
        avg_probs = torch.logsumexp(torch.stack(log_probs, dim=0), dim=0) - math.log(self.models_size)
        if avg_attn is not None:
            avg_attn.div_(self.models_size)
        return avg_probs, avg_attn

    @torch.jit.export
    def reorder_encoder_out(self, encoder_outs: Optional[List[Dict[str, List[Tensor]]]], new_order):
        """
        Reorder encoder output according to *new_order*.

        Args:
            encoder_out: output from the ``forward()`` method
            new_order (LongTensor): desired order

        Returns:
            *encoder_out* rearranged according to *new_order*
        """
        new_outs: List[Dict[str, List[Tensor]]] = []
        if not self.has_encoder():
            return new_outs
        for i, model in enumerate(self.models):
            assert encoder_outs is not None
            new_outs.append(model.encoder.reorder_encoder_out(encoder_outs[i], new_order))
        return new_outs

    @torch.jit.export
    def reorder_incremental_state(self, incremental_states: List[Dict[str, Dict[str, Optional[Tensor]]]], new_order):
        if not self.has_incremental_states():
            return
        for i, model in enumerate(self.models):
            model.decoder.reorder_incremental_state_scripting(incremental_states[i], new_order)


class SequenceGenerator(nn.Module):

    def __init__(self, models, tgt_dict, beam_size=1, max_len_a=0, max_len_b=200, max_len=0, min_len=1, normalize_scores=True, len_penalty=1.0, unk_penalty=0.0, temperature=1.0, match_source_len=False, no_repeat_ngram_size=0, search_strategy=None, eos=None, symbols_to_strip_from_output=None, lm_model=None, lm_weight=1.0, tokens_to_suppress=()):
        """Generates translations of a given source sentence.

        Args:
            models (List[~fairseq.models.FairseqModel]): ensemble of models,
                currently support fairseq.models.TransformerModel for scripting
            beam_size (int, optional): beam width (default: 1)
            max_len_a/b (int, optional): generate sequences of maximum length
                ax + b, where x is the source length
            max_len (int, optional): the maximum length of the generated output
                (not including end-of-sentence)
            min_len (int, optional): the minimum length of the generated output
                (not including end-of-sentence)
            normalize_scores (bool, optional): normalize scores by the length
                of the output (default: True)
            len_penalty (float, optional): length penalty, where <1.0 favors
                shorter, >1.0 favors longer sentences (default: 1.0)
            unk_penalty (float, optional): unknown word penalty, where <0
                produces more unks, >0 produces fewer (default: 0.0)
            temperature (float, optional): temperature, where values
                >1.0 produce more uniform samples and values <1.0 produce
                sharper samples (default: 1.0)
            match_source_len (bool, optional): outputs should match the source
                length (default: False)
        """
        super().__init__()
        if isinstance(models, EnsembleModel):
            self.model = models
        else:
            self.model = EnsembleModel(models)
        self.tgt_dict = tgt_dict
        self.pad = tgt_dict.pad()
        self.unk = tgt_dict.unk()
        self.eos = tgt_dict.eos() if eos is None else eos
        self.symbols_to_strip_from_output = symbols_to_strip_from_output.union({self.eos}) if symbols_to_strip_from_output is not None else {self.eos}
        self.token_indices_to_suppress: Optional[Tensor] = None
        token_indices_to_suppress = []
        for token_string in tokens_to_suppress:
            token_index = tgt_dict.index(token_string)
            assert token_index != self.unk
            token_indices_to_suppress.append(token_index)
        if len(token_indices_to_suppress) > 0:
            self.token_indices_to_suppress = torch.Tensor(token_indices_to_suppress).long()
        self.vocab_size = len(tgt_dict)
        self.beam_size = beam_size
        self.beam_size = min(beam_size, self.vocab_size - 1)
        self.model.set_decoder_beam_size(self.beam_size)
        self.max_len_a = max_len_a
        self.max_len_b = max_len_b
        self.min_len = min_len
        self.max_len = max_len or self.model.max_decoder_positions()
        self.normalize_scores = normalize_scores
        self.len_penalty = len_penalty
        self.unk_penalty = unk_penalty
        self.temperature = temperature
        self.match_source_len = match_source_len
        if no_repeat_ngram_size > 0:
            self.repeat_ngram_blocker = NGramRepeatBlock(no_repeat_ngram_size)
        else:
            self.repeat_ngram_blocker = None
        assert temperature > 0, '--temperature must be greater than 0'
        self.search = search.BeamSearch(tgt_dict) if search_strategy is None else search_strategy
        self.should_set_src_lengths = hasattr(self.search, 'needs_src_lengths') and self.search.needs_src_lengths
        self.model.eval()
        self.lm_model = lm_model
        self.lm_weight = lm_weight
        if self.lm_model is not None:
            self.lm_model.eval()

    def cuda(self):
        self.model
        return self

    @torch.no_grad()
    def forward(self, sample: Dict[str, Dict[str, Tensor]], prefix_tokens: Optional[Tensor]=None, bos_token: Optional[int]=None):
        """Generate a batch of translations.

        Args:
            sample (dict): batch
            prefix_tokens (torch.LongTensor, optional): force decoder to begin
                with these tokens
            bos_token (int, optional): beginning of sentence token
                (default: self.eos)
        """
        return self._generate(sample, prefix_tokens, bos_token=bos_token)

    def generate_batched_itr(self, data_itr, beam_size=None, cuda=False, timer=None):
        """Iterate over a batched dataset and yield individual translations.
        Args:
            cuda (bool, optional): use GPU for generation
            timer (StopwatchMeter, optional): time generations
        """
        for sample in data_itr:
            s = utils.move_to_cuda(sample) if cuda else sample
            if 'net_input' not in s:
                continue
            input = s['net_input']
            encoder_input = {k: v for k, v in input.items() if k != 'prev_output_tokens'}
            if timer is not None:
                timer.start()
            with torch.no_grad():
                hypos = self.generate(encoder_input)
            if timer is not None:
                timer.stop(sum(len(h[0]['tokens']) for h in hypos))
            for i, id in enumerate(s['id'].data):
                src = utils.strip_pad(input['src_tokens'].data[i, :], self.pad)
                ref = utils.strip_pad(s['target'].data[i, :], self.pad) if s['target'] is not None else None
                yield id, src, ref, hypos[i]

    @torch.no_grad()
    def generate(self, models, sample: Dict[str, Dict[str, Tensor]], **kwargs) ->List[List[Dict[str, Tensor]]]:
        """Generate translations. Match the api of other fairseq generators.

        Args:
            models (List[~fairseq.models.FairseqModel]): ensemble of models
            sample (dict): batch
            prefix_tokens (torch.LongTensor, optional): force decoder to begin
                with these tokens
            constraints (torch.LongTensor, optional): force decoder to include
                the list of constraints
            bos_token (int, optional): beginning of sentence token
                (default: self.eos)
        """
        return self._generate(sample, **kwargs)

    def _generate(self, sample: Dict[str, Dict[str, Tensor]], prefix_tokens: Optional[Tensor]=None, constraints: Optional[Tensor]=None, bos_token: Optional[int]=None):
        incremental_states = torch.jit.annotate(List[Dict[str, Dict[str, Optional[Tensor]]]], [torch.jit.annotate(Dict[str, Dict[str, Optional[Tensor]]], {}) for i in range(self.model.models_size)])
        net_input = sample['net_input']
        if 'src_tokens' in net_input:
            src_tokens = net_input['src_tokens']
            if 'src_lengths' in net_input:
                src_lengths = net_input['src_lengths']
            else:
                src_lengths = (src_tokens.ne(self.eos) & src_tokens.ne(self.pad)).long().sum(dim=1)
        elif 'source' in net_input:
            src_tokens = net_input['source']
            src_lengths = net_input['padding_mask'].size(-1) - net_input['padding_mask'].sum(-1) if net_input['padding_mask'] is not None else torch.tensor(src_tokens.size(-1))
        elif 'features' in net_input:
            src_tokens = net_input['features']
            src_lengths = net_input['padding_mask'].size(-1) - net_input['padding_mask'].sum(-1) if net_input['padding_mask'] is not None else torch.tensor(src_tokens.size(-1))
        else:
            raise Exception('expected src_tokens or source in net input. input keys: ' + str(net_input.keys()))
        bsz, src_len = src_tokens.size()[:2]
        beam_size = self.beam_size
        if constraints is not None and not self.search.supports_constraints:
            raise NotImplementedError("Target-side constraints were provided, but search method doesn't support them")
        self.search.init_constraints(constraints, beam_size)
        max_len: int = -1
        if self.match_source_len:
            max_len = src_lengths.max().item()
        else:
            max_len = min(int(self.max_len_a * src_len + self.max_len_b), self.max_len - 1)
        assert self.min_len <= max_len, 'min_len cannot be larger than max_len, please adjust these!'
        with torch.autograd.profiler.record_function('EnsembleModel: forward_encoder'):
            encoder_outs = self.model.forward_encoder(net_input)
        new_order = torch.arange(bsz).view(-1, 1).repeat(1, beam_size).view(-1)
        new_order = new_order.long()
        encoder_outs = self.model.reorder_encoder_out(encoder_outs, new_order)
        assert encoder_outs is not None
        scores = torch.zeros(bsz * beam_size, max_len + 1).float()
        tokens = torch.zeros(bsz * beam_size, max_len + 2).long().fill_(self.pad)
        tokens[:, 0] = self.eos if bos_token is None else bos_token
        attn: Optional[Tensor] = None
        cands_to_ignore = torch.zeros(bsz, beam_size).eq(-1)
        finalized = torch.jit.annotate(List[List[Dict[str, Tensor]]], [torch.jit.annotate(List[Dict[str, Tensor]], []) for i in range(bsz)])
        finished = [(False) for i in range(bsz)]
        num_remaining_sent = bsz
        cand_size = 2 * beam_size
        bbsz_offsets = (torch.arange(0, bsz) * beam_size).unsqueeze(1).type_as(tokens)
        cand_offsets = torch.arange(0, cand_size).type_as(tokens)
        reorder_state: Optional[Tensor] = None
        batch_idxs: Optional[Tensor] = None
        original_batch_idxs: Optional[Tensor] = None
        if 'id' in sample and isinstance(sample['id'], Tensor):
            original_batch_idxs = sample['id']
        else:
            original_batch_idxs = torch.arange(0, bsz).type_as(tokens)
        for step in range(max_len + 1):
            if reorder_state is not None:
                if batch_idxs is not None:
                    corr = batch_idxs - torch.arange(batch_idxs.numel()).type_as(batch_idxs)
                    reorder_state.view(-1, beam_size).add_(corr.unsqueeze(-1) * beam_size)
                    original_batch_idxs = original_batch_idxs[batch_idxs]
                self.model.reorder_incremental_state(incremental_states, reorder_state)
                encoder_outs = self.model.reorder_encoder_out(encoder_outs, reorder_state)
            with torch.autograd.profiler.record_function('EnsembleModel: forward_decoder'):
                lprobs, avg_attn_scores = self.model.forward_decoder(tokens[:, :step + 1], encoder_outs, incremental_states, self.temperature)
            if self.lm_model is not None:
                lm_out = self.lm_model(tokens[:, :step + 1])
                probs = self.lm_model.get_normalized_probs(lm_out, log_probs=True, sample=None)
                probs = probs[:, -1, :] * self.lm_weight
                lprobs += probs
            lprobs[lprobs != lprobs] = torch.tensor(-math.inf)
            lprobs[:, self.pad] = -math.inf
            lprobs[:, self.unk] -= self.unk_penalty
            if step >= max_len:
                lprobs[:, :self.eos] = -math.inf
                lprobs[:, self.eos + 1:] = -math.inf
            if prefix_tokens is not None and step < prefix_tokens.size(1) and step < max_len:
                lprobs, tokens, scores = self._prefix_tokens(step, lprobs, scores, tokens, prefix_tokens, beam_size)
            else:
                if step < self.min_len:
                    lprobs[:, self.eos] = -math.inf
                if self.token_indices_to_suppress is not None:
                    lprobs[:, self.token_indices_to_suppress] = -math.inf
            if avg_attn_scores is not None:
                if attn is None:
                    attn = torch.empty(bsz * beam_size, avg_attn_scores.size(1), max_len + 2)
                attn[:, :, step + 1].copy_(avg_attn_scores)
            scores = scores.type_as(lprobs)
            eos_bbsz_idx = torch.empty(0)
            eos_scores = torch.empty(0)
            if self.should_set_src_lengths:
                self.search.set_src_lengths(src_lengths)
            if self.repeat_ngram_blocker is not None:
                lprobs = self.repeat_ngram_blocker(tokens, lprobs, bsz, beam_size, step)
            cand_scores, cand_indices, cand_beams = self.search.step(step, lprobs.view(bsz, -1, self.vocab_size), scores.view(bsz, beam_size, -1)[:, :, :step], tokens[:, :step + 1], original_batch_idxs)
            cand_bbsz_idx = cand_beams.add(bbsz_offsets)
            eos_mask = cand_indices.eq(self.eos) & cand_scores.ne(-math.inf)
            eos_mask[:, :beam_size][cands_to_ignore] = torch.tensor(0)
            eos_bbsz_idx = torch.masked_select(cand_bbsz_idx[:, :beam_size], mask=eos_mask[:, :beam_size])
            finalized_sents: List[int] = []
            if eos_bbsz_idx.numel() > 0:
                eos_scores = torch.masked_select(cand_scores[:, :beam_size], mask=eos_mask[:, :beam_size])
                finalized_sents = self.finalize_hypos(step, eos_bbsz_idx, eos_scores, tokens, scores, finalized, finished, beam_size, attn, src_lengths, max_len)
                num_remaining_sent -= len(finalized_sents)
            assert num_remaining_sent >= 0
            if num_remaining_sent == 0:
                break
            if self.search.stop_on_max_len and step >= max_len:
                break
            assert step < max_len, f'{step} < {max_len}'
            if len(finalized_sents) > 0:
                new_bsz = bsz - len(finalized_sents)
                batch_mask = torch.ones(bsz, dtype=torch.bool, device=cand_indices.device)
                batch_mask[finalized_sents] = False
                batch_idxs = torch.arange(bsz, device=cand_indices.device).masked_select(batch_mask)
                self.search.prune_sentences(batch_idxs)
                eos_mask = eos_mask[batch_idxs]
                cand_beams = cand_beams[batch_idxs]
                bbsz_offsets.resize_(new_bsz, 1)
                cand_bbsz_idx = cand_beams.add(bbsz_offsets)
                cand_scores = cand_scores[batch_idxs]
                cand_indices = cand_indices[batch_idxs]
                if prefix_tokens is not None:
                    prefix_tokens = prefix_tokens[batch_idxs]
                src_lengths = src_lengths[batch_idxs]
                cands_to_ignore = cands_to_ignore[batch_idxs]
                scores = scores.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)
                tokens = tokens.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, -1)
                if attn is not None:
                    attn = attn.view(bsz, -1)[batch_idxs].view(new_bsz * beam_size, attn.size(1), -1)
                bsz = new_bsz
            else:
                batch_idxs = None
            eos_mask[:, :beam_size] = ~(~cands_to_ignore & ~eos_mask[:, :beam_size])
            active_mask = torch.add(eos_mask.type_as(cand_offsets) * cand_size, cand_offsets[:eos_mask.size(1)])
            new_cands_to_ignore, active_hypos = torch.topk(active_mask, k=beam_size, dim=1, largest=False)
            cands_to_ignore = new_cands_to_ignore.ge(cand_size)[:, :beam_size]
            assert (~cands_to_ignore).any(dim=1).all()
            active_bbsz_idx = torch.gather(cand_bbsz_idx, dim=1, index=active_hypos)
            active_scores = torch.gather(cand_scores, dim=1, index=active_hypos)
            active_bbsz_idx = active_bbsz_idx.view(-1)
            active_scores = active_scores.view(-1)
            tokens[:, :step + 1] = torch.index_select(tokens[:, :step + 1], dim=0, index=active_bbsz_idx)
            tokens.view(bsz, beam_size, -1)[:, :, step + 1] = torch.gather(cand_indices, dim=1, index=active_hypos)
            if step > 0:
                scores[:, :step] = torch.index_select(scores[:, :step], dim=0, index=active_bbsz_idx)
            scores.view(bsz, beam_size, -1)[:, :, step] = torch.gather(cand_scores, dim=1, index=active_hypos)
            self.search.update_constraints(active_hypos)
            if attn is not None:
                attn[:, :, :step + 2] = torch.index_select(attn[:, :, :step + 2], dim=0, index=active_bbsz_idx)
            reorder_state = active_bbsz_idx
        for sent in range(len(finalized)):
            scores = torch.tensor([float(elem['score'].item()) for elem in finalized[sent]])
            _, sorted_scores_indices = torch.sort(scores, descending=True)
            finalized[sent] = [finalized[sent][ssi] for ssi in sorted_scores_indices]
            finalized[sent] = torch.jit.annotate(List[Dict[str, Tensor]], finalized[sent])
        return finalized

    def _prefix_tokens(self, step: int, lprobs, scores, tokens, prefix_tokens, beam_size: int):
        """Handle prefix tokens"""
        prefix_toks = prefix_tokens[:, step].unsqueeze(-1).repeat(1, beam_size).view(-1)
        prefix_lprobs = lprobs.gather(-1, prefix_toks.unsqueeze(-1))
        prefix_mask = prefix_toks.ne(self.pad)
        lprobs[prefix_mask] = torch.tensor(-math.inf)
        lprobs[prefix_mask] = lprobs[prefix_mask].scatter(-1, prefix_toks[prefix_mask].unsqueeze(-1), prefix_lprobs[prefix_mask])
        eos_mask = prefix_toks.eq(self.eos)
        if eos_mask.any():
            first_beam = tokens[eos_mask].view(-1, beam_size, tokens.size(-1))[:, 0, 1:step + 1]
            eos_mask_batch_dim = eos_mask.view(-1, beam_size)[:, 0]
            target_prefix = prefix_tokens[eos_mask_batch_dim][:, :step]
            assert (first_beam == target_prefix).all()
            tokens = self.replicate_first_beam(tokens, eos_mask_batch_dim, beam_size)
            scores = self.replicate_first_beam(scores, eos_mask_batch_dim, beam_size)
            lprobs = self.replicate_first_beam(lprobs, eos_mask_batch_dim, beam_size)
        return lprobs, tokens, scores

    def replicate_first_beam(self, tensor, mask, beam_size: int):
        tensor = tensor.view(-1, beam_size, tensor.size(-1))
        tensor[mask] = tensor[mask][:, :1, :]
        return tensor.view(-1, tensor.size(-1))

    def finalize_hypos(self, step: int, bbsz_idx, eos_scores, tokens, scores, finalized: List[List[Dict[str, Tensor]]], finished: List[bool], beam_size: int, attn: Optional[Tensor], src_lengths, max_len: int):
        """Finalize hypothesis, store finalized information in `finalized`, and change `finished` accordingly.
        A sentence is finalized when {beam_size} finished items have been collected for it.

        Returns number of sentences (not beam items) being finalized.
        These will be removed from the batch and not processed further.
        Args:
            bbsz_idx (Tensor):
        """
        assert bbsz_idx.numel() == eos_scores.numel()
        tokens_clone = tokens.index_select(0, bbsz_idx)[:, 1:step + 2]
        tokens_clone[:, step] = self.eos
        attn_clone = attn.index_select(0, bbsz_idx)[:, :, 1:step + 2] if attn is not None else None
        pos_scores = scores.index_select(0, bbsz_idx)[:, :step + 1]
        pos_scores[:, step] = eos_scores
        pos_scores[:, 1:] = pos_scores[:, 1:] - pos_scores[:, :-1]
        if self.normalize_scores:
            eos_scores /= (step + 1) ** self.len_penalty
        cum_unfin: List[int] = []
        prev = 0
        for f in finished:
            if f:
                prev += 1
            else:
                cum_unfin.append(prev)
        cum_fin_tensor = torch.tensor(cum_unfin, dtype=torch.int)
        unfin_idx = torch.div(bbsz_idx, beam_size, rounding_mode='trunc')
        sent = unfin_idx + torch.index_select(cum_fin_tensor, 0, unfin_idx)
        seen = (sent << 32) + unfin_idx
        unique_seen: List[int] = torch.unique(seen).tolist()
        if self.match_source_len:
            condition = step > torch.index_select(src_lengths, 0, unfin_idx)
            eos_scores = torch.where(condition, torch.tensor(-math.inf), eos_scores)
        sent_list: List[int] = sent.tolist()
        for i in range(bbsz_idx.size()[0]):
            if len(finalized[sent_list[i]]) < beam_size:
                if attn_clone is not None:
                    hypo_attn = attn_clone[i]
                else:
                    hypo_attn = torch.empty(0)
                finalized[sent_list[i]].append({'tokens': tokens_clone[i], 'score': eos_scores[i], 'attention': hypo_attn, 'alignment': torch.empty(0), 'positional_scores': pos_scores[i]})
        newly_finished: List[int] = []
        for unique_s in unique_seen:
            unique_sent: int = unique_s >> 32
            unique_unfin_idx: int = unique_s - (unique_sent << 32)
            if not finished[unique_sent] and self.is_finished(step, unique_unfin_idx, max_len, len(finalized[unique_sent]), beam_size):
                finished[unique_sent] = True
                newly_finished.append(unique_unfin_idx)
        return newly_finished

    def is_finished(self, step: int, unfin_idx: int, max_len: int, finalized_sent_len: int, beam_size: int):
        """
        Check whether decoding for a sentence is finished, which
        occurs when the list of finalized sentences has reached the
        beam size, or when we reach the maximum length.
        """
        assert finalized_sent_len <= beam_size
        if finalized_sent_len == beam_size or step == max_len:
            return True
        return False


class EnsembleModelWithAlignment(EnsembleModel):
    """A wrapper around an ensemble of models."""

    def __init__(self, models):
        super().__init__(models)

    def forward_align(self, src_tokens, src_lengths, prev_output_tokens):
        avg_attn = None
        for model in self.models:
            decoder_out = model(src_tokens, src_lengths, prev_output_tokens)
            attn = decoder_out[1]['attn'][0]
            if avg_attn is None:
                avg_attn = attn
            else:
                avg_attn.add_(attn)
        if len(self.models) > 1:
            avg_attn.div_(len(self.models))
        return avg_attn


class SequenceGeneratorWithAlignment(SequenceGenerator):

    def __init__(self, models, tgt_dict, left_pad_target=False, print_alignment='hard', **kwargs):
        """Generates translations of a given source sentence.

        Produces alignments following "Jointly Learning to Align and
        Translate with Transformer Models" (Garg et al., EMNLP 2019).

        Args:
            left_pad_target (bool, optional): Whether or not the
                hypothesis should be left padded or not when they are
                teacher forced for generating alignments.
        """
        super().__init__(EnsembleModelWithAlignment(models), tgt_dict, **kwargs)
        self.left_pad_target = left_pad_target
        if print_alignment == 'hard':
            self.extract_alignment = utils.extract_hard_alignment
        elif print_alignment == 'soft':
            self.extract_alignment = utils.extract_soft_alignment

    @torch.no_grad()
    def generate(self, models, sample, **kwargs):
        finalized = super()._generate(sample, **kwargs)
        src_tokens = sample['net_input']['src_tokens']
        bsz = src_tokens.shape[0]
        beam_size = self.beam_size
        src_tokens, src_lengths, prev_output_tokens, tgt_tokens = self._prepare_batch_for_alignment(sample, finalized)
        if any(getattr(m, 'full_context_alignment', False) for m in self.model.models):
            attn = self.model.forward_align(src_tokens, src_lengths, prev_output_tokens)
        else:
            attn = [finalized[i // beam_size][i % beam_size]['attention'].transpose(1, 0) for i in range(bsz * beam_size)]
        if src_tokens.device != 'cpu':
            src_tokens = src_tokens
            tgt_tokens = tgt_tokens
            attn = [i for i in attn]
        for i in range(bsz * beam_size):
            alignment = self.extract_alignment(attn[i], src_tokens[i], tgt_tokens[i], self.pad, self.eos)
            finalized[i // beam_size][i % beam_size]['alignment'] = alignment
        return finalized

    def _prepare_batch_for_alignment(self, sample, hypothesis):
        src_tokens = sample['net_input']['src_tokens']
        bsz = src_tokens.shape[0]
        src_tokens = src_tokens[:, None, :].expand(-1, self.beam_size, -1).contiguous().view(bsz * self.beam_size, -1)
        src_lengths = sample['net_input']['src_lengths']
        src_lengths = src_lengths[:, None].expand(-1, self.beam_size).contiguous().view(bsz * self.beam_size)
        prev_output_tokens = data_utils.collate_tokens([beam['tokens'] for example in hypothesis for beam in example], self.pad, self.eos, self.left_pad_target, move_eos_to_beginning=True)
        tgt_tokens = data_utils.collate_tokens([beam['tokens'] for example in hypothesis for beam in example], self.pad, self.eos, self.left_pad_target, move_eos_to_beginning=False)
        return src_tokens, src_lengths, prev_output_tokens, tgt_tokens


class StackUnitSequenceGenerator(nn.Module):

    def __init__(self, tgt_dict, vocab_size):
        super().__init__()
        self.pad = tgt_dict.pad()
        self.eos = tgt_dict.eos()
        self.unk = tgt_dict.unk()
        self.offset = len(tgt_dict) - vocab_size
        self.vocab_size = vocab_size

    def pack_units(self, input: torch.Tensor, n_frames_per_step) ->torch.Tensor:
        if n_frames_per_step <= 1:
            return input
        bsz, _, n = input.shape
        assert n == n_frames_per_step
        scale = [pow(self.vocab_size, n_frames_per_step - 1 - i) for i in range(n_frames_per_step)]
        scale = torch.LongTensor(scale).squeeze(0)
        mask = input >= self.offset
        res = ((input - self.offset) * scale * mask).sum(dim=2) + self.offset
        return res

    @torch.no_grad()
    def generate(self, models, sample, **kwargs):
        model = models[0]
        model.eval()
        max_len = model.max_decoder_positions()
        src_tokens = sample['net_input']['src_tokens']
        src_lengths = sample['net_input']['src_lengths']
        bsz, src_len, _ = src_tokens.size()
        n_frames_per_step = model.decoder.n_frames_per_step
        encoder_out = model.forward_encoder(src_tokens, src_lengths, speaker=sample['speaker'])
        incremental_state = {}
        pred_out, attn, scores = [], [], []
        finished = src_tokens.new_zeros((bsz,)).bool()
        prev_output_tokens = src_lengths.new_zeros((bsz, 1)).long().fill_(self.eos)
        for _ in range(max_len):
            cur_out, cur_extra = model.forward_decoder(prev_output_tokens, encoder_out=encoder_out, incremental_state=incremental_state)
            lprobs = model.get_normalized_probs([cur_out], log_probs=True)
            lprobs[:, :, self.pad] = -math.inf
            lprobs[:, :, self.unk] = -math.inf
            cur_pred_lprob, cur_pred_out = torch.max(lprobs, dim=2)
            scores.append(cur_pred_lprob)
            pred_out.append(cur_pred_out)
            prev_output_tokens = torch.cat((prev_output_tokens, self.pack_units(cur_pred_out.view(bsz, 1, n_frames_per_step), n_frames_per_step)), dim=1)
            attn.append(cur_extra['attn'][0])
            cur_finished = torch.any(cur_pred_out.squeeze(1) == self.eos, dim=1)
            finished = finished | cur_finished
            if finished.sum().item() == bsz:
                break
        pred_out = torch.cat(pred_out, dim=1).view(bsz, -1)
        attn = torch.cat(attn, dim=2)
        alignment = attn.max(dim=1)[1]
        attn = attn.repeat_interleave(n_frames_per_step, dim=2)
        alignment = alignment.repeat_interleave(n_frames_per_step, dim=1)
        scores = torch.cat(scores, dim=1)
        eos_idx = (pred_out == self.eos).nonzero(as_tuple=True)
        out_lens = src_lengths.new_zeros((bsz,)).long().fill_(max_len)
        for b, l in zip(eos_idx[0], eos_idx[1]):
            out_lens[b] = min(l, out_lens[b])
        hypos = [[{'tokens': pred_out[b, :out_len], 'attn': attn[b, :, :out_len], 'alignment': alignment[b, :out_len], 'positional_scores': scores[b, :out_len], 'score': utils.item(scores[b, :out_len].sum().data)}] for b, out_len in zip(range(bsz), out_lens)]
        return hypos


class Model(nn.Module):

    def __init__(self, use_pytorch_checkpoint=False, use_fairseq_checkpoint=False, **kwargs):
        super().__init__()
        torch.manual_seed(0)
        self.use_pytorch_checkpoint = use_pytorch_checkpoint
        self.ffn = nn.Sequential(nn.Linear(32, 128), nn.Dropout(p=0.5), nn.Linear(128, 32))
        if use_fairseq_checkpoint:
            self.ffn = checkpoint_wrapper(self.ffn, **kwargs)
        self.out = nn.Linear(32, 1)

    def forward(self, x):
        if self.use_pytorch_checkpoint:
            x = checkpoint(self.ffn, x)
        else:
            x = self.ffn(x)
        return self.out(x)


class ModuleWithDelay(nn.Module):

    def __init__(self, delay):
        super().__init__()
        self.delay = delay

    def forward(self, x):
        time.sleep(self.delay)
        return x


class MockDDPWrapper(nn.Module):
    """A simple wrapper with an interface similar to DistributedDataParallel."""

    def __init__(self, module):
        super().__init__()
        self.module = module

    def forward(self, x):
        return self.module(x)


class DummyModule(torch.nn.Module):

    def __init__(self) ->None:
        """LightningModule for testing purposes

        Args:
            epoch_min_loss_override (int, optional): Pass in an epoch that will be set to the minimum
                validation loss for testing purposes (zero based). If None this is ignored. Defaults to None.
        """
        super().__init__()
        self.layer = torch.nn.Linear(in_features=32, out_features=2)
        self.another_layer = torch.nn.Linear(in_features=2, out_features=2)

    def forward(self, x: torch.Tensor) ->torch.Tensor:
        x = self.layer(x)
        return self.another_layer(x)


class DummyEncoder(FairseqEncoder):

    def __init__(self):
        super().__init__(None)

    def forward(self, src_tokens, src_lengths):
        mask, max_len = lengths_to_encoder_padding_mask(src_lengths)
        return {'encoder_out': src_tokens, 'encoder_padding_mask': mask}


class ModelWithSharedParameter(nn.Module):

    def __init__(self):
        super(ModelWithSharedParameter, self).__init__()
        self.embedding = nn.Embedding(1000, 200)
        self.FC1 = nn.Linear(200, 200)
        self.FC2 = nn.Linear(200, 200)
        self.FC2.weight = nn.Parameter(self.FC1.weight)
        self.FC2.bias = nn.Parameter(self.FC1.bias)
        self.relu = nn.ReLU()

    def forward(self, input):
        return self.FC2(self.ReLU(self.FC1(input))) + self.FC1(input)


class TestEncoder(FairseqEncoder):

    def __init__(self, args, dictionary):
        super().__init__(dictionary)
        self.args = args

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        return EncoderOut(encoder_out=src_tokens, encoder_padding_mask=None, encoder_embedding=None, encoder_states=None, src_tokens=None, src_lengths=None)

    def reorder_encoder_out(self, encoder_out, new_order):
        return EncoderOut(encoder_out=encoder_out.encoder_out.index_select(0, new_order), encoder_padding_mask=None, encoder_embedding=None, encoder_states=None, src_tokens=None, src_lengths=None)


class TestIncrementalDecoder(FairseqIncrementalDecoder):

    def __init__(self, args, dictionary):
        super().__init__(dictionary)
        assert hasattr(args, 'beam_probs') or hasattr(args, 'probs')
        args.max_decoder_positions = getattr(args, 'max_decoder_positions', 100)
        self.args = args

    def forward(self, prev_output_tokens, encoder_out=None, incremental_state=None):
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
        bbsz = prev_output_tokens.size(0)
        vocab = len(self.dictionary)
        src_len = encoder_out.encoder_out.size(1)
        tgt_len = prev_output_tokens.size(1)
        if incremental_state is not None:
            step = utils.get_incremental_state(self, incremental_state, 'step')
            if step is None:
                step = 0
            utils.set_incremental_state(self, incremental_state, 'step', step + 1)
            steps = [step]
        else:
            steps = list(range(tgt_len))
        if hasattr(self.args, 'probs'):
            assert self.args.probs.dim() == 3, 'expected probs to have size bsz*steps*vocab'
            probs = self.args.probs.index_select(1, torch.LongTensor(steps))
        else:
            probs = torch.FloatTensor(bbsz, len(steps), vocab).zero_()
            for i, step in enumerate(steps):
                if step < len(self.args.beam_probs):
                    probs[:, i, self.dictionary.eos():] = self.args.beam_probs[step]
                else:
                    probs[:, i, self.dictionary.eos()] = 1.0
        attn = torch.rand(bbsz, tgt_len, src_len)
        dev = prev_output_tokens.device
        return probs, {'attn': [attn]}

    def get_normalized_probs(self, net_output, log_probs, _):
        probs = net_output[0]
        if log_probs:
            return probs.log()
        else:
            return probs

    def max_positions(self):
        return self.args.max_decoder_positions


class TestReshapingEncoder(FairseqEncoder):

    def __init__(self, args, dictionary):
        super().__init__(dictionary)
        self.args = args

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        b_sz, t_sz = src_tokens.shape
        padding_needed = t_sz % 2
        x = src_tokens
        if padding_needed > 0:
            padding_needed = 2 - padding_needed
            x = F.pad(x, (0, padding_needed))
        return EncoderOut(encoder_out=x.view(b_sz, -1, 2), encoder_padding_mask=None, encoder_embedding=None, encoder_states=None, src_tokens=None, src_lengths=None)

    def reorder_encoder_out(self, encoder_out, new_order):
        return EncoderOut(encoder_out=encoder_out.encoder_out.index_select(0, new_order), encoder_padding_mask=None, encoder_embedding=None, encoder_states=None, src_tokens=None, src_lengths=None)


class TestAdditionalInputEncoder(FairseqEncoder):

    def __init__(self, args, dictionary):
        super().__init__(dictionary)
        self.args = args

    def forward(self, src_tokens, src_lengths=None, **kwargs):
        assert 'fancy_other_input' in kwargs
        assert kwargs['fancy_other_input'] is not None
        return EncoderOut(encoder_out=src_tokens, encoder_padding_mask=None, encoder_embedding=None, encoder_states=None, src_tokens=None, src_lengths=None)

    def reorder_encoder_out(self, encoder_out, new_order):
        return EncoderOut(encoder_out=encoder_out.encoder_out.index_select(0, new_order), encoder_padding_mask=None, encoder_embedding=None, encoder_states=None, src_tokens=None, src_lengths=None)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AdaptiveMask,
     lambda: ([], {'max_size': 4, 'ramp_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (AdaptiveSoftmax,
     lambda: ([], {'vocab_size': 4, 'input_dim': 4, 'cutoff': [4, 4], 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (AdaptiveSpan,
     lambda: ([], {'attn_span': 4, 'adapt_span_ramp': 4, 'adapt_span_init': 4, 'n_head': 4, 'adapt_span_layer': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (AlignHead,
     lambda: ([], {'config': _mock_config(hidden_size=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BLSTM,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (BeamableMM,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     True),
    (CPCAR,
     lambda: ([], {'dim_encoded': 4, 'dim_output': 4, 'keep_hidden': 4, 'num_layers': 1}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (CPCModel,
     lambda: ([], {'encoder': _mock_layer(), 'ar_net': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     True),
    (CTCDecoder,
     lambda: ([], {'dictionary': [4, 4], 'in_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (ChannelNorm,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Chunk,
     lambda: ([], {'chunks': 4, 'fn': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Conv1dSubsampler,
     lambda: ([], {'in_channels': 4, 'mid_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (ConvNorm,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (ConvTBC,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (Demucs,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (DepthWiseConv1d,
     lambda: ([], {'dim_in': 4, 'dim_out': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (Downsample,
     lambda: ([], {'index': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DownsampledMultiHeadAttention,
     lambda: ([], {'out_channels': 4, 'embed_dim': 4, 'num_heads': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (DropPath,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (DynamicConv_scripatable,
     lambda: ([], {'input_size': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (ESPNETMultiHeadedAttention,
     lambda: ([], {'n_feat': 4, 'n_head': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (FairseqDropout,
     lambda: ([], {'p': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FeedForward,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FeedForwardLayer,
     lambda: ([], {'d_model': 4, 'd_inner': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FixedPositionalEmbedding,
     lambda: ([], {'dim': 4, 'max_seq_len': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FixedPositionalEncoder,
     lambda: ([], {'pos_embed': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (Fp32BatchNorm,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (Fp32GroupNorm,
     lambda: ([], {'num_groups': 1, 'num_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Fp32InstanceNorm,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Fp32LayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GELU_,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GlobalAvgPool,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GumbelVectorQuantizer,
     lambda: ([], {'dim': 4, 'num_vars': 4, 'temp': [4, 4, 4], 'groups': 1, 'combine_groups': 1, 'vq_dim': 4, 'time_first': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (HeadSelectionLoss,
     lambda: ([], {'args': _mock_config(kl_weight=4)}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (Highway,
     lambda: ([], {'input_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (InceptionBlock,
     lambda: ([], {'input_dim': 4, 'num_outputs_0_0a': 4, 'num_outputs_1_0a': 4, 'num_outputs_1_0b': 4, 'num_outputs_2_0a': 4, 'num_outputs_2_0b': 4, 'num_outputs_3_0b': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     False),
    (Invertible1x1Conv,
     lambda: ([], {'c': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (Kmeans,
     lambda: ([], {'num_heads': 4, 'head_dim': 4, 'num_clusters': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (KmeansAttention,
     lambda: ([], {'num_clusters': 4, 'window_size': 4, 'num_heads': 4, 'head_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (KmeansVectorQuantizer,
     lambda: ([], {'dim': 4, 'num_vars': 4, 'groups': 1, 'combine_groups': 1, 'vq_dim': 4, 'time_first': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (LightweightConv1d,
     lambda: ([], {'input_size': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (LinearNorm,
     lambda: ([], {'in_dim': 4, 'out_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LinearizedConvolution,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (LocationLayer,
     lambda: ([], {'attention_n_filters': 4, 'attention_kernel_size': 4, 'attention_dim': 4}),
     lambda: ([torch.rand([4, 2, 64])], {}),
     True),
    (MockDDPWrapper,
     lambda: ([], {'module': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ModuleWithDelay,
     lambda: ([], {'delay': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (NoOp,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PatchEmbed,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (PositionalEncoding,
     lambda: ([], {'d_model': 4, 'dropout_rate': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PreNorm,
     lambda: ([], {'norm_class': _mock_layer, 'dim': 4, 'fn': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Prenet,
     lambda: ([], {'in_dim': 4, 'n_layers': 1, 'n_units': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReZero,
     lambda: ([], {'fn': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (RelPositionMultiHeadedAttention,
     lambda: ([], {'n_feat': 4, 'n_head': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4]), torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (RelPositionalEncoding,
     lambda: ([], {'max_len': 4, 'd_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (RelativePositionBias,
     lambda: ([], {'window_size': [4, 4], 'num_heads': 4}),
     lambda: ([], {}),
     True),
    (RelativePositionEmbedding,
     lambda: ([], {'head_dim': 4, 'max_position': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ResBlock,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (RotaryPositionalEmbedding,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (STConv3D,
     lambda: ([], {'input_dim': 4, 'output_dim': 4, 'kernel_size': [4, 4, 4]}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     False),
    (SamePad,
     lambda: ([], {'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SamePad2d,
     lambda: ([], {'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ScaleNorm,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SelfGating,
     lambda: ([], {'input_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     True),
    (SeqAttention,
     lambda: ([], {'d_model': 4, 'n_head': 4, 'attn_span': 4, 'dropout': 0.5, 'adapt_span_layer': 1, 'adapt_span_ramp': 4, 'adapt_span_init': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4]), torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (SingleHeadAttention,
     lambda: ([], {'out_channels': 4, 'embed_dim': 4, 'head_dim': 4, 'head_index': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (TestEncoder,
     lambda: ([], {'args': _mock_config(), 'dictionary': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (TestReshapingEncoder,
     lambda: ([], {'args': _mock_config(), 'dictionary': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (TextFeatPositionalEncoder,
     lambda: ([], {'pos_encoder': _mock_layer()}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (TransposeLast,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (VGGBlock,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'conv_kernel_size': 4, 'pooling_kernel_size': 4, 'num_conv_layers': 1, 'input_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (VariancePredictor,
     lambda: ([], {'args': _mock_config(encoder_embed_dim=4, var_pred_hidden_dim=4, var_pred_kernel_size=4, var_pred_dropout=0.5)}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (Wav2VecPredictionsModel,
     lambda: ([], {'in_dim': 4, 'out_dim': 4, 'prediction_steps': 4, 'n_negatives': 4, 'cross_sample_negatives': 4, 'sample_distance': 4, 'dropout': 0.5, 'offset': 4, 'balanced_classes': 4, 'infonce': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (WaveGlowLoss,
     lambda: ([], {}),
     lambda: ([(torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]))], {}),
     False),
    (ZeroPad1d,
     lambda: ([], {'pad_left': 4, 'pad_right': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
]

class Test_facebookresearch_fairseq(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

    def test_041(self):
        self._check(*TESTCASES[41])

    def test_042(self):
        self._check(*TESTCASES[42])

    def test_043(self):
        self._check(*TESTCASES[43])

    def test_044(self):
        self._check(*TESTCASES[44])

    def test_045(self):
        self._check(*TESTCASES[45])

    def test_046(self):
        self._check(*TESTCASES[46])

    def test_047(self):
        self._check(*TESTCASES[47])

    def test_048(self):
        self._check(*TESTCASES[48])

    def test_049(self):
        self._check(*TESTCASES[49])

    def test_050(self):
        self._check(*TESTCASES[50])

    def test_051(self):
        self._check(*TESTCASES[51])

    def test_052(self):
        self._check(*TESTCASES[52])

    def test_053(self):
        self._check(*TESTCASES[53])

    def test_054(self):
        self._check(*TESTCASES[54])

    def test_055(self):
        self._check(*TESTCASES[55])

    def test_056(self):
        self._check(*TESTCASES[56])

    def test_057(self):
        self._check(*TESTCASES[57])

    def test_058(self):
        self._check(*TESTCASES[58])

    def test_059(self):
        self._check(*TESTCASES[59])

    def test_060(self):
        self._check(*TESTCASES[60])

    def test_061(self):
        self._check(*TESTCASES[61])

    def test_062(self):
        self._check(*TESTCASES[62])

    def test_063(self):
        self._check(*TESTCASES[63])

    def test_064(self):
        self._check(*TESTCASES[64])

    def test_065(self):
        self._check(*TESTCASES[65])

    def test_066(self):
        self._check(*TESTCASES[66])

    def test_067(self):
        self._check(*TESTCASES[67])

    def test_068(self):
        self._check(*TESTCASES[68])

    def test_069(self):
        self._check(*TESTCASES[69])

    def test_070(self):
        self._check(*TESTCASES[70])

    def test_071(self):
        self._check(*TESTCASES[71])

    def test_072(self):
        self._check(*TESTCASES[72])

    def test_073(self):
        self._check(*TESTCASES[73])

