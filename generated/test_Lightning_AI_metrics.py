import sys
_module = sys.modules[__name__]
del sys
assistant = _module
conf = _module
detection_map = _module
plotting = _module
setup = _module
__about__ = _module
torchmetrics = _module
aggregation = _module
audio = _module
pesq = _module
pit = _module
sdr = _module
snr = _module
stoi = _module
classification = _module
accuracy = _module
auroc = _module
average_precision = _module
calibration_error = _module
cohen_kappa = _module
confusion_matrix = _module
dice = _module
exact_match = _module
f_beta = _module
hamming = _module
hinge = _module
jaccard = _module
matthews_corrcoef = _module
precision_recall = _module
precision_recall_curve = _module
ranking = _module
recall_at_fixed_precision = _module
roc = _module
specificity = _module
stat_scores = _module
collections = _module
detection = _module
mean_ap = _module
functional = _module
pesq = _module
pit = _module
sdr = _module
snr = _module
stoi = _module
accuracy = _module
auroc = _module
average_precision = _module
calibration_error = _module
cohen_kappa = _module
confusion_matrix = _module
dice = _module
exact_match = _module
f_beta = _module
hamming = _module
hinge = _module
jaccard = _module
matthews_corrcoef = _module
precision_recall = _module
precision_recall_curve = _module
ranking = _module
recall_at_fixed_precision = _module
roc = _module
specificity = _module
stat_scores = _module
image = _module
d_lambda = _module
ergas = _module
gradients = _module
helper = _module
psnr = _module
sam = _module
ssim = _module
tv = _module
uqi = _module
multimodal = _module
clip_score = _module
nominal = _module
cramers = _module
pearson = _module
theils_u = _module
tschuprows = _module
utils = _module
pairwise = _module
cosine = _module
euclidean = _module
helpers = _module
linear = _module
manhattan = _module
regression = _module
concordance = _module
cosine_similarity = _module
explained_variance = _module
kendall = _module
kl_divergence = _module
log_cosh = _module
log_mse = _module
mae = _module
mape = _module
mse = _module
pearson = _module
r2 = _module
spearman = _module
symmetric_mape = _module
tweedie_deviance = _module
utils = _module
wmape = _module
retrieval = _module
average_precision = _module
fall_out = _module
hit_rate = _module
ndcg = _module
precision = _module
precision_recall_curve = _module
r_precision = _module
recall = _module
reciprocal_rank = _module
text = _module
bert = _module
bleu = _module
cer = _module
chrf = _module
eed = _module
helper_embedding_metric = _module
infolm = _module
mer = _module
perplexity = _module
rouge = _module
sacre_bleu = _module
squad = _module
ter = _module
wer = _module
wil = _module
wip = _module
d_lambda = _module
ergas = _module
fid = _module
inception = _module
kid = _module
lpip = _module
psnr = _module
sam = _module
ssim = _module
tv = _module
uqi = _module
metric = _module
clip_score = _module
cramers = _module
pearson = _module
theils_u = _module
tschuprows = _module
concordance = _module
cosine_similarity = _module
explained_variance = _module
kendall = _module
kl_divergence = _module
log_cosh = _module
log_mse = _module
mae = _module
mape = _module
mse = _module
pearson = _module
r2 = _module
spearman = _module
symmetric_mape = _module
tweedie_deviance = _module
wmape = _module
average_precision = _module
base = _module
fall_out = _module
hit_rate = _module
ndcg = _module
precision = _module
precision_recall_curve = _module
r_precision = _module
recall = _module
reciprocal_rank = _module
bert = _module
bleu = _module
cer = _module
chrf = _module
eed = _module
infolm = _module
mer = _module
perplexity = _module
rouge = _module
squad = _module
ter = _module
wer = _module
wil = _module
wip = _module
utilities = _module
checks = _module
compute = _module
data = _module
distributed = _module
enums = _module
exceptions = _module
imports = _module
plot = _module
prints = _module
wrappers = _module
bootstrapping = _module
classwise = _module
minmax = _module
multioutput = _module
tracker = _module
integrations = _module
conftest = _module
lightning = _module
boring_model = _module
test_lightning = _module
unittests = _module
test_pesq = _module
test_pit = _module
test_sdr = _module
test_si_sdr = _module
test_si_snr = _module
test_snr = _module
test_stoi = _module
bases = _module
test_aggregation = _module
test_collections = _module
test_composition = _module
test_ddp = _module
test_hashing = _module
test_metric = _module
inputs = _module
test_accuracy = _module
test_auroc = _module
test_average_precision = _module
test_calibration_error = _module
test_cohen_kappa = _module
test_confusion_matrix = _module
test_dice = _module
test_exact_match = _module
test_f_beta = _module
test_hamming_distance = _module
test_hinge = _module
test_inputs = _module
test_jaccard = _module
test_matthews_corrcoef = _module
test_precision_recall = _module
test_precision_recall_curve = _module
test_ranking = _module
test_recall_at_fixed_precision = _module
test_roc = _module
test_specificity = _module
test_stat_scores = _module
test_map = _module
helpers = _module
reference_metrics = _module
testers = _module
test_d_lambda = _module
test_ergas = _module
test_fid = _module
test_image_gradients = _module
test_inception = _module
test_kid = _module
test_lpips = _module
test_ms_ssim = _module
test_psnr = _module
test_sam = _module
test_ssim = _module
test_tv = _module
test_uqi = _module
test_clip_score = _module
test_cramers = _module
test_pearson = _module
test_theils_u = _module
test_tschuprows = _module
test_pairwise_distance = _module
test_concordance = _module
test_cosine_similarity = _module
test_explained_variance = _module
test_kendall = _module
test_kl_divergence = _module
test_log_cosh_error = _module
test_mean_error = _module
test_pearson = _module
test_r2 = _module
test_spearman = _module
test_tweedie_deviance = _module
helpers = _module
inputs = _module
test_fallout = _module
test_hit_rate = _module
test_map = _module
test_mrr = _module
test_ndcg = _module
test_precision = _module
test_precision_recall_curve = _module
test_r_precision = _module
test_recall = _module
helpers = _module
inputs = _module
test_bertscore = _module
test_bleu = _module
test_cer = _module
test_chrf = _module
test_eed = _module
test_infolm = _module
test_mer = _module
test_perplexity = _module
test_rouge = _module
test_sacre_bleu = _module
test_squad = _module
test_ter = _module
test_wer = _module
test_wil = _module
test_wip = _module
test_auc = _module
test_plot = _module
test_utilities = _module
test_bootstrapping = _module
test_classwise = _module
test_minmax = _module
test_multioutput = _module
test_tracker = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import logging


import re


from typing import List


from typing import Optional


from typing import Tuple


from typing import Union


import inspect


from typing import Dict


import torch


import torch.nn as nn


from torch import Tensor


from torch.nn import Module


from torch import BoolTensor


from torch import IntTensor


import matplotlib.pyplot as plt


from functools import partial


from itertools import chain


import warnings


from typing import Any


from typing import Callable


from torch import tensor


from typing import Sequence


from typing import no_type_check


from collections import OrderedDict


from copy import deepcopy


from typing import Hashable


from typing import Iterable


from torch.nn import ModuleDict


import numpy as np


from itertools import permutations


from warnings import warn


import math


from torch.linalg import norm


from torch.nn import functional as F


import torch.nn.functional as F


import itertools


from torch import cumsum


from torch.nn.functional import pad


from torch.utils.data import DataLoader


from collections import Counter


from collections import defaultdict


from math import inf


from torch import stack


from typing import Set


from torch.utils.data import Dataset


from enum import unique


import string


from functools import lru_cache


from typing import Iterator


from torch.autograd import Function


import functools


from abc import ABC


from abc import abstractmethod


from typing import Generator


from time import perf_counter


from typing import Mapping


from itertools import product


from math import ceil


from math import floor


from math import sqrt


from torch.nn import ModuleList


from collections import namedtuple


from scipy.io import wavfile


from scipy.optimize import linear_sum_assignment


import time


from scipy.special import expit as sigmoid


from sklearn.metrics import accuracy_score as sk_accuracy


from sklearn.metrics import confusion_matrix as sk_confusion_matrix


from scipy.special import softmax


from sklearn.metrics import roc_auc_score as sk_roc_auc_score


from sklearn.metrics import average_precision_score as sk_average_precision_score


from sklearn.metrics import cohen_kappa_score as sk_cohen_kappa


from scipy.spatial.distance import dice as _sc_dice


from sklearn.metrics import f1_score as sk_f1_score


from sklearn.metrics import fbeta_score as sk_fbeta_score


from sklearn.metrics import hamming_loss as sk_hamming_loss


from sklearn.metrics import hinge_loss as sk_hinge


from sklearn.preprocessing import OneHotEncoder


from torch import rand


from torch import randint


from sklearn.metrics import jaccard_score as sk_jaccard_index


from sklearn.metrics import matthews_corrcoef as sk_matthews_corrcoef


from sklearn.metrics import precision_score as sk_precision_score


from sklearn.metrics import recall_score as sk_recall_score


from sklearn.metrics import precision_recall_curve as sk_precision_recall_curve


from sklearn.metrics import coverage_error as sk_coverage_error


from sklearn.metrics import label_ranking_average_precision_score as sk_label_ranking


from sklearn.metrics import label_ranking_loss as sk_label_ranking_loss


from sklearn.metrics import precision_recall_curve as _sk_precision_recall_curve


from sklearn.metrics import roc_curve as sk_roc_curve


import random


import numpy


from sklearn.metrics._regression import _check_reg_targets


from sklearn.utils import assert_all_finite


from sklearn.utils import check_consistent_length


from sklearn.utils import column_or_1d


from torch.multiprocessing import Pool


from torch.multiprocessing import set_start_method


from scipy.linalg import sqrtm as scipy_sqrtm


import pandas as pd


from scipy.stats.contingency import association


from sklearn.metrics.pairwise import cosine_similarity


from sklearn.metrics.pairwise import euclidean_distances


from sklearn.metrics.pairwise import linear_kernel


from sklearn.metrics.pairwise import manhattan_distances


from scipy.stats import pearsonr


from sklearn.metrics.pairwise import cosine_similarity as sk_cosine


from sklearn.metrics import explained_variance_score


from scipy.stats import kendalltau


from scipy.stats import entropy


from sklearn.metrics import mean_absolute_error as sk_mean_absolute_error


from sklearn.metrics import mean_absolute_percentage_error as sk_mean_abs_percentage_error


from sklearn.metrics import mean_squared_error as sk_mean_squared_error


from sklearn.metrics import mean_squared_log_error as sk_mean_squared_log_error


from sklearn.metrics import r2_score as sk_r2score


from scipy.stats import rankdata


from scipy.stats import spearmanr


from sklearn.metrics import mean_tweedie_deviance


from typing import Type


from numpy import array


from sklearn.metrics import label_ranking_average_precision_score


from sklearn.metrics import ndcg_score


from functools import wraps


import torch.distributed as dist


import torch.multiprocessing as mp


from sklearn.metrics import auc as _sk_auc


import matplotlib


from sklearn.metrics import mean_squared_error


from sklearn.metrics import precision_score


from sklearn.metrics import recall_score


from sklearn.metrics import accuracy_score


class TorchMetricsUserError(Exception):
    """Error used to inform users of a wrong combination of Metric API calls."""


def _flatten(x: Sequence) ->list:
    """Flatten list of list into single list."""
    return [item for sublist in x for item in sublist]


def _neg(x: Tensor) ->Tensor:
    return -torch.abs(x)


def _squeeze_scalar_element_tensor(x: Tensor) ->Tensor:
    return x.squeeze() if x.numel() == 1 else x


def apply_to_collection(data: Any, dtype: Union[type, tuple], function: Callable, *args: Any, wrong_dtype: Optional[Union[type, tuple]]=None, **kwargs: Any) ->Any:
    """Recursively applies a function to all elements of a certain dtype.

    Args:
        data: the collection to apply the function to
        dtype: the given function will be applied to all elements of this dtype
        function: the function to apply
        *args: positional arguments (will be forwarded to call of ``function``)
        wrong_dtype: the given function won't be applied if this type is specified and the given collections is of
            the :attr:`wrong_type` even if it is of type :attr`dtype`
        **kwargs: keyword arguments (will be forwarded to call of ``function``)

    Returns:
        the resulting collection

    Example:
        >>> apply_to_collection(torch.tensor([8, 0, 2, 6, 7]), dtype=Tensor, function=lambda x: x ** 2)
        tensor([64,  0,  4, 36, 49])
        >>> apply_to_collection([8, 0, 2, 6, 7], dtype=int, function=lambda x: x ** 2)
        [64, 0, 4, 36, 49]
        >>> apply_to_collection(dict(abc=123), dtype=int, function=lambda x: x ** 2)
        {'abc': 15129}
    """
    elem_type = type(data)
    if isinstance(data, dtype) and (wrong_dtype is None or not isinstance(data, wrong_dtype)):
        return function(data, *args, **kwargs)
    if isinstance(data, Mapping):
        return elem_type({k: apply_to_collection(v, dtype, function, *args, **kwargs) for k, v in data.items()})
    if isinstance(data, tuple) and hasattr(data, '_fields'):
        return elem_type(*(apply_to_collection(d, dtype, function, *args, **kwargs) for d in data))
    if isinstance(data, Sequence) and not isinstance(data, str):
        return elem_type([apply_to_collection(d, dtype, function, *args, **kwargs) for d in data])
    return data


def _squeeze_if_scalar(data: Any) ->Any:
    return apply_to_collection(data, Tensor, _squeeze_scalar_element_tensor)


def dim_zero_cat(x: Union[Tensor, List[Tensor]]) ->Tensor:
    """Concatenation along the zero dimension."""
    x = x if isinstance(x, (list, tuple)) else [x]
    x = [(y.unsqueeze(0) if y.numel() == 1 and y.ndim == 0 else y) for y in x]
    if not x:
        raise ValueError('No samples to concatenate')
    return torch.cat(x, dim=0)


def dim_zero_max(x: Tensor) ->Tensor:
    """Max along the zero dimension."""
    return torch.max(x, dim=0).values


def dim_zero_mean(x: Tensor) ->Tensor:
    """Average along the zero dimension."""
    return torch.mean(x, dim=0)


def dim_zero_min(x: Tensor) ->Tensor:
    """Min along the zero dimension."""
    return torch.min(x, dim=0).values


def dim_zero_sum(x: Tensor) ->Tensor:
    """Summation along the zero dimension."""
    return torch.sum(x, dim=0)


def _simple_gather_all_tensors(result: Tensor, group: Any, world_size: int) ->List[Tensor]:
    gathered_result = [torch.zeros_like(result) for _ in range(world_size)]
    torch.distributed.all_gather(gathered_result, result, group)
    return gathered_result


def gather_all_tensors(result: Tensor, group: Optional[Any]=None) ->List[Tensor]:
    """Function to gather all tensors from several ddp processes onto a list that is broadcasted to all processes.
    Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case
    tensors are padded, gathered and then trimmed to secure equal workload for all processes.

    Args:
        result: the value to sync
        group: the process group to gather results from. Defaults to all processes (world)

    Return:
        gathered_result: list with size equal to the process group where
            ``gathered_result[i]`` corresponds to result tensor from process ``i``
    """
    if group is None:
        group = torch.distributed.group.WORLD
    result = result.contiguous()
    world_size = torch.distributed.get_world_size(group)
    torch.distributed.barrier(group=group)
    if result.ndim == 0:
        return _simple_gather_all_tensors(result, group, world_size)
    local_size = torch.tensor(result.shape, device=result.device)
    local_sizes = [torch.zeros_like(local_size) for _ in range(world_size)]
    torch.distributed.all_gather(local_sizes, local_size, group=group)
    max_size = torch.stack(local_sizes).max(dim=0).values
    all_sizes_equal = all(all(ls == max_size) for ls in local_sizes)
    if all_sizes_equal:
        return _simple_gather_all_tensors(result, group, world_size)
    pad_dims = []
    pad_by = (max_size - local_size).detach().cpu()
    for val in reversed(pad_by):
        pad_dims.append(0)
        pad_dims.append(val.item())
    result_padded = F.pad(result, pad_dims)
    gathered_result = [torch.zeros_like(result_padded) for _ in range(world_size)]
    torch.distributed.all_gather(gathered_result, result_padded, group)
    for idx, item_size in enumerate(local_sizes):
        slice_param = [slice(dim_size) for dim_size in item_size]
        gathered_result[idx] = gathered_result[idx][slice_param]
    return gathered_result


def jit_distributed_available() ->bool:
    return torch.distributed.is_available() and torch.distributed.is_initialized()


def _warn(*args: Any, **kwargs: Any) ->None:
    warnings.warn(*args, **kwargs)


def rank_zero_only(fn: Callable) ->Callable:

    @wraps(fn)
    def wrapped_fn(*args: Any, **kwargs: Any) ->Any:
        if rank_zero_only.rank == 0:
            return fn(*args, **kwargs)
    return wrapped_fn


rank_zero_warn = rank_zero_only(_warn)


def _flatten_dict(x: Dict) ->Dict:
    """Flatten dict of dicts into single dict."""
    new_dict = {}
    for key, value in x.items():
        if isinstance(value, dict):
            for k, v in value.items():
                new_dict[k] = v
        else:
            new_dict[key] = value
    return new_dict


def allclose(tensor1: Tensor, tensor2: Tensor) ->bool:
    """Wrapper of torch.allclose that is robust towards dtype difference."""
    if tensor1.dtype != tensor2.dtype:
        tensor2 = tensor2
    return torch.allclose(tensor1, tensor2)


def _compute_phi_squared_corrected(phi_squared: Tensor, n_rows: int, n_cols: int, confmat_sum: Tensor) ->Tensor:
    """Compute bias-corrected Phi Squared."""
    return torch.max(torch.tensor(0.0, device=phi_squared.device), phi_squared - (n_rows - 1) * (n_cols - 1) / (confmat_sum - 1))


def _compute_rows_and_cols_corrected(n_rows: int, n_cols: int, confmat_sum: Tensor) ->Tuple[Tensor, Tensor]:
    """Compute bias-corrected number of rows and columns."""
    rows_corrected = n_rows - (n_rows - 1) ** 2 / (confmat_sum - 1)
    cols_corrected = n_cols - (n_cols - 1) ** 2 / (confmat_sum - 1)
    return rows_corrected, cols_corrected


def _compute_bias_corrected_values(phi_squared: Tensor, n_rows: int, n_cols: int, confmat_sum: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
    """Compute bias-corrected Phi Squared and number of rows and columns."""
    phi_squared_corrected = _compute_phi_squared_corrected(phi_squared, n_rows, n_cols, confmat_sum)
    rows_corrected, cols_corrected = _compute_rows_and_cols_corrected(n_rows, n_cols, confmat_sum)
    return phi_squared_corrected, rows_corrected, cols_corrected


def _compute_expected_freqs(confmat: Tensor) ->Tensor:
    """Compute the expected frequenceis from the provided confusion matrix."""
    margin_sum_rows, margin_sum_cols = confmat.sum(1), confmat.sum(0)
    expected_freqs = torch.einsum('r, c -> rc', margin_sum_rows, margin_sum_cols) / confmat.sum()
    return expected_freqs


def _compute_chi_squared(confmat: Tensor, bias_correction: bool) ->Tensor:
    """Chi-square test of independenc of variables in a confusion matrix table.

    Adapted from: https://github.com/scipy/scipy/blob/v1.9.2/scipy/stats/contingency.py.
    """
    expected_freqs = _compute_expected_freqs(confmat)
    df = expected_freqs.numel() - sum(expected_freqs.shape) + expected_freqs.ndim - 1
    if df == 0:
        return torch.tensor(0.0, device=confmat.device)
    if df == 1 and bias_correction:
        diff = expected_freqs - confmat
        direction = diff.sign()
        confmat += direction * torch.minimum(0.5 * torch.ones_like(direction), direction.abs())
    return torch.sum((confmat - expected_freqs) ** 2 / expected_freqs)


def _drop_empty_rows_and_cols(confmat: Tensor) ->Tensor:
    """Drop all rows and columns containing only zeros.

    Example:
        >>> import torch
        >>> from torchmetrics.functional.nominal.utils import _drop_empty_rows_and_cols
        >>> _ = torch.manual_seed(22)
        >>> matrix = torch.randint(10, size=(3, 3))
        >>> matrix[1, :] = matrix[:, 1] = 0
        >>> matrix
        tensor([[9, 0, 6],
                [0, 0, 0],
                [2, 0, 8]])
        >>> _drop_empty_rows_and_cols(matrix)
        tensor([[9, 6],
                [2, 8]])
    """
    confmat = confmat[confmat.sum(1) != 0]
    confmat = confmat[:, confmat.sum(0) != 0]
    return confmat


def _unable_to_use_bias_correction_warning(metric_name: str) ->None:
    rank_zero_warn(f'Unable to compute {metric_name} using bias correction. Please consider to set `bias_correction=False`.')


def _cramers_v_compute(confmat: Tensor, bias_correction: bool) ->Tensor:
    """Compute Cramers' V statistic based on a pre-computed confusion matrix.

    Args:
        confmat: Confusion matrix for observed data
        bias_correction: Indication of whether to use bias correction.

    Returns:
        Cramer's V statistic
    """
    confmat = _drop_empty_rows_and_cols(confmat)
    cm_sum = confmat.sum()
    chi_squared = _compute_chi_squared(confmat, bias_correction)
    phi_squared = chi_squared / cm_sum
    n_rows, n_cols = confmat.shape
    if bias_correction:
        phi_squared_corrected, rows_corrected, cols_corrected = _compute_bias_corrected_values(phi_squared, n_rows, n_cols, cm_sum)
        if torch.min(rows_corrected, cols_corrected) == 1:
            _unable_to_use_bias_correction_warning(metric_name="Cramer's V")
            return torch.tensor(float('nan'), device=confmat.device)
        cramers_v_value = torch.sqrt(phi_squared_corrected / torch.min(rows_corrected - 1, cols_corrected - 1))
    else:
        cramers_v_value = torch.sqrt(phi_squared / min(n_rows - 1, n_cols - 1))
    return cramers_v_value.clamp(0.0, 1.0)


def _bincount(x: Tensor, minlength: Optional[int]=None) ->Tensor:
    """PyTorch currently does not support``torch.bincount`` for:

        - deterministic mode on GPU.
        - MPS devices

    This implementation fallback to a for-loop counting occurrences in that case.

    Args:
        x: tensor to count
        minlength: minimum length to count

    Returns:
        Number of occurrences for each unique element in x
    """
    if minlength is None:
        minlength = len(torch.unique(x))
    if torch.are_deterministic_algorithms_enabled() or _TORCH_GREATER_EQUAL_1_12 and x.is_mps:
        output = torch.zeros(minlength, device=x.device, dtype=torch.long)
        for i in range(minlength):
            output[i] = (x == i).sum()
        return output
    return torch.bincount(x, minlength=minlength)


def _multiclass_confusion_matrix_update(preds: Tensor, target: Tensor, num_classes: int) ->Tensor:
    """Computes the bins to update the confusion matrix with."""
    unique_mapping = target * num_classes + preds
    bins = _bincount(unique_mapping, minlength=num_classes ** 2)
    return bins.reshape(num_classes, num_classes)


def _nominal_input_validation(nan_strategy: str, nan_replace_value: Optional[Union[int, float]]) ->None:
    if nan_strategy not in ['replace', 'drop']:
        raise ValueError(f"Argument `nan_strategy` is expected to be one of `['replace', 'drop']`, but got {nan_strategy}")
    if nan_strategy == 'replace' and not isinstance(nan_replace_value, (int, float)):
        raise ValueError(f"Argument `nan_replace` is expected to be of a type `int` or `float` when `nan_strategy = 'replace`, but got {nan_replace_value}")


def _pearsons_contingency_coefficient_compute(confmat: Tensor) ->Tensor:
    """Compute Pearson's Contingency Coefficient based on a pre-computed confusion matrix.

    Args:
        confmat: Confusion matrix for observed data

    Returns:
        Pearson's Contingency Coefficient
    """
    confmat = _drop_empty_rows_and_cols(confmat)
    cm_sum = confmat.sum()
    chi_squared = _compute_chi_squared(confmat, bias_correction=False)
    phi_squared = chi_squared / cm_sum
    tschuprows_t_value = torch.sqrt(phi_squared / (1 + phi_squared))
    return tschuprows_t_value.clamp(0.0, 1.0)


def _conditional_entropy_compute(confmat: Tensor) ->Tensor:
    """Compute Conditional Entropy Statistic based on a pre-computed confusion matrix.

    .. math::
        H(X|Y) = \\sum_{x, y ~ (X, Y)} p(x, y)\\frac{p(y)}{p(x, y)}

    Args:
        confmat: Confusion matrix for observed data

    Returns:
        Conditional Entropy Value
    """
    confmat = _drop_empty_rows_and_cols(confmat)
    total_occurrences = confmat.sum()
    p_xy_m = confmat / total_occurrences
    p_y = confmat.sum(1) / total_occurrences
    p_y_m = p_y.unsqueeze(1).repeat(1, p_xy_m.shape[1])
    return torch.nansum(p_xy_m * torch.log(p_y_m / p_xy_m))


def _theils_u_compute(confmat: Tensor) ->Tensor:
    """Compute Theil's U statistic based on a pre-computed confusion matrix.

    Args:
        confmat: Confusion matrix for observed data

    Returns:
        Theil's U statistic
    """
    confmat = _drop_empty_rows_and_cols(confmat)
    s_xy = _conditional_entropy_compute(confmat)
    total_occurrences = confmat.sum()
    p_x = confmat.sum(0) / total_occurrences
    s_x = -torch.sum(p_x * torch.log(p_x))
    if s_x == 0:
        return torch.tensor(0, device=confmat.device)
    return (s_x - s_xy) / s_x


def _tschuprows_t_compute(confmat: Tensor, bias_correction: bool) ->Tensor:
    """Compute Tschuprow's T statistic based on a pre-computed confusion matrix.

    Args:
        confmat: Confusion matrix for observed data
        bias_correction: Indication of whether to use bias correction.

    Returns:
        Tschuprow's T statistic
    """
    confmat = _drop_empty_rows_and_cols(confmat)
    cm_sum = confmat.sum()
    chi_squared = _compute_chi_squared(confmat, bias_correction)
    phi_squared = chi_squared / cm_sum
    n_rows, n_cols = confmat.shape
    if bias_correction:
        phi_squared_corrected, rows_corrected, cols_corrected = _compute_bias_corrected_values(phi_squared, n_rows, n_cols, cm_sum)
        if torch.min(rows_corrected, cols_corrected) == 1:
            _unable_to_use_bias_correction_warning(metric_name="Tschuprow's T")
            return torch.tensor(float('nan'), device=confmat.device)
        tschuprows_t_value = torch.sqrt(phi_squared_corrected / torch.sqrt((rows_corrected - 1) * (cols_corrected - 1)))
    else:
        n_rows_tensor = torch.tensor(n_rows, device=phi_squared.device)
        n_cols_tensor = torch.tensor(n_cols, device=phi_squared.device)
        tschuprows_t_value = torch.sqrt(phi_squared / torch.sqrt((n_rows_tensor - 1) * (n_cols_tensor - 1)))
    return tschuprows_t_value.clamp(0.0, 1.0)


def _cosine_similarity_compute(preds: Tensor, target: Tensor, reduction: Optional[str]='sum') ->Tensor:
    """Computes Cosine Similarity.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
        reduction:
            The method of reducing along the batch dimension using sum, mean or taking the individual scores

    Example:
        >>> target = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 4]])
        >>> preds = torch.tensor([[1, 2, 3, 4], [-1, -2, -3, -4]])
        >>> preds, target = _cosine_similarity_update(preds, target)
        >>> _cosine_similarity_compute(preds, target, 'none')
        tensor([ 1.0000, -1.0000])
    """
    dot_product = (preds * target).sum(dim=-1)
    preds_norm = preds.norm(dim=-1)
    target_norm = target.norm(dim=-1)
    similarity = dot_product / (preds_norm * target_norm)
    reduction_mapping = {'sum': torch.sum, 'mean': torch.mean, 'none': lambda x: x, None: lambda x: x}
    return reduction_mapping[reduction](similarity)


def _check_same_shape(preds: Tensor, target: Tensor) ->None:
    """Check that predictions and target have the same shape, else raise error."""
    if preds.shape != target.shape:
        raise RuntimeError(f'Predictions and targets are expected to have the same shape, but got {preds.shape} and {target.shape}.')


def _cosine_similarity_update(preds: Tensor, target: Tensor) ->Tuple[Tensor, Tensor]:
    """Updates and returns variables required to compute Cosine Similarity. Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    preds = preds.float()
    target = target.float()
    return preds, target


def _explained_variance_compute(n_obs: Tensor, sum_error: Tensor, sum_squared_error: Tensor, sum_target: Tensor, sum_squared_target: Tensor, multioutput: str='uniform_average') ->Tensor:
    """Computes Explained Variance.

    Args:
        n_obs: Number of predictions or observations
        sum_error: Sum of errors over all observations
        sum_squared_error: Sum of square of errors over all observations
        sum_target: Sum of target values
        sum_squared_target: Sum of squares of target values
        multioutput: Defines aggregation in the case of multiple output scores. Can be one
            of the following strings:

            * ``'raw_values'`` returns full set of scores
            * ``'uniform_average'`` scores are uniformly averaged
            * ``'variance_weighted'`` scores are weighted by their individual variances

    Example:
        >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
        >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
        >>> n_obs, sum_error, ss_error, sum_target, ss_target = _explained_variance_update(preds, target)
        >>> _explained_variance_compute(n_obs, sum_error, ss_error, sum_target, ss_target, multioutput='raw_values')
        tensor([0.9677, 1.0000])
    """
    diff_avg = sum_error / n_obs
    numerator = sum_squared_error / n_obs - diff_avg * diff_avg
    target_avg = sum_target / n_obs
    denominator = sum_squared_target / n_obs - target_avg * target_avg
    nonzero_numerator = numerator != 0
    nonzero_denominator = denominator != 0
    valid_score = nonzero_numerator & nonzero_denominator
    output_scores = torch.ones_like(diff_avg)
    output_scores[valid_score] = 1.0 - numerator[valid_score] / denominator[valid_score]
    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.0
    if multioutput == 'raw_values':
        return output_scores
    if multioutput == 'uniform_average':
        return torch.mean(output_scores)
    if multioutput == 'variance_weighted':
        denom_sum = torch.sum(denominator)
        return torch.sum(denominator / denom_sum * output_scores)


def _explained_variance_update(preds: Tensor, target: Tensor) ->Tuple[int, Tensor, Tensor, Tensor, Tensor]:
    """Updates and returns variables required to compute Explained Variance. Checks for same shape of input
    tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    n_obs = preds.size(0)
    sum_error = torch.sum(target - preds, dim=0)
    diff = target - preds
    sum_squared_error = torch.sum(diff * diff, dim=0)
    sum_target = torch.sum(target, dim=0)
    sum_squared_target = torch.sum(target * target, dim=0)
    return n_obs, sum_error, sum_squared_error, sum_target, sum_squared_target


def _get_p_value_for_t_value_from_dist(t_value: Tensor) ->Tensor:
    """Obtain p-value for a given Tensor of t-values. Handle ``nan`` which cannot be passed into torch
    distributions.

    When t-value is ``nan``, a resulted p-value should be alson ``nan``.
    """
    device = t_value
    normal_dist = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))
    is_nan = t_value.isnan()
    t_value = t_value.nan_to_num()
    p_value = normal_dist.cdf(t_value)
    return p_value.where(~is_nan, torch.tensor(float('nan'), dtype=p_value.dtype, device=p_value.device))


def _convert_sequence_to_dense_rank(x: Tensor, sort: bool=False) ->Tensor:
    """Convert a sequence to the rank tensor."""
    if sort:
        x = x.sort(dim=0).values
    _ones = torch.zeros(1, x.shape[1], dtype=torch.int32, device=x.device)
    return torch.cat([_ones, (x[1:] != x[:-1]).int()], dim=0).cumsum(0)


def _concordant_element_sum(x: Tensor, y: Tensor, i: int) ->Tensor:
    """Count a total number of concordant pairs in a single sequence."""
    return torch.logical_and(x[i] < x[i + 1:], y[i] < y[i + 1:]).sum(0).unsqueeze(0)


def _count_concordant_pairs(preds: Tensor, target: Tensor) ->Tensor:
    """Count a total number of concordant pairs in given sequences."""
    return torch.cat([_concordant_element_sum(preds, target, i) for i in range(preds.shape[0])]).sum(0)


def _discordant_element_sum(x: Tensor, y: Tensor, i: int) ->Tensor:
    """Count a total number of discordant pairs in a single sequences."""
    return torch.logical_or(torch.logical_and(x[i] > x[i + 1:], y[i] < y[i + 1:]), torch.logical_and(x[i] < x[i + 1:], y[i] > y[i + 1:])).sum(0).unsqueeze(0)


def _count_discordant_pairs(preds: Tensor, target: Tensor) ->Tensor:
    """Count a total number of discordant pairs in given sequences."""
    return torch.cat([_discordant_element_sum(preds, target, i) for i in range(preds.shape[0])]).sum(0)


def _get_ties(x: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
    """Get a total number of ties and staistics for p-value calculation for  a given sequence."""
    ties = torch.zeros(x.shape[1], dtype=x.dtype, device=x.device)
    ties_p1 = torch.zeros(x.shape[1], dtype=x.dtype, device=x.device)
    ties_p2 = torch.zeros(x.shape[1], dtype=x.dtype, device=x.device)
    for dim in range(x.shape[1]):
        n_ties = _bincount(x[:, dim])
        n_ties = n_ties[n_ties > 1]
        ties[dim] = (n_ties * (n_ties - 1) // 2).sum()
        ties_p1[dim] = (n_ties * (n_ties - 1.0) * (n_ties - 2)).sum()
        ties_p2[dim] = (n_ties * (n_ties - 1.0) * (2 * n_ties + 5)).sum()
    return ties, ties_p1, ties_p2


def _sort_on_first_sequence(x: Tensor, y: Tensor) ->Tuple[Tensor, Tensor]:
    """Sort sequences in an ascent order according to the sequence ``x``."""
    y = torch.clone(y)
    x, y = x.T, y.T
    x, perm = x.sort()
    for i in range(x.shape[0]):
        y[i] = y[i][perm[i]]
    return x.T, y.T


def _check_data_shape_to_num_outputs(preds: Tensor, target: Tensor, num_outputs: int) ->None:
    """Check that predictions and target have the correct shape, else raise error."""
    if preds.ndim > 2 or target.ndim > 2:
        raise ValueError(f'Expected both predictions and target to be either 1- or 2-dimensional tensors, but got {target.ndim} and {preds.ndim}.')
    if num_outputs == 1 and preds.ndim != 1 or num_outputs > 1 and num_outputs != preds.shape[1]:
        raise ValueError(f'Expected argument `num_outputs` to match the second dimension of input, but got {num_outputs} and {preds.shape[1]}.')


def _kendall_corrcoef_update(preds: Tensor, target: Tensor, concat_preds: List[Tensor]=[], concat_target: List[Tensor]=[], num_outputs: int=1) ->Tuple[List[Tensor], List[Tensor]]:
    """Update variables required to compute Kendall rank correlation coefficient.

    Args:
        preds: Sequence of data
        target: Sequence of data
        concat_preds: List of batches of preds sequence to be concatenated
        concat_target: List of batches of target sequence to be concatenated
        num_outputs: Number of outputs in multioutput setting

    Raises:
        RuntimeError: If ``preds`` and ``target`` do not have the same shape
    """
    _check_same_shape(preds, target)
    _check_data_shape_to_num_outputs(preds, target, num_outputs)
    if num_outputs == 1:
        preds = preds.unsqueeze(1)
        target = target.unsqueeze(1)
    concat_preds.append(preds)
    concat_target.append(target)
    return concat_preds, concat_target


def _safe_xlogy(x: Tensor, y: Tensor) ->Tensor:
    """Computes x * log(y). Returns 0 if x=0.

    Example:
        >>> import torch
        >>> x = torch.zeros(1)
        >>> _safe_xlogy(x, 1/x)
        tensor([0.])

    """
    res = x * torch.log(y)
    res[x == 0] = 0.0
    return res


def _kld_update(p: Tensor, q: Tensor, log_prob: bool) ->Tuple[Tensor, int]:
    """Updates and returns KL divergence scores for each observation and the total number of observations. Checks
    same shape and 2D nature of the input tensors else raises ValueError.

    Args:
        p: data distribution with shape ``[N, d]``
        q: prior or approximate distribution with shape ``[N, d]``
        log_prob: bool indicating if input is log-probabilities or probabilities. If given as probabilities,
            will normalize to make sure the distributes sum to 1
    """
    _check_same_shape(p, q)
    if p.ndim != 2 or q.ndim != 2:
        raise ValueError(f'Expected both p and q distribution to be 2D but got {p.ndim} and {q.ndim} respectively')
    total = p.shape[0]
    if log_prob:
        measures = torch.sum(p.exp() * (p - q), axis=-1)
    else:
        p = p / p.sum(axis=-1, keepdim=True)
        q = q / q.sum(axis=-1, keepdim=True)
        measures = _safe_xlogy(p, p / q).sum(axis=-1)
    return measures, total


def _log_cosh_error_compute(sum_log_cosh_error: Tensor, n_obs: Tensor) ->Tensor:
    """Computes Mean Squared Error.

    Args:
        sum_squared_error: Sum of LogCosh errors over all observations
        n_obs: Number of predictions or observations
    """
    return (sum_log_cosh_error / n_obs).squeeze()


def _unsqueeze_tensors(preds: Tensor, target: Tensor) ->Tuple[Tensor, Tensor]:
    if preds.ndim == 2:
        return preds, target
    return preds.unsqueeze(1), target.unsqueeze(1)


def _log_cosh_error_update(preds: Tensor, target: Tensor, num_outputs: int) ->Tuple[Tensor, Tensor]:
    """Updates and returns variables required to compute LogCosh error.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor

    Return:
        Sum of LogCosh error over examples, and total number of examples
    """
    _check_same_shape(preds, target)
    _check_data_shape_to_num_outputs(preds, target, num_outputs)
    preds, target = _unsqueeze_tensors(preds, target)
    diff = preds - target
    sum_log_cosh_error = torch.log((torch.exp(diff) + torch.exp(-diff)) / 2).sum(0).squeeze()
    n_obs = torch.tensor(target.shape[0], device=preds.device)
    return sum_log_cosh_error, n_obs


def _mean_squared_log_error_compute(sum_squared_log_error: Tensor, n_obs: int) ->Tensor:
    """Computes Mean Squared Log Error.

    Args:
        sum_squared_log_error:
            Sum of square of log errors over all observations ``(log error = log(target) - log(prediction))``
        n_obs: Number of predictions or observations

    Example:
        >>> preds = torch.tensor([0., 1, 2, 3])
        >>> target = torch.tensor([0., 1, 2, 2])
        >>> sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)
        >>> _mean_squared_log_error_compute(sum_squared_log_error, n_obs)
        tensor(0.0207)
    """
    return sum_squared_log_error / n_obs


def _mean_squared_log_error_update(preds: Tensor, target: Tensor) ->Tuple[Tensor, int]:
    """Returns variables required to compute Mean Squared Log Error. Checks for same shape of tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    sum_squared_log_error = torch.sum(torch.pow(torch.log1p(preds) - torch.log1p(target), 2))
    n_obs = target.numel()
    return sum_squared_log_error, n_obs


def _mean_absolute_error_compute(sum_abs_error: Tensor, n_obs: int) ->Tensor:
    """Computes Mean Absolute Error.

    Args:
        sum_abs_error: Sum of absolute value of errors over all observations
        n_obs: Number of predictions or observations

    Example:
        >>> preds = torch.tensor([0., 1, 2, 3])
        >>> target = torch.tensor([0., 1, 2, 2])
        >>> sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)
        >>> _mean_absolute_error_compute(sum_abs_error, n_obs)
        tensor(0.2500)
    """
    return sum_abs_error / n_obs


def _mean_absolute_error_update(preds: Tensor, target: Tensor) ->Tuple[Tensor, int]:
    """Updates and returns variables required to compute Mean Absolute Error.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    preds = preds if preds.is_floating_point else preds.float()
    target = target if target.is_floating_point else target.float()
    sum_abs_error = torch.sum(torch.abs(preds - target))
    n_obs = target.numel()
    return sum_abs_error, n_obs


def _mean_absolute_percentage_error_compute(sum_abs_per_error: Tensor, num_obs: int) ->Tensor:
    """Computes Mean Absolute Percentage Error.

    Args:
        sum_abs_per_error: Sum of absolute value of percentage errors over all observations
            ``(percentage error = (target - prediction) / target)``
        num_obs: Number of predictions or observations

    Example:
        >>> target = torch.tensor([1, 10, 1e6])
        >>> preds = torch.tensor([0.9, 15, 1.2e6])
        >>> sum_abs_per_error, num_obs = _mean_absolute_percentage_error_update(preds, target)
        >>> _mean_absolute_percentage_error_compute(sum_abs_per_error, num_obs)
        tensor(0.2667)
    """
    return sum_abs_per_error / num_obs


def _mean_absolute_percentage_error_update(preds: Tensor, target: Tensor, epsilon: float=1.17e-06) ->Tuple[Tensor, int]:
    """Updates and returns variables required to compute Mean Percentage Error.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
        epsilon: Specifies the lower bound for target values. Any target value below epsilon
            is set to epsilon (avoids ``ZeroDivisionError``).
    """
    _check_same_shape(preds, target)
    abs_diff = torch.abs(preds - target)
    abs_per_error = abs_diff / torch.clamp(torch.abs(target), min=epsilon)
    sum_abs_per_error = torch.sum(abs_per_error)
    num_obs = target.numel()
    return sum_abs_per_error, num_obs


def _mean_squared_error_compute(sum_squared_error: Tensor, n_obs: int, squared: bool=True) ->Tensor:
    """Computes Mean Squared Error.

    Args:
        sum_squared_error: Sum of square of errors over all observations
        n_obs: Number of predictions or observations
        squared: Returns RMSE value if set to False.

    Example:
        >>> preds = torch.tensor([0., 1, 2, 3])
        >>> target = torch.tensor([0., 1, 2, 2])
        >>> sum_squared_error, n_obs = _mean_squared_error_update(preds, target)
        >>> _mean_squared_error_compute(sum_squared_error, n_obs)
        tensor(0.2500)
    """
    return sum_squared_error / n_obs if squared else torch.sqrt(sum_squared_error / n_obs)


def _mean_squared_error_update(preds: Tensor, target: Tensor) ->Tuple[Tensor, int]:
    """Updates and returns variables required to compute Mean Squared Error.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    diff = preds - target
    sum_squared_error = torch.sum(diff * diff)
    n_obs = target.numel()
    return sum_squared_error, n_obs


def _error_on_missing_matplotlib() ->None:
    """Raise error if matplotlib is not installed."""
    if not _MATPLOTLIB_AVAILABLE:
        raise ModuleNotFoundError('Plot function expects `matplotlib` to be installed. Please install with `pip install matplotlib`')


def _final_aggregation(means_x: Tensor, means_y: Tensor, vars_x: Tensor, vars_y: Tensor, corrs_xy: Tensor, nbs: Tensor) ->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
    """Aggregate the statistics from multiple devices.

    Formula taken from here: `Aggregate the statistics from multiple devices`_
    """
    mx1, my1, vx1, vy1, cxy1, n1 = means_x[0], means_y[0], vars_x[0], vars_y[0], corrs_xy[0], nbs[0]
    for i in range(1, len(means_x)):
        mx2, my2, vx2, vy2, cxy2, n2 = means_x[i], means_y[i], vars_x[i], vars_y[i], corrs_xy[i], nbs[i]
        nb = n1 + n2
        mean_x = (n1 * mx1 + n2 * mx2) / nb
        mean_y = (n1 * my1 + n2 * my2) / nb
        element_x1 = (n1 + 1) * mean_x - n1 * mx1
        vx1 += (element_x1 - mx1) * (element_x1 - mean_x) - (element_x1 - mean_x) ** 2
        element_x2 = (n2 + 1) * mean_x - n2 * mx2
        vx2 += (element_x2 - mx2) * (element_x2 - mean_x) - (element_x2 - mean_x) ** 2
        var_x = vx1 + vx2
        element_y1 = (n1 + 1) * mean_y - n1 * my1
        vy1 += (element_y1 - my1) * (element_y1 - mean_y) - (element_y1 - mean_y) ** 2
        element_y2 = (n2 + 1) * mean_y - n2 * my2
        vy2 += (element_y2 - my2) * (element_y2 - mean_y) - (element_y2 - mean_y) ** 2
        var_y = vy1 + vy2
        cxy1 += (element_x1 - mx1) * (element_y1 - mean_y) - (element_x1 - mean_x) * (element_y1 - mean_y)
        cxy2 += (element_x2 - mx2) * (element_y2 - mean_y) - (element_x2 - mean_x) * (element_y2 - mean_y)
        corr_xy = cxy1 + cxy2
        mx1, my1, vx1, vy1, cxy1, n1 = mean_x, mean_y, var_x, var_y, corr_xy, nb
    return mean_x, mean_y, var_x, var_y, corr_xy, nb


def _pearson_corrcoef_compute(var_x: Tensor, var_y: Tensor, corr_xy: Tensor, nb: Tensor) ->Tensor:
    """Computes the final pearson correlation based on accumulated statistics.

    Args:
        var_x: variance estimate of x tensor
        var_y: variance estimate of y tensor
        corr_xy: covariance estimate between x and y tensor
        nb: number of observations
    """
    var_x /= nb - 1
    var_y /= nb - 1
    corr_xy /= nb - 1
    corrcoef = (corr_xy / (var_x * var_y).sqrt()).squeeze()
    return torch.clamp(corrcoef, -1.0, 1.0)


def _pearson_corrcoef_update(preds: Tensor, target: Tensor, mean_x: Tensor, mean_y: Tensor, var_x: Tensor, var_y: Tensor, corr_xy: Tensor, n_prior: Tensor, num_outputs: int) ->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
    """Updates and returns variables required to compute Pearson Correlation Coefficient.

    Checks for same shape of input tensors.

    Args:
        mean_x: current mean estimate of x tensor
        mean_y: current mean estimate of y tensor
        var_x: current variance estimate of x tensor
        var_y: current variance estimate of y tensor
        corr_xy: current covariance estimate between x and y tensor
        n_prior: current number of observed observations
    """
    _check_same_shape(preds, target)
    _check_data_shape_to_num_outputs(preds, target, num_outputs)
    n_obs = preds.shape[0]
    mx_new = (n_prior * mean_x + preds.mean(0) * n_obs) / (n_prior + n_obs)
    my_new = (n_prior * mean_y + target.mean(0) * n_obs) / (n_prior + n_obs)
    n_prior += n_obs
    var_x += ((preds - mx_new) * (preds - mean_x)).sum(0)
    var_y += ((target - my_new) * (target - mean_y)).sum(0)
    corr_xy += ((preds - mx_new) * (target - mean_y)).sum(0)
    mean_x = mx_new
    mean_y = my_new
    return mean_x, mean_y, var_x, var_y, corr_xy, n_prior


def _r2_score_compute(sum_squared_obs: Tensor, sum_obs: Tensor, rss: Tensor, n_obs: Tensor, adjusted: int=0, multioutput: str='uniform_average') ->Tensor:
    """Computes R2 score.

    Args:
        sum_squared_obs: Sum of square of all observations
        sum_obs: Sum of all observations
        rss: Residual sum of squares
        n_obs: Number of predictions or observations
        adjusted: number of independent regressors for calculating adjusted r2 score.
        multioutput: Defines aggregation in the case of multiple output scores. Can be one of the following strings:

            * `'raw_values'` returns full set of scores
            * `'uniform_average'` scores are uniformly averaged
            * `'variance_weighted'` scores are weighted by their individual variances

    Example:
        >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
        >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
        >>> sum_squared_obs, sum_obs, rss, n_obs = _r2_score_update(preds, target)
        >>> _r2_score_compute(sum_squared_obs, sum_obs, rss, n_obs, multioutput="raw_values")
        tensor([0.9654, 0.9082])
    """
    if n_obs < 2:
        raise ValueError('Needs at least two samples to calculate r2 score.')
    mean_obs = sum_obs / n_obs
    tss = sum_squared_obs - sum_obs * mean_obs
    raw_scores = 1 - rss / tss
    if multioutput == 'raw_values':
        r2 = raw_scores
    elif multioutput == 'uniform_average':
        r2 = torch.mean(raw_scores)
    elif multioutput == 'variance_weighted':
        tss_sum = torch.sum(tss)
        r2 = torch.sum(tss / tss_sum * raw_scores)
    else:
        raise ValueError(f'Argument `multioutput` must be either `raw_values`, `uniform_average` or `variance_weighted`. Received {multioutput}.')
    if adjusted < 0 or not isinstance(adjusted, int):
        raise ValueError('`adjusted` parameter should be an integer larger or equal to 0.')
    if adjusted != 0:
        if adjusted > n_obs - 1:
            rank_zero_warn('More independent regressions than data points in adjusted r2 score. Falls back to standard r2 score.', UserWarning)
        elif adjusted == n_obs - 1:
            rank_zero_warn('Division by zero in adjusted r2 score. Falls back to standard r2 score.', UserWarning)
        else:
            r2 = 1 - (1 - r2) * (n_obs - 1) / (n_obs - adjusted - 1)
    return r2


def _r2_score_update(preds: Tensor, target: Tensor) ->Tuple[Tensor, Tensor, Tensor, Tensor]:
    """Updates and returns variables required to compute R2 score.

    Checks for same shape and 1D/2D input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    if preds.ndim > 2:
        raise ValueError(f'Expected both prediction and target to be 1D or 2D tensors, but received tensors with dimension {preds.shape}')
    sum_obs = torch.sum(target, dim=0)
    sum_squared_obs = torch.sum(target * target, dim=0)
    residual = target - preds
    rss = torch.sum(residual * residual, dim=0)
    n_obs = target.size(0)
    return sum_squared_obs, sum_obs, rss, n_obs


def _find_repeats(data: Tensor) ->Tensor:
    """find and return values which have repeats i.e. the same value are more than once in the tensor."""
    temp = data.detach().clone()
    temp = temp.sort()[0]
    change = torch.cat([torch.tensor([True], device=temp.device), temp[1:] != temp[:-1]])
    unique = temp[change]
    change_idx = torch.cat([torch.nonzero(change), torch.tensor([[temp.numel()]], device=temp.device)]).flatten()
    freq = change_idx[1:] - change_idx[:-1]
    atleast2 = freq > 1
    return unique[atleast2]


def _rank_data(data: Tensor) ->Tensor:
    """Calculate the rank for each element of a tensor.

    The rank refers to the indices of an element in the corresponding sorted tensor (starting from 1).
    Duplicates of the same value will be assigned the mean of their rank.

    Adopted from `Rank of element tensor`_
    """
    n = data.numel()
    rank = torch.empty_like(data)
    idx = data.argsort()
    rank[idx[:n]] = torch.arange(1, n + 1, dtype=data.dtype, device=data.device)
    repeats = _find_repeats(data)
    for r in repeats:
        condition = data == r
        rank[condition] = rank[condition].mean()
    return rank


def _spearman_corrcoef_compute(preds: Tensor, target: Tensor, eps: float=1e-06) ->Tensor:
    """Computes Spearman Correlation Coefficient.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
        eps: Avoids ``ZeroDivisionError``.

    Example:
        >>> target = torch.tensor([3, -0.5, 2, 7])
        >>> preds = torch.tensor([2.5, 0.0, 2, 8])
        >>> preds, target = _spearman_corrcoef_update(preds, target, num_outputs=1)
        >>> _spearman_corrcoef_compute(preds, target)
        tensor(1.0000)
    """
    if preds.ndim == 1:
        preds = _rank_data(preds)
        target = _rank_data(target)
    else:
        preds = torch.stack([_rank_data(p) for p in preds.T]).T
        target = torch.stack([_rank_data(t) for t in target.T]).T
    preds_diff = preds - preds.mean(0)
    target_diff = target - target.mean(0)
    cov = (preds_diff * target_diff).mean(0)
    preds_std = torch.sqrt((preds_diff * preds_diff).mean(0))
    target_std = torch.sqrt((target_diff * target_diff).mean(0))
    corrcoef = cov / (preds_std * target_std + eps)
    return torch.clamp(corrcoef, -1.0, 1.0)


def _spearman_corrcoef_update(preds: Tensor, target: Tensor, num_outputs: int) ->Tuple[Tensor, Tensor]:
    """Updates and returns variables required to compute Spearman Correlation Coefficient.

    Checks for same shape and type of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    if not (preds.is_floating_point() and target.is_floating_point()):
        raise TypeError('Expected `preds` and `target` both to be floating point tensors, but got {pred.dtype} and {target.dtype}')
    _check_same_shape(preds, target)
    _check_data_shape_to_num_outputs(preds, target, num_outputs)
    return preds, target


def _symmetric_mean_absolute_percentage_error_compute(sum_abs_per_error: Tensor, num_obs: int) ->Tensor:
    """Computes Symmetric Mean Absolute Percentage Error.

    Args:
        sum_abs_per_error: Sum of values of symmetric absolute percentage errors over all observations
            ``(symmetric absolute percentage error = 2 * |target - prediction| / (target + prediction))``
        num_obs: Number of predictions or observations

    Example:
        >>> target = torch.tensor([1, 10, 1e6])
        >>> preds = torch.tensor([0.9, 15, 1.2e6])
        >>> sum_abs_per_error, num_obs = _symmetric_mean_absolute_percentage_error_update(preds, target)
        >>> _symmetric_mean_absolute_percentage_error_compute(sum_abs_per_error, num_obs)
        tensor(0.2290)
    """
    return sum_abs_per_error / num_obs


def _symmetric_mean_absolute_percentage_error_update(preds: Tensor, target: Tensor, epsilon: float=1.17e-06) ->Tuple[Tensor, int]:
    """Updates and returns variables required to compute Symmetric Mean Absolute Percentage Error.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
        epsilon: Avoids ``ZeroDivisionError``.
    """
    _check_same_shape(preds, target)
    abs_diff = torch.abs(preds - target)
    abs_per_error = abs_diff / torch.clamp(torch.abs(target) + torch.abs(preds), min=epsilon)
    sum_abs_per_error = 2 * torch.sum(abs_per_error)
    num_obs = target.numel()
    return sum_abs_per_error, num_obs


def _tweedie_deviance_score_compute(sum_deviance_score: Tensor, num_observations: Tensor) ->Tensor:
    """Computes Deviance Score.

    Args:
        sum_deviance_score: Sum of deviance scores accumalated until now.
        num_observations: Number of observations encountered until now.

    Example:
        >>> targets = torch.tensor([1.0, 2.0, 3.0, 4.0])
        >>> preds = torch.tensor([4.0, 3.0, 2.0, 1.0])
        >>> sum_deviance_score, num_observations = _tweedie_deviance_score_update(preds, targets, power=2)
        >>> _tweedie_deviance_score_compute(sum_deviance_score, num_observations)
        tensor(1.2083)
    """
    return sum_deviance_score / num_observations


def _tweedie_deviance_score_update(preds: Tensor, targets: Tensor, power: float=0.0) ->Tuple[Tensor, Tensor]:
    """Updates and returns variables required to compute Deviance Score for the given power.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        targets: Ground truth tensor
        power: see :func:`tweedie_deviance_score`

    Example:
        >>> targets = torch.tensor([1.0, 2.0, 3.0, 4.0])
        >>> preds = torch.tensor([4.0, 3.0, 2.0, 1.0])
        >>> _tweedie_deviance_score_update(preds, targets, power=2)
        (tensor(4.8333), tensor(4))
    """
    _check_same_shape(preds, targets)
    zero_tensor = torch.zeros(preds.shape, device=preds.device)
    if 0 < power < 1:
        raise ValueError(f'Deviance Score is not defined for power={power}.')
    if power == 0:
        deviance_score = torch.pow(targets - preds, exponent=2)
    elif power == 1:
        if torch.any(preds <= 0) or torch.any(targets < 0):
            raise ValueError(f"For power={power}, 'preds' has to be strictly positive and 'targets' cannot be negative.")
        deviance_score = 2 * (_safe_xlogy(targets, targets / preds) + preds - targets)
    elif power == 2:
        if torch.any(preds <= 0) or torch.any(targets <= 0):
            raise ValueError(f"For power={power}, both 'preds' and 'targets' have to be strictly positive.")
        deviance_score = 2 * (torch.log(preds / targets) + targets / preds - 1)
    else:
        if power < 0:
            if torch.any(preds <= 0):
                raise ValueError(f"For power={power}, 'preds' has to be strictly positive.")
        elif 1 < power < 2:
            if torch.any(preds <= 0) or torch.any(targets < 0):
                raise ValueError(f"For power={power}, 'targets' has to be strictly positive and 'preds' cannot be negative.")
        elif torch.any(preds <= 0) or torch.any(targets <= 0):
            raise ValueError(f"For power={power}, both 'preds' and 'targets' have to be strictly positive.")
        term_1 = torch.pow(torch.max(targets, zero_tensor), 2 - power) / ((1 - power) * (2 - power))
        term_2 = targets * torch.pow(preds, 1 - power) / (1 - power)
        term_3 = torch.pow(preds, 2 - power) / (2 - power)
        deviance_score = 2 * (term_1 - term_2 + term_3)
    sum_deviance_score = torch.sum(deviance_score)
    num_observations = torch.tensor(torch.numel(deviance_score), device=preds.device)
    return sum_deviance_score, num_observations


def _weighted_mean_absolute_percentage_error_compute(sum_abs_error: Tensor, sum_scale: Tensor, epsilon: float=1.17e-06) ->Tensor:
    """Computes Weighted Absolute Percentage Error.

    Args:
        sum_abs_error: scalar with sum of absolute errors
        sum_scale: scalar with sum of target values
        epsilon: small float to prevent division by zero
    """
    return sum_abs_error / torch.clamp(sum_scale, min=epsilon)


def _weighted_mean_absolute_percentage_error_update(preds: Tensor, target: Tensor) ->Tuple[Tensor, int]:
    """Updates and returns variables required to compute Weighted Absolute Percentage Error.

    Checks for same shape of input tensors.

    Args:
        preds: Predicted tensor
        target: Ground truth tensor
    """
    _check_same_shape(preds, target)
    sum_abs_error = (preds - target).abs().sum()
    sum_scale = target.abs().sum()
    return sum_abs_error, sum_scale


def _check_retrieval_target_and_prediction_types(preds: Tensor, target: Tensor, allow_non_binary_target: bool=False) ->Tuple[Tensor, Tensor]:
    """Check ``preds`` and ``target`` tensors are of the same shape and of the correct data type.

    Args:
        preds: either tensor with scores/logits
        target: tensor with ground true labels
        allow_non_binary_target: whether to allow target to contain non-binary values

    Raises:
        ValueError:
            If ``preds`` and ``target`` don't have the same shape, if they are empty or not of the correct ``dtypes``.
    """
    if target.dtype not in (torch.bool, torch.long, torch.int) and not torch.is_floating_point(target):
        raise ValueError('`target` must be a tensor of booleans, integers or floats')
    if not preds.is_floating_point():
        raise ValueError('`preds` must be a tensor of floats')
    if not allow_non_binary_target and (target.max() > 1 or target.min() < 0):
        raise ValueError('`target` must contain `binary` values')
    target = target.float() if target.is_floating_point() else target.long()
    preds = preds.float()
    return preds.flatten(), target.flatten()


def _check_retrieval_inputs(indexes: Tensor, preds: Tensor, target: Tensor, allow_non_binary_target: bool=False, ignore_index: Optional[int]=None) ->Tuple[Tensor, Tensor, Tensor]:
    """Check ``indexes``, ``preds`` and ``target`` tensors are of the same shape and of the correct data type.

    Args:
        indexes: tensor with queries indexes
        preds: tensor with scores/logits
        target: tensor with ground true labels
        ignore_index: ignore predictions where targets are equal to this number

    Raises:
        ValueError:
            If ``preds`` and ``target`` don't have the same shape, if they are empty or not of the correct ``dtypes``.

    Returns:
        indexes: as ``torch.long``
        preds: as ``torch.float32``
        target: as ``torch.long``
    """
    if indexes.shape != preds.shape or preds.shape != target.shape:
        raise ValueError('`indexes`, `preds` and `target` must be of the same shape')
    if indexes.dtype is not torch.long:
        raise ValueError('`indexes` must be a tensor of long integers')
    if ignore_index is not None:
        valid_positions = target != ignore_index
        indexes, preds, target = indexes[valid_positions], preds[valid_positions], target[valid_positions]
    if not indexes.numel() or not indexes.size():
        raise ValueError('`indexes`, `preds` and `target` must be non-empty and non-scalar tensors')
    preds, target = _check_retrieval_target_and_prediction_types(preds, target, allow_non_binary_target=allow_non_binary_target)
    return indexes.long().flatten(), preds, target


def _flexible_bincount(x: Tensor) ->Tensor:
    """Similar to `_bincount`, but works also with tensor that do not contain continuous values.

    Args:
        x: tensor to count

    Returns:
        Number of occurrences for each unique element in x
    """
    x = x - x.min()
    unique_x = torch.unique(x)
    output = _bincount(x, minlength=torch.max(unique_x) + 1)
    return output[unique_x]


def _check_retrieval_functional_inputs(preds: Tensor, target: Tensor, allow_non_binary_target: bool=False) ->Tuple[Tensor, Tensor]:
    """Check ``preds`` and ``target`` tensors are of the same shape and of the correct data type.

    Args:
        preds: either tensor with scores/logits
        target: tensor with ground true labels
        allow_non_binary_target: whether to allow target to contain non-binary values

    Raises:
        ValueError:
            If ``preds`` and ``target`` don't have the same shape, if they are empty
            or not of the correct ``dtypes``.

    Returns:
        preds: as torch.float32
        target: as torch.long if not floating point else torch.float32
    """
    if preds.shape != target.shape:
        raise ValueError('`preds` and `target` must be of the same shape')
    if not preds.numel() or not preds.size():
        raise ValueError('`preds` and `target` must be non-empty and non-scalar tensors')
    return _check_retrieval_target_and_prediction_types(preds, target, allow_non_binary_target=allow_non_binary_target)


def retrieval_fall_out(preds: Tensor, target: Tensor, k: Optional[int]=None) ->Tensor:
    """Computes the Fall-out (for information retrieval), as explained in `IR Fall-out`_ Fall-out is the fraction
    of non-relevant documents retrieved among all the non-relevant documents.

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised. If you want to measure Fall-out@K, ``k`` must be a positive integer.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.
        k: consider only the top k elements (default: ``None``, which considers them all)

    Returns:
        a single-value tensor with the fall-out (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.

    Raises:
        ValueError:
            If ``k`` parameter is not `None` or an integer larger than 0

    Example:
        >>> from  torchmetrics.functional import retrieval_fall_out
        >>> preds = tensor([0.2, 0.3, 0.5])
        >>> target = tensor([True, False, True])
        >>> retrieval_fall_out(preds, target, k=2)
        tensor(1.)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    k = preds.shape[-1] if k is None else k
    if not (isinstance(k, int) and k > 0):
        raise ValueError('`k` has to be a positive integer or None')
    target = 1 - target
    if not target.sum():
        return tensor(0.0, device=preds.device)
    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:k].sum().float()
    return relevant / target.sum()


def retrieval_hit_rate(preds: Tensor, target: Tensor, k: Optional[int]=None) ->Tensor:
    """Computes the hit rate (for information retrieval). The hit rate is 1.0 if there is at least one relevant
    document among all the top `k` retrieved documents.

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised. If you want to measure HitRate@K, ``k`` must be a positive integer.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.
        k: consider only the top k elements (default: `None`, which considers them all)

    Returns:
        a single-value tensor with the hit rate (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.

    Raises:
        ValueError:
            If ``k`` parameter is not `None` or an integer larger than 0

    Example:
        >>> preds = tensor([0.2, 0.3, 0.5])
        >>> target = tensor([True, False, True])
        >>> retrieval_hit_rate(preds, target, k=2)
        tensor(1.)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    if k is None:
        k = preds.shape[-1]
    if not (isinstance(k, int) and k > 0):
        raise ValueError('`k` has to be a positive integer or None')
    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:k].sum()
    return (relevant > 0).float()


def _dcg(target: Tensor) ->Tensor:
    """Computes Discounted Cumulative Gain for input tensor."""
    denom = torch.log2(torch.arange(target.shape[-1], device=target.device) + 2.0)
    return (target / denom).sum(dim=-1)


def retrieval_normalized_dcg(preds: Tensor, target: Tensor, k: Optional[int]=None) ->Tensor:
    """Computes `Normalized Discounted Cumulative Gain`_ (for information retrieval).

    ``preds`` and ``target`` should be of the same shape and live on the same device.
    ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document relevance.
        k: consider only the top k elements (default: ``None``, which considers them all)

    Return:
        a single-value tensor with the nDCG of the predictions ``preds`` w.r.t. the labels ``target``.

    Raises:
        ValueError:
            If ``k`` parameter is not `None` or an integer larger than 0

    Example:
        >>> from torchmetrics.functional import retrieval_normalized_dcg
        >>> preds = torch.tensor([.1, .2, .3, 4, 70])
        >>> target = torch.tensor([10, 0, 0, 1, 5])
        >>> retrieval_normalized_dcg(preds, target)
        tensor(0.6957)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target, allow_non_binary_target=True)
    k = preds.shape[-1] if k is None else k
    if not (isinstance(k, int) and k > 0):
        raise ValueError('`k` has to be a positive integer or None')
    sorted_target = target[torch.argsort(preds, dim=-1, descending=True)][:k]
    ideal_target = torch.sort(target, descending=True)[0][:k]
    ideal_dcg = _dcg(ideal_target)
    target_dcg = _dcg(sorted_target)
    all_irrelevant = ideal_dcg == 0
    target_dcg[all_irrelevant] = 0
    target_dcg[~all_irrelevant] /= ideal_dcg[~all_irrelevant]
    return target_dcg.mean()


def retrieval_precision(preds: Tensor, target: Tensor, k: Optional[int]=None, adaptive_k: bool=False) ->Tensor:
    """Computes the precision metric (for information retrieval). Precision is the fraction of relevant documents
    among all the retrieved documents.

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised. If you want to measure Precision@K, ``k`` must be a positive integer.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.
        k: consider only the top k elements (default: ``None``, which considers them all)
        adaptive_k: adjust `k` to `min(k, number of documents)` for each query

    Returns:
        a single-value tensor with the precision (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.

    Raises:
        ValueError:
            If ``k`` is not `None` or an integer larger than 0.
        ValueError:
            If ``adaptive_k`` is not boolean.

    Example:
        >>> preds = tensor([0.2, 0.3, 0.5])
        >>> target = tensor([True, False, True])
        >>> retrieval_precision(preds, target, k=2)
        tensor(0.5000)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    if not isinstance(adaptive_k, bool):
        raise ValueError('`adaptive_k` has to be a boolean')
    if k is None or adaptive_k and k > preds.shape[-1]:
        k = preds.shape[-1]
    if not (isinstance(k, int) and k > 0):
        raise ValueError('`k` has to be a positive integer or None')
    if not target.sum():
        return tensor(0.0, device=preds.device)
    relevant = target[preds.topk(min(k, preds.shape[-1]), dim=-1)[1]].sum().float()
    return relevant / k


def retrieval_precision_recall_curve(preds: Tensor, target: Tensor, max_k: Optional[int]=None, adaptive_k: bool=False) ->Tuple[Tensor, Tensor, Tensor]:
    """Computes precision-recall pairs for different k (from 1 to `max_k`).

    In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by
    the top k retrieved documents.

    Recall is the fraction of relevant documents retrieved among all the relevant documents.
    Precision is the fraction of relevant documents among all the retrieved documents.

    For each such set, precision and recall values can be plotted to give a recall-precision
    curve.

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.
        max_k: Calculate recall and precision for all possible top k from 1 to max_k
               (default: `None`, which considers all possible top k)
        adaptive_k: adjust `max_k` to `min(max_k, number of documents)` for each query

    Returns:
        tensor with the precision values for each k (at ``k``) from 1 to `max_k`
        tensor with the recall values for each k (at ``k``) from 1 to `max_k`
        tensor with all possibles k

    Raises:
        ValueError:
            If ``max_k`` is not `None` or an integer larger than 0.
        ValueError:
            If ``adaptive_k`` is not boolean.

    Example:
        >>> from  torchmetrics.functional import retrieval_precision_recall_curve
        >>> preds = tensor([0.2, 0.3, 0.5])
        >>> target = tensor([True, False, True])
        >>> precisions, recalls, top_k = retrieval_precision_recall_curve(preds, target, max_k=2)
        >>> precisions
        tensor([1.0000, 0.5000])
        >>> recalls
        tensor([0.5000, 0.5000])
        >>> top_k
        tensor([1, 2])
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    if not isinstance(adaptive_k, bool):
        raise ValueError('`adaptive_k` has to be a boolean')
    if max_k is None:
        max_k = preds.shape[-1]
    if not (isinstance(max_k, int) and max_k > 0):
        raise ValueError('`max_k` has to be a positive integer or None')
    if adaptive_k and max_k > preds.shape[-1]:
        topk = torch.arange(1, preds.shape[-1] + 1, device=preds.device)
        topk = pad(topk, (0, max_k - preds.shape[-1]), 'constant', float(preds.shape[-1]))
    else:
        topk = torch.arange(1, max_k + 1, device=preds.device)
    if not target.sum():
        return torch.zeros(max_k, device=preds.device), torch.zeros(max_k, device=preds.device), topk
    relevant = target[preds.topk(min(max_k, preds.shape[-1]), dim=-1)[1]].float()
    relevant = cumsum(pad(relevant, (0, max(0, max_k - len(relevant))), 'constant', 0.0), dim=0)
    recall = relevant / target.sum()
    precision = relevant / topk
    return precision, recall, topk


def _retrieval_recall_at_fixed_precision(precision: Tensor, recall: Tensor, top_k: Tensor, min_precision: float) ->Tuple[Tensor, Tensor]:
    """Computes maximum recall with condition that corresponding precision >= `min_precision`.

    Args:
        top_k: tensor with all possible k
        precision: tensor with all values precisions@k for k from top_k tensor
        recall: tensor with all values recall@k for k from top_k tensor
        min_precision: float value specifying minimum precision threshold.

    Returns:
        Maximum recall value, corresponding it best k
    """
    try:
        max_recall, best_k = max((r, k) for p, r, k in zip(precision, recall, top_k) if p >= min_precision)
    except ValueError:
        max_recall = torch.tensor(0.0, device=recall.device, dtype=recall.dtype)
        best_k = torch.tensor(len(top_k))
    if max_recall == 0.0:
        best_k = torch.tensor(len(top_k), device=top_k.device, dtype=top_k.dtype)
    return max_recall, best_k


def retrieval_r_precision(preds: Tensor, target: Tensor) ->Tensor:
    """Computes the r-precision metric (for information retrieval). R-Precision is the fraction of relevant
    documents among all the top ``k`` retrieved documents where ``k`` is equal to the total number of relevant
    documents.

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised. If you want to measure Precision@K, ``k`` must be a positive integer.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.

    Returns:
        a single-value tensor with the r-precision of the predictions ``preds`` w.r.t. the labels ``target``.

    Example:
        >>> preds = tensor([0.2, 0.3, 0.5])
        >>> target = tensor([True, False, True])
        >>> retrieval_r_precision(preds, target)
        tensor(0.5000)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    relevant_number = target.sum()
    if not relevant_number:
        return tensor(0.0, device=preds.device)
    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:relevant_number].sum().float()
    return relevant / relevant_number


def retrieval_recall(preds: Tensor, target: Tensor, k: Optional[int]=None) ->Tensor:
    """Computes the recall metric (for information retrieval). Recall is the fraction of relevant documents
    retrieved among all the relevant documents.

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised. If you want to measure Recall@K, ``k`` must be a positive integer.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.
        k: consider only the top k elements (default: `None`, which considers them all)

    Returns:
        a single-value tensor with the recall (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.

    Raises:
        ValueError:
            If ``k`` parameter is not `None` or an integer larger than 0

    Example:
        >>> from  torchmetrics.functional import retrieval_recall
        >>> preds = tensor([0.2, 0.3, 0.5])
        >>> target = tensor([True, False, True])
        >>> retrieval_recall(preds, target, k=2)
        tensor(0.5000)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    if k is None:
        k = preds.shape[-1]
    if not (isinstance(k, int) and k > 0):
        raise ValueError('`k` has to be a positive integer or None')
    if not target.sum():
        return tensor(0.0, device=preds.device)
    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:k].sum().float()
    return relevant / target.sum()


def retrieval_reciprocal_rank(preds: Tensor, target: Tensor) ->Tensor:
    """Computes reciprocal rank (for information retrieval). See `Mean Reciprocal Rank`_

    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
    0 is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
    otherwise an error is raised.

    Args:
        preds: estimated probabilities of each document to be relevant.
        target: ground truth about each document being relevant or not.

    Return:
        a single-value tensor with the reciprocal rank (RR) of the predictions ``preds`` wrt the labels ``target``.

    Example:
        >>> from torchmetrics.functional import retrieval_reciprocal_rank
        >>> preds = torch.tensor([0.2, 0.3, 0.5])
        >>> target = torch.tensor([False, True, False])
        >>> retrieval_reciprocal_rank(preds, target)
        tensor(0.5000)
    """
    preds, target = _check_retrieval_functional_inputs(preds, target)
    if not target.sum():
        return tensor(0.0, device=preds.device)
    target = target[torch.argsort(preds, dim=-1, descending=True)]
    position = torch.nonzero(target).view(-1)
    res = 1.0 / (position[0] + 1.0)
    return res


_DEFAULT_MODEL = 'roberta-large'


def _get_input_dict(input_ids: List[Tensor], attention_mask: List[Tensor]) ->Dict[str, Tensor]:
    """Create an input dictionary of ``input_ids`` and ``attention_mask`` for BERTScore calculation."""
    output_dict = {'input_ids': torch.cat(input_ids), 'attention_mask': torch.cat(attention_mask)}
    return output_dict


def _sort_data_according_length(input_ids: Tensor, attention_mask: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
    """Sort tokenized sentence from the shortest to the longest one."""
    sorted_indices = attention_mask.sum(1).argsort()
    input_ids = input_ids[sorted_indices]
    attention_mask = attention_mask[sorted_indices]
    return input_ids, attention_mask, sorted_indices


def _preprocess_text(text: List[str], tokenizer: Any, max_length: int=512, truncation: bool=True, sort_according_length: bool=True, own_tokenizer: bool=False) ->Tuple[Dict[str, Tensor], Optional[Tensor]]:
    """Default text pre-processing function using `transformers` `AutoTokenizer` instance.

    Args:
        text:
            An iterable of sentences.
        tokenizer:
            Either `AutoTokenizer` instance from `transformers` package, or a user's own tokenizer.
        max_length:
            A maximum sequence length.
        truncation:
            An indication of whether tokenized sequences should be padded only to the length of the longest sequence.
        sort_according_length:
            An indication of whether tokenized sequences should be sorted from shortest to longest. This is appropriate
            to do for leveraging dynamic padding during embedding calculation and thereby to hasten inference.
        own_tokenizer:
            An indication of whether a non-default user's own tokenizer is used.

    Return:
        A dictionary of tokenized sentences including input_ids and attention_mask.

    Raises:
        BaseException:
            If a tokenization with a user's own tokenizer is not successful.
    """
    if not own_tokenizer:
        tokenized_data = tokenizer(text, padding='max_length', max_length=max_length, truncation=truncation, return_tensors='pt')
    else:
        try:
            tokenized_data = tokenizer(text, max_length)
        except BaseException as e:
            raise BaseException(f'Tokenization was not successful: {e}')
    if sort_according_length:
        input_ids, attention_mask, sorting_indices = _sort_data_according_length(tokenized_data['input_ids'], tokenized_data['attention_mask'])
        input_dict = {'input_ids': input_ids, 'attention_mask': attention_mask}
    else:
        input_dict = {'input_ids': tokenized_data['input_ids'], 'attention_mask': tokenized_data['attention_mask']}
        sorting_indices = None
    return input_dict, sorting_indices


class TextDataset(Dataset):
    """PyTorch dataset class for storing tokenized sentences and other properties used for BERT score
    calculation."""

    def __init__(self, text: List[str], tokenizer: Any, max_length: int=512, preprocess_text_fn: Callable[[List[str], Any, int], Union[Dict[str, Tensor], Tuple[Dict[str, Tensor], Optional[Tensor]]]]=_preprocess_text, idf: bool=False, tokens_idf: Optional[Dict[int, float]]=None) ->None:
        """
        Args:
            text:
                An iterable of sentences.
            tokenizer:
                `AutoTokenizer` instance from `transformers` package.
            max_length:
                A maximum sequence length.
            preprocess_text_fn:
                A function used for processing the input sentences.
            idf:
                An indication of whether calculate token inverse document frequencies to weight the model embeddings.
            tokens_idf:
                Inverse document frequencies (these should be calculated on reference sentences).
        """
        _text = preprocess_text_fn(text, tokenizer, max_length)
        if isinstance(_text, tuple):
            self.text, self.sorting_indices = _text
        else:
            self.text = _text
        self.max_length = self.text['input_ids'].shape[1]
        self.num_sentences = len(text)
        self.idf = idf
        self.tokens_idf = {}
        if idf:
            self.tokens_idf = tokens_idf if tokens_idf is not None else self._get_tokens_idf()

    def __getitem__(self, idx: int) ->Dict[str, Tensor]:
        input_ids = self.text['input_ids'][idx, :]
        attention_mask = self.text['attention_mask'][idx, :]
        inputs_dict = {'input_ids': input_ids, 'attention_mask': attention_mask}
        if self.idf:
            input_ids_idf = torch.tensor([self.tokens_idf[input_idx] for input_idx in input_ids.tolist()])
            inputs_dict['input_ids_idf'] = input_ids_idf
        return inputs_dict

    def __len__(self) ->int:
        return self.num_sentences

    def _get_tokens_idf(self) ->Dict[int, float]:
        """Calculate token inverse document frequences.

        Return:
            A python dictionary containing inverse document frequences for token ids.
        """
        token_counter: Counter = Counter()
        for tokens in map(self._set_of_tokens, self.text['input_ids']):
            token_counter.update(tokens)
        tokens_idf: Dict[int, float] = defaultdict(self._get_tokens_idf_default_value)
        tokens_idf.update({idx: math.log((self.num_sentences + 1) / (occurrence + 1)) for idx, occurrence in token_counter.items()})
        return tokens_idf

    def _get_tokens_idf_default_value(self) ->float:
        """Helper function that ensures `defaultdict` to be pickled."""
        return math.log((self.num_sentences + 1) / 1)

    @staticmethod
    def _set_of_tokens(input_ids: Tensor) ->Set:
        """Return set of tokens from the `input_ids` :class:`~torch.Tensor`."""
        return set(input_ids.tolist())


def _input_data_collator(batch: Dict[str, Tensor], device: Optional[Union[str, torch.device]]=None) ->Dict[str, Tensor]:
    """Helper function that trims model inputs to the longest sequence within the batch and put the input on the
    proper device."""
    max_len = int(batch['attention_mask'].sum(1).max().item())
    input_ids = batch['input_ids'][:, :max_len]
    attention_mask = batch['attention_mask'][:, :max_len]
    batch.update({'input_ids': input_ids, 'attention_mask': attention_mask})
    return batch


class TokenizedDataset(TextDataset):
    """The child class of `TextDataset` class used with already tokenized data."""

    def __init__(self, input_ids: Tensor, attention_mask: Tensor, idf: bool=False, tokens_idf: Optional[Dict[int, float]]=None) ->None:
        """
        Args:
            input_ids: Input indexes
            attention_mask: Attention mask
            idf:
                An indication of whether calculate token inverse document frequencies to weight the model embeddings.
            tokens_idf:
                Inverse document frequencies (these should be calculated on reference sentences).
        """
        text = dict(zip(['input_ids', 'attention_mask', 'sorting_indices'], _sort_data_according_length(input_ids, attention_mask)))
        self.sorting_indices = text.pop('sorting_indices')
        self.text = _input_data_collator(text)
        self.num_sentences = len(self.text['input_ids'])
        self.max_length = self.text['input_ids'].shape[1]
        self.idf = idf
        self.tokens_idf = {}
        if idf:
            self.tokens_idf = tokens_idf if tokens_idf is not None else self._get_tokens_idf()


def _check_shape_of_model_output(output: Tensor, input_ids: Tensor) ->None:
    """Check if the shape of the user's own model output."""
    bs, seq_len = input_ids.shape[:2]
    invalid_out_shape = len(output.shape) != 3 or output.shape[0] != bs or output.shape[1] != seq_len
    if invalid_out_shape:
        raise ValueError(f'The model output must be `Tensor` of a shape `[batch_size, seq_len, model_dim]` i.e. [{bs}, {seq_len}. , `model_dim`], but got {output.shape}.')


def _get_progress_bar(dataloader: DataLoader, verbose: bool=False) ->Union[DataLoader, 'tqdm.auto.tqdm']:
    """Helper function returning either the dataloader itself when `verbose = False`, or it wraps the dataloader with
    `tqdm.auto.tqdm`, when `verbose = True` to display a progress bar during the embbeddings calculation."""
    return tqdm.auto.tqdm(dataloader) if verbose else dataloader


def _output_data_collator(model_output: Tensor, attention_mask: Tensor, target_len: int) ->Tuple[Tensor, Tensor]:
    """Helper function that pads the model output and attention mask to the target length."""
    zeros_shape = list(model_output.shape)
    zeros_shape[2] = target_len - zeros_shape[2]
    model_output = torch.cat([model_output, torch.zeros(zeros_shape, dtype=model_output.dtype)], dim=2)
    zeros = torch.zeros(zeros_shape[0], zeros_shape[2], dtype=attention_mask.dtype)
    attention_mask = torch.cat([attention_mask, zeros], dim=1)
    return model_output, attention_mask


def _process_attention_mask_for_special_tokens(attention_mask: Tensor) ->Tensor:
    """Process attention mask to be zero for special [CLS] and [SEP] tokens as they're not included in a
    calculation for BERT score.

    Args:
        attention_mask: An attention mask to be returned, for example, by a `transformers` tokenizer.

    Return:
        A processed attention mask.
    """
    attention_mask[:, 0] = 0
    sep_token_position = (attention_mask - 0.1).cumsum(-1).argmax(-1)
    attention_mask[torch.arange(attention_mask.size(0)).long(), sep_token_position] = 0
    return attention_mask


def _get_embeddings_and_idf_scale(dataloader: DataLoader, target_len: int, model: Module, device: Optional[Union[str, torch.device]]=None, num_layers: Optional[int]=None, all_layers: bool=False, idf: bool=False, verbose: bool=False, user_forward_fn: Callable[[Module, Dict[str, Tensor]], Tensor]=None) ->Tuple[Tensor, Tensor]:
    """Calculate sentence embeddings and the inverse-document-frequency scaling factor.
    Args:
        dataloader: dataloader instance.
        target_len: A length of the longest sequence in the data. Used for padding the model output.
        model: BERT model.
        device: A device to be used for calculation.
        num_layers: The layer of representation to use.
        all_layers: An indication whether representation from all model layers should be used for BERTScore.
        idf: An Indication whether normalization using inverse document frequencies should be used.
        verbose: An indication of whether a progress bar to be displayed during the embeddings' calculation.
        user_forward_fn:
            A user's own forward function used in a combination with ``user_model``. This function must
            take ``user_model`` and a python dictionary of containing ``"input_ids"`` and ``"attention_mask"``
            represented by :class:`~torch.Tensor` as an input and return the model's output represented by the single
            :class:`~torch.Tensor`.

    Return:
        A tuple of :class:`~torch.Tensor`s containing the model's embeddings and the normalized tokens IDF.
        When ``idf = False``, tokens IDF is not calculated, and a matrix of mean weights is returned instead.
        For a single sentence, ``mean_weight = 1/seq_len``, where ``seq_len`` is a sum over the corresponding
        ``attention_mask``.

    Raises:
        ValueError:
            If ``all_layers = True`` and a model, which is not from the ``transformers`` package, is used.
    """
    embeddings_list: List[Tensor] = []
    idf_scale_list: List[Tensor] = []
    for batch in _get_progress_bar(dataloader, verbose):
        with torch.no_grad():
            batch = _input_data_collator(batch, device)
            if not all_layers:
                if not user_forward_fn:
                    out = model(batch['input_ids'], batch['attention_mask'], output_hidden_states=True)
                    out = out.hidden_states[num_layers if num_layers is not None else -1]
                else:
                    out = user_forward_fn(model, batch)
                    _check_shape_of_model_output(out, batch['input_ids'])
                out = out.unsqueeze(1)
            else:
                if user_forward_fn:
                    raise ValueError('The option `all_layers=True` can be used only with default `transformers` models.')
                out = model(batch['input_ids'], batch['attention_mask'], output_hidden_states=True)
                out = torch.cat([o.unsqueeze(1) for o in out.hidden_states], dim=1)
        out /= out.norm(dim=-1).unsqueeze(-1)
        out, attention_mask = _output_data_collator(out, batch['attention_mask'], target_len)
        processed_attention_mask = _process_attention_mask_for_special_tokens(attention_mask)
        out = torch.einsum('blsd, bs -> blsd', out, processed_attention_mask)
        embeddings_list.append(out.cpu())
        input_ids_idf = batch['input_ids_idf'] * processed_attention_mask if idf else processed_attention_mask.type(out.dtype)
        input_ids_idf /= input_ids_idf.sum(-1, keepdim=True)
        idf_scale_list.append(input_ids_idf.cpu())
    embeddings = torch.cat(embeddings_list)
    idf_scale = torch.cat(idf_scale_list)
    return embeddings, idf_scale


def _get_hash(model_name_or_path: Optional[str]=None, num_layers: Optional[int]=None, idf: bool=False) ->str:
    """Compute `BERT_score`_ (copied and adjusted)"""
    msg = f"{model_name_or_path}_L{num_layers}{'_idf' if idf else '_no-idf'}"
    return msg


def _get_scaled_precision_or_recall(cos_sim: Tensor, metric: str, idf_scale: Tensor) ->Tensor:
    """Helper function that calculates precision or recall, transpose it and scale it with idf_scale factor."""
    dim = 3 if metric == 'precision' else 2
    res = cos_sim.max(dim=dim).values
    res = torch.einsum('bls, bs -> bls', res, idf_scale).sum(-1)
    res = res.transpose(0, 1).squeeze()
    return res


def _get_precision_recall_f1(preds_embeddings: Tensor, target_embeddings: Tensor, preds_idf_scale: Tensor, target_idf_scale: Tensor) ->Tuple[Tensor, Tensor, Tensor]:
    """Calculate precision, recall and F1 score over candidate and reference sentences.

    Args:
        preds_embeddings: Embeddings of candidate sentences.
        target_embeddings: Embeddings of reference sentences.
        preds_idf_scale: An IDF scale factor for candidate sentences.
        target_idf_scale: An IDF scale factor for reference sentences.

    Return:
        Tensors containing precision, recall and F1 score, respectively.
    """
    cos_sim = torch.einsum('blpd, blrd -> blpr', preds_embeddings, target_embeddings)
    precision = _get_scaled_precision_or_recall(cos_sim, 'precision', preds_idf_scale)
    recall = _get_scaled_precision_or_recall(cos_sim, 'recall', target_idf_scale)
    f1_score = 2 * precision * recall / (precision + recall)
    f1_score = f1_score.masked_fill(torch.isnan(f1_score), 0.0)
    return precision, recall, f1_score


def _read_csv_from_local_file(baseline_path: str) ->Tensor:
    """Helper function which reads baseline the csv file from the local file.

    This method implemented to avoid `pandas` dependency.
    """
    with open(baseline_path) as fname:
        csv_file = csv.reader(fname)
        baseline_list = [[float(item) for item in row] for idx, row in enumerate(csv_file) if idx > 0]
    baseline = torch.tensor(baseline_list)[:, 1:]
    return baseline


def _read_csv_from_url(baseline_url: str) ->Tensor:
    """Helper function which reads the baseline csv file from URL.

    This method is implemented to avoid `pandas` dependency.
    """
    with urllib.request.urlopen(baseline_url) as http_request:
        baseline_list = [[float(item) for item in row.strip().decode('utf-8').split(',')] for idx, row in enumerate(http_request) if idx > 0]
        baseline = torch.tensor(baseline_list)[:, 1:]
    return baseline


def _load_baseline(lang: str='en', model_name_or_path: Optional[str]=None, baseline_path: Optional[str]=None, baseline_url: Optional[str]=None) ->Optional[Tensor]:
    """Load a CSV file with the baseline values used for rescaling."""
    if baseline_path:
        baseline: Optional[Tensor] = _read_csv_from_local_file(baseline_path)
    elif baseline_url:
        baseline = _read_csv_from_url(baseline_url)
    elif lang and model_name_or_path:
        _URL_BASE = 'https://raw.githubusercontent.com/Tiiiger/bert_score/master/bert_score/rescale_baseline'
        baseline_url = f'{_URL_BASE}/{lang}/{model_name_or_path}.tsv'
        baseline = _read_csv_from_url(baseline_url)
    else:
        baseline = None
        warn('Baseline was not successfully loaded. No baseline is going to be used.')
    return baseline


def _rescale_metrics_with_baseline(precision: Tensor, recall: Tensor, f1_score: Tensor, baseline: Tensor, num_layers: Optional[int]=None, all_layers: bool=False) ->Tuple[Tensor, Tensor, Tensor]:
    """Rescale the computed metrics with the pre-computed baseline."""
    if num_layers is None and all_layers is False:
        num_layers = -1
    all_metrics = torch.stack([precision, recall, f1_score], dim=-1)
    baseline_scale = baseline.unsqueeze(1) if all_layers else baseline[num_layers]
    all_metrics = (all_metrics - baseline_scale) / (1 - baseline_scale)
    return all_metrics[..., 0], all_metrics[..., 1], all_metrics[..., 2]


def bert_score(preds: Union[List[str], Dict[str, Tensor]], target: Union[List[str], Dict[str, Tensor]], model_name_or_path: Optional[str]=None, num_layers: Optional[int]=None, all_layers: bool=False, model: Optional[Module]=None, user_tokenizer: Any=None, user_forward_fn: Callable[[Module, Dict[str, Tensor]], Tensor]=None, verbose: bool=False, idf: bool=False, device: Optional[Union[str, torch.device]]=None, max_length: int=512, batch_size: int=64, num_threads: int=4, return_hash: bool=False, lang: str='en', rescale_with_baseline: bool=False, baseline_path: Optional[str]=None, baseline_url: Optional[str]=None) ->Dict[str, Union[List[float], str]]:
    """`Bert_score Evaluating Text Generation`_ leverages the pre-trained contextual embeddings from BERT and
    matches words in candidate and reference sentences by cosine similarity.

    It has been shown to correlate with human judgment on sentence-level and system-level evaluation.
    Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different
    language generation tasks.

    This implemenation follows the original implementation from `BERT_score`_.

    Args:
        preds: Either an iterable of predicted sentences or a ``Dict[input_ids, attention_mask]``.
        target: Either an iterable of target sentences or a  ``Dict[input_ids, attention_mask]``.
        model_name_or_path: A name or a model path used to load ``transformers`` pretrained model.
        num_layers: A layer of representation to use.
        all_layers:
            An indication of whether the representation from all model's layers should be used.
            If ``all_layers = True``, the argument ``num_layers`` is ignored.
        model: A user's own model.
        user_tokenizer:
            A user's own tokenizer used with the own model. This must be an instance with the ``__call__`` method.
            This method must take an iterable of sentences (``List[str]``) and must return a python dictionary
            containing ``"input_ids"`` and ``"attention_mask"`` represented by :class:`~torch.Tensor`.
            It is up to the user's model of whether ``"input_ids"`` is a :class:`~torch.Tensor` of input ids
            or embedding vectors. his tokenizer must prepend an equivalent of ``[CLS]`` token and append an equivalent
            of ``[SEP]`` token as `transformers` tokenizer does.
        user_forward_fn:
            A user's own forward function used in a combination with ``user_model``.
            This function must take ``user_model`` and a python dictionary of containing ``"input_ids"``
            and ``"attention_mask"`` represented by :class:`~torch.Tensor` as an input and return the model's output
            represented by the single :class:`~torch.Tensor`.
        verbose: An indication of whether a progress bar to be displayed during the embeddings' calculation.
        idf: An indication of whether normalization using inverse document frequencies should be used.
        device: A device to be used for calculation.
        max_length: A maximum length of input sequences. Sequences longer than ``max_length`` are to be trimmed.
        batch_size: A batch size used for model processing.
        num_threads: A number of threads to use for a dataloader.
        return_hash: An indication of whether the correspodning ``hash_code`` should be returned.
        lang: A language of input sentences. It is used when the scores are rescaled with a baseline.
        rescale_with_baseline:
            An indication of whether bertscore should be rescaled with a pre-computed baseline.
            When a pretrained model from ``transformers`` model is used, the corresponding baseline is downloaded
            from the original ``bert-score`` package from `BERT_score`_ if available.
            In other cases, please specify a path to the baseline csv/tsv file, which must follow the formatting
            of the files from `BERT_score`_
        baseline_path: A path to the user's own local csv/tsv file with the baseline scale.
        baseline_url: A url path to the user's own  csv/tsv file with the baseline scale.

    Returns:
        Python dictionary containing the keys ``precision``, ``recall`` and ``f1`` with corresponding values.

    Raises:
        ValueError:
            If ``len(preds) != len(target)``.
        ModuleNotFoundError:
            If `tqdm` package is required and not installed.
        ModuleNotFoundError:
            If ``transformers`` package is required and not installed.
        ValueError:
            If ``num_layer`` is larger than the number of the model layers.
        ValueError:
            If invalid input is provided.

    Example:
        >>> from torchmetrics.functional.text.bert import bert_score
        >>> preds = ["hello there", "general kenobi"]
        >>> target = ["hello there", "master kenobi"]
        >>> score = bert_score(preds, target)
        >>> from pprint import pprint
        >>> rounded_score = {k: [round(v, 3) for v in vv] for k, vv in score.items()}
        >>> pprint(rounded_score)
        {'f1': [1.0, 0.996], 'precision': [1.0, 0.996], 'recall': [1.0, 0.996]}
    """
    if len(preds) != len(target):
        raise ValueError('Number of predicted and reference sententes must be the same!')
    if verbose and not _TQDM_AVAILABLE:
        raise ModuleNotFoundError('An argument `verbose = True` requires `tqdm` package be installed. Install with `pip install tqdm`.')
    if model is None:
        if not _TRANSFORMERS_AVAILABLE:
            raise ModuleNotFoundError('`bert_score` metric with default models requires `transformers` package be installed. Either install with `pip install transformers>=4.0` or `pip install torchmetrics[text]`.')
        if model_name_or_path is None:
            warn(f'The argument `model_name_or_path` was not specified while it is required when default `transformers` model are used.It is, therefore, used the default recommended model - {_DEFAULT_MODEL}.')
        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path or _DEFAULT_MODEL)
        model = AutoModel.from_pretrained(model_name_or_path or _DEFAULT_MODEL)
    else:
        tokenizer = user_tokenizer
    model.eval()
    model
    try:
        if num_layers and num_layers > model.config.num_hidden_layers:
            raise ValueError(f'num_layers={num_layers} is forbidden for {model_name_or_path}. Please use num_layers <= {model.config.num_hidden_layers}')
    except AttributeError:
        warn('It was not possible to retrieve the parameter `num_layers` from the model specification.')
    _are_empty_lists = all(isinstance(text, list) and len(text) == 0 for text in (preds, target))
    _are_valid_lists = all(isinstance(text, list) and len(text) > 0 and isinstance(text[0], str) for text in (preds, target))
    _are_valid_tensors = all(isinstance(text, dict) and isinstance(text['input_ids'], Tensor) for text in (preds, target))
    if _are_empty_lists:
        warn('Predictions and references are empty.')
        output_dict: Dict[str, Union[List[float], str]] = {'precision': [0.0], 'recall': [0.0], 'f1': [0.0]}
        if return_hash:
            output_dict.update({'hash': _get_hash(model_name_or_path, num_layers, idf)})
        return output_dict
    baseline = _load_baseline(lang, model_name_or_path, baseline_path, baseline_url) if rescale_with_baseline else None
    if _are_valid_lists:
        target_dataset = TextDataset(target, tokenizer, max_length, idf=idf)
        preds_dataset = TextDataset(preds, tokenizer, max_length, idf=idf, tokens_idf=target_dataset.tokens_idf)
    elif _are_valid_tensors:
        target_dataset = TokenizedDataset(**target, idf=idf)
        preds_dataset = TokenizedDataset(**preds, idf=idf, tokens_idf=target_dataset.tokens_idf)
    else:
        raise ValueError('Invalid input provided.')
    target_loader = DataLoader(target_dataset, batch_size=batch_size, num_workers=num_threads)
    preds_loader = DataLoader(preds_dataset, batch_size=batch_size, num_workers=num_threads)
    target_embeddings, target_idf_scale = _get_embeddings_and_idf_scale(target_loader, target_dataset.max_length, model, device, num_layers, all_layers, idf, verbose, user_forward_fn)
    preds_embeddings, preds_idf_scale = _get_embeddings_and_idf_scale(preds_loader, preds_dataset.max_length, model, device, num_layers, all_layers, idf, verbose, user_forward_fn)
    precision, recall, f1_score = _get_precision_recall_f1(preds_embeddings, target_embeddings, preds_idf_scale, target_idf_scale)
    if baseline is not None:
        precision, recall, f1_score = _rescale_metrics_with_baseline(precision, recall, f1_score, baseline, num_layers, all_layers)
    output_dict = {'precision': precision.tolist(), 'recall': recall.tolist(), 'f1': f1_score.tolist()}
    if return_hash:
        output_dict.update({'hash': _get_hash(model_name_or_path, num_layers, idf)})
    return output_dict


def _bleu_score_compute(preds_len: Tensor, target_len: Tensor, numerator: Tensor, denominator: Tensor, n_gram: int, weights: Sequence[float], smooth: bool) ->Tensor:
    """Computes the BLEU score.

    Args:
        preds_len: count of words in a candidate translation
        target_len: count of words in a reference translation
        numerator: Numerator of precision score (true positives)
        denominator: Denominator of precision score (true positives + false positives)
        n_gram: gram value ranged 1 to 4
        weights: Weights used for unigrams, bigrams, etc. to calculate BLEU score.
        smooth: Whether to apply smoothing
    """
    device = numerator.device
    if min(numerator) == 0.0:
        return tensor(0.0, device=device)
    if smooth:
        precision_scores = torch.div(torch.add(numerator, torch.ones(n_gram, device=device)), torch.add(denominator, torch.ones(n_gram, device=device)))
        precision_scores[0] = numerator[0] / denominator[0]
    else:
        precision_scores = numerator / denominator
    log_precision_scores = tensor(weights, device=device) * torch.log(precision_scores)
    geometric_mean = torch.exp(torch.sum(log_precision_scores))
    brevity_penalty = tensor(1.0, device=device) if preds_len > target_len else torch.exp(1 - target_len / preds_len)
    bleu = brevity_penalty * geometric_mean
    return bleu


def _count_ngram(ngram_input_list: Sequence[str], n_gram: int) ->Counter:
    """Counting how many times each word appears in a given text with ngram.

    Args:
        ngram_input_list: A list of translated text or reference texts
        n_gram: gram value ranged 1 to 4

    Return:
        ngram_counter: a collections.Counter object of ngram
    """
    ngram_counter: Counter = Counter()
    for i in range(1, n_gram + 1):
        for j in range(len(ngram_input_list) - i + 1):
            ngram_key = tuple(ngram_input_list[j:i + j])
            ngram_counter[ngram_key] += 1
    return ngram_counter


def _tokenize_fn(sentence: str) ->Sequence[str]:
    """Tokenizes sentence into list of words.

    Args:
        sentence: A sentence separated by white space.

    Return:
        List of words
    """
    return sentence.split()


def _bleu_score_update(preds: Sequence[str], target: Sequence[Sequence[str]], numerator: Tensor, denominator: Tensor, preds_len: Tensor, target_len: Tensor, n_gram: int=4, tokenizer: Callable[[str], Sequence[str]]=_tokenize_fn) ->Tuple[Tensor, Tensor]:
    """Updates and returns variables required to compute the BLEU score.

    Args:
        preds: An iterable of machine translated corpus
        target: An iterable of iterables of reference corpus
        numerator: Numerator of precision score (true positives)
        denominator: Denominator of precision score (true positives + false positives)
        preds_len: count of words in a candidate prediction
        target: count of words in a reference translation
        n_gram: gram value ranged 1 to 4
        tokenizer: A function that turns sentence into list of words
    """
    target_: Sequence[Sequence[Sequence[str]]] = [[(tokenizer(line) if line else []) for line in t] for t in target]
    preds_: Sequence[Sequence[str]] = [(tokenizer(line) if line else []) for line in preds]
    for pred, targets in zip(preds_, target_):
        preds_len += len(pred)
        target_len_list = [len(tgt) for tgt in targets]
        target_len_diff = [abs(len(pred) - x) for x in target_len_list]
        target_len += target_len_list[target_len_diff.index(min(target_len_diff))]
        preds_counter: Counter = _count_ngram(pred, n_gram)
        target_counter: Counter = Counter()
        for tgt in targets:
            target_counter |= _count_ngram(tgt, n_gram)
        ngram_counter_clip = preds_counter & target_counter
        for counter_clip in ngram_counter_clip:
            numerator[len(counter_clip) - 1] += ngram_counter_clip[counter_clip]
        for counter in preds_counter:
            denominator[len(counter) - 1] += preds_counter[counter]
    return preds_len, target_len


def _cer_compute(errors: Tensor, total: Tensor) ->Tensor:
    """Compute the Character error rate.

    Args:
        errors: Number of edit operations to get from the reference to the prediction, summed over all samples
        total: Number of characters over all references

    Returns:
        Character error rate score
    """
    return errors / total


def _edit_distance(prediction_tokens: List[str], reference_tokens: List[str]) ->int:
    """Standard dynamic programming algorithm to compute the edit distance.

    Args:
        prediction_tokens: A tokenized predicted sentence
        reference_tokens: A tokenized reference sentence
    Returns:
        Edit distance between the predicted sentence and the reference sentence
    """
    dp = [([0] * (len(reference_tokens) + 1)) for _ in range(len(prediction_tokens) + 1)]
    for i in range(len(prediction_tokens) + 1):
        dp[i][0] = i
    for j in range(len(reference_tokens) + 1):
        dp[0][j] = j
    for i in range(1, len(prediction_tokens) + 1):
        for j in range(1, len(reference_tokens) + 1):
            if prediction_tokens[i - 1] == reference_tokens[j - 1]:
                dp[i][j] = dp[i - 1][j - 1]
            else:
                dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1
    return dp[-1][-1]


def _cer_update(preds: Union[str, List[str]], target: Union[str, List[str]]) ->Tuple[Tensor, Tensor]:
    """Update the cer score with the current set of references and predictions.

    Args:
        preds: Transcription(s) to score as a string or list of strings
        target: Reference(s) for each speech input as a string or list of strings

    Returns:
        Number of edit operations to get from the reference to the prediction, summed over all samples
        Number of character overall references
    """
    if isinstance(preds, str):
        preds = [preds]
    if isinstance(target, str):
        target = [target]
    errors = tensor(0, dtype=torch.float)
    total = tensor(0, dtype=torch.float)
    for pred, tgt in zip(preds, target):
        pred_tokens = pred
        tgt_tokens = tgt
        errors += _edit_distance(list(pred_tokens), list(tgt_tokens))
        total += len(tgt_tokens)
    return errors, total


_DICT_STATES_NAMES = 'total_preds_char_n_grams', 'total_preds_word_n_grams', 'total_target_char_n_grams', 'total_target_word_n_grams', 'total_matching_char_n_grams', 'total_matching_word_n_grams'


_DICT_STATES_TYPES = Tuple[Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor]]


_N_GRAM_LEVELS = 'char', 'word'


_TEXT_LEVELS = 'preds', 'target', 'matching'


_EPS_SMOOTHING = tensor(1e-16)


def _calculate_fscore(matching_char_n_grams: Dict[int, Tensor], matching_word_n_grams: Dict[int, Tensor], hyp_char_n_grams: Dict[int, Tensor], hyp_word_n_grams: Dict[int, Tensor], ref_char_n_grams: Dict[int, Tensor], ref_word_n_grams: Dict[int, Tensor], n_order: float, beta: float) ->Tensor:
    """Calculate sentence-level chrF/chrF++ score.

    For given hypothesis and reference statistics (either sentence-level or corpus-level)
    the chrF/chrF++ score is returned.

    Args:
        matching_char_n_grams:
            A total number of matching character n-grams between the best matching reference and hypothesis.
        matching_word_n_grams:
            A total number of matching word n-grams between the best matching reference and hypothesis.
        hyp_char_n_grams: A total number of hypothesis character n-grams.
        hyp_word_n_grams: A total number of hypothesis word n-grams.
        ref_char_n_grams: A total number of reference character n-grams.
        ref_word_n_grams: A total number of reference word n-grams.
        n_order: A sum of character and word n-gram order.
        beta: A parameter determining an importance of recall w.r.t. precision. If `beta=1`, their importance is equal.

    Return:
        A chrF/chrF++ score. This function is universal both for sentence-level and corpus-level calucation.
    """

    def _get_n_gram_fscore(matching_n_grams: Dict[int, Tensor], ref_n_grams: Dict[int, Tensor], hyp_n_grams: Dict[int, Tensor], beta: float) ->Dict[int, Tensor]:
        """Get n-gram level f-score."""
        precision: Dict[int, Tensor] = {n: (matching_n_grams[n] / hyp_n_grams[n] if hyp_n_grams[n] > 0 else tensor(0.0)) for n in matching_n_grams}
        recall: Dict[int, Tensor] = {n: (matching_n_grams[n] / ref_n_grams[n] if ref_n_grams[n] > 0 else tensor(0.0)) for n in matching_n_grams}
        denominator: Dict[int, Tensor] = {n: torch.max(beta ** 2 * precision[n] + recall[n], _EPS_SMOOTHING) for n in matching_n_grams}
        f_score: Dict[int, Tensor] = {n: ((1 + beta ** 2) * precision[n] * recall[n] / denominator[n]) for n in matching_n_grams}
        return f_score
    char_n_gram_f_score = _get_n_gram_fscore(matching_char_n_grams, ref_char_n_grams, hyp_char_n_grams, beta)
    word_n_gram_f_score = _get_n_gram_fscore(matching_word_n_grams, ref_word_n_grams, hyp_word_n_grams, beta)
    f_score = (sum(char_n_gram_f_score.values()) + sum(word_n_gram_f_score.values())) / tensor(n_order)
    return f_score


def _chrf_score_compute(total_preds_char_n_grams: Dict[int, Tensor], total_preds_word_n_grams: Dict[int, Tensor], total_target_char_n_grams: Dict[int, Tensor], total_target_word_n_grams: Dict[int, Tensor], total_matching_char_n_grams: Dict[int, Tensor], total_matching_word_n_grams: Dict[int, Tensor], n_order: float, beta: float) ->Tensor:
    """Compute chrF/chrF++ score based on pre-computed target, prediction and matching character and word n-grams.

    Args:
        total_preds_char_n_grams: number of hypothesis character n-grams.
        total_preds_word_n_grams: number of hypothesis word n-grams.
        total_target_char_n_grams: number of reference character n-grams.
        total_target_word_n_grams: number of reference word n-grams.
        total_matching_char_n_grams: number of matching character n-grams between references and hypotheses.
        total_matching_word_n_grams: number of total matching word n-grams between references and hypotheses.
        n_order: A sum of character and word n-gram order.
        beta:
            A parameter determining an importance of recall w.r.t. precision. If `beta=1`, their importance is equal.

    Return:
        A corpus-level chrF/chrF++ score.
    """
    chrf_f_score = _calculate_fscore(total_matching_char_n_grams, total_matching_word_n_grams, total_preds_char_n_grams, total_preds_word_n_grams, total_target_char_n_grams, total_target_word_n_grams, n_order, beta)
    return chrf_f_score


def _get_characters(sentence: str, whitespace: bool) ->List[str]:
    """Split sentence into individual characters.

    Args:
        sentence: An input sentence to split.
        whitespace: An indication whether to keep whitespaces during character n-gram extraction.

    Return:
        A list of separated characters.
    """
    if whitespace:
        return list(sentence)
    return list(sentence.strip().replace(' ', ''))


_PUNCTUATIONS = set('!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~')


def _separate_word_and_punctiation(word: str) ->List[str]:
    """
    Separates out punctuations from beginning and end of words for chrF. Adapted from https://github.com/m-popovic/chrF
    and https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/chrf.py.
    Args:
        word: An input word to be separated from a punctuation if present.

    Return:
        A list of a single word or a separated word and punctuation.
    """
    if len(word) == 1:
        return [word]
    if word[-1] in _PUNCTUATIONS:
        return [word[:-1], word[-1]]
    if word[0] in _PUNCTUATIONS:
        return [word[0], word[1:]]
    return [word]


def _get_words_and_punctiation(sentence: str) ->List[str]:
    """Separates out punctuations from beginning and end of words for chrF for all words in the sentence.

    Args:
        sentence: An input sentence to split

    Return:
        An aggregated list of separated words and punctuations.
    """
    return sum((_separate_word_and_punctiation(word) for word in sentence.strip().split()), [])


def _ngram_counts(char_or_word_list: List[str], n_gram_order: int) ->Dict[int, Dict[Tuple[str, ...], Tensor]]:
    """
    Args:
        char_or_word_list: A list of characters of words
        n_gram_order: The largest number of n-gram.

    Return:
        A dictionary of dictionaries with a counts of given n-grams.
    """
    ngrams: Dict[int, Dict[Tuple[str, ...], Tensor]] = defaultdict(lambda : defaultdict(lambda : tensor(0.0)))
    for n in range(1, n_gram_order + 1):
        for ngram in (tuple(char_or_word_list[i:i + n]) for i in range(len(char_or_word_list) - n + 1)):
            ngrams[n][ngram] += tensor(1)
    return ngrams


def _get_n_grams_counts_and_total_ngrams(sentence: str, n_char_order: int, n_word_order: int, lowercase: bool, whitespace: bool) ->Tuple[Dict[int, Dict[Tuple[str, ...], Tensor]], Dict[int, Dict[Tuple[str, ...], Tensor]], Dict[int, Tensor], Dict[int, Tensor]]:
    """
    Args:
        sentence: An input sentence
        n_char_order: A character n-gram order.
        n_word_order: A word n-gram order.
        lowercase: An indication whether to enable case-insensitivity.
        whitespace: An indication whether to keep whitespaces during character n-gram extraction.

    Return:
        char_n_grams_counts: A dictionary of dictionaries with sentence character n-grams.
        word_n_grams_counts: A dictionary of dictionaries with sentence word n-grams.
        total_char_n_grams: A dictionary containing a total number of sentence character n-grams.
        total_word_n_grams: A dictionary containing a total number of sentence word n-grams.
    """

    def _char_and_word_ngrams_counts(sentence: str, n_char_order: int, n_word_order: int, lowercase: bool) ->Tuple[Dict[int, Dict[Tuple[str, ...], Tensor]], Dict[int, Dict[Tuple[str, ...], Tensor]]]:
        """Get a dictionary of dictionaries with a counts of given n-grams."""
        if lowercase:
            sentence = sentence.lower()
        char_n_grams_counts = _ngram_counts(_get_characters(sentence, whitespace), n_char_order)
        word_n_grams_counts = _ngram_counts(_get_words_and_punctiation(sentence), n_word_order)
        return char_n_grams_counts, word_n_grams_counts

    def _get_total_ngrams(n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]]) ->Dict[int, Tensor]:
        """Get total sum of n-grams over n-grams w.r.t n."""
        total_n_grams: Dict[int, Tensor] = defaultdict(lambda : tensor(0.0))
        for n in n_grams_counts:
            total_n_grams[n] = tensor(sum(n_grams_counts[n].values()))
        return total_n_grams
    char_n_grams_counts, word_n_grams_counts = _char_and_word_ngrams_counts(sentence, n_char_order, n_word_order, lowercase)
    total_char_n_grams = _get_total_ngrams(char_n_grams_counts)
    total_word_n_grams = _get_total_ngrams(word_n_grams_counts)
    return char_n_grams_counts, word_n_grams_counts, total_char_n_grams, total_word_n_grams


def _get_ngram_matches(hyp_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]], ref_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]]) ->Dict[int, Tensor]:
    """Get a number of n-gram matches between reference and hypothesis n-grams.

    Args:
        hyp_n_grams_counts:
        ref_n_grams_counts:

    Return:
        matching_n_grams
    """
    matching_n_grams: Dict[int, Tensor] = defaultdict(lambda : tensor(0.0))
    for n in hyp_n_grams_counts:
        matching_n_grams[n] = tensor(sum(torch.min(ref_n_grams_counts[n][n_gram], hyp_n_grams_counts[n][n_gram]) for n_gram in hyp_n_grams_counts[n]))
    return matching_n_grams


def _calculate_sentence_level_chrf_score(targets: List[str], pred_char_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]], pred_word_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]], preds_char_n_grams: Dict[int, Tensor], preds_word_n_grams: Dict[int, Tensor], n_char_order: int, n_word_order: int, n_order: float, beta: float, lowercase: bool, whitespace: bool) ->Tuple[Tensor, Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor]]:
    """Calculate the best sentence-level chrF/chrF++ score.

    For a given pre-processed hypothesis, all references are evaluated and score and statistics
    for the best matching reference is returned.

    Args:
        targets: An iterable of references.
        preds_char_n_grams_counts: A dictionary of dictionaries with hypothesis character n-grams.
        preds_word_n_grams_counts: A dictionary of dictionaries with hypothesis word n-grams.
        pred_char_n_grams: A total number of hypothesis character n-grams.
        pred_word_n_grams: A total number of hypothesis word n-grams.
        n_char_order: A character n-gram order.
        n_word_order: A word n-gram order.
        n_order: A sum of character and word n-gram order.
        beta: A parameter determining an importance of recall w.r.t. precision. If `beta=1`, their importance is equal.
        lowercase: An indication whether to enable case-insensitivity.
        whitespace: An indication whether to keep whitespaces during character n-gram extraction.

    Return:
        Return chrF/chrF++ score and statistics for the best matching hypothesis and reference.

        f_score: A sentence-level chrF/chrF++ score.
        matching_char_n_grams:
            A total number of matching character n-grams between the best matching reference and hypothesis.
        matching_word_n_grams:
            A total number of matching word n-grams between the best matching reference and hypothesis.
        target_char_n_grams: A total number of reference character n-grams.
        target_word_n_grams: A total number of reference word n-grams.
    """
    best_f_score = tensor(0.0)
    best_matching_char_n_grams: Dict[int, Tensor] = defaultdict(lambda : tensor(0.0))
    best_matching_word_n_grams: Dict[int, Tensor] = defaultdict(lambda : tensor(0.0))
    best_target_char_n_grams: Dict[int, Tensor] = defaultdict(lambda : tensor(0.0))
    best_target_word_n_grams: Dict[int, Tensor] = defaultdict(lambda : tensor(0.0))
    for target in targets:
        target_char_n_grams_counts, target_word_n_grams_counts, target_char_n_grams, target_word_n_grams = _get_n_grams_counts_and_total_ngrams(target, n_char_order, n_word_order, lowercase, whitespace)
        matching_char_n_grams = _get_ngram_matches(target_char_n_grams_counts, pred_char_n_grams_counts)
        matching_word_n_grams = _get_ngram_matches(target_word_n_grams_counts, pred_word_n_grams_counts)
        f_score = _calculate_fscore(matching_char_n_grams, matching_word_n_grams, preds_char_n_grams, preds_word_n_grams, target_char_n_grams, target_word_n_grams, n_order, beta)
        if f_score > best_f_score:
            best_f_score = f_score
            best_matching_char_n_grams = matching_char_n_grams
            best_matching_word_n_grams = matching_word_n_grams
            best_target_char_n_grams = target_char_n_grams
            best_target_word_n_grams = target_word_n_grams
    return best_f_score, best_matching_char_n_grams, best_matching_word_n_grams, best_target_char_n_grams, best_target_word_n_grams


def _sum_over_dicts(total_n_grams: Dict[int, Tensor], n_grams: Dict[int, Tensor]) ->Dict[int, Tensor]:
    """Aggregate total n-grams to keep corpus-level statistics.

    Args:
        total_n_grams: A dictionary containing a total corpus-level number of n-grams.
        n_grams: A dictionary containing a sentence-level number of n-grams.

    Return:
        A dictionary containing a total corpus-level number of n-grams.
    """
    for n in n_grams:
        total_n_grams[n] += n_grams[n]
    return total_n_grams


def _validate_inputs(reference_corpus: Union[Sequence[str], Sequence[Sequence[str]]], hypothesis_corpus: Union[str, Sequence[str]]) ->Tuple[Sequence[Sequence[str]], Sequence[str]]:
    """Check and update (if needed) the format of reference and hypothesis corpora for various text evaluation
    metrics.

    Args:
        reference_corpus: An iterable of iterables of reference corpus.
        hypothesis_corpus: An iterable of hypothesis corpus.

    Return:
        reference_corpus: An iterable of iterables of reference corpus.
        hypothesis_corpus: An iterable of hypothesis corpus.

    Raises:
        ValueError:
            If length of `reference_corpus` and `hypothesis_corpus` differs.
    """
    if isinstance(hypothesis_corpus, str):
        hypothesis_corpus = [hypothesis_corpus]
    if all(isinstance(ref, str) for ref in reference_corpus):
        if len(hypothesis_corpus) == 1:
            reference_corpus = [reference_corpus]
        else:
            reference_corpus = [[ref] for ref in reference_corpus]
    if hypothesis_corpus and all(ref for ref in reference_corpus) and len(reference_corpus) != len(hypothesis_corpus):
        raise ValueError(f'Corpus has different size {len(reference_corpus)} != {len(hypothesis_corpus)}')
    return reference_corpus, hypothesis_corpus


def _chrf_score_update(preds: Union[str, Sequence[str]], target: Union[Sequence[str], Sequence[Sequence[str]]], total_preds_char_n_grams: Dict[int, Tensor], total_preds_word_n_grams: Dict[int, Tensor], total_target_char_n_grams: Dict[int, Tensor], total_target_word_n_grams: Dict[int, Tensor], total_matching_char_n_grams: Dict[int, Tensor], total_matching_word_n_grams: Dict[int, Tensor], n_char_order: int, n_word_order: int, n_order: float, beta: float, lowercase: bool, whitespace: bool, sentence_chrf_score: Optional[List[Tensor]]=None) ->Tuple[Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Optional[List[Tensor]]]:
    """
    Args:
        preds: An iterable of hypothesis corpus.
        target: An iterable of iterables of reference corpus.
        total_preds_char_n_grams: A dictionary containing a total number of hypothesis character n-grams.
        total_preds_word_n_grams: A dictionary containing a total number of hypothesis word n-grams.
        total_target_char_n_grams: A dictionary containing a total number of reference character n-grams.
        total_target_word_n_grams: A dictionary containing a total number of reference word n-grams.
        total_matching_char_n_grams:
            A dictionary containing a total number of matching character n-grams between references and hypotheses.
        total_matching_word_n_grams:
            A dictionary containing a total number of total matching word n-grams between references and hypotheses.
        n_char_order: A character n-gram order.
        n_word_order: A word n-gram order.
        n_order: Sum of character and word n-gram order.
        beta: A parameter determining an importance of recall w.r.t. precision. If `beta=1`, their importance is equal.
        lowercase: An indication whether to enable case-insensitivity.
        whitespace: An indication whether to keep whitespaces during character n-gram extraction.
        sentence_chrf_score: A list of sentence-level chrF/chrF++ scores.

    Return:
        total_target_char_n_grams: number of reference character n-grams.
        total_target_word_n_grams: number of reference word n-grams.
        total_preds_char_n_grams: number of hypothesis character n-grams.
        total_preds_word_n_grams: number of hypothesis word n-grams.
        total_matching_char_n_grams: number of matching character n-grams between references and hypotheses.
        total_matching_word_n_grams: number of total matching word n-grams between references and hypotheses.
        sentence_chrf_score: A list of sentence-level chrF/chrF++ scores.

    Raises:
        ValueError:
            If length of ``preds`` and ``target`` differs.
    """
    target_corpus, preds = _validate_inputs(target, preds)
    for pred, targets in zip(preds, target_corpus):
        pred_char_n_grams_counts, pred_word_n_grams_counts, pred_char_n_grams, pred_word_n_grams = _get_n_grams_counts_and_total_ngrams(pred, n_char_order, n_word_order, lowercase, whitespace)
        total_preds_char_n_grams = _sum_over_dicts(total_preds_char_n_grams, pred_char_n_grams)
        total_preds_word_n_grams = _sum_over_dicts(total_preds_word_n_grams, pred_word_n_grams)
        sentence_level_f_score, matching_char_n_grams, matching_word_n_grams, target_char_n_grams, target_word_n_grams = _calculate_sentence_level_chrf_score(targets, pred_char_n_grams_counts, pred_word_n_grams_counts, pred_char_n_grams, pred_word_n_grams, n_char_order, n_word_order, n_order, beta, lowercase, whitespace)
        if sentence_chrf_score is not None:
            sentence_chrf_score.append(sentence_level_f_score.unsqueeze(0))
        total_target_char_n_grams = _sum_over_dicts(total_target_char_n_grams, target_char_n_grams)
        total_target_word_n_grams = _sum_over_dicts(total_target_word_n_grams, target_word_n_grams)
        total_matching_char_n_grams = _sum_over_dicts(total_matching_char_n_grams, matching_char_n_grams)
        total_matching_word_n_grams = _sum_over_dicts(total_matching_word_n_grams, matching_word_n_grams)
    return total_preds_char_n_grams, total_preds_word_n_grams, total_target_char_n_grams, total_target_word_n_grams, total_matching_char_n_grams, total_matching_word_n_grams, sentence_chrf_score


def _prepare_n_grams_dicts(n_char_order: int, n_word_order: int) ->Tuple[Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor]]:
    """Prepare dictionaries dictionaries with default zero values for total reference, hypothesis and matching
    character and word n-grams.

    Args:
        n_char_order: A character n-gram order.
        n_word_order: A word n-gram order.

    Return:
        Dictionaries with default zero values for total reference, hypothesis and matching character and word
        n-grams.
    """
    total_preds_char_n_grams: Dict[int, Tensor] = {(n + 1): tensor(0.0) for n in range(n_char_order)}
    total_preds_word_n_grams: Dict[int, Tensor] = {(n + 1): tensor(0.0) for n in range(n_word_order)}
    total_target_char_n_grams: Dict[int, Tensor] = {(n + 1): tensor(0.0) for n in range(n_char_order)}
    total_target_word_n_grams: Dict[int, Tensor] = {(n + 1): tensor(0.0) for n in range(n_word_order)}
    total_matching_char_n_grams: Dict[int, Tensor] = {(n + 1): tensor(0.0) for n in range(n_char_order)}
    total_matching_word_n_grams: Dict[int, Tensor] = {(n + 1): tensor(0.0) for n in range(n_word_order)}
    return total_preds_char_n_grams, total_preds_word_n_grams, total_target_char_n_grams, total_target_word_n_grams, total_matching_char_n_grams, total_matching_word_n_grams


def _eed_compute(sentence_level_scores: List[Tensor]) ->Tensor:
    """Final step in extended edit distance.

    Args:
        sentence_level_scores: list of sentence-level scores as floats

    Return:
        average of scores as a tensor
    """
    if len(sentence_level_scores) == 0:
        return tensor(0.0)
    average = sum(sentence_level_scores) / tensor(len(sentence_level_scores))
    return average


_BEAM_WIDTH = 25


_INT_INFINITY = int(1e+16)


_MAX_CACHE_SIZE = 10000


_MAX_SHIFT_CANDIDATES = 1000


_MAX_SHIFT_DIST = 50


_MAX_SHIFT_SIZE = 10


def _find_shifted_pairs(pred_words: List[str], target_words: List[str]) ->Iterator[Tuple[int, int, int]]:
    """Find matching word sub-sequences in two lists of words. Ignores sub-sequences starting at the same position.

    Args:
        pred_words: A list of a tokenized hypothesis sentence.
        target_words: A list of a tokenized reference sentence.

    Return:
        Yields tuples of ``target_start, pred_start, length`` such that:
        ``target_words[target_start : target_start + length] == pred_words[pred_start : pred_start + length]``

        pred_start:
            A list of hypothesis start indices.
        target_start:
            A list of reference start indices.
        length:
            A length of a word span to be considered.
    """
    for pred_start in range(len(pred_words)):
        for target_start in range(len(target_words)):
            if abs(target_start - pred_start) > _MAX_SHIFT_DIST:
                continue
            for length in range(1, _MAX_SHIFT_SIZE):
                if pred_words[pred_start + length - 1] != target_words[target_start + length - 1]:
                    break
                yield pred_start, target_start, length
                _hyp = len(pred_words) == pred_start + length
                _ref = len(target_words) == target_start + length
                if _hyp or _ref:
                    break


def _handle_corner_cases_during_shifting(alignments: Dict[int, int], pred_errors: List[int], target_errors: List[int], pred_start: int, target_start: int, length: int) ->bool:
    """A helper function which returns ``True`` if any of corner cases has been met. Otherwise, ``False`` is
    returned.

    Args:
        alignments: A dictionary mapping aligned positions between a reference and a hypothesis.
        pred_errors: A list of error positions in a hypothesis.
        target_errors: A list of error positions in a reference.
        pred_start: A hypothesis start index.
        target_start: A reference start index.
        length: A length of a word span to be considered.

    Return:
        An indication whether any of conrner cases has been met.
    """
    if sum(pred_errors[pred_start:pred_start + length]) == 0:
        return True
    if sum(target_errors[target_start:target_start + length]) == 0:
        return True
    if pred_start <= alignments[target_start] < pred_start + length:
        return True
    return False


def _perform_shift(words: List[str], start: int, length: int, target: int) ->List[str]:
    """Perform a shift in ``words`` from ``start`` to ``target``.

    Args:
        words: A words to shift.
        start: An index where to start shifting from.
        length: A number of how many words to be considered.
        target: An index where to end shifting.

    Return:
        A list of shifted words.
    """

    def _shift_word_before_previous_position(words: List[str], start: int, target: int, length: int) ->List[str]:
        return words[:target] + words[start:start + length] + words[target:start] + words[start + length:]

    def _shift_word_after_previous_position(words: List[str], start: int, target: int, length: int) ->List[str]:
        return words[:start] + words[start + length:target] + words[start:start + length] + words[target:]

    def _shift_word_within_shifted_string(words: List[str], start: int, target: int, length: int) ->List[str]:
        shifted_words = words[:start]
        shifted_words += words[start + length:length + target]
        shifted_words += words[start:start + length]
        shifted_words += words[length + target:]
        return shifted_words
    if target < start:
        return _shift_word_before_previous_position(words, start, target, length)
    if target > start + length:
        return _shift_word_after_previous_position(words, start, target, length)
    return _shift_word_within_shifted_string(words, start, target, length)


def _translation_edit_rate(pred_words: List[str], target_words: List[str]) ->Tensor:
    """Compute translation edit rate between hypothesis and reference sentences.

    Args:
        pred_words: A list of a tokenized hypothesis sentence.
        target_words: A list of lists of tokenized reference sentences.

    Return:
        A number of required edits to match hypothesis and reference sentences.
    """
    if len(target_words) == 0:
        return tensor(0.0)
    cached_edit_distance = _LevenshteinEditDistance(target_words)
    num_shifts = 0
    checked_candidates = 0
    input_words = pred_words
    while True:
        delta, new_input_words, checked_candidates = _shift_words(input_words, target_words, cached_edit_distance, checked_candidates)
        if checked_candidates >= _MAX_SHIFT_CANDIDATES or delta <= 0:
            break
        num_shifts += 1
        input_words = new_input_words
    edit_distance, _ = cached_edit_distance(input_words)
    total_edits = num_shifts + edit_distance
    return tensor(total_edits)


def _compute_sentence_statistics(pred_words: List[str], target_words: List[List[str]]) ->Tuple[Tensor, Tensor]:
    """Compute sentence TER statistics between hypothesis and provided references.

    Args:
        pred_words: A list of tokenized hypothesis sentence.
        target_words: A list of lists of tokenized reference sentences.

    Return:
        best_num_edits:
            The best (lowest) number of required edits to match hypothesis and reference sentences.
        avg_tgt_len:
            Average length of tokenized reference sentences.
    """
    tgt_lengths = tensor(0.0)
    best_num_edits = tensor(2e+16)
    for tgt_words in target_words:
        num_edits = _translation_edit_rate(tgt_words, pred_words)
        tgt_lengths += len(tgt_words)
        if num_edits < best_num_edits:
            best_num_edits = num_edits
    avg_tgt_len = tgt_lengths / len(target_words)
    return best_num_edits, avg_tgt_len


def _preprocess_en(sentence: str) ->str:
    """Copied from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/util.py.

    Raises:
        ValueError: If input sentence is not of a type `str`.
    """
    if not isinstance(sentence, str):
        raise ValueError(f'Only strings allowed during preprocessing step, found {type(sentence)} instead')
    sentence = sentence.rstrip()
    rules_interpunction = [('.', ' .'), ('!', ' !'), ('?', ' ?'), (',', ' ,')]
    for pattern, replacement in rules_interpunction:
        sentence = sentence.replace(pattern, replacement)
    rules_re = [('\\s+', ' '), ('(\\d) ([.,]) (\\d)', '\\1\\2\\3'), ('(Dr|Jr|Prof|Rev|Gen|Mr|Mt|Mrs|Ms) .', '\\1.')]
    for pattern, replacement in rules_re:
        sentence = re.sub(pattern, replacement, sentence)
    rules_interpunction = [('e . g .', 'e.g.'), ('i . e .', 'i.e.'), ('U . S .', 'U.S.')]
    for pattern, replacement in rules_interpunction:
        sentence = sentence.replace(pattern, replacement)
    sentence = ' ' + sentence + ' '
    return sentence


def _preprocess_ja(sentence: str) ->str:
    """Copied from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/util.py.

    Raises:
        ValueError: If input sentence is not of a type `str`.
    """
    if not isinstance(sentence, str):
        raise ValueError(f'Only strings allowed during preprocessing step, found {type(sentence)} instead')
    sentence = sentence.rstrip()
    sentence = unicodedata.normalize('NFKC', sentence)
    return sentence


def _get_dataloader(input_ids: Tensor, attention_mask: Tensor, idf: bool, batch_size: int, num_workers: int) ->DataLoader:
    """Prepare dataloader.

    Args:
        input_ids:
            Indices of input sequence tokens in the vocabulary.
        attention_mask:
            Mask to avoid performing attention on padding token indices.
        idf:
            A batch size used for model processing.
        num_threads:
            A number of workers to use for a dataloader.

    Return:
        An instance of ``torch.utils.data.DataLoader`` used for iterating over examples.
    """
    dataset = TokenizedDataset(input_ids, attention_mask, idf)
    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)
    return dataloader


def _get_token_mask(input_ids: Tensor, pad_token_id: int, sep_token_id: int, cls_token_id: int) ->Tensor:
    """Generate a token mask for differentiating all special tokens in the input batch. There are 0s for special
    tokens and 1s otherwise.

    Args:
        input_ids:
            Indices of input sequence tokens in the vocabulary.
        pad_token_id:
            An id of ``<PAD>`` tokens that are used to make arrays of tokens the same size for batching purpose
        cls_token_id:
            An id of ``<CLS>`` token that represents the class of the input. (It might be ``<BOS>`` token for some
            models.)
        sep_token_id:
            An id of ``<SEP>`` token that separates two different sentences in the same input. (It might be ``<EOS>``
            token for some models.)

    Return:
        Tensor mask of 0s and 1s that masks all special tokens in the ``input_ids`` tensor.
    """
    token_mask = input_ids.eq(pad_token_id) | input_ids.eq(sep_token_id) | input_ids.eq(cls_token_id)
    return ~token_mask


def _mer_compute(errors: Tensor, total: Tensor) ->Tensor:
    """Compute the match error rate.

    Args:
        errors: Number of edit operations to get from the reference to the prediction, summed over all samples
        total: Number of words overall references

    Returns:
        Match error rate score
    """
    return errors / total


def _mer_update(preds: Union[str, List[str]], target: Union[str, List[str]]) ->Tuple[Tensor, Tensor]:
    """Update the mer score with the current set of references and predictions.

    Args:
        preds: Transcription(s) to score as a string or list of strings
        target: Reference(s) for each speech input as a string or list of strings

    Returns:
        Number of edit operations to get from the reference to the prediction, summed over all samples
        Number of words overall references
    """
    if isinstance(preds, str):
        preds = [preds]
    if isinstance(target, str):
        target = [target]
    errors = tensor(0, dtype=torch.float)
    total = tensor(0, dtype=torch.float)
    for pred, tgt in zip(preds, target):
        pred_tokens = pred.split()
        tgt_tokens = tgt.split()
        errors += _edit_distance(pred_tokens, tgt_tokens)
        total += max(len(tgt_tokens), len(pred_tokens))
    return errors, total


def _perplexity_compute(total: Tensor, count: Tensor) ->Tensor:
    """Compute the Perplexity.

    Args:
        total: Log probabilities, summed over all samples
        count: Number of samples
    Returns:
        Perplexity
    """
    return torch.exp(total / count)


def _perplexity_update(preds: Tensor, target: Tensor, ignore_index: Optional[int]=None) ->Tuple[Tensor, Tensor]:
    """Compute intermediate statistics for Perplexity.

    Args:
        preds:
            Probabilities assigned to each token in a sequence with shape [batch_size, seq_len, vocab_size].
        target:
            Ground truth values with a shape [batch_size, seq_len].
        ignore_index:
            Integer specifying a target class to ignore. If given, this class index does not contribute
            to the returned score.

    Returns:
        Log probabilities, summed over all samples
        Number of samples
    """
    _check_shape_and_type_consistency(preds, target)
    probs = F.softmax(preds.reshape(-1, preds.shape[-1]), dim=1)
    target = target.reshape(-1)
    if ignore_index is not None:
        mask = target.ne(ignore_index)
        target = target.where(target != ignore_index, torch.tensor(0, device=target.device))
    else:
        mask = torch.ones_like(target, dtype=torch.bool)
    probs = probs[:, target].diagonal()[mask]
    total_log_probs = -probs.log().sum()
    count = mask.sum()
    return total_log_probs, count


ALLOWED_ACCUMULATE_VALUES = 'avg', 'best'


def _rouge_score_compute(sentence_results: Dict[str, List[Tensor]]) ->Dict[str, Tensor]:
    """Compute the combined ROUGE metric for all the input set of predicted and target sentences.

    Args:
        sentence_results: Rouge-N/Rouge-L/Rouge-LSum metrics calculated for single sentence.
    """
    results: Dict[str, Tensor] = {}
    if sentence_results == {}:
        return results
    for rouge_key, scores in sentence_results.items():
        results[rouge_key] = torch.tensor(scores).mean()
    return results


def _normalize_and_tokenize_text(text: str, stemmer: Optional[Any]=None, normalizer: Callable[[str], str]=None, tokenizer: Callable[[str], Sequence[str]]=None) ->Sequence[str]:
    """Rouge score should be calculated only over lowercased words and digits. Optionally, Porter stemmer can be
    used to strip word suffixes to improve matching. The text normalization follows the implemantion from `Rouge
    score_Text Normalizition`_

    Args:
        text: An input sentence.
        stemmer: Porter stemmer instance to strip word suffixes to improve matching.
        normalizer: A user's own normalizer function.
            If this is ``None``, replacing any non-alpha-numeric characters with spaces is default.
            This function must take a ``str`` and return a ``str``.
        tokenizer:
            A user's own tokenizer function. If this is ``None``, splitting by spaces is default
            This function must take a ``str`` and return ``Sequence[str]``
    """
    text = normalizer(text) if callable(normalizer) else re.sub('[^a-z0-9]+', ' ', text.lower())
    tokens = tokenizer(text) if callable(tokenizer) else re.split('\\s+', text)
    if stemmer:
        tokens = [(stemmer.stem(x) if len(x) > 3 else x) for x in tokens]
    tokens = [x for x in tokens if isinstance(x, str) and len(x) > 0]
    return tokens


def _compute_metrics(hits_or_lcs: int, pred_len: int, target_len: int) ->Dict[str, Tensor]:
    """This computes precision, recall and F1 score based on hits/lcs, and the length of lists of tokenizer
    predicted and target sentences.

    Args:
        hits_or_lcs: A number of matches or a length of the longest common subsequence.
        pred_len: A length of a tokenized predicted sentence.
        target_len: A length of a tokenized target sentence.
    """
    precision = hits_or_lcs / pred_len
    recall = hits_or_lcs / target_len
    if precision == recall == 0.0:
        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
    fmeasure = 2 * precision * recall / (precision + recall)
    return dict(precision=tensor(precision), recall=tensor(recall), fmeasure=tensor(fmeasure))


def _lcs(pred_tokens: Sequence[str], target_tokens: Sequence[str], return_full_table: bool=False) ->Union[int, Sequence[Sequence[int]]]:
    """Common DP algorithm to compute the length of the longest common subsequence.

    Args:
        pred_tokens: A tokenized predicted sentence.
        target_tokens: A tokenized target sentence.
    """
    lcs = [([0] * (len(pred_tokens) + 1)) for _ in range(len(target_tokens) + 1)]
    for i in range(1, len(target_tokens) + 1):
        for j in range(1, len(pred_tokens) + 1):
            if target_tokens[i - 1] == pred_tokens[j - 1]:
                lcs[i][j] = lcs[i - 1][j - 1] + 1
            else:
                lcs[i][j] = max(lcs[i - 1][j], lcs[i][j - 1])
    if return_full_table:
        return lcs
    return lcs[-1][-1]


def _rouge_l_score(pred: Sequence[str], target: Sequence[str]) ->Dict[str, Tensor]:
    """This computes precision, recall and F1 score for the Rouge-L metric.

    Args:
        pred: A predicted sentence.
        target: A target sentence.
    """
    pred_len, target_len = len(pred), len(target)
    if 0 in (pred_len, target_len):
        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
    lcs: int = _lcs(pred, target)
    return _compute_metrics(lcs, pred_len, target_len)


def _backtracked_lcs(lcs_table: Sequence[Sequence[int]], pred_tokens: Sequence[str], target_tokens: Sequence[str]) ->Sequence[int]:
    """Backtrack LCS table.

    Args:
        lcs_table: A table containing information for the calculation of the longest common subsequence.
        pred_tokens: A tokenized predicted sentence.
        target_tokens: A tokenized target sentence.
    """
    i = len(pred_tokens)
    j = len(target_tokens)
    backtracked_lcs: List[int] = []
    while i > 0 and j > 0:
        if pred_tokens[i - 1] == target_tokens[j - 1]:
            backtracked_lcs.insert(0, j - 1)
            i -= 1
            j -= 1
        elif lcs_table[j][i - 1] > lcs_table[j - 1][i]:
            i -= 1
        else:
            j -= 1
    return backtracked_lcs


def _union_lcs(pred_tokens_list: Sequence[Sequence[str]], target_tokens: Sequence[str]) ->Sequence[str]:
    """Find union LCS between a target sentence and iterable of predicted tokens.

    Args:
        pred_tokens_list: A tokenized predicted sentence split by '
'.
        target_tokens: A tokenized single part of target sentence split by '
'.

    Return:
    """

    def lcs_ind(pred_tokens: Sequence[str], target_tokens: Sequence[str]) ->Sequence[int]:
        """Returns one of the longest of longest common subsequence via backtracked lcs table."""
        lcs_table: Sequence[Sequence[int]] = _lcs(pred_tokens, target_tokens, return_full_table=True)
        backtracked_lcs_table = _backtracked_lcs(lcs_table, pred_tokens, target_tokens)
        return backtracked_lcs_table

    def find_union(lcs_tables: Sequence[Sequence[int]]) ->Sequence[int]:
        """Find union LCS given a list of LCS."""
        return sorted(list(set().union(*lcs_tables)))
    lcs_tables = [lcs_ind(pred_tokens, target_tokens) for pred_tokens in pred_tokens_list]
    union_lcs = [target_tokens[i] for i in find_union(lcs_tables)]
    return union_lcs


def _rouge_lsum_score(pred: Sequence[Sequence[str]], target: Sequence[Sequence[str]]) ->Dict[str, Tensor]:
    """This computes precision, recall and F1 score for the Rouge-LSum metric. More information can be found in Section
    3.2 of the referenced paper [1]. This implementation follow the official implementation from:
    https://github.com/google-research/google-research/blob/master/rouge/rouge_scorer.py

    Args:
        pred: An iterable of predicted sentence split by '
'.
        target: An iterable target sentence split by '
'.

    References
        [1] ROUGE: A Package for Automatic Evaluation of Summaries by Chin-Yew Lin. https://aclanthology.org/W04-1013/
    """
    pred_len = sum(map(len, pred))
    target_len = sum(map(len, target))
    if 0 in (pred_len, target_len):
        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))

    def _get_token_counts(sentences: Sequence[Sequence[str]]) ->Counter:
        ngrams: Counter = Counter()
        for sentence in sentences:
            ngrams.update(sentence)
        return ngrams
    pred_tokens_count = _get_token_counts(pred)
    target_tokens_count = _get_token_counts(target)
    hits = 0
    for tgt in target:
        lcs = _union_lcs(pred, tgt)
        for token in lcs:
            if pred_tokens_count[token] > 0 and target_tokens_count[token] > 0:
                hits += 1
                pred_tokens_count[token] -= 1
                target_tokens_count[token] -= 1
    return _compute_metrics(hits, pred_len, target_len)


def _rouge_n_score(pred: Sequence[str], target: Sequence[str], n_gram: int) ->Dict[str, Tensor]:
    """This computes precision, recall and F1 score for the Rouge-N metric.

    Args:
        pred: A predicted sentence.
        target: A target sentence.
        n_gram: N-gram overlap.
    """

    def _create_ngrams(tokens: Sequence[str], n: int) ->Counter:
        ngrams: Counter = Counter()
        for ngram in (tuple(tokens[i:i + n]) for i in range(len(tokens) - n + 1)):
            ngrams[ngram] += 1
        return ngrams
    pred_ngrams, target_ngrams = _create_ngrams(pred, n_gram), _create_ngrams(target, n_gram)
    pred_len, target_len = sum(pred_ngrams.values()), sum(target_ngrams.values())
    if 0 in (pred_len, target_len):
        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
    hits = sum(min(pred_ngrams[w], target_ngrams[w]) for w in set(pred_ngrams))
    return _compute_metrics(hits, max(pred_len, 1), max(target_len, 1))


def _split_sentence(x: str) ->Sequence[str]:
    """The sentence is split to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
    if not _NLTK_AVAILABLE:
        raise ModuleNotFoundError('ROUGE-Lsum calculation requires that `nltk` is installed. Use `pip install nltk`.')
    nltk.download('punkt', quiet=True, force=False)
    re.sub('<n>', '', x)
    return nltk.sent_tokenize(x)


def _rouge_score_update(preds: Sequence[str], target: Sequence[Sequence[str]], rouge_keys_values: List[Union[int, str]], accumulate: str, stemmer: Optional[Any]=None, normalizer: Callable[[str], str]=None, tokenizer: Callable[[str], Sequence[str]]=None) ->Dict[Union[int, str], List[Dict[str, Tensor]]]:
    """Update the rouge score with the current set of predicted and target sentences.

    Args:
        preds: An iterable of predicted sentences.
        target: An iterable of iterable of target sentences.
        rouge_keys_values: List of N-grams/'L'/'Lsum' arguments.
        accumulate: Useful incase of multi-reference rouge score.
            ``avg`` takes the avg of all references with respect to predictions
            ``best`` takes the best fmeasure score obtained between prediction and multiple corresponding references.
            Allowed values are ``avg`` and ``best``.
        stemmer: Porter stemmer instance to strip word suffixes to improve matching.
        normalizer:
            A user's own normalizer function.
            If this is ``None``, replacing any non-alpha-numeric characters with spaces is default.
            This function must take a `str` and return a `str`.
        tokenizer:
            A user's own tokenizer function. If this is ``None``, spliting by spaces is default
            This function must take a `str` and return `Sequence[str]`

    Example:
        >>> preds = "My name is John".split()
        >>> target = "Is your name John".split()
        >>> from pprint import pprint
        >>> score = _rouge_score_update(preds, target, rouge_keys_values=[1, 2, 3, 'L'], accumulate='best')
        >>> pprint(score)
        {1: [{'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)}],
         2: [{'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)}],
         3: [{'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
             {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)}],
         'L': [{'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
               {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
               {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)},
               {'fmeasure': tensor(0.), 'precision': tensor(0.), 'recall': tensor(0.)}]}
    """
    results: Dict[Union[int, str], List[Dict[str, Tensor]]] = {rouge_key: [] for rouge_key in rouge_keys_values}
    for pred_raw, target_raw in zip(preds, target):
        result_inner: Dict[Union[int, str], Dict[str, Tensor]] = {rouge_key: {} for rouge_key in rouge_keys_values}
        result_avg: Dict[Union[int, str], List[Dict[str, Tensor]]] = {rouge_key: [] for rouge_key in rouge_keys_values}
        list_results = []
        pred = _normalize_and_tokenize_text(pred_raw, stemmer, normalizer, tokenizer)
        if 'Lsum' in rouge_keys_values:
            pred_lsum = [_normalize_and_tokenize_text(pred_sentence, stemmer, normalizer, tokenizer) for pred_sentence in _split_sentence(pred_raw)]
        for target_raw_inner in target_raw:
            tgt = _normalize_and_tokenize_text(target_raw_inner, stemmer, normalizer, tokenizer)
            if 'Lsum' in rouge_keys_values:
                target_lsum = [_normalize_and_tokenize_text(tgt_sentence, stemmer, normalizer, tokenizer) for tgt_sentence in _split_sentence(target_raw_inner)]
            for rouge_key in rouge_keys_values:
                if isinstance(rouge_key, int):
                    score = _rouge_n_score(pred, tgt, rouge_key)
                elif rouge_key == 'L':
                    score = _rouge_l_score(pred, tgt)
                elif rouge_key == 'Lsum':
                    score = _rouge_lsum_score(pred_lsum, target_lsum)
                result_inner[rouge_key] = score
                result_avg[rouge_key].append(score)
            list_results.append(result_inner.copy())
        if accumulate == 'best':
            key_curr = rouge_keys_values[0]
            all_fmeasure = torch.tensor([v[key_curr]['fmeasure'] for v in list_results])
            highest_idx = int(torch.argmax(all_fmeasure).item())
            for rouge_key in rouge_keys_values:
                results[rouge_key].append(list_results[highest_idx][rouge_key])
        elif accumulate == 'avg':
            new_result_avg: Dict[Union[int, str], Dict[str, Tensor]] = {rouge_key: {} for rouge_key in rouge_keys_values}
            for rouge_key, metrics in result_avg.items():
                _dict_metric_score_batch: Dict[str, List[Tensor]] = {}
                for metric in metrics:
                    for _type, value in metric.items():
                        if _type not in _dict_metric_score_batch:
                            _dict_metric_score_batch[_type] = []
                        _dict_metric_score_batch[_type].append(value)
                new_result_avg[rouge_key] = {_type: torch.tensor(_dict_metric_score_batch[_type]).mean() for _type in _dict_metric_score_batch}
            for rouge_key in rouge_keys_values:
                results[rouge_key].append(new_result_avg[rouge_key])
    return results


SINGLE_PRED_TYPE = Dict[str, str]


PREDS_TYPE = Union[SINGLE_PRED_TYPE, List[SINGLE_PRED_TYPE]]


SINGLE_TARGET_TYPE = Dict[str, Union[str, Dict[str, Union[List[str], List[int]]]]]


TARGETS_TYPE = Union[SINGLE_TARGET_TYPE, List[SINGLE_TARGET_TYPE]]


def _squad_compute(f1: Tensor, exact_match: Tensor, total: Tensor) ->Dict[str, Tensor]:
    """Aggregate the F1 Score and Exact match for the batch.

    Return:
        Dictionary containing the F1 score, Exact match score for the batch.
    """
    exact_match = 100.0 * exact_match / total
    f1 = 100.0 * f1 / total
    return {'exact_match': exact_match, 'f1': f1}


SQuAD_FORMAT = {'answers': {'answer_start': [1], 'text': ['This is a test text']}, 'context': 'This is a test context.', 'id': '1', 'question': 'Is this a test?', 'title': 'train test'}


def _squad_input_check(preds: PREDS_TYPE, targets: TARGETS_TYPE) ->Tuple[Dict[str, str], List[Dict[str, List[Dict[str, List[Dict[str, Any]]]]]]]:
    """Check for types and convert the input to necessary format to compute the input."""
    if isinstance(preds, Dict):
        preds = [preds]
    if isinstance(targets, Dict):
        targets = [targets]
    for pred in preds:
        keys = pred.keys()
        if 'prediction_text' not in keys or 'id' not in keys:
            raise KeyError("Expected keys in a single prediction are 'prediction_text' and 'id'.Please make sure that 'prediction_text' maps to the answer string and 'id' maps to the key string.")
    for target in targets:
        keys = target.keys()
        if 'answers' not in keys or 'id' not in keys:
            raise KeyError(f"Expected keys in a single target are 'answers' and 'id'.Please make sure that 'answers' maps to a `SQuAD` format dictionary and 'id' maps to the key string.\nSQuAD Format: {SQuAD_FORMAT}")
        answers: Dict[str, Union[List[str], List[int]]] = target['answers']
        if 'text' not in answers.keys():
            raise KeyError(f"Expected keys in a 'answers' are 'text'.Please make sure that 'answer' maps to a `SQuAD` format dictionary.\nSQuAD Format: {SQuAD_FORMAT}")
    preds_dict = {prediction['id']: prediction['prediction_text'] for prediction in preds}
    _fn_answer = lambda tgt: dict(answers=[dict(text=txt) for txt in tgt['answers']['text']], id=tgt['id'])
    targets_dict = [{'paragraphs': [{'qas': [_fn_answer(target) for target in targets]}]}]
    return preds_dict, targets_dict


def _normalize_text(s: str) ->str:
    """Lower text and remove punctuation, articles and extra whitespace."""

    def remove_articles(text: str) ->str:
        return re.sub('\\b(a|an|the)\\b', ' ', text)

    def white_space_fix(text: str) ->str:
        return ' '.join(text.split())

    def remove_punc(text: str) ->str:
        exclude = set(string.punctuation)
        return ''.join(ch for ch in text if ch not in exclude)

    def lower(text: str) ->str:
        return text.lower()
    return white_space_fix(remove_articles(remove_punc(lower(s))))


def _compute_exact_match_score(prediction: str, ground_truth: str) ->Tensor:
    """Compute Exact Match for two sentences."""
    return tensor(int(_normalize_text(prediction) == _normalize_text(ground_truth)))


def _get_tokens(s: str) ->List[str]:
    """Split a sentence into separate tokens."""
    return [] if not s else _normalize_text(s).split()


def _compute_f1_score(predicted_answer: str, target_answer: str) ->Tensor:
    """Compute F1 Score for two sentences."""
    target_tokens = _get_tokens(target_answer)
    predicted_tokens = _get_tokens(predicted_answer)
    common = Counter(target_tokens) & Counter(predicted_tokens)
    num_same = tensor(sum(common.values()))
    if len(target_tokens) == 0 or len(predicted_tokens) == 0:
        return tensor(int(target_tokens == predicted_tokens))
    if num_same == 0:
        return tensor(0.0)
    precision = 1.0 * num_same / tensor(len(predicted_tokens))
    recall = 1.0 * num_same / tensor(len(target_tokens))
    f1 = 2 * precision * recall / (precision + recall)
    return f1


def _metric_max_over_ground_truths(metric_fn: Callable[[str, str], Tensor], prediction: str, ground_truths: List[str]) ->Tensor:
    """Calculate maximum score for a predicted answer with all reference answers."""
    return max(metric_fn(prediction, truth) for truth in ground_truths)


def _squad_update(preds: Dict[str, str], target: List[Dict[str, List[Dict[str, List[Dict[str, Any]]]]]]) ->Tuple[Tensor, Tensor, Tensor]:
    """Compute F1 Score and Exact Match for a collection of predictions and references.

    Args:
        preds: A dictionary mapping an `id` to the predicted `answer`.
        target:
            A list of dictionary mapping `paragraphs` to list of dictionary mapping `qas` to a list of dictionary
            containing `id` and list of all possible `answers`.

    Return:
        Tuple containing F1 score, Exact match score and total number of examples.

    Example:
        >>> from torchmetrics.functional.text.squad import _squad_update
        >>> preds = [{"prediction_text": "1976", "id": "56e10a3be3433e1400422b22"}]
        >>> target = [{"answers": {"answer_start": [97], "text": ["1976"]}, "id": "56e10a3be3433e1400422b22"}]
        >>> preds_dict = {pred["id"]: pred["prediction_text"] for pred in preds}
        >>> targets_dict = [
        ...     dict(paragraphs=[dict(qas=[dict(answers=[
        ...         {"text": txt} for txt in tgt["answers"]["text"]], id=tgt["id"]) for tgt in target
        ...     ])])
        ... ]
        >>> _squad_update(preds_dict, targets_dict)
        (tensor(1.), tensor(1.), tensor(1))
    """
    f1 = tensor(0.0)
    exact_match = tensor(0.0)
    total = tensor(0)
    for article in target:
        for paragraph in article['paragraphs']:
            for qa in paragraph['qas']:
                total += 1
                if qa['id'] not in preds:
                    rank_zero_warn(f"Unanswered question {qa['id']} will receive score 0.")
                    continue
                ground_truths = list(map(lambda x: x['text'], qa['answers']))
                pred = preds[qa['id']]
                exact_match += _metric_max_over_ground_truths(_compute_exact_match_score, pred, ground_truths)
                f1 += _metric_max_over_ground_truths(_compute_f1_score, pred, ground_truths)
    return f1, exact_match, total


class _TercomTokenizer:
    """Re-implementation of Tercom Tokenizer in Python 3.

    See src/ter/core/Normalizer.java in https://github.com/jhclark/tercom Note that Python doesn't support named Unicode
    blocks so the mapping for relevant blocks was taken from here: https://unicode-table.com/en/blocks/

    This implementation follows the implemenation from
    https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/tokenizers/tokenizer_ter.py.
    """
    _ASIAN_PUNCTUATION = '([\\u3001\\u3002\\u3008-\\u3011\\u3014-\\u301f\\uff61-\\uff65\\u30fb])'
    _FULL_WIDTH_PUNCTUATION = '([\\uff0e\\uff0c\\uff1f\\uff1a\\uff1b\\uff01\\uff02\\uff08\\uff09])'

    def __init__(self, normalize: bool=False, no_punctuation: bool=False, lowercase: bool=True, asian_support: bool=False) ->None:
        """Initialize the tokenizer.

        Args:
            normalize: An indication whether a general tokenization to be applied.
            no_punctuation: An indication whteher a punctuation to be removed from the sentences.
            lowercase: An indication whether to enable case-insesitivity.
            asian_support: An indication whether asian characters to be processed.
        """
        self.normalize = normalize
        self.no_punctuation = no_punctuation
        self.lowercase = lowercase
        self.asian_support = asian_support

    @lru_cache(maxsize=2 ** 16)
    def __call__(self, sentence: str) ->str:
        """Apply a different tokenization techniques according.

        Args:
            sentence: An input sentence to pre-process and tokenize.

        Return:
            A tokenized and pre-processed sentence.
        """
        if not sentence:
            return ''
        if self.lowercase:
            sentence = sentence.lower()
        if self.normalize:
            sentence = self._normalize_general_and_western(sentence)
            if self.asian_support:
                sentence = self._normalize_asian(sentence)
        if self.no_punctuation:
            sentence = self._remove_punct(sentence)
            if self.asian_support:
                sentence = self._remove_asian_punct(sentence)
        return ' '.join(sentence.split())

    @staticmethod
    def _normalize_general_and_western(sentence: str) ->str:
        """Apply a language-independent (general) tokenization."""
        sentence = f' {sentence} '
        rules = [('\\n-', ''), ('\\n', ' '), ('&quot;', '"'), ('&amp;', '&'), ('&lt;', '<'), ('&gt;', '>'), ('([{-~[-` -&(-+:-@/])', ' \\1 '), ("'s ", " 's "), ("'s$", " 's"), ('([^0-9])([\\.,])', '\\1 \\2 '), ('([\\.,])([^0-9])', ' \\1 \\2'), ('([0-9])(-)', '\\1 \\2 ')]
        for pattern, replacement in rules:
            sentence = re.sub(pattern, replacement, sentence)
        return sentence

    @classmethod
    def _normalize_asian(cls, sentence: str) ->str:
        """Split Chinese chars and Japanese kanji down to character level."""
        sentence = re.sub('([\\u4e00-\\u9fff\\u3400-\\u4dbf])', ' \\1 ', sentence)
        sentence = re.sub('([\\u31c0-\\u31ef\\u2e80-\\u2eff])', ' \\1 ', sentence)
        sentence = re.sub('([\\u3300-\\u33ff\\uf900-\\ufaff\\ufe30-\\ufe4f])', ' \\1 ', sentence)
        sentence = re.sub('([\\u3200-\\u3f22])', ' \\1 ', sentence)
        sentence = re.sub('(^|^[\\u3040-\\u309f])([\\u3040-\\u309f]+)(?=$|^[\\u3040-\\u309f])', '\\1 \\2 ', sentence)
        sentence = re.sub('(^|^[\\u30a0-\\u30ff])([\\u30a0-\\u30ff]+)(?=$|^[\\u30a0-\\u30ff])', '\\1 \\2 ', sentence)
        sentence = re.sub('(^|^[\\u31f0-\\u31ff])([\\u31f0-\\u31ff]+)(?=$|^[\\u31f0-\\u31ff])', '\\1 \\2 ', sentence)
        sentence = re.sub(cls._ASIAN_PUNCTUATION, ' \\1 ', sentence)
        sentence = re.sub(cls._FULL_WIDTH_PUNCTUATION, ' \\1 ', sentence)
        return sentence

    @staticmethod
    def _remove_punct(sentence: str) ->str:
        """Remove punctuation from an input sentence string."""
        return re.sub('[\\.,\\?:;!\\"\\(\\)]', '', sentence)

    @classmethod
    def _remove_asian_punct(cls, sentence: str) ->str:
        """Remove asian punctuation from an input sentence string."""
        sentence = re.sub(cls._ASIAN_PUNCTUATION, '', sentence)
        sentence = re.sub(cls._FULL_WIDTH_PUNCTUATION, '', sentence)
        return sentence


def _compute_ter_score_from_statistics(num_edits: Tensor, tgt_length: Tensor) ->Tensor:
    """Compute TER score based on pre-computed a number of edits and an average reference length.

    Args:
        num_edits: A number of required edits to match hypothesis and reference sentences.
        tgt_length: An average length of reference sentences.

    Return:
        A corpus-level TER score or 1 if reference_length == 0.
    """
    if tgt_length > 0 and num_edits > 0:
        score = num_edits / tgt_length
    elif tgt_length == 0 and num_edits > 0:
        score = tensor(1.0)
    else:
        score = tensor(0.0)
    return score


def _ter_compute(total_num_edits: Tensor, total_tgt_length: Tensor) ->Tensor:
    """Compute TER based on pre-computed a total number of edits and a total average reference length.
    Args:
        total_num_edits: A total number of required edits to match hypothesis and reference sentences.
        total_tgt_length: A total average length of reference sentences.

    Return:
        A corpus-level TER score.
    """
    return _compute_ter_score_from_statistics(total_num_edits, total_tgt_length)


def _preprocess_sentence(sentence: str, tokenizer: _TercomTokenizer) ->str:
    """Given a sentence, apply tokenization.

    Args:
        sentence: The input sentence string.
        tokenizer: An instance of ``_TercomTokenizer`` handling a sentence tokenization.

    Return:
        The pre-processed output sentence string.
    """
    return tokenizer(sentence.rstrip())


def _ter_update(preds: Union[str, Sequence[str]], target: Sequence[Union[str, Sequence[str]]], tokenizer: _TercomTokenizer, total_num_edits: Tensor, total_tgt_length: Tensor, sentence_ter: Optional[List[Tensor]]=None) ->Tuple[Tensor, Tensor, Optional[List[Tensor]]]:
    """Update TER statistics.

    Args:
        preds: An iterable of hypothesis corpus.
        target: An iterable of iterables of reference corpus.
        tokenizer:
        total_num_edits: A total number of required edits to match hypothesis and reference sentences.
        total_tgt_length: A total average length of reference sentences.

    Return:
        total_num_edits:
            A total number of required edits to match hypothesis and reference sentences.
        total_tgt_length:
            A total average length of reference sentences.
        sentence_ter:
            (Optionally) A list of sentence-level TER.

    Raises:
        ValueError:
            If length of ``preds`` and ``target`` differs.
    """
    target, preds = _validate_inputs(target, preds)
    for pred, tgt in zip(preds, target):
        tgt_words_: List[List[str]] = [_preprocess_sentence(_tgt, tokenizer).split() for _tgt in tgt]
        pred_words_: List[str] = _preprocess_sentence(pred, tokenizer).split()
        num_edits, tgt_length = _compute_sentence_statistics(pred_words_, tgt_words_)
        total_num_edits += num_edits
        total_tgt_length += tgt_length
        if sentence_ter is not None:
            sentence_ter.append(_compute_ter_score_from_statistics(num_edits, tgt_length).unsqueeze(0))
    return total_num_edits, total_tgt_length, sentence_ter


def _wer_compute(errors: Tensor, total: Tensor) ->Tensor:
    """Compute the word error rate.

    Args:
        errors: Number of edit operations to get from the reference to the prediction, summed over all samples
        total: Number of words overall references

    Returns:
        Word error rate score
    """
    return errors / total


def _wer_update(preds: Union[str, List[str]], target: Union[str, List[str]]) ->Tuple[Tensor, Tensor]:
    """Update the wer score with the current set of references and predictions.

    Args:
        preds: Transcription(s) to score as a string or list of strings
        target: Reference(s) for each speech input as a string or list of strings

    Returns:
        Number of edit operations to get from the reference to the prediction, summed over all samples
        Number of words overall references
    """
    if isinstance(preds, str):
        preds = [preds]
    if isinstance(target, str):
        target = [target]
    errors = tensor(0, dtype=torch.float)
    total = tensor(0, dtype=torch.float)
    for pred, tgt in zip(preds, target):
        pred_tokens = pred.split()
        tgt_tokens = tgt.split()
        errors += _edit_distance(pred_tokens, tgt_tokens)
        total += len(tgt_tokens)
    return errors, total


def _wil_compute(errors: Tensor, target_total: Tensor, preds_total: Tensor) ->Tensor:
    """Compute the Word Information Lost.

    Args:
        errors: Number of edit operations to get from the reference to the prediction, summed over all samples
        target_total: Number of words overall references
        preds_total: Number of words overall prediction

    Returns:
        Word Information Lost score
    """
    return 1 - errors / target_total * (errors / preds_total)


def _wil_update(preds: Union[str, List[str]], target: Union[str, List[str]]) ->Tuple[Tensor, Tensor, Tensor]:
    """Update the wil score with the current set of references and predictions.

    Args:
        preds: Transcription(s) to score as a string or list of strings
        target: Reference(s) for each speech input as a string or list of strings

    Returns:
        Number of edit operations to get from the reference to the prediction, summed over all samples
        Number of words overall references
        Number of words overall predictions
    """
    if isinstance(preds, str):
        preds = [preds]
    if isinstance(target, str):
        target = [target]
    total = tensor(0.0)
    errors = tensor(0.0)
    target_total = tensor(0.0)
    preds_total = tensor(0.0)
    for pred, tgt in zip(preds, target):
        pred_tokens = pred.split()
        target_tokens = tgt.split()
        errors += _edit_distance(pred_tokens, target_tokens)
        target_total += len(target_tokens)
        preds_total += len(pred_tokens)
        total += max(len(target_tokens), len(pred_tokens))
    return errors - total, target_total, preds_total


def _wip_compute(errors: Tensor, target_total: Tensor, preds_total: Tensor) ->Tensor:
    """Compute the Word Information Perserved.

    Args:
        errors: Number of edit operations to get from the reference to the prediction, summed over all samples
        target_total: Number of words overall references
        preds_total: Number of words overall prediction

    Returns:
        Word Information Perserved score
    """
    return errors / target_total * (errors / preds_total)


def _wip_update(preds: Union[str, List[str]], target: Union[str, List[str]]) ->Tuple[Tensor, Tensor, Tensor]:
    """Update the wip score with the current set of references and predictions.

    Args:
        preds: Transcription(s) to score as a string or list of strings
        target: Reference(s) for each speech input as a string or list of strings

    Returns:
        Number of edit operations to get from the reference to the prediction, summed over all samples
        Number of words overall references
        Number of words overall prediction
    """
    if isinstance(preds, str):
        preds = [preds]
    if isinstance(target, str):
        target = [target]
    total = tensor(0.0)
    errors = tensor(0.0)
    target_total = tensor(0.0)
    preds_total = tensor(0.0)
    for pred, tgt in zip(preds, target):
        pred_tokens = pred.split()
        target_tokens = tgt.split()
        errors += _edit_distance(pred_tokens, target_tokens)
        target_total += len(target_tokens)
        preds_total += len(pred_tokens)
        total += max(len(target_tokens), len(pred_tokens))
    return errors - total, target_total, preds_total


def _bootstrap_sampler(size: int, sampling_strategy: str='poisson') ->Tensor:
    """Resample a tensor along its first dimension with replacement.

    Args:
        size: number of samples
        sampling_strategy: the strategy to use for sampling, either ``'poisson'`` or ``'multinomial'``

    Returns:
        resampled tensor
    """
    if sampling_strategy == 'poisson':
        p = torch.distributions.Poisson(1)
        n = p.sample((size,))
        return torch.arange(size).repeat_interleave(n.long(), dim=0)
    if sampling_strategy == 'multinomial':
        idx = torch.multinomial(torch.ones(size), num_samples=size, replacement=True)
        return idx
    raise ValueError('Unknown sampling strategy')


def _get_nan_indices(*tensors: Tensor) ->Tensor:
    """Get indices of rows along dim 0 which have NaN values."""
    if len(tensors) == 0:
        raise ValueError('Must pass at least one tensor as argument')
    sentinel = tensors[0]
    nan_idxs = torch.zeros(len(sentinel), dtype=torch.bool, device=sentinel.device)
    for tensor in tensors:
        permuted_tensor = tensor.flatten(start_dim=1)
        nan_idxs |= torch.any(torch.isnan(permuted_tensor), dim=1)
    return nan_idxs

