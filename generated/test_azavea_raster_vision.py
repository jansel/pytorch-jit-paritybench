import sys
_module = sys.modules[__name__]
del sys
configs = _module
test_pipeline = _module
setup = _module
conf = _module
integration_tests = _module
config = _module
object_detection = _module
semantic_segmentation = _module
util = _module
flip_scene = _module
generate_scene = _module
aws_batch = _module
aws_batch_runner = _module
aws_s3 = _module
s3_file_system = _module
core = _module
analyzer = _module
analyzer_config = _module
stats_analyzer = _module
stats_analyzer_config = _module
backend = _module
backend_config = _module
box = _module
cli = _module
data = _module
class_config = _module
crs_transformer = _module
identity_crs_transformer = _module
rasterio_crs_transformer = _module
dataset_config = _module
label = _module
chip_classification_labels = _module
labels = _module
object_detection_labels = _module
semantic_segmentation_labels = _module
tfod_utils = _module
np_box_list = _module
np_box_list_ops = _module
np_box_ops = _module
utils = _module
label_source = _module
chip_classification_label_source = _module
chip_classification_label_source_config = _module
label_source_config = _module
object_detection_label_source = _module
object_detection_label_source_config = _module
semantic_segmentation_label_source = _module
semantic_segmentation_label_source_config = _module
label_store = _module
chip_classification_geojson_store = _module
chip_classification_geojson_store_config = _module
label_store_config = _module
object_detection_geojson_store = _module
object_detection_geojson_store_config = _module
semantic_segmentation_label_store = _module
semantic_segmentation_label_store_config = _module
raster_source = _module
multi_raster_source = _module
multi_raster_source_config = _module
raster_source_config = _module
rasterio_source = _module
rasterio_source_config = _module
rasterized_source = _module
rasterized_source_config = _module
raster_transformer = _module
cast_transformer = _module
cast_transformer_config = _module
min_max_transformer = _module
min_max_transformer_config = _module
nan_transformer = _module
nan_transformer_config = _module
raster_transformer_config = _module
reclass_transformer = _module
reclass_transformer_config = _module
rgb_class_transformer = _module
rgb_class_transformer_config = _module
stats_transformer = _module
stats_transformer_config = _module
scene = _module
scene_config = _module
factory = _module
geojson = _module
misc = _module
vectorization = _module
vector_source = _module
geojson_vector_source = _module
geojson_vector_source_config = _module
vector_source_config = _module
vector_transformer = _module
buffer_transformer = _module
buffer_transformer_config = _module
class_inference_transformer = _module
class_inference_transformer_config = _module
label_maker = _module
filter = _module
shift_transformer = _module
shift_transformer_config = _module
vector_transformer_config = _module
data_sample = _module
evaluation = _module
chip_classification_evaluation = _module
chip_classification_evaluator = _module
chip_classification_evaluator_config = _module
class_evaluation_item = _module
classification_evaluation = _module
classification_evaluator = _module
classification_evaluator_config = _module
evaluation_item = _module
evaluator = _module
evaluator_config = _module
object_detection_evaluation = _module
object_detection_evaluator = _module
object_detection_evaluator_config = _module
semantic_segmentation_evaluation = _module
semantic_segmentation_evaluator = _module
semantic_segmentation_evaluator_config = _module
predictor = _module
raster_stats = _module
rv_pipeline = _module
chip_classification = _module
chip_classification_config = _module
object_detection_config = _module
rv_pipeline_config = _module
semantic_segmentation_config = _module
cog = _module
filter_geojson = _module
stac = _module
gdal_vsi = _module
vsi_file_system = _module
pipeline = _module
file_system = _module
http_file_system = _module
local_file_system = _module
pipeline_config = _module
registry = _module
runner = _module
inprocess_runner = _module
local_runner = _module
rv_config = _module
verbosity = _module
version = _module
pipeline_example_plugin1 = _module
config1 = _module
config2 = _module
sample_pipeline = _module
sample_pipeline2 = _module
pipeline_example_plugin2 = _module
config3 = _module
deluxe_message_maker = _module
pytorch_backend = _module
examples = _module
spacenet_rio = _module
cowc_potsdam = _module
cowc_potsdam_data_prep = _module
merge_geojson = _module
prepare_potsdam = _module
resample_geotiffs = _module
transfer_georeference = _module
xview = _module
isprs_potsdam = _module
isprs_potsdam_multi_source = _module
spacenet_vegas = _module
test = _module
tiny_spacenet = _module
pytorch_chip_classification = _module
pytorch_chip_classification_config = _module
pytorch_learner_backend = _module
pytorch_learner_backend_config = _module
pytorch_object_detection = _module
pytorch_object_detection_config = _module
pytorch_semantic_segmentation = _module
pytorch_semantic_segmentation_config = _module
pytorch_learner = _module
classification_learner = _module
classification_learner_config = _module
dataset = _module
classification_dataset = _module
dataset = _module
object_detection_dataset = _module
regression_dataset = _module
semantic_segmentation_dataset = _module
transform = _module
aoi_sampler = _module
visualizer = _module
classification_visualizer = _module
object_detection_visualizer = _module
regression_visualizer = _module
semantic_segmentation_visualizer = _module
visualizer = _module
learner = _module
learner_config = _module
learner_pipeline = _module
learner_pipeline_config = _module
object_detection_learner = _module
object_detection_learner_config = _module
object_detection_utils = _module
regression_learner = _module
regression_learner_config = _module
semantic_segmentation_learner = _module
semantic_segmentation_learner_config = _module
torch_hub = _module
utils = _module
tests = _module
test_rasterio_crs_transformer = _module
test_chip_classification_labels = _module
test_object_detection_labels = _module
test_semantic_segmentation_labels = _module
test_chip_classification_label_source = _module
test_object_detection_label_source = _module
test_semantic_segmentation_label_source = _module
test_semantic_segmentation_label_store = _module
test_utils = _module
mock_crs_transformer = _module
mock_raster_source = _module
test_multi_raster_source = _module
test_rasterio_source = _module
test_rasterized_source = _module
test_cast_transformer = _module
test_rgb_class_transformer = _module
test_stats_transformer = _module
test_class_config = _module
test_dataset = _module
test_geojson = _module
test_geojson_vector_source = _module
test_buffer_transformer = _module
test_class_inference_transformer = _module
test_shift_transformer = _module
test_chip_classification_evaluator = _module
test_class_evaluation_item = _module
test_evaluator = _module
test_object_detection_evaluation = _module
test_semantic_segmentation_evaluation = _module
test_semantic_segmentation_evaluator = _module
test_semantic_segmentation_config = _module
test_box = _module
test_stats_analyzer = _module
test_stac = _module
data_files = _module
lambda_transforms = _module
test_config = _module
test_file_system = _module
test_pytorch_learner_backend = _module
test_aoi_sampler = _module
test_dataset = _module
test_transform = _module
test_classification_learner = _module
test_data_config = _module
test_model_config = _module
test_object_detection_learner = _module
test_object_detection_utils = _module
test_regression_learner = _module
test_semantic_segmentation_learner = _module
test_solver_config = _module
test_torch_hub = _module
test_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import TYPE_CHECKING


from typing import List


import logging


import warnings


import torch


from typing import Union


from typing import Optional


from enum import Enum


from torch import nn


from typing import Tuple


from typing import Any


from typing import TypeVar


import numpy as np


from torch.utils.data import Dataset


from typing import Dict


from collections import defaultdict


from typing import Iterable


from typing import Callable


from typing import Sequence


import matplotlib.pyplot as plt


import matplotlib.colors as mcolors


import matplotlib.patches as mpatches


from abc import ABC


from abc import abstractmethod


from torch import Tensor


from torch.utils.data import DataLoader


from typing import Iterator


from typing import Type


import time


import numbers


import torch.nn as nn


from torch.utils.tensorboard import SummaryWriter


import random


import uuid


from torch import optim


from torch.optim.lr_scheduler import CyclicLR


from torch.optim.lr_scheduler import MultiStepLR


from torch.optim.lr_scheduler import _LRScheduler


from torch.utils.data import ConcatDataset


from torch.utils.data import Subset


from functools import reduce


from torchvision.ops import box_area


from torchvision.ops import box_convert


from torchvision.ops import batched_nms


from torchvision.ops import clip_boxes_to_image


from torchvision.utils import draw_bounding_boxes


import matplotlib.gridspec as gridspec


import torch.nn.functional as F


from torchvision import models


from torch.nn import functional as F


import torch.hub


from torch.hub import _import_module


from uuid import uuid4


from matplotlib import pyplot as plt


class BoxList:

    def __init__(self, boxes: torch.Tensor, format: str='xyxy', **extras) ->None:
        """Representation of a list of bounding boxes and associated data.

        Internally, boxes are always stored in the xyxy format.

        Args:
            boxes: tensor<n, 4>
            format: format of input boxes.
            extras: dict with values that are tensors with first dimension corresponding
                to boxes first dimension
        """
        self.extras = extras
        if format == 'xyxy':
            self.boxes = boxes
        elif format == 'yxyx':
            self.boxes = boxes[:, [1, 0, 3, 2]]
        else:
            self.boxes = box_convert(boxes, format, 'xyxy')

    def __contains__(self, key: str) ->bool:
        return key == 'boxes' or key in self.extras

    def get_field(self, name: str) ->Any:
        if name == 'boxes':
            return self.boxes
        else:
            return self.extras.get(name)

    def _map_extras(self, func: Callable, cond: Callable=lambda k, v: True) ->dict:
        new_extras = {}
        for k, v in self.extras.items():
            if cond(k, v):
                new_extras[k] = func(k, v)
            else:
                new_extras[k] = v
        return new_extras

    def copy(self) ->'BoxList':
        return BoxList(self.boxes.copy(), **self._map_extras(lambda k, v: v.copy()), cond=lambda k, v: torch.is_tensor(v))

    def to(self, *args, **kwargs) ->'BoxList':
        boxes = self.boxes
        extras = self._map_extras(func=lambda k, v: v, cond=lambda k, v: torch.is_tensor(v))
        return BoxList(boxes, **extras)

    def convert_boxes(self, out_fmt: str) ->torch.Tensor:
        if out_fmt == 'yxyx':
            boxes = self.boxes[:, [1, 0, 3, 2]]
        else:
            boxes = box_convert(self.boxes, 'xyxy', out_fmt)
        return boxes

    def __len__(self) ->int:
        return len(self.boxes)

    @staticmethod
    def cat(box_lists: Iterable['BoxList']) ->'BoxList':
        boxes = []
        extras = defaultdict(list)
        for bl in box_lists:
            boxes.append(bl.boxes)
            for k, v in bl.extras.items():
                extras[k].append(v)
        boxes = torch.cat(boxes)
        for k, v in extras.items():
            extras[k] = torch.cat(v)
        return BoxList(boxes, **extras)

    def equal(self, other: 'BoxList') ->bool:
        if len(other) != len(self):
            return False
        extras = [(v.float().unsqueeze(1) if v.ndim == 1 else v.float()) for v in self.extras.values()]
        cat_arr = torch.cat([self.boxes] + extras, 1)
        self_tups = set([tuple([x.item() for x in row]) for row in cat_arr])
        extras = [(v.float().unsqueeze(1) if v.ndim == 1 else v.float()) for v in other.extras.values()]
        cat_arr = torch.cat([other.boxes] + extras, 1)
        other_tups = set([tuple([x.item() for x in row]) for row in cat_arr])
        return self_tups == other_tups

    def ind_filter(self, inds: Sequence[int]) ->'BoxList':
        boxes = self.boxes[inds]
        extras = self._map_extras(func=lambda k, v: v[inds], cond=lambda k, v: torch.is_tensor(v))
        return BoxList(boxes, **extras)

    def score_filter(self, score_thresh: float=0.25) ->'BoxList':
        scores = self.extras.get('scores')
        if scores is not None:
            return self.ind_filter(scores > score_thresh)
        else:
            raise ValueError('must have scores as key in extras')

    def clip_boxes(self, img_height: int, img_width: int) ->'BoxList':
        boxes = clip_boxes_to_image(self.boxes, (img_height, img_width))
        return BoxList(boxes, **self.extras)

    def nms(self, iou_thresh: float=0.5) ->torch.Tensor:
        if len(self) == 0:
            return self
        good_inds = batched_nms(self.boxes, self.get_field('scores'), self.get_field('class_ids'), iou_thresh)
        return self.ind_filter(good_inds)

    def scale(self, yscale: float, xscale: float) ->'BoxList':
        boxes = self.boxes * torch.tensor([[yscale, xscale, yscale, xscale]], device=self.boxes.device)
        return BoxList(boxes, **self.extras)

    def pin_memory(self) ->'BoxList':
        self.boxes = self.boxes.pin_memory()
        for k, v in self.extras.items():
            if torch.is_tensor(v):
                self.extras[k] = v.pin_memory()
        return self


class TorchVisionODAdapter(nn.Module):
    """Adapter for interfacing with TorchVision's object detection models.

    The purpose of this adapter is:
    1) to convert input BoxLists to dicts before feeding them into the model
    2) to convert detections output by the model as dicts into BoxLists

    Additionally, it automatically converts to/from 1-indexed class labels
    (which is what the TorchVision models expect).
    """

    def __init__(self, model: nn.Module, ignored_output_inds: Sequence[int]=[0]) ->None:
        """Constructor.

        Args:
            model (nn.Module): A torchvision object detection model.
            ignored_output_inds (Iterable[int], optional): Class labels to exclude.
                Defaults to [0].
        """
        super().__init__()
        self.model = model
        self.ignored_output_inds = ignored_output_inds

    def forward(self, input: torch.Tensor, targets: Optional[Iterable[BoxList]]=None) ->Union[dict, List[BoxList]]:
        """Forward pass.

        Args:
            input (Tensor[batch_size, in_channels, in_height, in_width]): batch
                of images.
            targets (Optional[Iterable[BoxList]], optional): In training mode,
                should be Iterable[BoxList]], with each BoxList having a
                'class_ids' field. In eval mode, should be None. Defaults to
                None.

        Returns:
            Union[dict, List[BoxList]]: In training mode,
                returns a dict of losses. In eval mode, returns a list of
                BoxLists containing predicted boxes, class_ids, and scores.
                Further filtering based on score should be done before
                considering the prediction "final".
        """
        if targets is not None:
            _targets = [self.boxlist_to_model_input_dict(bl) for bl in targets]
            loss_dict = self.model(input, _targets)
            loss_dict['total_loss'] = sum(list(loss_dict.values()))
            return loss_dict
        outs = self.model(input)
        boxlists = [self.model_output_dict_to_boxlist(out) for out in outs]
        return boxlists

    def boxlist_to_model_input_dict(self, boxlist: BoxList) ->dict:
        """Convert BoxList to a dict compatible with torchvision detection
        models. Also, make class labels 1-indexed.

        Args:
            boxlist (BoxList): A BoxList with a "class_ids" field.

        Returns:
            dict: A dict with keys: "boxes" and "labels".
        """
        return {'boxes': boxlist.boxes, 'labels': boxlist.get_field('class_ids') + 1}

    def model_output_dict_to_boxlist(self, out: dict) ->BoxList:
        """Convert torchvision detection dict to BoxList. Also, exclude any
        null classes and make class labels 0-indexed.

        Args:
            out (dict): A dict output by a torchvision detection model in eval
                mode.

        Returns:
            BoxList: A BoxList with "class_ids" and "scores" fields.
        """
        exclude_masks = [(out['labels'] != i) for i in self.ignored_output_inds]
        mask = reduce(iand, exclude_masks)
        boxlist = BoxList(boxes=out['boxes'][mask], class_ids=out['labels'][mask] - 1, scores=out['scores'][mask])
        return boxlist


class RegressionModel(nn.Module):

    def __init__(self, backbone_arch, out_features, pretrained=True, pos_out_inds=None, prob_out_inds=None):
        super().__init__()
        self.backbone = getattr(models, backbone_arch)(pretrained=pretrained)
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_features, out_features)
        self.pos_out_inds = pos_out_inds
        self.prob_out_inds = prob_out_inds

    def forward(self, x):
        out = self.backbone(x)
        if self.pos_out_inds:
            for ind in self.pos_out_inds:
                out[:, ind] = out[:, ind].exp()
        if self.prob_out_inds:
            for ind in self.prob_out_inds:
                out[:, ind] = out[:, ind].sigmoid()
        return out


class SplitTensor(nn.Module):
    """ Wrapper around `torch.split` """

    def __init__(self, size_or_sizes, dim):
        super().__init__()
        self.size_or_sizes = size_or_sizes
        self.dim = dim

    def forward(self, X):
        return X.split(self.size_or_sizes, dim=self.dim)


class Parallel(nn.ModuleList):
    """ Passes inputs through multiple `nn.Module`s in parallel.
        Returns a tuple of outputs.
    """

    def __init__(self, *args):
        super().__init__(args)

    def forward(self, xs):
        if isinstance(xs, torch.Tensor):
            return tuple(m(xs) for m in self)
        assert len(xs) == len(self)
        return tuple(m(x) for m, x in zip(self, xs))


class AddTensors(nn.Module):
    """ Adds all its inputs together. """

    def forward(self, xs):
        return sum(xs)


class MockModel(nn.Module):

    def __init__(self, num_classes: int) ->None:
        super().__init__()
        self.num_classes = num_classes

    def forward(self, x, y=None):
        if self.training:
            assert y is not None
            return {'loss1': 0, 'loss2': 0}
        else:
            N = len(x)
            nboxes = np.random.randint(0, 10)
            outs = [{'boxes': torch.rand((nboxes, 4)), 'labels': torch.randint(0, self.num_classes, (nboxes,)), 'scores': torch.rand((nboxes,))} for _ in range(N)]
            return outs


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AddTensors,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MockModel,
     lambda: ([], {'num_classes': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Parallel,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SplitTensor,
     lambda: ([], {'size_or_sizes': 4, 'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     True),
]

class Test_azavea_raster_vision(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

