import sys
_module = sys.modules[__name__]
del sys
data_processor = _module
source = _module
tensorizer = _module
atis = _module
server = _module
metric = _module
model = _module
my_tagging_task = _module
output = _module
pytext = _module
builtin_task = _module
common = _module
constants = _module
utils = _module
config = _module
component = _module
config_adapter = _module
contextual_intent_slot = _module
doc_classification = _module
field_config = _module
module_config = _module
pair_classification = _module
pytext_config = _module
query_document_pairwise_ranking = _module
serialize = _module
component_test = _module
config_adapter_test = _module
pytext_all_config_test = _module
pytext_config_test = _module
serialize_test = _module
data = _module
batch_sampler = _module
bert_tensorizer = _module
data_handler = _module
data_structures = _module
annotation = _module
node = _module
annotation_test = _module
decoupled_data = _module
dense_retrieval_tensorizer = _module
disjoint_multitask_data = _module
disjoint_multitask_data_handler = _module
dynamic_pooling_batcher = _module
featurizer = _module
simple_featurizer = _module
masked_tensorizer = _module
masked_util = _module
packed_lm_data = _module
pickleable_gpt2bpe_encoder = _module
pickleable_gpt2bpe_encoder = _module
roberta_tensorizer = _module
sources = _module
conllu = _module
data_source = _module
dense_retrieval = _module
pandas = _module
session = _module
squad = _module
tsv = _module
squad_for_bert_tensorizer = _module
squad_tensorizer = _module
tensorizers = _module
test = _module
batch_sampler_test = _module
data_test = _module
dynamic_pooling_batcher_test = _module
mask_tensorizers_test = _module
pandas_data_source_test = _module
round_robin_batchiterator_test = _module
simple_featurizer_test = _module
tensorizers_test = _module
tokenizers_test = _module
tsv_data_source_test = _module
utils_test = _module
token_tensorizer = _module
tokenizers = _module
tokenizer = _module
utils = _module
xlm_constants = _module
xlm_dictionary = _module
xlm_tensorizer = _module
make_config_docs = _module
conf = _module
exporters = _module
custom_exporters = _module
exporter = _module
new_text_model_exporter_test = _module
text_model_exporter_test = _module
fields = _module
char_field = _module
contextual_token_embedding_field = _module
dict_field = _module
field = _module
char_field_test = _module
contextual_token_embedding_field_test = _module
dict_field_test = _module
field_test = _module
text_field_with_special_unk = _module
legacy = _module
batch = _module
dataset = _module
example = _module
field = _module
iterator = _module
pipeline = _module
datasets = _module
babi = _module
imdb = _module
language_modeling = _module
nli = _module
sequence_tagging = _module
sst = _module
text_classification = _module
translation = _module
trec = _module
unsupervised_learning = _module
vocab = _module
loss = _module
loss = _module
regularized_loss = _module
regularizer = _module
structured_loss = _module
ctc_loss_test = _module
focal_loss_test = _module
label_smoothing_loss_test = _module
samplewise_label_smoothing_loss_test = _module
main = _module
metric_reporters = _module
calibration_metric_reporter = _module
channel = _module
classification_metric_reporter = _module
compositional_metric_reporter = _module
compositional_utils = _module
dense_retrieval_metric_reporter = _module
disjoint_multitask_metric_reporter = _module
intent_slot_detection_metric_reporter = _module
language_model_metric_reporter = _module
mask_compositional = _module
mask_seq2seq_topk = _module
metric_reporter = _module
multi_span_qa_metric_reporter = _module
pairwise_ranking_metric_reporter = _module
regression_metric_reporter = _module
seq2seq_compositional = _module
seq2seq_metric_reporter = _module
seq2seq_utils = _module
squad_metric_reporter = _module
classification_metric_reporter_test = _module
compositional_metric_reporter_test = _module
intent_slot_metric_reporter_test = _module
language_model_metric_reporter_test = _module
multi_label_seq_tagging_metric_reporter_test = _module
tensorboard_test = _module
word_tagging_metric_reporter = _module
metrics = _module
calibration_metrics = _module
dense_retrieval_metrics = _module
intent_slot_metrics = _module
language_model_metrics = _module
mask_metrics = _module
seq2seq_metrics = _module
squad_metrics = _module
basic_metrics_test = _module
calibration_metrics_test = _module
intent_slot_metrics_test = _module
metrics_test_base = _module
multilabel_metrics_test = _module
models = _module
bert_classification_models = _module
bert_regression_model = _module
crf = _module
decoders = _module
decoder_base = _module
intent_slot_model_decoder = _module
mlp_decoder = _module
mlp_decoder_n_tower = _module
mlp_decoder_query_response = _module
mlp_decoder_tri_tower = _module
mlp_decoder_two_tower = _module
multilabel_decoder = _module
disjoint_multitask_model = _module
distributed_model = _module
doc_model = _module
embeddings = _module
char_embedding = _module
contextual_token_embedding = _module
dict_embedding = _module
embedding_base = _module
embedding_list = _module
int_single_category_embedding = _module
int_weighted_multi_category_embedding = _module
mlp_embedding = _module
scriptable_embedding_list = _module
word_embedding = _module
word_seq_embedding = _module
ensembles = _module
bagging_doc_ensemble = _module
bagging_intent_slot_ensemble = _module
ensemble = _module
joint_model = _module
language_models = _module
lmlstm = _module
masked_lm = _module
masking_utils = _module
model = _module
module = _module
output_layers = _module
distance_output_layer = _module
doc_classification_output_layer = _module
doc_regression_output_layer = _module
intent_slot_output_layer = _module
lm_output_layer = _module
multi_label_classification_layer = _module
output_layer_base = _module
pairwise_ranking_output_layer = _module
squad_output_layer = _module
utils = _module
word_tagging_output_layer = _module
pair_classification_model = _module
qna = _module
bert_squad_qa = _module
dr_qa = _module
query_document_pairwise_ranking_model = _module
r3f_models = _module
representations = _module
attention = _module
augmented_lstm = _module
bilstm = _module
bilstm_doc_attention = _module
bilstm_doc_slot_attention = _module
bilstm_slot_attn = _module
biseqcnn = _module
contextual_intent_slot_rep = _module
deepcnn = _module
docnn = _module
huggingface_bert_sentence_encoder = _module
huggingface_electra_sentence_encoder = _module
jointcnn_rep = _module
lightconv = _module
ordered_neuron_lstm = _module
pair_rep = _module
pass_through = _module
pooling = _module
pure_doc_attention = _module
representation_base = _module
seq_rep = _module
slot_attention = _module
sparse_transformer_sentence_encoder = _module
stacked_bidirectional_rnn = _module
augmented_lstm_test = _module
ordered_neuron_lstm_test = _module
transformer_test = _module
traced_transformer_encoder = _module
transformer = _module
luna_attention = _module
luna_sentence_encoder = _module
multihead_linear_attention = _module
representation = _module
sentence_encoder = _module
transformer_sentence_encoder = _module
transformer_sentence_encoder_base = _module
roberta = _module
semantic_parsers = _module
rnng = _module
rnng_constant = _module
rnng_data_structures = _module
rnng_parser = _module
seq_models = _module
attention = _module
base = _module
conv_decoder = _module
conv_encoder = _module
conv_model = _module
light_conv = _module
mask_generator = _module
nar_length = _module
nar_modules = _module
nar_output_layer = _module
nar_seq2seq_model = _module
positional = _module
projection_layers = _module
rnn_decoder = _module
rnn_encoder = _module
rnn_encoder_decoder = _module
seq2seq_model = _module
seq2seq_output_layer = _module
seqnn = _module
utils = _module
bilstm_test = _module
crf_test = _module
dict_embedding_test = _module
int_single_category_embedding_test = _module
int_weighted_multi_category_embedding_test = _module
mlp_decoder_test = _module
mlp_embedding_test = _module
module_test = _module
output_layer_test = _module
personalized_doc_model_test = _module
rnng_test = _module
scripted_seq2seq_generator_test = _module
transformer_sentence_encoder_test = _module
word_embedding_test = _module
word_seq_embedding_test = _module
tri_tower_classification_model = _module
two_tower_classification_model = _module
two_tower_regression_model = _module
utils = _module
word_model = _module
optimizer = _module
activations = _module
adabelief = _module
fairseq_fp16_utils = _module
fp16_optimizer = _module
lamb = _module
madgrad = _module
optimizers = _module
radam = _module
scheduler = _module
sparsifiers = _module
blockwise_sparsifier = _module
sparsifier = _module
sparsifier_test = _module
swa = _module
fp16optimizer_test = _module
test_swa = _module
resources = _module
task = _module
disjoint_multitask = _module
new_task = _module
nop_decorator = _module
pytext_checkpoint_management = _module
serialize = _module
tasks = _module
torchscript = _module
batchutils = _module
module = _module
seq2seq = _module
beam_decode = _module
beam_search = _module
decoder = _module
encoder = _module
export_model = _module
scripted_seq2seq_generator = _module
seq2seq_rnn_decoder_utils = _module
bert = _module
normalizer = _module
roberta = _module
tensorizer = _module
xlm = _module
test_batchutils = _module
test_module = _module
test_tensorizer = _module
test_tokenizer = _module
test_vocab = _module
bpe = _module
tokenizer = _module
utils = _module
vocab = _module
trainers = _module
ensemble_trainer = _module
hogwild_trainer = _module
trainer = _module
training_state = _module
utils = _module
ascii_table = _module
config_utils = _module
cuda = _module
distributed = _module
documentation = _module
file_io = _module
label = _module
lazy = _module
loss = _module
meter = _module
mobile_onnx = _module
model = _module
onnx = _module
path = _module
precision = _module
tensor = _module
ascii_table_test = _module
embeddings_utils_test = _module
label_test = _module
lazy_test = _module
path_test = _module
timing_test = _module
utils_test = _module
timing = _module
typing = _module
usage = _module
workflow = _module
setup = _module
tests = _module
data_utils = _module
main_test = _module
model_utils_test = _module
module_load_save_test = _module
predictor_test = _module
seq2seq_model_tests = _module
task_load_save_test = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import List


import torch


import torch.nn.functional as F


from enum import Enum


from typing import Tuple


import collections


import enum


from typing import Any


from typing import Dict


from typing import Type


from typing import Union


from collections import OrderedDict


from typing import Optional


import math


from copy import deepcopy


from typing import Generator


from typing import Iterable


from typing import MutableMapping


from typing import Set


import copy


import itertools


from itertools import chain


import numpy as np


import pandas as pd


import re


from typing import NamedTuple


from collections import Counter


from logging import getLogger


from typing import Callable


import torchtext


from functools import partial


import torch.utils.data


from torchtext.data.utils import RandomShuffler


from torchtext.utils import download_from_url


from torchtext.data.utils import dtype_to_attr


from torchtext.data.utils import get_tokenizer


from torchtext.data.utils import is_tokenizer_serializable


import logging


import random


from torchtext.data.utils import ngrams_iterator


from torchtext.utils import extract_archive


from torchtext.data.functional import custom_replace


from torchtext.data.functional import simple_space_split


from collections import defaultdict


from torchtext.vocab import pretrained_aliases


from torchtext.vocab import Vectors


from torch import nn


import torch.nn as nn


from scipy.special import logsumexp


from torch.multiprocessing.spawn import spawn


from torch import Tensor


from numpy import linalg as LA


from torch.utils.tensorboard import SummaryWriter


from itertools import tee


from itertools import zip_longest


from typing import Sequence


import time


from torch import optim


import torch.jit as jit


from torch.serialization import default_restore_location


from torch import jit


import warnings


from torch.nn import Linear


import torch.onnx.operators


from torch.nn import ModuleList


import inspect


from inspect import signature


import torch.jit


from enum import IntEnum


from enum import unique


import functools


from scipy.special import comb


from torch.nn.utils.rnn import pack_padded_sequence


from torch.nn.utils.rnn import PackedSequence


from torch.nn.utils.rnn import pad_packed_sequence


import torch.onnx


import torch.cuda


from torch.nn import Parameter


from torch.nn import functional as F


from typing import Sized


from torch.nn import LayerNorm


import string


from torch.onnx import ExportTypes


from torch.onnx import OperatorExportTypes


from torch.optim.optimizer import Optimizer as PT_Optimizer


from collections import namedtuple


from torch.optim import Optimizer as PT_Optimizer


from torch.optim.lr_scheduler import _LRScheduler


from torch.optim.lr_scheduler import CosineAnnealingLR as TorchCosineAnnealingLR


from torch.optim.lr_scheduler import CyclicLR as TorchCyclicLR


from torch.optim.lr_scheduler import ExponentialLR as TorchExponentialLR


from torch.optim.lr_scheduler import ReduceLROnPlateau as TorchReduceLROnPlateau


from torch.optim.lr_scheduler import StepLR as TorchStepLR


from torch.autograd import Variable


from torch import sparse


from torch.utils import data


from torch import sort


import abc


import torch.jit.quantized


import torch.multiprocessing as mp


from torch.distributed.algorithms.ddp_comm_hooks.default_hooks import fp16_compress_hook


from collections.abc import Sequence


import torch.distributed as dist_c10d


from inspect import getmembers


from inspect import isclass


from inspect import isfunction


import numpy


from typing import get_type_hints


from typing import IO


from typing import Iterator


class ScriptBatchInput(NamedTuple):
    """A batch of inputs for TorchScript Module(bundle of Tensorizer and Model)
    texts or tokens is required but multually exclusive
    Args:
        texts: a batch of raw text inputs
        tokens: a batch of pre-tokenized inputs
        languages: language for each input in the batch
    """
    texts: Optional[List[List[str]]]
    tokens: Optional[List[List[List[str]]]]
    languages: Optional[List[List[str]]]


@torch.jit.script
def validate_padding_control(padding_control: Optional[List[int]]) ->bool:
    if padding_control is not None:
        if len(padding_control) < 2:
            return False
        elif padding_control[0] != 0:
            return False
    return True


class TensorizerScriptImpl(torch.nn.Module):
    device: str
    seq_padding_control: Optional[List[int]]
    batch_padding_control: Optional[List[int]]

    def __init__(self):
        super().__init__()
        self.device: str = ''
        self.seq_padding_control = None
        self.batch_padding_control = None

    @torch.jit.export
    def set_device(self, device: str):
        self.device = device

    @torch.jit.export
    def set_padding_control(self, dimension: str, padding_control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        if not validate_padding_control(padding_control):
            raise RuntimeError('Malformed padding_control value')
        if dimension == 'sequence_length':
            self.seq_padding_control = padding_control
        elif dimension == 'batch_length':
            self.batch_padding_control = padding_control
        else:
            raise RuntimeError('Illegal padding dimension specified.')

    def batch_size(self, inputs: ScriptBatchInput) ->int:
        texts: Optional[List[List[str]]] = inputs.texts
        tokens: Optional[List[List[List[str]]]] = inputs.tokens
        if texts is not None:
            return len(texts)
        elif tokens is not None:
            return len(tokens)
        else:
            raise RuntimeError('Empty input for both texts and tokens.')

    def row_size(self, inputs: ScriptBatchInput) ->int:
        texts: Optional[List[List[str]]] = inputs.texts
        tokens: Optional[List[List[List[str]]]] = inputs.tokens
        if texts is not None:
            return len(texts[0])
        elif tokens is not None:
            return len(tokens[0])
        else:
            raise RuntimeError('Empty input for both texts and tokens.')

    def get_texts_by_index(self, texts: Optional[List[List[str]]], index: int) ->Optional[List[str]]:
        if texts is None or len(texts) == 0:
            return None
        return texts[index]

    def get_tokens_by_index(self, tokens: Optional[List[List[List[str]]]], index: int) ->Optional[List[List[str]]]:
        if tokens is None or len(tokens) == 0:
            return None
        return tokens[index]

    def tokenize(self, *args, **kwargs):
        """
        This functions will receive the inputs from Clients, usually there are
        two possible inputs
        1) a row of texts: List[str]
        2) a row of pre-processed tokens: List[List[str]]

        Override this function to be TorchScriptable, e.g you need to declare
        concrete input arguments with type hints.
        """
        raise NotImplementedError

    def numberize(self, *args, **kwargs):
        """
        This functions will receive the outputs from function: tokenize() or
        will be called directly from PyTextTensorizer function: numberize().

        Override this function to be TorchScriptable, e.g you need to declare
        concrete input arguments with type hints.
        """
        raise NotImplementedError

    def tensorize(self, *args, **kwargs):
        """
        This functions will receive a list(e.g a batch) of outputs
        from function numberize(), padding and convert to output tensors.

        Override this function to be TorchScriptable, e.g you need to declare
        concrete input arguments with type hints.
        """
        raise NotImplementedError

    @torch.jit.ignore
    def tensorize_wrapper(self, *args, **kwargs):
        """
        This functions will receive a list(e.g a batch) of outputs
        from function numberize(), padding and convert to output tensors.

        It will be called in PyText Tensorizer during training time, this
        function is not torchscriptiable because it depends on cuda.device().
        """
        with to_device(self, cuda.device()):
            return self.tensorize(*args, **kwargs)

    @torch.jit.ignore
    def torchscriptify(self):
        return torch.jit.script(self)


class ScriptTokenizerBase(torch.jit.ScriptModule):

    @torch.jit.script_method
    def tokenize(self, input: str) ->List[Tuple[str, int, int]]:
        """
        Process a single line of raw inputs into tokens, it supports
        two input formats:
        1) a single text
        2) a token

        Returns a list of tokens with start and end indices in original input.
        """
        raise NotImplementedError


class ScriptDoNothingTokenizer(ScriptTokenizerBase):

    @torch.jit.script_method
    def tokenize(self, raw_token: str) ->List[Tuple[str, int, int]]:
        return [(raw_token, -1, -1)]


class Token(NamedTuple):
    value: str
    start: int
    end: int


class ScriptVocabulary(nn.Module):
    idx: Dict[str, int]

    def __init__(self, vocab_list, unk_idx: int=0, pad_idx: int=-1, bos_idx: int=-1, eos_idx: int=-1, mask_idx: int=-1, unk_token: Optional[str]=None):
        super().__init__()
        self.vocab: List[str] = vocab_list
        self.unk_idx: int = unk_idx
        self.pad_idx: int = pad_idx
        self.eos_idx: int = eos_idx
        self.bos_idx: int = bos_idx
        self.mask_idx: int = mask_idx
        self.idx: Dict[str, int] = {word: i for i, word in enumerate(vocab_list)}
        pad_token = vocab_list[pad_idx] if pad_idx >= 0 else SpecialTokens.PAD
        self.pad_token: str = pad_token
        self.unk_token = unk_token

    def get_pad_index(self):
        return self.pad_idx

    def get_unk_index(self):
        return self.unk_idx

    def lookup_indices_1d(self, values: List[str]) ->List[int]:
        result: List[int] = []
        for value in values:
            result.append(self.idx.get(value, self.unk_idx))
        return result

    def lookup_indices_2d(self, values: List[List[str]]) ->List[List[int]]:
        result: List[List[int]] = []
        for value in values:
            result.append(self.lookup_indices_1d(value))
        return result

    def lookup_indices_3d(self, values: List[List[List[str]]]) ->List[List[List[int]]]:
        result: List[List[List[int]]] = []
        for value in values:
            result.append(self.lookup_indices_2d(value))
        return result

    def lookup_words_1d(self, values: torch.Tensor, filter_token_list: List[int]=(), possible_unk_token: Optional[str]=None) ->List[str]:
        """If possible_unk_token is not None, then all UNK id's will be replaced
        by possible_unk_token instead of the default UNK string which is <UNK>.
        This is a simple way to resolve UNK's when there's a correspondence
        between source and target translations.
        """
        result: List[str] = []
        for idx in range(values.size(0)):
            value = int(values[idx])
            if not value in filter_token_list:
                result.append(self.lookup_word(value, possible_unk_token))
        return result

    def lookup_words_1d_cycle_heuristic(self, values: torch.Tensor, filter_token_list: List[int], ordered_unks_token: List[str]) ->List[str]:
        """This function is a extension of the possible_unk_token heuristic
        in lookup_words_1d, which fails in the case when multiple unks are
        available. The way we deal with this is we increment every unk token in
        ordered_unks_token everytime we substitute an unk token. This solves a
        substantial amount of queries with multiple unk tokens.
        """
        unk_idx = 0
        unk_idx_length: int = len(ordered_unks_token)
        unk_copy: bool = unk_idx_length != 0
        vocab_length: int = len(self.vocab)
        result: List[str] = []
        for idx in range(values.size(0)):
            value = int(values[idx])
            if not value in filter_token_list:
                if value < vocab_length and value != self.unk_idx:
                    result.append(self.vocab[value])
                elif not unk_copy:
                    result.append(self.vocab[self.unk_idx])
                else:
                    unk_value = ordered_unks_token[unk_idx % unk_idx_length]
                    result.append(unk_value)
                    unk_idx += 1
        return result

    def lookup_word(self, idx: int, possible_unk_token: Optional[str]=None):
        if idx < len(self.vocab) and idx != self.unk_idx:
            return self.vocab[idx]
        else:
            return self.vocab[self.unk_idx] if possible_unk_token is None else possible_unk_token

    def __len__(self):
        return len(self.vocab)


class ConfigBaseMeta(type):

    def annotations_and_defaults(cls):
        annotations = OrderedDict()
        defaults = {}
        for base in reversed(cls.__bases__):
            if base is ConfigBase:
                continue
            annotations.update(getattr(base, '__annotations__', {}))
            defaults.update(getattr(base, '_field_defaults', {}))
        annotations.update(vars(cls).get('__annotations__', {}))
        defaults.update({k: getattr(cls, k) for k in annotations if hasattr(cls, k)})
        return annotations, defaults

    @property
    def __annotations__(cls):
        annotations, _ = cls.annotations_and_defaults()
        return annotations
    _field_types = __annotations__

    @property
    def _fields(cls):
        return cls.__annotations__.keys()

    @property
    def _field_defaults(cls):
        _, defaults = cls.annotations_and_defaults()
        return defaults


class ConfigBase(metaclass=ConfigBaseMeta):

    def items(self):
        return self._asdict().items()

    def _asdict(self):
        return {k: getattr(self, k) for k in type(self).__annotations__}

    def _replace(self, **kwargs):
        args = self._asdict()
        args.update(kwargs)
        return type(self)(**args)

    def __init__(self, **kwargs):
        """Configs can be constructed by specifying values by keyword.
        If a keyword is supplied that isn't in the config, or if a config requires
        a value that isn't specified and doesn't have a default, a TypeError will be
        raised."""
        specified = kwargs.keys() | type(self)._field_defaults.keys()
        required = type(self).__annotations__.keys()
        unspecified_fields = required - specified
        if unspecified_fields:
            raise TypeError(f'Failed to specify {unspecified_fields} for {type(self)}')
        overspecified_fields = specified - required
        if overspecified_fields:
            raise TypeError(f'Specified non-existent fields {overspecified_fields} for {type(self)}')
        vars(self).update(kwargs)

    def __str__(self):
        lines = [self.__class__.__name__ + ':']
        for key, val in sorted(self._asdict().items()):
            lines += f'{key}: {val}'.split('\n')
        return '\n    '.join(lines)

    def __eq__(self, other):
        """Mainly a convenience utility for unit testing."""
        return type(self) == type(other) and self._asdict() == other._asdict()


class ComponentType(enum.Enum):
    TASK = 'task'
    COLUMN = 'column'
    DATA_TYPE = 'data_type'
    DATA_HANDLER = 'data_handler'
    DATA_SOURCE = 'data_source'
    TOKENIZER = 'tokenizer'
    TENSORIZER = 'tensorizer'
    BATCHER = 'batcher'
    BATCH_SAMPLER = 'batch_sampler'
    FEATURIZER = 'featurizer'
    TRAINER = 'trainer'
    LOSS = 'loss'
    OPTIMIZER = 'optimizer'
    SCHEDULER = 'scheduler'
    MODEL = 'model'
    MODEL2 = 'model2'
    MODULE = 'module'
    PREDICTOR = 'predictor'
    EXPORTER = 'exporter'
    METRIC_REPORTER = 'metric_reporter'
    SPARSIFIER = 'sparsifier'
    MASKING_FUNCTION = 'masking_function'


class RegistryError(Exception):
    pass


class Registry:
    _registered_components: Dict[ComponentType, Dict[Type, Type]] = collections.defaultdict(dict)

    @classmethod
    def add(cls, component_type: ComponentType, cls_to_add: Type, config_cls: Type):
        component = cls._registered_components[component_type]
        if config_cls in component:
            raise RegistryError(f"Cannot add {cls_to_add} to {component_type} for task_config type {config_cls}; it's already registered for {component[config_cls]}")
        component[config_cls] = cls_to_add

    @classmethod
    def get(cls, component_type: ComponentType, config_cls: Type) ->Type:
        if component_type not in cls._registered_components:
            raise RegistryError(f"type {component_type} doesn't exist")
        if config_cls not in cls._registered_components[component_type]:
            raise RegistryError(f'unregistered config class {config_cls.__name__} for {component_type}')
        return cls._registered_components[component_type][config_cls]

    @classmethod
    def values(cls, component_type: ComponentType) ->Tuple[Type, ...]:
        if component_type not in cls._registered_components:
            raise RegistryError(f"type {component_type} doesn't exist")
        return tuple(cls._registered_components[component_type].values())

    @classmethod
    def configs(cls, component_type: ComponentType) ->Tuple[Type, ...]:
        if component_type not in cls._registered_components:
            raise RegistryError(f"type {component_type} doesn't exist")
        return tuple(cls._registered_components[component_type].keys())

    @classmethod
    def subconfigs(cls, config_cls: Type) ->Tuple[Type, ...]:
        return tuple(sub_cls for sub_cls in cls.configs(config_cls.__COMPONENT_TYPE__) if issubclass(sub_cls.__COMPONENT__, config_cls.__COMPONENT__))


class ComponentMeta(type):

    def __new__(metacls, typename, bases, namespace):
        if 'Config' not in namespace:
            parent_config = next((base.Config for base in bases if hasattr(base, 'Config')), None)
            if parent_config is not None:


                class Config(parent_config):
                    pass
            else:


                class Config(ConfigBase):
                    pass
            namespace['Config'] = Config
        component_type = next((base.__COMPONENT_TYPE__ for base in bases if hasattr(base, '__COMPONENT_TYPE__')), namespace.get('__COMPONENT_TYPE__'))
        new_cls = super().__new__(metacls, typename, bases, namespace)
        new_cls.Config.__COMPONENT_TYPE__ = component_type
        new_cls.Config.__name__ = f'{typename}.Config'
        new_cls.Config.__COMPONENT__ = new_cls
        new_cls.Config.__EXPANSIBLE__ = namespace.get('__EXPANSIBLE__')
        if component_type:
            Registry.add(component_type, new_cls, new_cls.Config)
        return new_cls

    def __dir__(cls):
        """Jit doesnt allow scripting of attributes whose classname includes "."

        Example Repro:

        class OldModule(Module):
            class Config(ConfigBase):
                a: int = 5

            @classmethod
            def from_config(cls, config: Config):
                return cls(config.a)

            def __init__(self, a):
                super().__init__()
                self.a = a

            def forward(self, b: int) -> int:
                return b + self.a

        m = OldModule.from_config(OldModule.Config())
        jit.script(m)

            > RuntimeError: Could not get qualified name for class 'OldModule.Config':
        'OldModule.Config' is not a valid identifier

        print(m.Config.__name__)
            > OldModule.Config

        At the sametime, we dont need to script the config classes because they
        are not needed during inference time. Hence in this workaround we skip
        the config classes.

        Ideal solution is that when building models they should be inheriting
        from nn.Module only and not Component. This requires significant changes
        to the way models are created in PyText.

        """
        result = super().__dir__()
        return [r for r in result if not (isinstance(getattr(cls, r, None), type) and issubclass(getattr(cls, r, None), ConfigBase))]


subsystem_name = 'PyText'


def log_class_usage(klass):
    identifier = subsystem_name
    if klass and hasattr(klass, '__name__'):
        identifier += f'.{klass.__name__}'
    torch._C._log_api_usage_once(identifier)


class Component(metaclass=ComponentMeta):


    class Config(ConfigBase):
        pass

    @classmethod
    def from_config(cls, config, *args, **kwargs):
        return cls(config, *args, **kwargs)

    def __init__(self, config=None, *args, **kwargs):
        self.config = config
        log_class_usage(self.__class__)


class ScriptWordTokenizer(ScriptTokenizerBase):

    def __init__(self, lowercase=True):
        super().__init__()
        self.lowercase = lowercase

    @torch.jit.script_method
    def tokenize(self, raw_token: str) ->List[Tuple[str, int, int]]:
        """
        This tokenizers splits a raw_token into its constituent words by splitting
        the raw_token on space. This function handle multiple spaces between
        words too.
        Note:
        torch scripting doesn't support try-except and since re.finditer uses
        try in its implemetation regex based tokenization is not supported.
        """
        tokenize_input = raw_token.lower() if self.lowercase else raw_token
        tokens = tokenize_input.split()
        torchify_tokens = torch.jit.annotate(List[Tuple[str, int, int]], [])
        start, end = 0, 0
        for token in tokens:
            start = tokenize_input.find(token, end, -1)
            end = start + len(token)
            torchify_tokens.append((token.strip(), start, end))
            start = end + 1
        return torchify_tokens


class Tokenizer(Component):
    """A simple regex-splitting tokenizer."""
    __COMPONENT_TYPE__ = ComponentType.TOKENIZER
    __EXPANSIBLE__ = True


    class Config(Component.Config):
        split_regex: str = '\\s+'
        lowercase: bool = True
        use_byte_offsets: bool = False

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.split_regex, config.lowercase, config.use_byte_offsets)

    def __init__(self, split_regex='\\s+', lowercase=True, use_byte_offsets=False):
        super().__init__(None)
        self.split_regex = split_regex
        self.lowercase = lowercase
        self.use_byte_offsets = use_byte_offsets

    def tokenize(self, input: str) ->List[Token]:
        tokens = []
        start = 0
        tokenize_input = input.lower() if self.lowercase else input
        for match in re.finditer(self.split_regex, tokenize_input):
            split_start, split_end = match.span()
            tokens.append(Token(tokenize_input[start:split_start], start, split_start))
            start = split_end
        tokens.append(Token(tokenize_input[start:len(input)], start, len(input)))
        if self.use_byte_offsets:
            return [self._convert_token(input, token) for token in tokens if token.value]
        else:
            return [token for token in tokens if token.value]

    def _convert_token(self, inp: str, token: Token) ->Token:
        return Token(token.value, self._convert_char_to_byte_offsets(inp, token.start), self._convert_char_to_byte_offsets(inp, token.end))

    def _convert_char_to_byte_offsets(self, input: str, char_offset: int) ->int:
        return len(input[:char_offset].encode('utf8'))

    def torchscriptify(self):
        if self.split_regex == '\\s+':
            return ScriptWordTokenizer(self.lowercase)
        else:
            NotImplementedError

    def decode(self, sentence: str):
        return sentence


def should_iter(i):
    """Whether or not an object looks like a python iterable (not including strings)."""
    return hasattr(i, '__iter__') and not isinstance(i, str) and not (isinstance(i, torch.Tensor) and (i.dim() == 0 or len(i) == 0))


class VocabLookup(torch.jit.ScriptModule):
    """
    TorchScript implementation of lookup_tokens() in pytext/data/tensorizers.py
    """

    def __init__(self, vocab: ScriptVocabulary):
        super().__init__()
        self.vocab = vocab

    @torch.jit.script_method
    def forward(self, tokens: List[Tuple[str, int, int]], bos_idx: Optional[int]=None, eos_idx: Optional[int]=None, use_eos_token_for_bos: bool=False, max_seq_len: int=2 ** 30) ->Tuple[List[int], List[int], List[int]]:
        """Convert tokens into ids by doing vocab look-up.

        Convert tokens into ids by doing vocab look-up. It will also append
        bos & eos index into token_ids if needed. A token is represented by
        a Tuple[str, int, int], which is [token, start_index, end_index].

        Args:
            tokens: List of tokens with start and end position in the original
                text. start and end index could be optional (e.g value is -1)
            bos_idx: index of begin of sentence, optional.
            eos_idx: index of end of sentence, optional.
            use_eos_token_for_bos: use eos index as bos.
            max_seq_len: maximum tokens length.
        """
        if bos_idx is None:
            bos_idx = -1
        if eos_idx is None:
            eos_idx = -1
        text_tokens: List[str] = []
        start_idxs: List[int] = []
        end_idxs: List[int] = []
        max_seq_len = max_seq_len - (1 if bos_idx >= 0 else 0) - (1 if eos_idx >= 0 else 0)
        for i in range(min(len(tokens), max_seq_len)):
            token: Tuple[str, int, int] = tokens[i]
            text_tokens.append(token[0])
            start_idxs.append(token[1])
            end_idxs.append(token[2])
        token_ids: List[int] = self.vocab.lookup_indices_1d(text_tokens)
        if bos_idx >= 0:
            if use_eos_token_for_bos:
                bos_idx = eos_idx
            token_ids = [bos_idx] + token_ids
            start_idxs = [-1] + start_idxs
            end_idxs = [-1] + end_idxs
        if eos_idx >= 0:
            token_ids.append(eos_idx)
            start_idxs.append(-1)
            end_idxs.append(-1)
        return token_ids, start_idxs, end_idxs


CUDA_ENABLED = False


def FloatTensor(*args):
    if CUDA_ENABLED:
        return torch.FloatTensor(*args)
    else:
        return torch.FloatTensor(*args)


class Loss(Component):
    """Base class for loss functions"""
    __COMPONENT_TYPE__ = ComponentType.LOSS

    def __init__(self, config=None, *args, **kwargs):
        super().__init__(config)

    def __call__(self, logit, targets, reduce=True):
        raise NotImplementedError


class AUCPRHingeLoss(nn.Module, Loss):
    """area under the precision-recall curve loss,
    Reference: "Scalable Learning of Non-Decomposable Objectives", Section 5     TensorFlow Implementation:     https://github.com/tensorflow/models/tree/master/research/global_objectives    """


    class Config(ConfigBase):
        """
        Attributes:
            precision_range_lower (float): the lower range of precision values over
                which to compute AUC. Must be nonnegative, `\\leq precision_range_upper`,
                and `leq 1.0`.
            precision_range_upper (float): the upper range of precision values over
                which to compute AUC. Must be nonnegative, `\\geq precision_range_lower`,
                and `leq 1.0`.
            num_classes (int): number of classes(aka labels)
            num_anchors (int): The number of grid points used to approximate the
                Riemann sum.
        """
        precision_range_lower: float = 0.0
        precision_range_upper: float = 1.0
        num_classes: int = 1
        num_anchors: int = 20

    def __init__(self, config, weights=None, *args, **kwargs):
        """Args:
        config: Config containing `precision_range_lower`, `precision_range_upper`,
            `num_classes`, `num_anchors`
        """
        nn.Module.__init__(self)
        Loss.__init__(self, config)
        self.num_classes = self.config.num_classes
        self.num_anchors = self.config.num_anchors
        self.precision_range = self.config.precision_range_lower, self.config.precision_range_upper
        self.precision_values, self.delta = loss_utils.range_to_anchors_and_delta(self.precision_range, self.num_anchors)
        self.biases = nn.Parameter(FloatTensor(self.config.num_classes, self.config.num_anchors).zero_())
        self.lambdas = nn.Parameter(FloatTensor(self.config.num_classes, self.config.num_anchors).data.fill_(1.0))

    def forward(self, logits, targets, reduce=True, size_average=True, weights=None):
        """
        Args:
            logits: Variable :math:`(N, C)` where `C = number of classes`
            targets: Variable :math:`(N)` where each value is
                `0 <= targets[i] <= C-1`
            weights: Coefficients for the loss. Must be a `Tensor` of shape
                [N] or [N, C], where `N = batch_size`, `C = number of classes`.
            size_average (bool, optional): By default, the losses are averaged
                    over observations for each minibatch. However, if the field
                    sizeAverage is set to False, the losses are instead summed
                    for each minibatch. Default: ``True``
            reduce (bool, optional): By default, the losses are averaged or summed over
                observations for each minibatch depending on size_average. When reduce
                is False, returns a loss per input/target element instead and ignores
                size_average. Default: True
        """
        C = 1 if logits.dim() == 1 else logits.size(1)
        if self.num_classes != C:
            raise ValueError('num classes is %d while logits width is %d' % (self.num_classes, C))
        labels, weights = AUCPRHingeLoss._prepare_labels_weights(logits, targets, weights=weights)
        lambdas = loss_utils.lagrange_multiplier(self.lambdas)
        hinge_loss = loss_utils.weighted_hinge_loss(labels.unsqueeze(-1), logits.unsqueeze(-1) - self.biases, positive_weights=1.0 + lambdas * (1.0 - self.precision_values), negative_weights=lambdas * self.precision_values)
        class_priors = loss_utils.build_class_priors(labels, weights=weights)
        lambda_term = class_priors.unsqueeze(-1) * (lambdas * (1.0 - self.precision_values))
        per_anchor_loss = weights.unsqueeze(-1) * hinge_loss - lambda_term
        loss = per_anchor_loss.sum(2) * self.delta
        loss /= self.precision_range[1] - self.precision_range[0]
        if not reduce:
            return loss
        elif size_average:
            return loss.mean()
        else:
            return loss.sum()

    @staticmethod
    def _prepare_labels_weights(logits, targets, weights=None):
        """
        Args:
            logits: Variable :math:`(N, C)` where `C = number of classes`
            targets: Variable :math:`(N)` where each value is
                `0 <= targets[i] <= C-1`
            weights: Coefficients for the loss. Must be a `Tensor` of shape
                [N] or [N, C], where `N = batch_size`, `C = number of classes`.
        Returns:
            labels: Tensor of shape [N, C], one-hot representation
            weights: Tensor of shape broadcastable to labels
        """
        N, C = logits.size()
        labels = FloatTensor(N, C).zero_().scatter(1, targets.unsqueeze(1).data, 1)
        if weights is None:
            weights = FloatTensor(N).data.fill_(1.0)
        if weights.dim() == 1:
            weights.unsqueeze_(-1)
        return labels, weights


class FCModelWithNanAndInfWts(nn.Module):
    """Simple FC model"""

    def __init__(self):
        super(FCModelWithNanAndInfWts, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 3)
        self.fc1.weight.data.fill_(float('NaN'))
        self.fc2.weight.data.fill_(float('Inf'))

    def forward(self, x):
        x = self.fc1(x)
        return self.fc2(x)


class CRF(nn.Module):
    """
    Compute the log-likelihood of the input assuming a conditional random field
    model.

    Args:
        num_tags: The number of tags
    """

    def __init__(self, num_tags: int, ignore_index: int, default_label_pad_index: int) ->None:
        if num_tags <= 0:
            raise ValueError(f'Invalid number of tags: {num_tags}')
        super().__init__()
        self.num_tags = num_tags
        self.transitions = nn.Parameter(torch.Tensor(num_tags + 2, num_tags + 2))
        self.start_tag = num_tags
        self.end_tag = num_tags + 1
        self.reset_parameters()
        self.ignore_index = ignore_index
        self.default_label_pad_index = default_label_pad_index
        log_class_usage(__class__)

    def reset_parameters(self) ->None:
        nn.init.uniform_(self.transitions, -0.1, 0.1)
        self.transitions.data[:, self.start_tag] = -10000
        self.transitions.data[self.end_tag, :] = -10000

    def get_transitions(self):
        return self.transitions.data

    def set_transitions(self, transitions: torch.Tensor=None):
        self.transitions.data = transitions

    def forward(self, emissions: torch.Tensor, tags: torch.Tensor, reduce: bool=True) ->torch.Tensor:
        """
        Compute log-likelihood of input.

        Args:
            emissions: Emission values for different tags for each input. The
                expected shape is batch_size * seq_len * num_labels. Padding is
                should be on the right side of the input.
            tags: Actual tags for each token in the input. Expected shape is
                batch_size * seq_len
        """
        mask = self._make_mask_from_targets(tags)
        numerator = self._compute_joint_llh(emissions, tags, mask)
        denominator = self._compute_log_partition_function(emissions, mask)
        llh = numerator - denominator
        return llh if not reduce else torch.mean(llh)

    @jit.export
    def decode(self, emissions: torch.Tensor, seq_lens: torch.Tensor) ->torch.Tensor:
        """
        Given a set of emission probabilities, return the predicted tags.

        Args:
            emissions: Emission probabilities with expected shape of
                batch_size * seq_len * num_labels
            seq_lens: Length of each input.
        """
        mask = self._make_mask_from_seq_lens(seq_lens)
        result = self._viterbi_decode(emissions, mask)
        return result

    def _compute_joint_llh(self, emissions: torch.Tensor, tags: torch.Tensor, mask: torch.Tensor) ->torch.Tensor:
        seq_len = emissions.shape[1]
        llh = self.transitions[self.start_tag, tags[:, 0]].unsqueeze(1)
        llh += emissions[:, 0, :].gather(1, tags[:, 0].view(-1, 1)) * mask[:, 0].unsqueeze(1)
        for idx in range(1, seq_len):
            old_state, new_state = tags[:, idx - 1].view(-1, 1), tags[:, idx].view(-1, 1)
            emission_scores = emissions[:, idx, :].gather(1, new_state)
            transition_scores = self.transitions[old_state, new_state]
            llh += (emission_scores + transition_scores) * mask[:, idx].unsqueeze(1)
        last_tag_indices = mask.sum(1, dtype=torch.long) - 1
        last_tags = tags.gather(1, last_tag_indices.view(-1, 1))
        llh += self.transitions[last_tags.squeeze(1), self.end_tag].unsqueeze(1)
        return llh.squeeze(1)

    def _compute_log_partition_function(self, emissions: torch.Tensor, mask: torch.Tensor) ->torch.Tensor:
        seq_len = emissions.shape[1]
        log_prob = emissions[:, 0].clone()
        log_prob += self.transitions[self.start_tag, :self.start_tag].unsqueeze(0)
        for idx in range(1, seq_len):
            broadcast_emissions = emissions[:, idx].unsqueeze(1)
            broadcast_transitions = self.transitions[:self.start_tag, :self.start_tag].unsqueeze(0)
            broadcast_logprob = log_prob.unsqueeze(2)
            score = broadcast_logprob + broadcast_emissions + broadcast_transitions
            score = torch.logsumexp(score, 1)
            log_prob = score * mask[:, idx].unsqueeze(1) + log_prob.squeeze(1) * (1 - mask[:, idx].unsqueeze(1))
        log_prob += self.transitions[:self.start_tag, self.end_tag].unsqueeze(0)
        return torch.logsumexp(log_prob.squeeze(1), 1)

    def _viterbi_decode(self, emissions: torch.Tensor, mask: torch.Tensor) ->torch.Tensor:
        tensor_device = emissions.device
        seq_len = emissions.shape[1]
        mask = mask
        log_prob = emissions[:, 0].clone()
        log_prob += self.transitions[self.start_tag, :self.start_tag].unsqueeze(0)
        end_scores = log_prob + self.transitions[:self.start_tag, self.end_tag].unsqueeze(0)
        best_scores_list: List[torch.Tensor] = []
        empty_data: List[int] = []
        best_paths_list = [torch.tensor(empty_data, device=tensor_device).long()]
        best_scores_list.append(end_scores.unsqueeze(1))
        for idx in range(1, seq_len):
            broadcast_emissions = emissions[:, idx].unsqueeze(1)
            broadcast_transmissions = self.transitions[:self.start_tag, :self.start_tag].unsqueeze(0)
            broadcast_log_prob = log_prob.unsqueeze(2)
            score = broadcast_emissions + broadcast_transmissions + broadcast_log_prob
            max_scores, max_score_indices = torch.max(score, 1)
            best_paths_list.append(max_score_indices.unsqueeze(1))
            end_scores = max_scores + self.transitions[:self.start_tag, self.end_tag].unsqueeze(0)
            best_scores_list.append(end_scores.unsqueeze(1))
            log_prob = max_scores
        best_scores = torch.cat(best_scores_list, 1).float()
        best_paths = torch.cat(best_paths_list, 1)
        _, max_indices_from_scores = torch.max(best_scores, 2)
        valid_index_tensor = torch.tensor(0, device=tensor_device).long()
        if self.ignore_index == self.default_label_pad_index:
            padding_tensor = valid_index_tensor
        else:
            padding_tensor = torch.tensor(self.ignore_index, device=tensor_device).long()
        labels = max_indices_from_scores[:, seq_len - 1]
        labels = self._mask_tensor(labels, 1 - mask[:, seq_len - 1], padding_tensor)
        all_labels = labels.unsqueeze(1).long()
        for idx in range(seq_len - 2, -1, -1):
            indices_for_lookup = all_labels[:, -1].clone()
            indices_for_lookup = self._mask_tensor(indices_for_lookup, indices_for_lookup == self.ignore_index, valid_index_tensor)
            indices_from_prev_pos = best_paths[:, idx, :].gather(1, indices_for_lookup.view(-1, 1).long()).squeeze(1)
            indices_from_prev_pos = self._mask_tensor(indices_from_prev_pos, 1 - mask[:, idx + 1], padding_tensor)
            indices_from_max_scores = max_indices_from_scores[:, idx]
            indices_from_max_scores = self._mask_tensor(indices_from_max_scores, mask[:, idx + 1], padding_tensor)
            labels = torch.where(indices_from_max_scores == self.ignore_index, indices_from_prev_pos, indices_from_max_scores)
            labels = self._mask_tensor(labels, 1 - mask[:, idx], padding_tensor)
            all_labels = torch.cat((all_labels, labels.view(-1, 1).long()), 1)
        return torch.flip(all_labels, [1])

    def _make_mask_from_targets(self, targets):
        mask = targets.ne(self.ignore_index).float()
        return mask

    def _make_mask_from_seq_lens(self, seq_lens):
        seq_lens = seq_lens.view(-1, 1)
        max_len = torch.max(seq_lens)
        range_tensor = torch.arange(max_len, device=seq_lens.device).unsqueeze(0)
        range_tensor = range_tensor.expand(seq_lens.size(0), range_tensor.size(1))
        mask = (range_tensor < seq_lens).float()
        return mask

    def _mask_tensor(self, score_tensor, mask_condition, mask_value):
        masked_tensor = torch.where(mask_condition, mask_value, score_tensor)
        return masked_tensor

    def export_to_caffe2(self, workspace, init_net, predict_net, logits_output_name):
        """
        Exports the crf layer to caffe2 by manually adding the necessary operators
        to the init_net and predict net.

        Args:
            init_net: caffe2 init net created by the current graph
            predict_net: caffe2 net created by the current graph
            workspace: caffe2 current workspace
            output_names: current output names of the caffe2 net
            py_model: original pytorch model object

        Returns:
            string: The updated predictions blob name
        """
        crf_transitions = init_net.AddExternalInput(init_net.NextName())
        workspace.FeedBlob(str(crf_transitions), self.get_transitions().numpy())
        logits_squeezed = predict_net.Squeeze(logits_output_name, dims=[0])
        new_logits = apply_crf(init_net, predict_net, crf_transitions, logits_squeezed, self.num_tags)
        new_logits = predict_net.ExpandDims(new_logits, dims=[0])
        predict_net.Copy(new_logits, logits_output_name)
        return logits_output_name


class Stage(Enum):
    TRAIN = 'Training'
    EVAL = 'Evaluation'
    TEST = 'Test'
    OTHERS = 'Others'
    PERSONALIZED_EVAL = 'Personalized Eval'
    PERSONALIZED_TEST = 'Personalized Test'


class DistributedModel(nn.parallel.DistributedDataParallel):
    """
    Wrapper model class to train models in distributed data parallel manner.
    The way to use this class to train your module in distributed manner is::

        distributed_model = DistributedModel(
            module=model,
            device_ids=[device_id0, device_id1],
            output_device=device_id0,
            broadcast_buffers=False,
        )


    where, `model` is the object of the actual model class you want to train in
    distributed manner.
    """

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        log_class_usage(__class__)

    def __getattr__(self, name):
        wrapped_module = super().__getattr__('module')
        if hasattr(wrapped_module, name):
            return getattr(wrapped_module, name)
        return super().__getattr__(name)

    def cpu(self):
        wrapped_module = super().__getattr__('module')
        return wrapped_module.cpu()

    def state_dict(self, *args, **kwargs):
        wrapped_module = super().__getattr__('module')
        return wrapped_module.state_dict(*args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        wrapped_module = super().__getattr__('module')
        return wrapped_module.load_state_dict(*args, **kwargs)

    def train(self, mode=True):
        """
        Override to set stage
        """
        super().train(mode)
        self._set_module_stage(Stage.TRAIN)

    def eval(self, stage=Stage.TEST):
        """
        Override to set stage
        """
        super().eval()
        self._set_module_stage(stage)

    def _set_module_stage(self, stage):
        wrapped_module = super().__getattr__('module')
        if hasattr(wrapped_module, 'stage'):
            wrapped_module.stage = stage


class EmbeddingBase(Module):
    """Base class for token level embedding modules.

    Args:
        embedding_dim (int): Size of embedding vector.

    Attributes:
        num_emb_modules (int): Number of ways to embed a token.
        embedding_dim (int): Size of embedding vector.

    """
    __EXPANSIBLE__ = True

    def __init__(self, embedding_dim: int):
        super().__init__()
        self.num_emb_modules = 1
        self.embedding_dim = embedding_dim
        log_class_usage(__class__)

    def visualize(self, summary_writer: SummaryWriter):
        """
        Overridden in sub classes to implement Tensorboard visualization of
        embedding space
        """
        pass


class EmbeddingList(EmbeddingBase, ModuleList):
    """
    There are more than one way to embed a token and this module provides a way
    to generate a list of sub-embeddings, concat embedding tensors into a single
    Tensor or return a tuple of Tensors that can be used by downstream modules.

    Args:
        embeddings (Iterable[EmbeddingBase]): A sequence of embedding modules to
        embed a token.
        concat (bool): Whether to concatenate the embedding vectors emitted from
        `embeddings` modules.

    Attributes:
        num_emb_modules (int): Number of flattened embeddings in `embeddings`,
            e.g: ((e1, e2), e3) has 3 in total
        input_start_indices (List[int]): List of indices of the sub-embeddings
            in the embedding list.
        concat (bool): Whether to concatenate the embedding vectors emitted from
            `embeddings` modules.
        embedding_dim: Total embedding size, can be a single int or tuple of
            int depending on concat setting
    """

    def __init__(self, embeddings: Iterable[EmbeddingBase], concat: bool) ->None:
        EmbeddingBase.__init__(self, 0)
        embeddings = list(filter(None, embeddings))
        self.num_emb_modules = sum(emb.num_emb_modules for emb in embeddings)
        embeddings_list, input_start_indices = [], []
        start = 0
        for emb in embeddings:
            if emb.embedding_dim > 0:
                embeddings_list.append(emb)
                input_start_indices.append(start)
            start += emb.num_emb_modules
        ModuleList.__init__(self, embeddings_list)
        self.input_start_indices = input_start_indices
        self.concat = concat
        assert len(self) > 0, 'must have at least 1 sub embedding'
        embedding_dims = tuple(emb.embedding_dim for emb in self)
        self.embedding_dim = sum(embedding_dims) if concat else embedding_dims
        log_class_usage(__class__)

    def forward(self, *emb_input) ->Union[torch.Tensor, Tuple[torch.Tensor]]:
        """
        Get embeddings from all sub-embeddings and either concatenate them
        into one Tensor or return them in a tuple.

        Args:
            *emb_input (type): Sequence of token level embeddings to combine.
                The inputs should match the size of configured embeddings. Each
                of them is either a Tensor or a tuple of Tensors.

        Returns:
            Union[torch.Tensor, Tuple[torch.Tensor]]: If `concat` is True then
                a Tensor is returned by concatenating all embeddings. Otherwise
                all embeddings are returned in a tuple.

        """
        if self.num_emb_modules != len(emb_input):
            raise Exception(f'expecting {self.num_emb_modules} embeddings, ' + f'but got {len(emb_input)} input')
        tensors = []
        for emb, start in zip(self, self.input_start_indices):
            end = start + emb.num_emb_modules
            input = emb_input[start:end]
            if len(input) == 1:
                if isinstance(input[0], list) or isinstance(input[0], tuple):
                    [input] = input
            emb_tensor = emb(*input)
            tensors.append(emb_tensor)
        if self.concat:
            return torch.cat(tensors, -1)
        else:
            return tuple(tensors) if len(tensors) > 1 else tensors[0]

    def visualize(self, summary_writer: SummaryWriter):
        for child in self:
            child.visualize(summary_writer)


class ModuleConfig(ConfigBase):
    load_path: Optional[str] = None
    save_path: Optional[str] = None
    freeze: bool = False
    shared_module_key: Optional[str] = None


class IntSingleCategoryEmbedding(EmbeddingBase):
    """Embed Dict of feature_id -> feature value to list of tensors (1 tensor per feature ID) after looking up feature value embedding,
    then apply optional pooling and MLP to final tensor.
    Passed in feature dict keys need to be in fixed order in forward.
    """


    class Config(ModuleConfig):
        embedding_dim: int = 32
        pooling_type: str = 'none'
        mlp_layer_dims: List[int] = []
        feature_buckets: Dict[int, int] = {}

    @classmethod
    def from_config(cls, config: Config):
        """Factory method to construct an instance of DictEmbedding from
        the module's config object and the field's metadata object.

        Args:
            config (DictFeatConfig): Configuration object specifying all the
            parameters of DictEmbedding.
            metadata (FieldMeta): Object containing this field's metadata.

        Returns:
            type: An instance of DictEmbedding.

        """
        return cls(embedding_dim=config.embedding_dim, pooling_type=config.pooling_type, mlp_layer_dims=config.mlp_layer_dims, feature_buckets=config.feature_buckets)

    def __init__(self, embedding_dim: int, pooling_type: str, mlp_layer_dims: List[int], feature_buckets: Dict[int, int]) ->None:
        super().__init__(embedding_dim)
        self.pooling_type = pooling_type
        self.mlp_layer_dims = mlp_layer_dims
        self.num_intput_features = len(feature_buckets)
        input_dim = self.num_intput_features * embedding_dim if self.pooling_type == 'none' else embedding_dim
        self.mlp = nn.Sequential(*(nn.Sequential(nn.Linear(m, n), nn.ReLU()) for m, n in zip([input_dim] + list(mlp_layer_dims), mlp_layer_dims)))
        self.feature_buckets = {int(k): v for k, v in feature_buckets.items()}
        self.feature_embeddings = nn.ModuleDict({str(k): nn.Embedding(v, embedding_dim) for k, v in feature_buckets.items()})
        log_class_usage(__class__)

    def get_output_dim(self):
        if self.mlp_layer_dims:
            return self.mlp_layer_dims[-1]
        if self.pooling_type == 'none':
            return self.num_intput_features * self.embedding_dim
        elif self.pooling_type == 'mean':
            return self.embedding_dim
        elif self.pooling_type == 'max':
            return self.embedding_dim
        else:
            raise RuntimeError(f'Pooling type {self.pooling_type} is unsupported.')

    def forward(self, feats: Dict[int, torch.Tensor]) ->torch.Tensor:
        embeddings: List[torch.Tensor] = []
        for k, buckets in self.feature_buckets.items():
            feat = feats[k]
            feats_remap = torch.remainder(feat, buckets)
            feat_emb: nn.Embedding = self.feature_embeddings[str(k)]
            embeddings.append(feat_emb(feats_remap))
        if self.pooling_type == 'none':
            reduced_embeds = torch.cat(embeddings, dim=1)
        elif self.pooling_type == 'mean':
            reduced_embeds = torch.sum(torch.stack(embeddings, dim=1), dim=1)
        elif self.pooling_type == 'max':
            reduced_embeds, _ = torch.max(torch.stack(embeddings, dim=1), dim=1)
        else:
            raise RuntimeError(f'Pooling type {self.pooling_type} is unsupported.')
        return self.mlp(reduced_embeds)


class IntWeightedMultiCategoryEmbedding(EmbeddingBase):
    """Embed Dict of feature_id -> (feature values, offsets, weights) to list of tensors (1 tensor per feature ID) with EmbeddingBag,
    then apply optional pooling and MLP to final tensor.
    Passed in feature dict keys need to be in fixed order in forward.
    """


    class Config(ModuleConfig):
        embedding_dim: int = 32
        weight_scale: float = 1.0
        embedding_bag_mode: Optional[str] = None
        ignore_weight: Optional[bool] = None
        features_embedding_bag_mode: Dict[int, str] = {}
        pooling_type: str = 'none'
        mlp_layer_dims: List[int] = []
        feature_buckets: Dict[int, int] = {}

    @classmethod
    def from_config(cls, config: Config):
        """Factory method to construct an instance of IntWeightedMultiCategoryEmbedding
        from the module's config object and the field's metadata object.

        Args:
            config (Config): Configuration object specifying all the
            parameters of IntWeightedMultiCategoryEmbedding.
            num_intput_features: Number of input features in forward.

        Returns:
            type: An instance of IntWeightedMultiCategoryEmbedding.

        """
        return cls(embedding_dim=config.embedding_dim, weight_scale=config.weight_scale, features_embedding_bag_mode=config.features_embedding_bag_mode, embedding_bag_mode=config.embedding_bag_mode, ignore_weight=config.ignore_weight, pooling_type=config.pooling_type, mlp_layer_dims=config.mlp_layer_dims, feature_buckets=config.feature_buckets)

    def __init__(self, embedding_dim: int, weight_scale: float, features_embedding_bag_mode: Dict[int, str], embedding_bag_mode: Optional[str], ignore_weight: Optional[bool], pooling_type: str, mlp_layer_dims: List[int], feature_buckets: Dict[int, int]) ->None:
        super().__init__(embedding_dim)
        features_embedding_bag_mode = {int(k): v for k, v in features_embedding_bag_mode.items()}
        if ignore_weight is not None or embedding_bag_mode is not None:
            assert len(features_embedding_bag_mode) == 0, "ignore_weight could only be set in old config and couldn't be set together with features_embedding_bag_mode"
            ignore_weight = ignore_weight or False
            embedding_bag_mode = embedding_bag_mode or 'sum'
            features_embedding_bag_mode = {int(k): (embedding_bag_mode if ignore_weight else 'sum') for k in feature_buckets.keys()}
        self.weight_scale = weight_scale
        self.features_embedding_bag_mode = features_embedding_bag_mode
        self.pooling_type = pooling_type
        self.mlp_layer_dims = mlp_layer_dims
        self.feature_buckets = {int(k): v for k, v in feature_buckets.items()}
        self.feature_embeddings = nn.ModuleDict({str(k): nn.EmbeddingBag(v, embedding_dim, mode=self.features_embedding_bag_mode.get(k, 'sum')) for k, v in feature_buckets.items()})
        self.num_intput_features = len(feature_buckets)
        input_dim = self.num_intput_features * embedding_dim if self.pooling_type == 'none' else embedding_dim
        self.mlp = nn.Sequential(*(nn.Sequential(nn.Linear(m, n), nn.ReLU()) for m, n in zip([input_dim] + list(mlp_layer_dims), mlp_layer_dims)))
        log_class_usage(__class__)

    def get_output_dim(self):
        if self.mlp_layer_dims:
            return self.mlp_layer_dims[-1]
        if self.pooling_type == 'none':
            return self.num_intput_features * self.embedding_dim
        elif self.pooling_type == 'mean':
            return self.embedding_dim
        elif self.pooling_type == 'max':
            return self.embedding_dim
        else:
            raise RuntimeError(f'Pooling type {self.pooling_type} is unsupported.')

    def forward(self, feats: Dict[int, Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]) ->torch.Tensor:
        embeddings: List[torch.Tensor] = []
        for k, buckets in self.feature_buckets.items():
            feat, offsets, weights = feats[k]
            feats_remap = torch.remainder(feat, buckets)
            feat_emb: nn.EmbeddingBag = self.feature_embeddings[str(k)]
            embeddings.append(feat_emb(feats_remap, offsets=offsets, per_sample_weights=weights if self.features_embedding_bag_mode.get(k, 'sum') == 'sum' else None) * self.weight_scale)
        if self.pooling_type == 'none':
            reduced_embeds = torch.cat(embeddings, dim=1)
        elif self.pooling_type == 'mean':
            reduced_embeds = torch.sum(torch.stack(embeddings, dim=1), dim=1)
        elif self.pooling_type == 'max':
            reduced_embeds, _ = torch.max(torch.stack(embeddings, dim=1), dim=1)
        else:
            raise RuntimeError(f'Pooling type {self.pooling_type} is unsupported.')
        return self.mlp(reduced_embeds)


class Vocab(object):
    """Defines a vocabulary object that will be used to numericalize a field.

    Attributes:
        freqs: A collections.Counter object holding the frequencies of tokens
            in the data used to build the Vocab.
        stoi: A collections.defaultdict instance mapping token strings to
            numerical identifiers.
        itos: A list of token strings indexed by their numerical identifiers.
    """
    UNK = '<unk>'

    def __init__(self, counter, max_size=None, min_freq=1, specials=('<unk>', '<pad>'), vectors=None, unk_init=None, vectors_cache=None, specials_first=True):
        """Create a Vocab object from a collections.Counter.

        Args:
            counter: collections.Counter object holding the frequencies of
                each value found in the data.
            max_size: The maximum size of the vocabulary, or None for no
                maximum. Default: None.
            min_freq: The minimum frequency needed to include a token in the
                vocabulary. Values less than 1 will be set to 1. Default: 1.
            specials: The list of special tokens (e.g., padding or eos) that
                will be prepended to the vocabulary. Default: ['<unk'>, '<pad>']
            vectors: One of either the available pretrained vectors
                or custom pretrained vectors (see Vocab.load_vectors);
                or a list of aforementioned vectors
            unk_init (callback): by default, initialize out-of-vocabulary word vectors
                to zero vectors; can be any function that takes in a Tensor and
                returns a Tensor of the same size. Default: 'torch.zeros'
            vectors_cache: directory for cached vectors. Default: '.vector_cache'
            specials_first: Whether to add special tokens into the vocabulary at first.
                If it is False, they are added into the vocabulary at last.
                Default: True.
        """
        self.freqs = counter
        counter = counter.copy()
        min_freq = max(min_freq, 1)
        self.itos = []
        self.unk_index = None
        if specials_first:
            self.itos = list(specials)
            max_size = None if max_size is None else max_size + len(specials)
        for tok in specials:
            del counter[tok]
        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])
        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)
        for word, freq in words_and_frequencies:
            if freq < min_freq or len(self.itos) == max_size:
                break
            self.itos.append(word)
        if Vocab.UNK in specials:
            unk_index = specials.index(Vocab.UNK)
            self.unk_index = unk_index if specials_first else len(self.itos) + unk_index
            self.stoi = defaultdict(self._default_unk_index)
        else:
            self.stoi = defaultdict()
        if not specials_first:
            self.itos.extend(list(specials))
        self.stoi.update({tok: i for i, tok in enumerate(self.itos)})
        self.vectors = None
        if vectors is not None:
            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)
        else:
            assert unk_init is None and vectors_cache is None

    def _default_unk_index(self):
        return self.unk_index

    def __getitem__(self, token):
        return self.stoi.get(token, self.stoi.get(Vocab.UNK))

    def __getstate__(self):
        attrs = dict(self.__dict__)
        attrs['stoi'] = dict(self.stoi)
        return attrs

    def __setstate__(self, state):
        if state.get('unk_index', None) is None:
            stoi = defaultdict()
        else:
            stoi = defaultdict(self._default_unk_index)
        stoi.update(state['stoi'])
        state['stoi'] = stoi
        self.__dict__.update(state)

    def __eq__(self, other):
        if self.freqs != other.freqs:
            return False
        if self.stoi != other.stoi:
            return False
        if self.itos != other.itos:
            return False
        if self.vectors != other.vectors:
            return False
        return True

    def __len__(self):
        return len(self.itos)

    def lookup_indices(self, tokens):
        indices = [self.__getitem__(token) for token in tokens]
        return indices

    def extend(self, v, sort=False):
        words = sorted(v.itos) if sort else v.itos
        for w in words:
            if w not in self.stoi:
                self.itos.append(w)
                self.stoi[w] = len(self.itos) - 1

    def load_vectors(self, vectors, **kwargs):
        """
        Args:
            vectors: one of or a list containing instantiations of the
                GloVe, CharNGram, or Vectors classes. Alternatively, one
                of or a list of available pretrained vectors:

                charngram.100d
                fasttext.en.300d
                fasttext.simple.300d
                glove.42B.300d
                glove.840B.300d
                glove.twitter.27B.25d
                glove.twitter.27B.50d
                glove.twitter.27B.100d
                glove.twitter.27B.200d
                glove.6B.50d
                glove.6B.100d
                glove.6B.200d
                glove.6B.300d

            Remaining keyword arguments: Passed to the constructor of Vectors classes.
        """
        if not isinstance(vectors, list):
            vectors = [vectors]
        for idx, vector in enumerate(vectors):
            if isinstance(vector, str):
                if vector not in pretrained_aliases:
                    raise ValueError('Got string input vector {}, but allowed pretrained vectors are {}'.format(vector, list(pretrained_aliases.keys())))
                vectors[idx] = pretrained_aliases[vector](**kwargs)
            elif not isinstance(vector, Vectors):
                raise ValueError('Got input vectors of type {}, expected str or Vectors object'.format(type(vector)))
        tot_dim = sum(v.dim for v in vectors)
        self.vectors = torch.Tensor(len(self), tot_dim)
        for i, token in enumerate(self.itos):
            start_dim = 0
            for v in vectors:
                end_dim = start_dim + v.dim
                self.vectors[i][start_dim:end_dim] = v[token.strip()]
                start_dim = end_dim
            assert start_dim == tot_dim

    def set_vectors(self, stoi, vectors, dim, unk_init=torch.Tensor.zero_):
        """
        Set the vectors for the Vocab instance from a collection of Tensors.

        Args:
            stoi: A dictionary of string to the index of the associated vector
                in the `vectors` input argument.
            vectors: An indexed iterable (or other structure supporting __getitem__) that
                given an input index, returns a FloatTensor representing the vector
                for the token associated with the index. For example,
                vector[stoi["string"]] should return the vector for "string".
            dim: The dimensionality of the vectors.
            unk_init (callback): by default, initialize out-of-vocabulary word vectors
                to zero vectors; can be any function that takes in a Tensor and
                returns a Tensor of the same size. Default: 'torch.zeros'
        """
        self.vectors = torch.Tensor(len(self), dim)
        for i, token in enumerate(self.itos):
            wv_index = stoi.get(token, None)
            if wv_index is not None:
                self.vectors[i] = vectors[wv_index]
            else:
                self.vectors[i] = unk_init(self.vectors[i])


class FieldMeta:
    vocab: Vocab
    vocab_size: int
    vocab_export_name: str
    pad_token_idx: int
    unk_token_idx: int
    init_token_idx: int
    eos_token_idx: int
    nesting_meta: Any
    dummy_model_input: Union[torch.Tensor, Tuple[torch.Tensor, ...], None]


class lazy_property(object):
    """
    More or less copy-pasta: http://stackoverflow.com/a/6849299
    Meant to be used for lazy evaluation of an object attribute.
    property should represent non-mutable data, as it replaces itself.
    """

    def __init__(self, fget):
        self._fget = fget
        self.__doc__ = fget.__doc__
        self.__name__ = fget.__name__

    def __get__(self, obj, obj_cls_type):
        if obj is None:
            return None
        value = self._fget(obj)
        setattr(obj, self.__name__, value)
        return value


class Tensorizer(Component):
    """Tensorizers are a component that converts from batches of
    `pytext.data.type.DataType` instances to tensors. These tensors will eventually
    be inputs to the model, but the model is aware of the tensorizers and can arrange
    the tensors they create to conform to its model.

    Tensorizers have an initialize function. This function allows the tensorizer to
    read through the training dataset to build up any data that it needs for
    creating the model. Commonly this is valuable for things like inferring a
    vocabulary from the training set, or learning the entire set of training labels,
    or slot labels, etc.
    """
    __COMPONENT_TYPE__ = ComponentType.TENSORIZER
    __EXPANSIBLE__ = True
    __TENSORIZER_SCRIPT_IMPL__ = None


    class Config(Component.Config):
        is_input: bool = True

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.is_input)

    def __init__(self, is_input: bool=True):
        self.is_input = is_input
        log_class_usage(__class__)

    @property
    def column_schema(self):
        """Generic types don't pickle well pre-3.7, so we don't actually want
        to store the schema as an attribute. We're already storing all of the
        columns anyway, so until there's a better solution, schema is a property."""
        return []

    def numberize(self, row):
        raise NotImplementedError

    def prepare_input(self, row):
        """Return preprocessed input tensors/blob for caffe2 prediction net."""
        return self.numberize(row)

    def sort_key(self, row):
        raise NotImplementedError

    def tensorize(self, batch):
        """Tensorizer knows how to pad and tensorize a batch of it's own output."""
        return batch

    def initialize(self, from_scratch=True):
        """
        The initialize function is carefully designed to allow us to read through the
        training dataset only once, and not store it in memory. As such, it can't itself
        manually iterate over the data source. Instead, the initialize function is a
        coroutine, which is sent row data. This should look roughly like::

            # set up variables here
            ...
            try:
                # start reading through data source
                while True:
                    # row has type Dict[str, types.DataType]
                    row = yield
                    # update any variables, vocabularies, etc.
                    ...
            except GeneratorExit:
                # finalize your initialization, set instance variables, etc.
                ...

        See `WordTokenizer.initialize` for a more concrete example.
        """
        return
        yield

    @lazy_property
    def tensorizer_script_impl(self):
        raise NotImplementedError

    def __getstate__(self):
        state = copy.copy(vars(self))
        state.pop('tensorizer_script_impl', None)
        return state

    def stringify(self, token_indices):
        res = ''
        if hasattr(self, 'vocab'):
            res = ' '.join([self.vocab._vocab[index] for index in token_indices])
            if hasattr(self, 'tokenizer'):
                if hasattr(self.tokenizer, 'decode'):
                    res = self.tokenizer.decode(res)
        return res

    def torchscriptify(self):
        return self.tensorizer_script_impl.torchscriptify()


class ScriptableEmbeddingList(EmbeddingBase):
    """

    This class is a Torchscript-friendly version of
    pytext.models.embeddings.EmbeddingList. The main differences are that it
    requires input arguments to be passed in as a list of Tensors, since
    Torchscript does not allow variable arguments, and that it only supports
    concat mode, since Torchscript does not support return value variance.

    """


    class Wrapper1(torch.nn.Module):

        def __init__(self, embedding: EmbeddingBase):
            super().__init__()
            self._embedding = embedding

        def forward(self, xs: List[torch.Tensor]):
            return self._embedding(xs[0])


    class Wrapper2(torch.nn.Module):

        def __init__(self, embedding: EmbeddingBase):
            super().__init__()
            self._embedding = embedding

        def forward(self, xs: List[torch.Tensor]):
            return self._embedding(xs[0], xs[1])


    class Wrapper3(torch.nn.Module):

        def __init__(self, embedding: EmbeddingBase):
            super().__init__()
            self._embedding = embedding

        def forward(self, xs: List[torch.Tensor]):
            return self._embedding(xs[0], xs[1], xs[2])

    @staticmethod
    def _adapt_embedding(embedding: torch.nn.Module) ->torch.nn.Module:
        param_count = len([p for p in signature(embedding.forward).parameters.values() if p.default == inspect._empty])
        if param_count == 1:
            return ScriptableEmbeddingList.Wrapper1(embedding)
        elif param_count == 2:
            return ScriptableEmbeddingList.Wrapper2(embedding)
        elif param_count == 3:
            return ScriptableEmbeddingList.Wrapper3(embedding)
        raise AssertionError(f'Unsupported parameter count {param_count}. If a new embedding class has been added, you will need to add support in this class.')

    def __init__(self, embeddings: Iterable[EmbeddingBase]):
        EmbeddingBase.__init__(self, 0)
        embeddings = list(filter(None, embeddings))
        self.num_emb_modules = sum(emb.num_emb_modules for emb in embeddings)
        embeddings_list: List[EmbeddingBase] = []
        input_start_indices: List[int] = []
        start = 0
        embedding_dim = 0
        for emb in embeddings:
            if emb.embedding_dim > 0:
                embeddings_list.append(emb)
                input_start_indices.append(start)
                embedding_dim += emb.embedding_dim
            start += emb.num_emb_modules
        self.embeddings_list = torch.nn.ModuleList(map(ScriptableEmbeddingList._adapt_embedding, embeddings_list))
        self.input_start_indices: Tuple[int] = tuple(input_start_indices)
        assert len(self.embeddings_list) > 0, 'must have at least 1 sub embedding'
        self.embedding_dim = embedding_dim

    def forward(self, emb_input: List[List[torch.Tensor]]) ->torch.Tensor:
        """
        Get embeddings from all sub-embeddings and either concatenate them
        into one Tensor or return them in a tuple.

        Args:
            emb_input (type): Sequence of token level embeddings to combine.
                The inputs should match the size of configured embeddings. Each
                of them is a List of Tensors.

        Returns:
            torch.Tensor: a Tensor is returned by concatenating all embeddings.

        """
        if self.num_emb_modules != len(emb_input):
            raise Exception(f'expecting {self.num_emb_modules} embeddings, ' + f'but got {len(emb_input)} input')
        tensors = []
        for emb, start in zip(self.embeddings_list, self.input_start_indices):
            tensors.append(emb(emb_input[start]))
        return torch.cat(tensors, 2)

    def visualize(self, summary_writer: SummaryWriter):
        for child in self:
            child.visualize(summary_writer)


class BiLSTM(torch.nn.Module):
    """Wrapper for nn.LSTM

    Differences include:
    * weight initialization
    * the bidirectional option makes the first layer bidirectional only
    (and in that case the hidden dim is divided by 2)
    """

    @staticmethod
    def LSTM(input_size, hidden_size, **kwargs):
        m = torch.nn.LSTM(input_size, hidden_size, **kwargs)
        for name, param in m.named_parameters():
            if 'weight' in name or 'bias' in name:
                param.data.uniform_(-0.1, 0.1)
        return m

    def __init__(self, num_layers, bidirectional, embed_dim, hidden_dim, dropout):
        super().__init__()
        self.num_layers = num_layers
        self.bidirectional = bidirectional
        if bidirectional:
            assert hidden_dim % 2 == 0, 'hidden_dim should be even if bidirectional'
        self.hidden_dim = hidden_dim
        self.layers = torch.nn.ModuleList([])
        for layer in range(num_layers):
            is_layer_bidirectional = bidirectional and layer == 0
            if is_layer_bidirectional:
                assert hidden_dim % 2 == 0, 'hidden_dim must be even if bidirectional (to be divided evenly between directions)'
            self.layers.append(BiLSTM.LSTM(embed_dim if layer == 0 else hidden_dim, hidden_dim // 2 if is_layer_bidirectional else hidden_dim, num_layers=1, dropout=dropout, bidirectional=is_layer_bidirectional))
        log_class_usage(__class__)

    def forward(self, embeddings: torch.Tensor, lengths: torch.Tensor, enforce_sorted: bool=True):
        bsz = embeddings.size()[1]
        packed_input = pack_padded_sequence(embeddings, lengths, enforce_sorted=enforce_sorted)
        final_hiddens, final_cells = [], []
        for i, rnn_layer in enumerate(self.layers):
            if self.bidirectional and i == 0:
                h0 = embeddings.new_full((2, bsz, self.hidden_dim // 2), 0)
                c0 = embeddings.new_full((2, bsz, self.hidden_dim // 2), 0)
            else:
                h0 = embeddings.new_full((1, bsz, self.hidden_dim), 0)
                c0 = embeddings.new_full((1, bsz, self.hidden_dim), 0)
            current_output, (h_last, c_last) = rnn_layer(packed_input, (h0, c0))
            if self.bidirectional and i == 0:
                h_last = torch.cat((h_last[0, :, :], h_last[1, :, :]), dim=1)
                c_last = torch.cat((c_last[0, :, :], c_last[1, :, :]), dim=1)
            else:
                h_last = h_last.squeeze(dim=0)
                c_last = c_last.squeeze(dim=0)
            final_hiddens.append(h_last)
            final_cells.append(c_last)
            packed_input = current_output
        final_hidden_size_list: List[int] = final_hiddens[0].size()
        final_hidden_size: Tuple[int, int] = (final_hidden_size_list[0], final_hidden_size_list[1])
        final_hiddens = torch.cat(final_hiddens, dim=0).view(self.num_layers, *final_hidden_size)
        final_cell_size_list: List[int] = final_cells[0].size()
        final_cell_size: Tuple[int, int] = (final_cell_size_list[0], final_cell_size_list[1])
        final_cells = torch.cat(final_cells, dim=0).view(self.num_layers, *final_cell_size)
        unpacked_output, _ = pad_packed_sequence(packed_input)
        return unpacked_output, final_hiddens, final_cells


def _assert_tensorizer_type(t):
    if t is not type(None) and not issubclass(t, Tensorizer.Config):
        raise TypeError(f'ModelInput configuration should only include tensorizers: {t}')


class ModelInputMeta(ConfigBaseMeta):

    def __new__(metacls, typename, bases, namespace):
        annotations = namespace.get('__annotations__', {})
        for t in annotations.values():
            if getattr(t, '__origin__', '') is Union:
                for ut in t.__args__:
                    _assert_tensorizer_type(ut)
            else:
                _assert_tensorizer_type(t)
        return super().__new__(metacls, typename, bases, namespace)


class ModelInputBase(ConfigBase, metaclass=ModelInputMeta):
    """Base class for model inputs."""


def is_absolute_path(file_path: str) ->bool:
    if file_path:
        return file_path.startswith('/') or file_path.startswith('manifold://')
    else:
        return False


FP16_ENABLED = False


def maybe_float(tensor):
    if FP16_ENABLED and tensor.type().split('.')[-1] == 'HalfTensor':
        return tensor.float()
    else:
        return tensor


class BaseModel(nn.Module, Component):
    """
    Base model class which inherits from nn.Module. Also has a stage flag to
    indicate it's in `train`, `eval`, or `test` stage.
    This is because the built-in train/eval flag in PyTorch can't distinguish eval
    and test, which is required to support some use cases.
    """
    __EXPANSIBLE__ = True
    __COMPONENT_TYPE__ = ComponentType.MODEL
    SUPPORT_FP16_OPTIMIZER = False


    class Config(Component.Config):


        class ModelInput(ModelInputBase):
            pass
        inputs: ModelInput = ModelInput()

    def __init__(self, stage: Stage=Stage.TRAIN) ->None:
        nn.Module.__init__(self)
        self.stage = stage
        self.module_list: List[nn.Module] = []
        self.find_unused_parameters = True
        if self.__class__ != __class__:
            log_class_usage(__class__)
            log_class_usage(self.__class__)

    def train(self, mode=True):
        """Override to explicitly maintain the stage (train, eval, test)."""
        super().train(mode)
        self.stage = Stage.TRAIN

    def eval(self, stage=Stage.TEST):
        """Override to explicitly maintain the stage (train, eval, test)."""
        super().eval()
        self.stage = stage

    def contextualize(self, context):
        """Add additional context into model. `context` can be anything that
        helps maintaining/updating state. For example, it is used by
        :class:`~DisjointMultitaskModel` for changing the task that should be
        trained with a given iterator.
        """
        self.context = context

    def get_loss(self, logit, target, context):
        return self.output_layer.get_loss(logit, target, context)

    def get_pred(self, logit, target=None, context=None, *args):
        return self.output_layer.get_pred(logit, target, context)

    def save_modules(self, base_path: str='', suffix: str=''):
        """Save each sub-module in separate files for reusing later."""

        def save(module):
            save_path = getattr(module, 'save_path', None)
            if save_path:
                if is_absolute_path(save_path):
                    path = save_path + suffix
                else:
                    path = os.path.join(base_path, save_path + suffix)
                None
                with PathManager.open(path, 'wb') as save_file:
                    if isinstance(module, torch.jit.ScriptModule):
                        module.save(save_file)
                    else:
                        torch.save(module.state_dict(), save_file)
        self.apply(save)

    def prepare_for_onnx_export_(self, **kwargs):
        """Make model exportable via ONNX trace."""

        def apply_prepare_for_onnx_export_(module):
            if module != self and hasattr(module, 'prepare_for_onnx_export_'):
                module.prepare_for_onnx_export_(**kwargs)
        self.apply(apply_prepare_for_onnx_export_)

    def quantize(self):
        """Quantize the model during export."""
        tq.quantize_dynamic(self, {torch.nn.Linear, torch.nn.LSTM}, dtype=torch.qint8, inplace=True)

    @classmethod
    def train_batch(cls, model, batch, state=None):
        model_inputs = model.arrange_model_inputs(batch)
        model_context = model.arrange_model_context(batch)
        targets = model.arrange_targets(batch)
        model_outputs = model(*model_inputs)
        if state:
            if model_context is None:
                model_context = {'stage': state.stage}
            else:
                model_context['stage'] = state.stage
        loss = maybe_float(model.get_loss(model_outputs, targets, model_context))
        predictions, scores = model.get_pred(model_outputs, context=model_context)
        metric_data = predictions, targets, scores, loss, model_inputs
        return loss, metric_data

    def arrange_model_inputs(self, tensor_dict):
        pass

    def arrange_targets(self, tensor_dict):
        pass

    def arrange_model_context(self, tensor_dict):
        return None

    def onnx_trace_input(self, tensor_dict):
        return self.arrange_model_inputs(tensor_dict)

    def trace(self, inputs):
        return torch.jit.trace(self, inputs)

    def caffe2_export(self, tensorizers, tensor_dict, path, export_onnx_path=None):
        pass

    def arrange_caffe2_model_inputs(self, tensor_dict):
        """
        Generate inputs for exported caffe2 model, default behavior is flatten the
        input tuples
        """
        model_inputs = self.arrange_model_inputs(tensor_dict)
        flat_model_inputs = []
        for model_input in model_inputs:
            if isinstance(model_input, tuple):
                flat_model_inputs.extend(model_input)
            else:
                flat_model_inputs.append(model_input)
        return flat_model_inputs

    def get_num_examples_from_batch(self, batch):
        pass


class CommonMetadata:
    features: Dict[str, FieldMeta]
    target: FieldMeta
    dataset_sizes: Dict[str, int]


class DecoderBase(Module):
    """Base class for all decoder modules.

    Args:
        config (ConfigBase): Configuration object.

    Attributes:
        in_dim (int): Dimension of input Tensor passed to the decoder.
        out_dim (int): Dimension of output Tensor produced by the decoder.

    """

    def __init__(self, config: ConfigBase):
        super().__init__(config)
        self.input_dim = 0
        self.target_dim = 0
        self.num_decoder_modules = 0
        log_class_usage(__class__)

    def forward(self, *input):
        raise NotImplementedError()

    def get_decoder(self):
        """Returns the decoder module."""
        raise NotImplementedError()

    def get_in_dim(self) ->int:
        """Returns the dimension of the input Tensor that the decoder accepts."""
        return self.in_dim

    def get_out_dim(self) ->int:
        """Returns the dimension of the input Tensor that the decoder emits."""
        return self.out_dim


class CNNParams(ConfigBase):
    kernel_num: int = 100
    kernel_sizes: List[int] = [3, 4]
    weight_norm: bool = False
    dilated: bool = False
    causal: bool = False


class ConnectionConfig(ConfigBase):
    connection_type: str = 'highway'
    num_layers: int = 0
    dropout: float = 0.1


class CharFeatConfig(ModuleConfig):
    embed_dim: int = 100
    sparse: bool = False
    cnn: CNNParams = CNNParams()
    connection: ConnectionConfig = ConnectionConfig()
    highway_layers: Optional[int] = None
    projection_dim: Optional[int] = None
    export_input_names: List[str] = ['char_vals']
    vocab_from_train_data: bool = True
    max_word_length: int = 20
    min_freq: int = 1
    dedicated_context_embedding: bool = False


class ContextualTokenEmbeddingConfig(ConfigBase):
    embed_dim: int = 0
    model_paths: Optional[Dict[str, str]] = None
    export_input_names: List[str] = ['contextual_token_embedding']
    downsample_dim: Optional[int] = None


class PoolingType(Enum):
    MEAN = 'mean'
    MAX = 'max'
    LOGSUMEXP = 'logsumexp'
    NONE = 'none'


class DictFeatConfig(ModuleConfig):
    embed_dim: int = 100
    sparse: bool = False
    pooling: PoolingType = PoolingType.MEAN
    export_input_names: List[str] = ['dict_vals', 'dict_weights', 'dict_lens']
    vocab_from_train_data: bool = True
    mobile: bool = False
    use_weights: bool = True


class FloatVectorConfig(ConfigBase):
    dim: int = 0
    export_input_names: List[str] = ['float_vec_vals']
    dim_error_check: bool = False


class ModelInput:
    TEXT = 'word_feat'
    DICT = 'dict_feat'
    CHAR = 'char_feat'
    CONTEXTUAL_TOKEN_EMBEDDING = 'contextual_token_embedding'
    SEQ = 'seq_word_feat'
    DENSE = 'dense_feat'


class RepresentationBase(Module):

    def __init__(self, config):
        super().__init__(config)
        self.representation_dim = None

    def forward(self, *inputs):
        raise NotImplementedError()

    def get_representation_dim(self):
        return self.representation_dim

    def _preprocess_inputs(self, inputs):
        raise NotImplementedError()


def create_component(component_type: ComponentType, config: Any, *args, **kwargs):
    config_cls = type(config)
    cls = Registry.get(component_type, config_cls)
    try:
        return cls.from_config(config, *args, **kwargs)
    except TypeError as e:
        raise Exception(f"Can't create component {cls}: {str(e)}")


def _create_module_from_registry(module_config, *args, **kwargs):
    return create_component(ComponentType.MODULE, module_config, *args, **kwargs)


def create_module(module_config, *args, create_fn=_create_module_from_registry, **kwargs):
    """Create module object given the module's config object. It depends on the
    global shared module registry. Hence, your module must be available for the
    registry. This entails that your module must be imported somewhere in the
    code path during module creation (ideally in your model class) for the module
    to be visible for registry.

    Args:
        module_config (type): Module config object.
        create_fn (type): The function to use for creating the module. Use this
            parameter if your module creation requires custom code and pass your
            function here. Defaults to `_create_module_from_registry()`.

    Returns:
        type: Description of returned object.

    """
    shared_module_key = getattr(module_config, 'shared_module_key', None)
    typed_shared_module_key = shared_module_key, type(module_config)
    load_path = getattr(module_config, 'load_path', None)
    module = SHARED_MODULE_REGISTRY.get(typed_shared_module_key)
    if not module:
        if load_path:
            with PathManager.open(load_path, 'rb') as load_file:
                loaded_module = torch.load(load_file, map_location='cpu')
            if isinstance(loaded_module, dict):
                module = create_fn(module_config, *args, **kwargs)
                module.load_state_dict(loaded_module)
            else:
                module = loaded_module
            name = type(module).__name__
            None
        else:
            module = create_fn(module_config, *args, **kwargs)
    name = type(module).__name__
    if getattr(module_config, 'freeze', False):
        None
        module.freeze()
    if shared_module_key:
        SHARED_MODULE_REGISTRY[typed_shared_module_key] = module
    module.save_path = getattr(module_config, 'save_path', None)
    return module


class Module(nn.Module, Component):
    """Generic module class that serves as base class for all PyText modules.

    Args:
        config (type): Module's `config` object. Specific contents of this object
            depends on the module. Defaults to None.

    """
    Config = ModuleConfig
    __COMPONENT_TYPE__ = ComponentType.MODULE

    def __init__(self, config=None) ->None:
        nn.Module.__init__(self)
        Component.__init__(self, config)

    def freeze(self) ->None:
        for param in self.parameters():
            param.requires_grad = False


class ClassificationScores(jit.ScriptModule):

    def __init__(self, classes, score_function, score_function_dim=None):
        super().__init__()
        self.classes = jit.Attribute(classes, List[str])
        self.score_function = score_function
        self.score_function_dim = score_function_dim

    @jit.script_method
    def forward(self, logits: torch.Tensor):
        if self.score_function_dim is None:
            scores = self.score_function(logits)
        else:
            scores = self.score_function(logits, dim=self.score_function_dim)
        results = jit.annotate(List[Dict[str, float]], [])
        for example_scores in scores.chunk(len(scores)):
            example_scores = example_scores.squeeze(dim=0)
            example_response = jit.annotate(Dict[str, float], {})
            for i in range(len(self.classes)):
                example_response[self.classes[i]] = float(example_scores[i].item())
            results.append(example_response)
        return results


class RegressionScores(torch.jit.ScriptModule):

    def __init__(self, squash_to_unit_range: bool):
        super().__init__()
        self.squash_to_unit_range = torch.jit.Attribute(squash_to_unit_range, bool)

    @torch.jit.script_method
    def forward(self, logits: torch.Tensor) ->List[float]:
        prediction = logits.squeeze(dim=1)
        if self.squash_to_unit_range:
            prediction = torch.sigmoid(prediction)
        scores: List[float] = prediction.tolist()
        return scores


def query_word_reprs(encoder_repr: Tensor, token_indices: Tensor) ->Tensor:
    """
    Given an encoder_repr (B x T_1 x H) and token_indices (B x T_2) where T_2 <= T_1,
    collect embeddings from encoder_repr pertaining to indices in token_indices. In the
    context of fine-tuning pre-trained encoders on sequence labeling, our goal is to
    build token-level representations as opposed to subword-level represenatations
    for alignment with other token-level cues, such as dictionary features. Currently,
    a token representation is built by taking its first subword representation.
    """
    return torch.gather(encoder_repr, 1, token_indices.unsqueeze(2).expand(-1, -1, encoder_repr.size(-1)))


class IntentSlotScores(nn.Module):

    def __init__(self, doc_scores: jit.ScriptModule, word_scores: jit.ScriptModule):
        super().__init__()
        self.doc_scores = doc_scores
        self.word_scores = word_scores
        log_class_usage(__class__)

    def forward(self, logits: Tuple[torch.Tensor, torch.Tensor], context: Dict[str, torch.Tensor]) ->Tuple[List[Dict[str, float]], List[List[Dict[str, float]]]]:
        d_logits, w_logits = logits
        if 'token_indices' in context:
            w_logits = query_word_reprs(w_logits, context['token_indices'])
        d_results = self.doc_scores(d_logits)
        w_results = self.word_scores(w_logits, context)
        return d_results, w_results


class MultiLabelClassificationScores(nn.Module):

    def __init__(self, scores: List[jit.ScriptModule]):
        super().__init__()
        self.scores = nn.ModuleList(scores)
        log_class_usage(__class__)

    def forward(self, logits: List[torch.Tensor]) ->List[List[Dict[str, float]]]:
        results: List[List[Dict[str, float]]] = []
        for idx, sc in enumerate(self.scores):
            logit = logits[idx]
            flattened_logit = logit.view(-1, logit.size()[-1])
            results.append(sc(flattened_logit))
        return results


class BinaryFocalLoss(Loss):
    """
    Focal loss for binary classification
    refrence: https://arxiv.org/pdf/1708.02002.pdf
    """


    class Config(ConfigBase):
        alpha: float = 0.25
        gamma: float = 2.0
        logits: bool = True

    def __init__(self, config, *args, **kwargs):
        self.alpha = config.alpha
        self.gamma = config.gamma
        self.logits = config.logits

    def __call__(self, logits, targets, reduce=True):
        if self.logits:
            bce_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')
        else:
            bce_loss = F.binary_cross_entropy(logits, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        loss = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return loss.mean() if reduce else loss


class CrossEntropyLoss(Loss):


    class Config(ConfigBase):
        pass

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        self.ignore_index = ignore_index
        self.weight = weight

    def __call__(self, logits, targets, reduce=True):
        return F.nll_loss(F.log_softmax(logits, 1, dtype=torch.float32), targets, weight=self.weight, ignore_index=self.ignore_index, reduction='mean' if reduce else 'none')


class FocalLoss(Loss):
    """
    Focal loss for multi-class outputs
    refrence: https://arxiv.org/pdf/1708.02002.pdf
    """


    class Config(ConfigBase):
        alpha: float = 0.25
        gamma: float = 2.0
        logits: bool = True

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        self.ignore_index = ignore_index
        self.weight = weight
        self.alpha = config.alpha
        self.gamma = config.gamma
        self.logits = config.logits

    def __call__(self, logits, targets, reduce=True):
        log_prob = F.log_softmax(logits, 1, dtype=torch.float32)
        pt = torch.exp(log_prob)
        return F.nll_loss(self.alpha * (1 - pt) ** self.gamma * log_prob, targets, weight=self.weight, ignore_index=self.ignore_index, reduction='mean' if reduce else 'none')


class KLDivergenceCELoss(Loss):


    class Config(ConfigBase):
        temperature: float = 1.0
        hard_weight: float = 0.0

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        assert ignore_index < 0
        assert 0.0 <= config.hard_weight < 1.0
        self.weight = weight
        self.t = config.temperature
        self.hard_weight = config.hard_weight

    def __call__(self, logits, targets, reduce=True, combine_loss=True):
        """
        Computes Kullback-Leibler divergence loss for multiclass classification
        probability distribution computed by CrossEntropyLoss loss.
        For, KL-divergence, batchmean is the right way to reduce, not just mean.
        """
        hard_targets, _, soft_targets_logits = targets
        soft_targets = F.softmax(soft_targets_logits.float() / self.t, dim=1)
        soft_targets = soft_targets.clamp(1e-10, 1 - 1e-10)
        log_probs = F.log_softmax(logits / self.t, 1)
        if self.weight is not None:
            soft_loss = F.kl_div(log_probs, soft_targets, reduction='none') * self.weight
            soft_loss = torch.sum(soft_loss, dim=1).mean() if reduce else torch.sum(soft_loss, dim=1)
        else:
            soft_loss = F.kl_div(log_probs, soft_targets, reduction='batchmean' if reduce else 'none')
        soft_loss *= self.t ** 2
        hard_loss = F.nll_loss(F.log_softmax(logits, 1, dtype=torch.float32), hard_targets, weight=self.weight, reduction='mean' if reduce else 'none')
        return (1.0 - self.hard_weight) * soft_loss + self.hard_weight * hard_loss if combine_loss else (soft_loss, hard_loss)


def create_loss(loss_config, *args, **kwargs):
    return create_component(ComponentType.LOSS, loss_config, *args, **kwargs)


@jit.script
def _get_prediction_from_scores(scores: torch.Tensor, classes: List[str]) ->List[List[Dict[str, float]]]:
    """
    Given scores for a batch, get the prediction for each word in the form of a
    List[List[Dict[str, float]]] for callers of the torchscript model to consume.
    The outer list iterates over batches of sentences and the inner iterates
    over each token in the sentence. The dictionary consists of
    `label:score` for each word.

    Example:

    Assuming slot labels are [No-Label, Number, Name]
    Utterances: [[call john please], [Brightness 25]]
    Output could look like:
    [
        [
            { No-Label: -0.1, Number: -1.5, Name: -9.01},
            { No-Label: -2.1, Number: -1.5, Name: -0.01},
            { No-Label: -0.1, Number: -1.5, Name: -2.01},
        ],
        [
            { No-Label: -0.1, Number: -1.5, Name: -9.01},
            { No-Label: -2.1, Number: -0.5, Name: -7.01},
            { No-Label: -0.1, Number: -1.5, Name: -2.01},
        ]
    ]
    """
    results: List[List[Dict[str, float]]] = []
    for sentence_scores in scores.chunk(len(scores)):
        sentence_scores = sentence_scores.squeeze(0)
        sentence_response: List[Dict[str, float]] = []
        for word_scores in sentence_scores.chunk(len(sentence_scores)):
            word_scores = word_scores.squeeze(0)
            word_response: Dict[str, float] = {}
            for i in range(len(classes)):
                word_response[classes[i]] = float(word_scores[i].item())
            sentence_response.append(word_response)
        results.append(sentence_response)
    return results


class WordTaggingScores(nn.Module):
    classes: List[str]

    def __init__(self, classes):
        super().__init__()
        self.classes = classes
        log_class_usage(__class__)

    def forward(self, logits: torch.Tensor, context: Optional[Dict[str, torch.Tensor]]=None) ->List[List[Dict[str, float]]]:
        scores: torch.Tensor = F.log_softmax(logits, 2)
        return _get_prediction_from_scores(scores, self.classes)


@jit.script
def _rearrange_output(logit, pred):
    """
    Rearrange the word logits so that the decoded word has the highest valued
    logits by swapping the indices predicted with those with maximum logits.
    """
    max_logits, max_logit_indices = torch.max(logit, 2, keepdim=True)
    pred_indices = pred.unsqueeze(2)
    pred_logits = torch.gather(logit, 2, pred_indices)
    logit_rearranged = logit.scatter(2, pred_indices, max_logits)
    logit_rearranged.scatter_(2, max_logit_indices, pred_logits)
    return logit_rearranged


class CRFWordTaggingScores(WordTaggingScores):

    def __init__(self, classes: List[str], crf):
        super().__init__(classes)
        self.crf = crf
        self.crf.eval()
        log_class_usage(__class__)

    def forward(self, logits: torch.Tensor, context: Dict[str, torch.Tensor]) ->List[List[Dict[str, float]]]:
        assert 'seq_lens' in context
        pred = self.crf.decode(logits, context['seq_lens'])
        logits_rearranged = _rearrange_output(logits, pred)
        scores: torch.Tensor = F.log_softmax(logits_rearranged, 2)
        return _get_prediction_from_scores(scores, self.classes)


class BinaryCrossEntropyLoss(Loss):


    class Config(ConfigBase):
        reweight_negative: bool = True
        reduce: bool = True

    def __call__(self, logits, targets, reduce=True):
        """
        Computes 1-vs-all binary cross entropy loss for multiclass
        classification.
        """
        targets = FloatTensor(targets.size(0), logits.size(1)).zero_().scatter_(1, targets.unsqueeze(1).data, 1) if len(logits.size()) > 1 else targets.float()
        """
        `F.binary_cross_entropy` or `torch.nn.BCELoss.` requires the
        output of the previous function be already a FloatTensor.
        """
        loss = F.binary_cross_entropy_with_logits(precision.maybe_float(logits), targets, reduction='none')
        if self.config.reweight_negative:
            weights = targets + (1.0 - targets) / max(1, targets.size(1) - 1.0)
            loss = loss * weights
        return loss.sum(-1).mean() if reduce else loss.sum(-1)


class KLDivergenceBCELoss(Loss):


    class Config(ConfigBase):
        temperature: float = 1.0
        hard_weight: float = 0.0

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        assert 0.0 <= config.hard_weight < 1.0
        self.ignore_index = ignore_index
        self.weight = weight
        self.t = config.temperature
        self.hard_weight = config.hard_weight

    def __call__(self, logits, targets, reduce=True):
        """
        Computes Kullback-Leibler divergence loss for multiclass classification
        probability distribution computed by BinaryCrossEntropyLoss loss
        """
        hard_targets, _, soft_targets_logits = targets
        soft_targets = F.sigmoid(FloatTensor(soft_targets_logits) / self.t).clamp(1e-20, 1 - 1e-20)
        probs = F.sigmoid(logits / self.t).clamp(1e-20, 1 - 1e-20)
        probs_neg = probs.neg().add(1).clamp(1e-20, 1 - 1e-20)
        soft_targets_neg = soft_targets.neg().add(1).clamp(1e-20, 1 - 1e-20)
        if self.weight is not None:
            soft_loss = F.kl_div(probs.log(), soft_targets, reduction='none') * self.weight + F.kl_div(probs_neg.log(), soft_targets_neg, reduction='none') * self.weight
            if reduce:
                soft_loss = soft_loss.mean()
        else:
            soft_loss = F.kl_div(probs.log(), soft_targets, reduction='mean' if reduce else 'none') + F.kl_div(probs_neg.log(), soft_targets_neg, reduction='mean' if reduce else 'none')
        soft_loss *= self.t ** 2
        hard_loss = 0.0
        if self.hard_weight > 0.0:
            one_hot_targets = FloatTensor(hard_targets.size(0), logits.size(1)).zero_().scatter_(1, hard_targets.unsqueeze(1).data, 1)
            hard_loss = F.binary_cross_entropy_with_logits(logits, one_hot_targets, reduction='mean' if reduce else 'none', weight=self.weight)
        return (1.0 - self.hard_weight) * soft_loss + self.hard_weight * hard_loss


class SourceType(Enum):
    LOG_PROBS = 'log_probs'
    LOGITS = 'logits'
    PROBS = 'probs'


class LabelSmoothedCrossEntropyLoss(Loss):


    class Config(ConfigBase):
        beta: float = 0.1
        source: SourceType = SourceType.LOGITS
        use_entropy: bool = False

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        if weight is not None:
            assert torch.sum(torch.abs(weight - 1.0)) < 1e-07
        self.ignore_index = ignore_index
        self.weight = weight
        self.beta = config.beta
        self.source = config.source
        self.use_entropy = config.use_entropy
        self.cross_entropy_loss = None
        self.label_smoothing_loss = None

    def __call__(self, logits, targets, reduce=True):
        """
        If use_entropy is False, returns the cross-entropy loss alongwith the KL divergence of the
        discrete uniform distribution with the logits. Refer to section 3.2
        If use_entopy is True, uses the entropy of the output distribution as
        the smoothing loss (i.e., higher entropy, better). Refer to section 3
        https://arxiv.org/pdf/1701.06548.pdf
        """
        if self.use_entropy:
            probs = F.softmax(logits, dim=1)
            log_probs = torch.log(probs)
            label_smoothing_loss = torch.sum(log_probs * probs, dim=1)
        else:
            if self.source == SourceType.LOGITS:
                log_probs = F.log_softmax(logits, dim=1)
            elif self.source == SourceType.PROBS:
                log_probs = logits.log()
            else:
                log_probs = logits
            label_smoothing_loss = -1 * log_probs.mean(dim=1)
        if reduce:
            non_ignored = targets != self.ignore_index
            if non_ignored.any():
                label_smoothing_loss = torch.mean(label_smoothing_loss[non_ignored])
            else:
                label_smoothing_loss = torch.tensor(0.0, device=logits.device)
        cross_entropy_loss = F.nll_loss(log_probs, targets, ignore_index=self.ignore_index, reduction='mean' if reduce else 'none', weight=self.weight)
        self.cross_entropy_loss = cross_entropy_loss
        self.label_smoothing_loss = label_smoothing_loss
        return (1.0 - self.beta) * cross_entropy_loss + self.beta * label_smoothing_loss


class Padding:
    WORD_LABEL_PAD = 'PAD_LABEL'
    WORD_LABEL_PAD_IDX = 0
    DEFAULT_LABEL_PAD_IDX = -1


class StructuredLoss(Loss):
    """Generic loss function applied to structured outputs."""

    def __init__(self, config, ignore_index=1):
        self.ignore_index = ignore_index

    def __call__(self, logits, targets, reduce=True):
        raise NotImplementedError


class CostFunctionType(Enum):
    HAMMING = 'hamming'


class HingeLoss(Loss):


    class Config(ConfigBase):
        margin: float = 1.0

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        self.margin = config.margin
        self.ignore_index = ignore_index
        self.weight = weight

    def __call__(self, logits, targets, reduce=True):
        return F.multi_margin_loss(logits, targets, margin=self.margin, weight=self.weight, reduction='mean' if reduce else 'none')


class NLLLoss(Loss):


    class Config(ConfigBase):
        pass

    def __init__(self, config, ignore_index=-100, weight=None, *args, **kwargs):
        self.ignore_index = ignore_index
        self.weight = weight

    def __call__(self, log_probs, targets, reduce=True):
        return F.nll_loss(log_probs, targets, ignore_index=self.ignore_index, reduction='mean' if reduce else 'none', weight=self.weight)


def hamming_distance(logits, targets, cost_scale=1.0):
    """
    Computes Hamming distance (https://en.wikipedia.org/wiki/Hamming_distance), which is
    defined as the number of positions where two sequences of equal length differ. We apply
    Hamming distance locally, incrementing non-gold token scores by `cost_scale`.

    ```
    Example:
    Given targets = [0, 1] and cost_scale = 1.0, we have the following:
    logits (before) = [[-1.0, 1.0, 2.0], [-2.0, -1.0, 1.0]]
    logits (after) = [[-1.0, 2.0, 3.0], [-1.0, -1.0, 2.0]]
    ```
    """
    hamming_cost = cost_scale * torch.ones_like(logits)
    gold_cost = torch.zeros_like(targets).unsqueeze(2)
    hamming_cost.scatter_(2, targets.unsqueeze(2), gold_cost)
    return hamming_cost


def get_cost_fn(cost_fn_type: CostFunctionType):
    """Retrieves a cost function corresponding to `cost_fn_type`."""
    if cost_fn_type == cost_fn_type.HAMMING:
        return hamming_distance
    else:
        raise RuntimeError('invalid cost type provideo')


class StructuredMarginLoss(StructuredLoss):
    """
    Margin-based loss which requires a gold structure Y to score at least
    `cost(Y, Y')` above a hypothesis structure `Y'`. The cost function used is
    variable, but should reflect the underlying semantics of the task (e.g.,
    BLEU in machine translation).
    """


    class Config(ConfigBase):
        cost_scale: float = 1.0
        cost_fn: CostFunctionType = CostFunctionType.HAMMING
        label_loss: Union[NLLLoss.Config, HingeLoss.Config] = NLLLoss.Config()

    def __init__(self, config, ignore_index=1, *args, **kwargs):
        super().__init__(config, ignore_index)
        self.cost_scale = config.cost_scale
        self.cost_fn = get_cost_fn(config.cost_fn)
        self.label_loss_fn = create_loss(config.label_loss, ignore_index=ignore_index)

    def __call__(self, logits, targets, reduce=True):
        cost = self.cost_fn(logits, targets, self.cost_scale)
        logits = logits.clone() + cost
        if isinstance(self.label_loss_fn, NLLLoss):
            logits = F.log_softmax(logits, 2)
        logits = logits.view(-1, logits.size(-1))
        targets = targets.view(-1)
        return self.label_loss_fn(logits, targets, reduce)


def device():
    return 'cuda:{}'.format(torch.cuda.current_device()) if CUDA_ENABLED else 'cpu'


def tensor(data, dtype):
    return torch.tensor(data, dtype=dtype, device=device())


def get_label_weights(vocab_dict: Dict[str, int], label_weights: Dict[str, float]):
    """
    Get label weights from user-provided label_weights field
    """
    pruned_label_weights = {vocab_dict[k]: v for k, v in label_weights.items() if k in vocab_dict}
    if len(pruned_label_weights) != len(label_weights):
        filtered_labels = [k for k in label_weights if k not in vocab_dict]
        logging.warning(f'WARNING: these labels are filtered from original label weights             {filtered_labels}')
    if len(pruned_label_weights) == 0:
        return None
    weights_tensor = [1] * len(vocab_dict)
    for k, v in pruned_label_weights.items():
        weights_tensor[k] = v
    return tensor(weights_tensor, dtype=torch.float)


class ConfigParseError(Exception):
    pass


class MissingValueError(ConfigParseError):
    pass


class BinaryCrossEntropyWithLogitsLoss(Loss):


    class Config(ConfigBase):
        reduce: bool = True

    def __init__(self, config, pos_weight=None, *args, **kwargs):
        self.pos_weight = pos_weight

    def __call__(self, logits, targets, reduce=True):
        """
        Computes 1-vs-all binary cross entropy loss for multiclass classification. However, unlike BinaryCrossEntropyLoss, we require targets to be a one-hot vector.
        """
        target_labels = targets[0].float()
        """
        `F.binary_cross_entropy_with_logits` requires the
        output of the previous function be already a FloatTensor.
        """
        loss = F.binary_cross_entropy_with_logits(precision.maybe_float(logits), target_labels, reduction='none', pos_weight=self.pos_weight)
        return loss.sum(-1).mean() if reduce else loss.sum(-1)


class MultiLabelSoftMarginLoss(Loss):


    class Config(ConfigBase):
        pass

    def __call__(self, m_out, targets, reduce=True):
        """
        Computes multi-label classification loss
        see details in torch.nn.MultiLabelSoftMarginLoss
        """
        num_classes = m_out.size()[1]
        target_labels = targets[0]
        tmp_target_labels = target_labels + 1
        n_hot_targets = FloatTensor(target_labels.size(0), num_classes + 1).zero_().scatter_(1, tmp_target_labels, 1)[:, 1:]
        """
        `F.multilabel_soft_margin_loss` or `torch.nn.MultiLabelSoftMarginLoss.`
        requires the
        output of the previous function be already a FloatTensor.
        """
        loss = F.multilabel_soft_margin_loss(precision.maybe_float(m_out), n_hot_targets, reduction='mean')
        return loss


class WeightingMethod(Enum):
    CLASS_RATIO = 'CLASS_RATIO'
    SQRT_RATIO = 'SQRT'
    CAPPED_RATIO = 'CAP'


def get_auto_label_weights(vocab_dict: Dict[str, int], label_counts: Counter):
    """
    label weights automatically calculated from training data
    """
    label_weights = {}
    for label in vocab_dict:
        if label in label_counts:
            pos_count = label_counts[label]
            neg_count = sum(label_counts.values()) - pos_count
            weight = neg_count / pos_count
            label_weights[label] = weight
        else:
            label_weights[label] = 1.0
    weights_tensor = torch.ones(1, len(label_weights.keys()))
    for label, weight in label_weights.items():
        weights_tensor[0][vocab_dict[label]] = weight
    return weights_tensor


def get_normalized_cap_label_weights(vocab_dict: Dict[str, int], label_counts: Counter):
    """
    label weights automatically calculated from training data, normalized and capped by 1
    """
    normalized_label_weights = {}
    avg_label_count = sum(label_counts.values()) / len(label_counts.keys())
    for label in vocab_dict:
        if label in label_counts:
            count = label_counts[label]
            if count > avg_label_count:
                label_weight = avg_label_count / count
                normalized_label_weights[label] = label_weight
            else:
                normalized_label_weights[label] = 1.0
        else:
            normalized_label_weights[label] = 1.0
    cap_label_weights_tensor = torch.ones(1, len(vocab_dict.keys()))
    for label, index in vocab_dict.items():
        cap_label_weights_tensor[0][index] = normalized_label_weights[label]
    return cap_label_weights_tensor


def get_normalized_sqrt_label_weights(vocab_dict: Dict[str, int], label_counts: Counter):
    """
    label weights automatically calculated from training data, normalized by sqrt
    """
    auto_weights_tensor = get_auto_label_weights(vocab_dict, label_counts)
    return torch.sqrt(auto_weights_tensor)


def get_automatic_label_weights(vocab_dict, label_counts, automatic_label_weighting_method):
    """
    This function contains the logic on which automatic label weighting method to use based on the config. Label weights are automatically calculated from all training examples.
    Due to the fact that the label distribution can be highly skewed resulting in excessively weights, we implemented two normalization method, 'sqrt' and 'cap', to normalize the weights.
    """
    if automatic_label_weighting_method == WeightingMethod.CLASS_RATIO:
        automatic_label_weights = get_auto_label_weights(vocab_dict, label_counts)
    elif automatic_label_weighting_method == WeightingMethod.SQRT_RATIO:
        automatic_label_weights = get_normalized_sqrt_label_weights(vocab_dict, label_counts)
    elif automatic_label_weighting_method == WeightingMethod.CAPPED_RATIO:
        automatic_label_weights = get_normalized_cap_label_weights(vocab_dict, label_counts)
    else:
        raise ValueError(f'ERROR: weighting method {automatic_label_weighting_method} not recognized.')
    logging.info(f'automatic_label_weighting_method is {automatic_label_weighting_method} and label_weights is {automatic_label_weights}')
    return automatic_label_weights


def get_custom_or_automatic_label_weights(vocab_dict, label_counts, label_weights_field, automatic_label_weighting_method_field):
    if label_weights_field and automatic_label_weighting_method_field:
        label_weights = get_automatic_label_weights(vocab_dict, label_counts, automatic_label_weighting_method_field)
        custom_labels = []
        for label, custom_weight in label_weights_field.items():
            if label in vocab_dict:
                label_weights[0][vocab_dict[label]] = custom_weight
                custom_labels.append(label)
        logging.warning(f'WARNING: following labels are updated with user provided custom values             {custom_labels}')
    elif label_weights_field:
        label_weights = get_label_weights(vocab_dict, label_weights_field)
    elif automatic_label_weighting_method_field:
        label_weights = get_automatic_label_weights(vocab_dict, label_counts, automatic_label_weighting_method_field)
    else:
        return
    logging.info(f'INFO: Final label weights are {label_weights}')
    return label_weights


class Activation(Enum):
    RELU = 'relu'
    LEAKYRELU = 'leakyrelu'
    TANH = 'tanh'
    GELU = 'gelu'
    GLU = 'glu'


def get_activation(name, dim=1):
    if name == Activation.RELU:
        return nn.ReLU()
    elif name == Activation.LEAKYRELU:
        return nn.LeakyReLU()
    elif name == Activation.TANH:
        return nn.Tanh()
    elif name == Activation.GELU:
        return nn.GELU()
    elif name == Activation.GLU:
        return nn.GLU(dim=dim)
    else:
        raise RuntimeError(f'{name} is not supported')


class CosineEmbeddingLoss(Loss):


    class Config(ConfigBase):
        margin: float = 0.0

    def __init__(self, config, *args, **kwargs):
        self.margin = config.margin

    def __call__(self, embeddings, targets, reduce=True):
        if len(embeddings) != 2:
            raise ValueError(f'Number of embeddings must be 2. Found {len(embeddings)} embeddings.')
        return F.cosine_embedding_loss(embeddings[0], embeddings[1], targets, margin=self.margin, reduction='mean' if reduce else 'none')


class MAELoss(Loss):
    """
    Mean absolute error or L1 loss, for regression tasks.
    """


    class Config(ConfigBase):
        pass

    def __call__(self, predictions, targets, reduce=True):
        return F.l1_loss(predictions, targets, reduction='mean' if reduce else 'none')


class MSELoss(Loss):
    """
    Mean squared error or L2 loss, for regression tasks.
    """


    class Config(ConfigBase):
        pass

    def __call__(self, predictions, targets, reduce=True):
        return F.mse_loss(predictions, targets, reduction='mean' if reduce else 'none')


@unique
class OutputScore(IntEnum):
    raw_cosine = 1
    norm_cosine = 2
    sigmoid_cosine = 3


def get_norm_cosine_scores(cosine_sim_scores):
    pos_scores = (cosine_sim_scores + 1.0) / 2.0
    neg_scores = 1.0 - pos_scores
    return pos_scores, neg_scores


def get_sigmoid_scores(cosine_sim_scores):
    pos_scores = torch.sigmoid(cosine_sim_scores)
    neg_scores = 1.0 - pos_scores
    return pos_scores, neg_scores


class LastTimestepPool(Module):

    def __init__(self, config: Module.Config, n_input: int) ->None:
        super().__init__(config)
        log_class_usage(__class__)

    def forward(self, inputs: torch.Tensor, seq_lengths: torch.Tensor) ->torch.Tensor:
        if torch._C._get_tracing_state():
            assert inputs.shape[0] == 1
            return inputs[:, -1, :]
        bsz, _, dim = inputs.shape
        idx = seq_lengths.unsqueeze(1).expand(bsz, dim).unsqueeze(1)
        return inputs.gather(1, idx - 1).squeeze(1)


class MaxPool(Module):

    def __init__(self, config: Module.Config, n_input: int) ->None:
        super().__init__(config)
        log_class_usage(__class__)

    def forward(self, inputs: torch.Tensor, seq_lengths: torch.Tensor=None) ->torch.Tensor:
        return torch.max(inputs, 1)[0]


class MeanPool(Module):

    def __init__(self, config: Module.Config, n_input: int) ->None:
        super().__init__(config)
        log_class_usage(__class__)

    def forward(self, inputs: torch.Tensor, seq_lengths: torch.Tensor) ->torch.Tensor:
        return torch.sum(inputs, 1) / seq_lengths.unsqueeze(1).float()


class NoPool(Module):

    def __init__(self, config: Module.Config, n_input: int) ->None:
        super().__init__(config)
        log_class_usage(__class__)

    def forward(self, inputs: torch.Tensor, seq_lengths: torch.Tensor=None) ->torch.Tensor:
        return inputs


class SelfAttention(Module):


    class Config(ConfigBase):
        attn_dimension: int = 64
        dropout: float = 0.4

    def __init__(self, config: Config, n_input: int) ->None:
        super().__init__(config)
        self.dropout = nn.Dropout(config.dropout)
        self.n_input = n_input
        self.n_attn = config.attn_dimension
        self.ws1 = nn.Linear(n_input, self.n_attn, bias=False)
        self.ws2 = nn.Linear(self.n_attn, 1, bias=False)
        self.tanh = nn.Tanh()
        self.softmax = nn.Softmax()
        self.init_weights()
        log_class_usage(__class__)

    def init_weights(self, init_range: float=0.1) ->None:
        self.ws1.weight.data.uniform_(-init_range, init_range)
        self.ws2.weight.data.uniform_(-init_range, init_range)

    def forward(self, inputs: torch.Tensor, seq_lengths: torch.Tensor=None) ->torch.Tensor:
        size = torch.onnx.operators.shape_as_tensor(inputs)
        flat_2d_shape = torch.cat((torch.LongTensor([-1]), size[2].view(1)))
        compressed_emb = torch.onnx.operators.reshape_from_tensor_shape(inputs, flat_2d_shape)
        hbar = self.tanh(self.ws1(self.dropout(compressed_emb)))
        alphas = self.ws2(hbar)
        alphas = torch.onnx.operators.reshape_from_tensor_shape(alphas, size[:2])
        alphas = self.softmax(alphas)
        return torch.bmm(alphas.unsqueeze(1), inputs).squeeze(1)


PAD_INDEX = 1


UNK_INDEX = 0


def _infer_pad_shape(nested_lists):
    """Return the minimal tensor shape which could contain the input data."""
    yield len(nested_lists)
    while nested_lists and all(should_iter(i) for i in nested_lists):
        yield precision.pad_length(max(len(nested) for nested in nested_lists))
        nested_lists = list(itertools.chain.from_iterable(nested_lists))


def _make_nested_padding(pad_shape, pad_token):
    """Create nested lists of pad_token of shape pad_shape."""
    result = [pad_token]
    for dimension in reversed(pad_shape):
        result = [result * dimension]
    return result[0]


def pad(nested_lists, pad_token, pad_shape=None):
    """Pad the input lists with the pad token. If pad_shape is provided, pad to that
    shape, otherwise infer the input shape and pad out to a square tensor shape."""
    if pad_shape is None:
        pad_shape = list(_infer_pad_shape(nested_lists))
    if not pad_shape:
        return nested_lists
    dimension, *rest = pad_shape
    result = [pad(nested, pad_token, rest) for nested in nested_lists]
    result += [_make_nested_padding(rest, pad_token)] * (dimension - len(result))
    return result


def pad_and_tensorize(batch, pad_token=0, pad_shape=None, dtype=torch.long):
    batch = list(batch)
    if not batch:
        return torch.Tensor()
    return cuda.tensor(pad(batch, pad_token=pad_token, pad_shape=pad_shape), dtype=dtype)


class LabelTensorizer(Tensorizer):
    """Numberize labels. Label can be used as either input or target.

    NB: if the labels are used as targets for binary classification with a loss
    such as cosine distance, the order of the `label_vocab` *does* matter,
    and it should be `[negative_class, positive_class]`.
    """
    __EXPANSIBLE__ = True


    class Config(Tensorizer.Config):
        column: str = 'label'
        allow_unknown: bool = False
        pad_in_vocab: bool = False
        label_vocab: Optional[List[str]] = None
        label_vocab_file: Optional[str] = None
        is_input: bool = False
        add_labels: Optional[List[str]] = None

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.column, config.allow_unknown, config.pad_in_vocab, config.label_vocab, config.label_vocab_file, config.is_input, config.add_labels)

    def __init__(self, label_column: str='label', allow_unknown: bool=False, pad_in_vocab: bool=False, label_vocab: Optional[List[str]]=None, label_vocab_file: Optional[str]=None, is_input: bool=Config.is_input, add_labels: Optional[List[str]]=None):
        self.label_column = label_column
        self.pad_in_vocab = pad_in_vocab
        self.vocab_builder = VocabBuilder()
        self.vocab_builder.use_pad = pad_in_vocab
        self.vocab_builder.use_unk = allow_unknown
        self.add_labels = add_labels
        self.vocab = None
        self.pad_idx = -1
        assert label_vocab is None or label_vocab_file is None, 'Cannot specify both label_vocab and label_vocab_file'
        if label_vocab:
            self.vocab_builder.add_all(label_vocab)
            self.vocab, self.pad_idx = self._create_vocab()
        elif label_vocab_file:
            with PathManager.open(label_vocab_file) as f:
                self.vocab_builder.add_from_file(f, skip_header_line=False, lowercase_tokens=False, size=None)
            self.vocab, self.pad_idx = self._create_vocab()
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.label_column, str)]

    def initialize(self, from_scratch=True):
        """
        Look through the dataset for all labels and create a vocab map for them.
        """
        if self.vocab and from_scratch:
            return
        try:
            while True:
                row = yield
                labels = row[self.label_column]
                self.vocab_builder.add_all(labels)
        except GeneratorExit:
            if self.add_labels:
                self.vocab_builder.add_all(self.add_labels)
            self.vocab, self.pad_idx = self._create_vocab()

    def _create_vocab(self):
        if not self.vocab_builder.has_added_tokens():
            error_msg = 'Label classes are not specified, and no examples or labels were found in training data. Either the training data is empty, or the data fields are misnamed and no examples are parsed (warnings would appear in preceding stdout logs).'
            raise ValueError(error_msg)
        vocab = self.vocab_builder.make_vocab()
        pad_idx = vocab.get_pad_index() if self.pad_in_vocab else Padding.DEFAULT_LABEL_PAD_IDX
        return vocab, pad_idx

    def numberize(self, row):
        """Numberize labels."""
        return self.vocab.lookup_all(row[self.label_column])

    def tensorize(self, batch):
        return pad_and_tensorize(batch, self.pad_idx)


class VocabFileConfig(Component.Config):
    filepath: str = ''
    skip_header_line: bool = False
    lowercase_tokens: bool = False
    size_limit: int = 0


class VocabConfig(Component.Config):
    build_from_data: bool = True
    size_from_data: int = 0
    min_counts: int = 0
    vocab_files: List[VocabFileConfig] = []
    additional_tokens: Optional[List[str]] = None


class TokenTensorizer(Tensorizer):
    """Convert text to a list of tokens. Do this based on a tokenizer configuration,
    and build a vocabulary for numberization. Finally, pad the batch to create
    a square tensor of the correct size.
    """


    class Config(Tensorizer.Config):
        column: str = 'text'
        tokenizer: Tokenizer.Config = Tokenizer.Config()
        add_bos_token: bool = False
        add_eos_token: bool = False
        use_eos_token_for_bos: bool = False
        max_seq_len: Optional[int] = None
        vocab: VocabConfig = VocabConfig()
        vocab_file_delimiter: str = ' '
        vocab_use_mask: bool = False

    @classmethod
    def from_config(cls, config: Config):
        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
        return cls(text_column=config.column, tokenizer=tokenizer, add_bos_token=config.add_bos_token, add_eos_token=config.add_eos_token, use_eos_token_for_bos=config.use_eos_token_for_bos, max_seq_len=config.max_seq_len, vocab_config=config.vocab, vocab_file_delimiter=config.vocab_file_delimiter, is_input=config.is_input, vocab_use_mask=config.vocab_use_mask)

    def __init__(self, text_column, tokenizer=None, add_bos_token=Config.add_bos_token, add_eos_token=Config.add_eos_token, use_eos_token_for_bos=Config.use_eos_token_for_bos, max_seq_len=Config.max_seq_len, vocab_config=None, vocab=None, vocab_file_delimiter=' ', is_input=Config.is_input, vocab_use_mask=Config.vocab_use_mask):
        self.text_column = text_column
        self.tokenizer = tokenizer or Tokenizer()
        self.vocab = vocab
        self.add_bos_token = add_bos_token
        self.add_eos_token = add_eos_token
        self.use_eos_token_for_bos = use_eos_token_for_bos
        self.max_seq_len = max_seq_len or 2 ** 30
        self.vocab_builder = None
        self.vocab_config = vocab_config or VocabConfig()
        self.vocab_file_delimiter = vocab_file_delimiter
        self.vocab_use_mask = vocab_use_mask
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.text_column, str)]

    def _tokenize(self, text=None, pre_tokenized=None):
        return tokenize(text=text, pre_tokenized=pre_tokenized, tokenizer=self.tokenizer, bos_token=self.vocab.bos_token if self.add_bos_token else None, eos_token=self.vocab.eos_token if self.add_eos_token else None, pad_token=self.vocab.pad_token, use_eos_token_for_bos=self.use_eos_token_for_bos, max_seq_len=self.max_seq_len)

    def _lookup_tokens(self, text=None, pre_tokenized=None):
        return lookup_tokens(text=text, pre_tokenized=pre_tokenized, tokenizer=self.tokenizer, vocab=self.vocab, bos_token=self.vocab.bos_token if self.add_bos_token else None, eos_token=self.vocab.eos_token if self.add_eos_token else None, pad_token=self.vocab.pad_token, use_eos_token_for_bos=self.use_eos_token_for_bos, max_seq_len=self.max_seq_len)

    def _reverse_lookup(self, token_ids):
        return [self.vocab[id] for id in token_ids]

    def initalize_vocab(self, vocab_builder=None, from_scratch=True) ->bool:
        """
        returns True if needs to initialize vocabulary through data
        returns False is vocab is initialized
        """
        if self.vocab and from_scratch:
            if self.vocab_config.build_from_data or self.vocab_config.vocab_files:
                None
            return True
        if not self.vocab_config.build_from_data and not self.vocab_config.vocab_files:
            raise ValueError(f"To create token tensorizer for '{self.text_column}', either `build_from_data` or `vocab_files` must be set.")
        if not self.vocab_builder:
            self.vocab_builder = vocab_builder or VocabBuilder(delimiter=self.vocab_file_delimiter)
            self.vocab_builder.use_bos = self.add_bos_token
            self.vocab_builder.use_eos = self.add_eos_token
            self.vocab_builder.use_mask = self.vocab_use_mask
        if not self.vocab_config.build_from_data:
            self._add_vocab_from_files()
            if self.vocab_config.additional_tokens:
                self.vocab_builder.add_all(self.vocab_config.additional_tokens)
            self.vocab = self.vocab_builder.make_vocab()
            return True
        return False

    def initialize(self, vocab_builder=None, from_scratch=True):
        """Build vocabulary based on training corpus."""
        if self.initalize_vocab(vocab_builder, from_scratch):
            return
        try:
            while True:
                row = yield
                raw_text = row[self.text_column]
                tokenized = self.tokenizer.tokenize(raw_text)
                self.vocab_builder.add_all([t.value for t in tokenized])
        except GeneratorExit:
            self.vocab_builder.truncate_to_vocab_size(self.vocab_config.size_from_data, self.vocab_config.min_counts)
            self._add_vocab_from_files()
            if self.vocab_config.additional_tokens:
                self.vocab_builder.add_all(self.vocab_config.additional_tokens)
            self.vocab = self.vocab_builder.make_vocab()

    def _add_vocab_from_files(self):
        for vocab_file in self.vocab_config.vocab_files:
            with PathManager.open(vocab_file.filepath) as f:
                self.vocab_builder.add_from_file(f, vocab_file.skip_header_line, vocab_file.lowercase_tokens, vocab_file.size_limit)

    def numberize(self, row):
        """Tokenize, look up in vocabulary."""
        tokens, start_idx, end_idx = self._lookup_tokens(row[self.text_column])
        token_ranges = list(zip(start_idx, end_idx))
        return tokens, len(tokens), token_ranges

    def prepare_input(self, row):
        """Tokenize, look up in vocabulary, return tokenized_texts in raw text"""
        tokenized_texts, start_idx, end_idx = self._tokenize(row[self.text_column])
        token_ranges = list(zip(start_idx, end_idx))
        return list(tokenized_texts), len(tokenized_texts), token_ranges

    def tensorize(self, batch):
        tokens, seq_lens, token_ranges = zip(*batch)
        return pad_and_tensorize(tokens, self.vocab.get_pad_index()), pad_and_tensorize(seq_lens), pad_and_tensorize(token_ranges)

    def sort_key(self, row):
        return row[1]


class DotProductSelfAttention(Module):
    """
    Given vector w and token vectors = {t1, t2, ..., t_n}, compute self attention
    weights to weighs the tokens
    * a_j = softmax(w . t_j)
    """


    class Config(Module.Config):
        input_dim: int = 32

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.input_dim)

    def __init__(self, input_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, 1)
        log_class_usage(__class__)

    def forward(self, tokens, tokens_mask):
        """
        Input:
            x: batch_size * seq_len * input_dim
            x_mask: batch_size * seq_len (1 for padding, 0 for true)
        Output:
            alpha: batch_size * seq_len
        """
        scores = self.linear(tokens).squeeze(2)
        scores.data.masked_fill_(tokens_mask.data, -float('inf'))
        return F.softmax(scores, dim=-1)


GLOVE_840B_300D = '/mnt/vol/pytext/users/kushall/pretrained/glove.840B.300d.txt'


class MultiplicativeAttention(Module):
    """
    Given sequence P and vector q, computes attention weights for each element
    in P by matching q with each element in P using multiplicative attention.
    * a_i = softmax(p_i . W . q)
    """


    class Config(Module.Config):
        p_hidden_dim: int = 32
        q_hidden_dim: int = 32
        normalize: bool = False

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.p_hidden_dim, config.q_hidden_dim, config.normalize)

    def __init__(self, p_hidden_dim, q_hidden_dim, normalize):
        super().__init__()
        self.normalize = normalize
        self.linear = nn.Linear(p_hidden_dim, q_hidden_dim)
        log_class_usage(__class__)

    def forward(self, p_seq: torch.Tensor, q: torch.Tensor, p_mask: torch.Tensor):
        """
        Input:
            p_seq: batch_size * p_seq_len * p_hidden_dim
            q: batch_size * q_hidden_dim
            p_mask: batch_size * p_seq_len (1 for padding, 0 for true)
        Output:
            attn_scores: batch_size * p_seq_len
        """
        Wq = self.linear(q) if self.linear is not None else q
        pWq = p_seq.bmm(Wq.unsqueeze(2)).squeeze(2)
        pWq.data.masked_fill_(p_mask.data, -float('inf'))
        attn_scores = F.softmax(pWq, dim=-1) if self.normalize else pWq.exp()
        return attn_scores


class SequenceAlignedAttention(Module):
    """
    Given sequences P and Q, computes attention weights for each element in P by
    matching Q with each element in P.
    * a_i_j = softmax(p_i . q_j) where softmax is computed by summing over q_j
    """


    class Config(Module.Config):
        proj_dim: int = 32

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.proj_dim)

    def __init__(self, proj_dim):
        super().__init__()
        self.linear = nn.Linear(proj_dim, proj_dim)
        self.proj_dim = proj_dim
        log_class_usage(__class__)

    def forward(self, p: torch.Tensor, q: torch.Tensor, q_mask: torch.Tensor):
        """
        Input:
            p: batch_size * p_seq_len * dim
            q: batch_size * q_seq_len * dim
            q_mask: batch_size * q_seq_len (1 for padding, 0 for true)
        Output:
            matched_seq: batch_size * doc_seq_len * dim
        """
        p_transform = F.relu(self.linear(p))
        q_transform = F.relu(self.linear(q))
        attn_scores = p_transform.bmm(q_transform.transpose(2, 1))
        q_mask = q_mask.unsqueeze(1).expand(attn_scores.size())
        attn_scores.data.masked_fill_(q_mask.data, -float('inf'))
        attn_scores_flattened = F.softmax(attn_scores.view(-1, q.size(1)), dim=-1)
        return attn_scores_flattened.view(-1, p.size(1), q.size(1))


class BERTInitialTokenizer(Tokenizer):
    """
    Basic initial tokenization for BERT.  This is run prior to word piece, does
    white space tokenization in addition to lower-casing and accent removal
    if specified.
    """


    class Config(Tokenizer.Config):
        """Config for this class."""

    @classmethod
    def from_config(cls, config: Config):
        basic_tokenizer = BasicTokenizer(do_lower_case=config.lowercase, never_split=('[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'))
        return cls(basic_tokenizer)

    def __init__(self, basic_tokenizer) ->None:
        self.tokenizer = basic_tokenizer
        log_class_usage(__class__)

    def tokenize(self, text):
        """Tokenizes a piece of text."""
        if self.tokenizer.do_lower_case:
            text = self.tokenizer._run_strip_accents(text.lower())
        tokens = self.tokenizer.tokenize(text)
        end = 0
        result = []
        for token in tokens:
            start = text.find(token, end)
            if start == -1:
                start = end
            end = start + len(token)
            result.append(Token(token, start, end))
        return result


class WordPieceTokenizer(Tokenizer):
    """Word piece tokenizer for BERT models."""


    class Config(ConfigBase):
        basic_tokenizer: BERTInitialTokenizer.Config = BERTInitialTokenizer.Config()
        wordpiece_vocab_path: str = 'manifold://nlp_technologies/tree/huggingface-models/bert-base-uncased/vocab.txt'

    def __init__(self, wordpiece_vocab, basic_tokenizer, wordpiece_tokenizer) ->None:
        self.vocab = wordpiece_vocab
        self.basic_tokenizer = basic_tokenizer
        self.wordpiece_tokenizer = wordpiece_tokenizer
        log_class_usage(__class__)

    @classmethod
    def from_config(cls, config: Config):
        basic_tokenizer = create_component(ComponentType.TOKENIZER, config.basic_tokenizer)
        vocab = WordPieceTokenizer.load_vocab(config.wordpiece_vocab_path)
        wordpiece_tokenizer = WordpieceTokenizer(vocab=vocab, unk_token='[UNK]')
        return cls(vocab, basic_tokenizer, wordpiece_tokenizer)

    @staticmethod
    def load_vocab(vocab_file):
        """Loads a vocabulary file into a dictionary."""
        vocab = OrderedDict()
        with PathManager.open(vocab_file, 'r') as reader:
            tokens = reader.readlines()
        for index, token in enumerate(tokens):
            token = token.rstrip('\n')
            vocab[token] = index
        return vocab

    def tokenize(self, input_str: str) ->List[Token]:
        tokens = []
        for token in self.basic_tokenizer.tokenize(input_str):
            start = token.start
            for sub_token in self.wordpiece_tokenizer.tokenize(token.value):
                piece_len = len(sub_token) if not sub_token.startswith('##') else len(sub_token) - 2
                if sub_token == '[UNK]':
                    piece_len = len(token.value)
                end = start + piece_len
                tokens.append(Token(sub_token, start, end))
                start = end
        return [token for token in tokens if token.value]


class SquadTensorizer(TokenTensorizer):
    """Produces inputs and answer spans for Squad."""
    __EXPANSIBLE__ = True
    SPAN_PAD_IDX = -100


    class Config(TokenTensorizer.Config):
        doc_column: str = 'doc'
        ques_column: str = 'question'
        answers_column: str = 'answers'
        answer_starts_column: str = 'answer_starts'
        tokenizer: Tokenizer.Config = Tokenizer.Config(split_regex='\\W+')
        max_ques_seq_len: int = 64
        max_doc_seq_len: int = 256

    @classmethod
    def from_config(cls, config: Config, **kwargs):
        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
        vocab = None
        if isinstance(tokenizer, WordPieceTokenizer):
            None
            replacements = {'[UNK]': SpecialTokens.UNK, '[PAD]': SpecialTokens.PAD, '[CLS]': SpecialTokens.BOS, '[SEP]': SpecialTokens.EOS, '[MASK]': SpecialTokens.MASK}
            vocab = Vocabulary([token for token, _ in tokenizer.vocab.items()], replacements=replacements)
        doc_tensorizer = TokenTensorizer(text_column=config.doc_column, tokenizer=tokenizer, vocab=vocab, max_seq_len=config.max_doc_seq_len)
        ques_tensorizer = TokenTensorizer(text_column=config.ques_column, tokenizer=tokenizer, vocab=vocab, max_seq_len=config.max_ques_seq_len)
        return cls(doc_tensorizer=doc_tensorizer, ques_tensorizer=ques_tensorizer, doc_column=config.doc_column, ques_column=config.ques_column, answers_column=config.answers_column, answer_starts_column=config.answer_starts_column, tokenizer=tokenizer, vocab=vocab, **kwargs)

    def __init__(self, doc_tensorizer: TokenTensorizer, ques_tensorizer: TokenTensorizer, doc_column: str=Config.doc_column, ques_column: str=Config.ques_column, answers_column: str=Config.answers_column, answer_starts_column: str=Config.answer_starts_column, **kwargs):
        super().__init__(text_column=None, **kwargs)
        self.ques_tensorizer = ques_tensorizer
        self.doc_tensorizer = doc_tensorizer
        self.doc_column = doc_column
        self.ques_column = ques_column
        self.answers_column = answers_column
        self.answer_starts_column = answer_starts_column

    def initialize(self, vocab_builder=None, from_scratch=True):
        """Build vocabulary based on training corpus."""
        if isinstance(self.tokenizer, WordPieceTokenizer):
            return
        if not self.vocab_builder or from_scratch:
            self.vocab_builder = vocab_builder or VocabBuilder()
            self.vocab_builder.pad_index = 0
            self.vocab_builder.unk_index = 1
        ques_initializer = self.ques_tensorizer.initialize(self.vocab_builder, from_scratch)
        doc_initializer = self.doc_tensorizer.initialize(self.vocab_builder, from_scratch)
        ques_initializer.send(None)
        doc_initializer.send(None)
        try:
            while True:
                row = yield
                ques_initializer.send(row)
                doc_initializer.send(row)
        except GeneratorExit:
            self.vocab = self.vocab_builder.make_vocab()

    def _lookup_tokens(self, text, source_is_doc=True):
        return self.doc_tensorizer._lookup_tokens(text) if source_is_doc else self.ques_tensorizer._lookup_tokens(text)

    def numberize(self, row):
        assert len(self.vocab) == len(self.ques_tensorizer.vocab)
        assert len(self.vocab) == len(self.doc_tensorizer.vocab)
        ques_tokens, _, _ = self.ques_tensorizer._lookup_tokens(row[self.ques_column])
        doc_tokens, orig_start_idx, orig_end_idx = self.doc_tensorizer._lookup_tokens(row[self.doc_column])
        start_idx_map = {}
        end_idx_map = {}
        for token_idx, (start_idx, end_idx) in enumerate(zip(orig_start_idx, orig_end_idx)):
            start_idx_map[start_idx] = token_idx
            end_idx_map[end_idx] = token_idx
        answer_start_token_indices = [start_idx_map.get(raw_idx, self.SPAN_PAD_IDX) for raw_idx in row[self.answer_starts_column]]
        answer_end_token_indices = [end_idx_map.get(raw_idx + len(answer), self.SPAN_PAD_IDX) for raw_idx, answer in zip(row[self.answer_starts_column], row[self.answers_column])]
        if not (answer_start_token_indices and answer_end_token_indices) or self._only_pad(answer_start_token_indices) or self._only_pad(answer_end_token_indices):
            answer_start_token_indices = [self.SPAN_PAD_IDX]
            answer_end_token_indices = [self.SPAN_PAD_IDX]
        return doc_tokens, len(doc_tokens), ques_tokens, len(ques_tokens), answer_start_token_indices, answer_end_token_indices

    def tensorize(self, batch):
        doc_tokens, doc_seq_len, ques_tokens, ques_seq_len, answer_start_idx, answer_end_idx = zip(*batch)
        doc_tokens = pad_and_tensorize(doc_tokens, self.vocab.get_pad_index())
        doc_mask = (doc_tokens == self.vocab.get_pad_index()).byte()
        ques_tokens = pad_and_tensorize(ques_tokens, self.vocab.get_pad_index())
        ques_mask = (ques_tokens == self.vocab.get_pad_index()).byte()
        answer_start_idx = pad_and_tensorize(answer_start_idx, self.SPAN_PAD_IDX)
        answer_end_idx = pad_and_tensorize(answer_end_idx, self.SPAN_PAD_IDX)
        return doc_tokens, pad_and_tensorize(doc_seq_len), doc_mask, ques_tokens, pad_and_tensorize(ques_seq_len), ques_mask, answer_start_idx, answer_end_idx

    def sort_key(self, row):
        raise NotImplementedError('SquadTensorizer.sort_key() should not be called.')

    def _only_pad(self, token_id_list: List[int]) ->bool:
        for token_id in token_id_list:
            if token_id != self.SPAN_PAD_IDX:
                return False
        return True


class RnnType(Enum):
    RNN = 'rnn'
    LSTM = 'lstm'
    GRU = 'gru'


RNN_TYPE_DICT = {RnnType.RNN: nn.RNN, RnnType.LSTM: nn.LSTM, RnnType.GRU: nn.GRU}


class StackedBidirectionalRNN(Module):
    """
    `StackedBidirectionalRNN` implements a multi-layer bidirectional RNN with an
    option to return outputs from all the layers of RNN.

    Args:
        config (Config): Configuration object of type BiLSTM.Config.
        embed_dim (int): The number of expected features in the input.
        padding_value (float): Value for the padded elements. Defaults to 0.0.

    Attributes:
        padding_value (float): Value for the padded elements.
        dropout (nn.Dropout): Dropout layer preceding the LSTM.
        lstm (nn.LSTM): LSTM layer that operates on the inputs.
        representation_dim (int): The calculated dimension of the output features
            of BiLSTM.
    """


    class Config(Module.Config):
        """
        Configuration class for `StackedBidirectionalRNN`.

        Attributes:
            hidden_size (int): Number of features in the hidden state of the RNN.
                Defaults to 32.
            num_layers (int): Number of recurrent layers. Eg. setting `num_layers=2`
                would mean stacking two RNNs together to form a stacked RNN,
                with the second RNN taking in the outputs of the first RNN and
                computing the final result. Defaults to 1.
            dropout (float): Dropout probability to use. Defaults to 0.4.
            bidirectional (bool): If `True`, becomes a bidirectional RNN. Defaults
                to `True`.
            rnn_type (str): Which RNN type to use. Options: "rnn", "lstm", "gru".
            concat_layers (bool): Whether to concatenate the outputs of each layer
                of stacked RNN.
        """
        hidden_size: int = 32
        num_layers: int = 1
        dropout: float = 0.0
        bidirectional: bool = True
        rnn_type: RnnType = RnnType.LSTM
        concat_layers: bool = True

    def __init__(self, config: Config, input_size: int, padding_value: float=0.0):
        super().__init__()
        self.num_layers = config.num_layers
        self.dropout = nn.Dropout(config.dropout)
        self.concat_layers = config.concat_layers
        self.padding_value = padding_value
        self.rnns = nn.ModuleList()
        rnn_module = RNN_TYPE_DICT.get(config.rnn_type)
        assert rnn_module is not None, 'rnn_cell cannot be None'
        for i in range(config.num_layers):
            input_size = input_size if i == 0 else 2 * config.hidden_size
            self.rnns.append(rnn_module(input_size, config.hidden_size, num_layers=1, bidirectional=config.bidirectional))
        self.representation_dim = (config.num_layers if config.concat_layers else 1) * config.hidden_size * (2 if config.bidirectional else 1)
        log_class_usage(__class__)

    def forward(self, tokens, tokens_mask):
        """
        Args:
            tokens: batch, max_seq_len, hidden_size
            tokens_mask: batch, max_seq_len (1 for padding, 0 for true)
        Output:
            tokens_encoded: batch, max_seq_len, hidden_size * num_layers if
                concat_layers = True else batch, max_seq_len, hidden_size
        """
        seq_lengths = tokens_mask.eq(0).long().sum(1)
        seq_lengths_sorted, idx_of_sorted = torch.sort(seq_lengths, dim=0, descending=True)
        tokens_sorted = tokens.index_select(0, idx_of_sorted)
        packed_tokens = nn.utils.rnn.pack_padded_sequence(tokens_sorted, seq_lengths_sorted, batch_first=True)
        outputs = [packed_tokens]
        for i in range(self.num_layers):
            rnn_input = outputs[-1]
            rnn_input = nn.utils.rnn.PackedSequence(self.dropout(rnn_input.data), rnn_input.batch_sizes)
            outputs.append(self.rnns[i](rnn_input)[0])
        outputs = outputs[1:]
        for i in range(len(outputs)):
            outputs[i] = nn.utils.rnn.pad_packed_sequence(outputs[i], padding_value=self.padding_value, batch_first=True)[0]
        output = torch.cat(outputs, 2) if self.concat_layers else outputs[-1]
        _, idx_of_original = torch.sort(idx_of_sorted, dim=0)
        output = output.index_select(0, idx_of_original)
        max_seq_len = tokens_mask.size(1)
        batch_size, output_seq_len, output_dim = output.size()
        if output_seq_len != max_seq_len:
            padding = torch.zeros(batch_size, max_seq_len - output_seq_len, output_dim).type(output.data.type())
            output = torch.cat([output, padding], 1)
        return output


class PairwiseRankingLoss(Loss):
    """
    Given embeddings for a query, positive response and negative response
    computes pairwise ranking hinge loss
    """


    class Config(ConfigBase):
        margin: float = 1.0

    @staticmethod
    def get_similarities(embeddings):
        pos_embed, neg_embed, query_embed = embeddings
        pos_similarity = F.cosine_similarity(query_embed, pos_embed)
        neg_similarity = F.cosine_similarity(query_embed, neg_embed)
        return pos_similarity, neg_similarity, query_embed.size(0)

    def __call__(self, logits, targets, reduce=True):
        pos_similarity, neg_similarity, batch_size = self.get_similarities(logits)
        targets_local = FloatTensor(batch_size)
        targets_local.fill_(1)
        return F.margin_ranking_loss(pos_similarity, neg_similarity, targets_local, self.config.margin)


class AugmentedLSTMCell(nn.Module):
    """
    `AugmentedLSTMCell` implements a AugmentedLSTM cell.
    Args:
        embed_dim (int): The number of expected features in the input.
        lstm_dim (int): Number of features in the hidden state of the LSTM.
        Defaults to 32.
        use_highway (bool): If `True` we append a highway network to the
        outputs of the LSTM.
        use_bias (bool): If `True` we use a bias in our LSTM calculations, otherwise
        we don't.

    Attributes:
        input_linearity (nn.Module): Fused weight matrix which
            computes a linear function over the input.
        state_linearity (nn.Module): Fused weight matrix which
            computes a linear function over the states.
    """

    def __init__(self, embed_dim: int, lstm_dim: int, use_highway: bool, use_bias: bool=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.lstm_dim = lstm_dim
        self.use_highway = use_highway
        self.use_bias = use_bias
        if use_highway:
            self._highway_inp_proj_start = 5 * self.lstm_dim
            self._highway_inp_proj_end = 6 * self.lstm_dim
            self.input_linearity = nn.Linear(self.embed_dim, self._highway_inp_proj_end, bias=self.use_bias)
            self.state_linearity = nn.Linear(self.lstm_dim, self._highway_inp_proj_start, bias=True)
        else:
            self.input_linearity = nn.Linear(self.embed_dim, 4 * self.lstm_dim, bias=self.use_bias)
            self.state_linearity = nn.Linear(self.lstm_dim, 4 * self.lstm_dim, bias=True)
        self.reset_parameters()
        log_class_usage(__class__)

    def reset_parameters(self):
        stdv = 1.0 / math.sqrt(self.lstm_dim)
        for weight in self.parameters():
            nn.init.uniform_(weight, -stdv, stdv)

    def forward(self, x: torch.Tensor, states=Tuple[torch.Tensor, torch.Tensor], variational_dropout_mask: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Warning: DO NOT USE THIS LAYER DIRECTLY, INSTEAD USE the AugmentedLSTM class

        Args:
            x (torch.Tensor): Input tensor of shape
                (bsize x input_dim).
            states (Tuple[torch.Tensor, torch.Tensor]): Tuple of tensors containing
                the hidden state and the cell state of each element in
                the batch. Each of these tensors have a dimension of
                (bsize x nhid). Defaults to `None`.

        Returns:
            Tuple[torch.Tensor, torch.Tensor]:
                Returned states. Shape of each state is (bsize x nhid).

        """
        hidden_state, memory_state = states
        projected_input = self.input_linearity(x)
        projected_state = self.state_linearity(hidden_state)
        input_gate = forget_gate = memory_init = output_gate = highway_gate = None
        if self.use_highway:
            fused_op = projected_input[:, :5 * self.lstm_dim] + projected_state
            fused_chunked = torch.chunk(fused_op, 5, 1)
            input_gate, forget_gate, memory_init, output_gate, highway_gate = fused_chunked
            highway_gate = torch.sigmoid(highway_gate)
        else:
            fused_op = projected_input + projected_state
            input_gate, forget_gate, memory_init, output_gate = torch.chunk(fused_op, 4, 1)
        input_gate = torch.sigmoid(input_gate)
        forget_gate = torch.sigmoid(forget_gate)
        memory_init = torch.tanh(memory_init)
        output_gate = torch.sigmoid(output_gate)
        memory = input_gate * memory_init + forget_gate * memory_state
        timestep_output: torch.Tensor = output_gate * torch.tanh(memory)
        if self.use_highway:
            highway_input_projection = projected_input[:, self._highway_inp_proj_start:self._highway_inp_proj_end]
            timestep_output = highway_gate * timestep_output + (1 - highway_gate) * highway_input_projection
        if variational_dropout_mask is not None and self.training:
            timestep_output = timestep_output * variational_dropout_mask
        return timestep_output, memory


class AugmentedLSTMUnidirectional(nn.Module):
    """
    `AugmentedLSTMUnidirectional` implements a one-layer single directional
    AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally
    appends an optional highway network to the output layer. Furthermore the
    dropout controlls the level of variational dropout done.

    Args:
        embed_dim (int): The number of expected features in the input.
        lstm_dim (int): Number of features in the hidden state of the LSTM.
            Defaults to 32.
        go_forward (bool): Whether to compute features left to right (forward)
            or right to left (backward).
        recurrent_dropout_probability (float): Variational dropout probability
            to use. Defaults to 0.0.
        use_highway (bool): If `True` we append a highway network to the
            outputs of the LSTM.
        use_input_projection_bias (bool): If `True` we use a bias in
            our LSTM calculations, otherwise we don't.

    Attributes:
        cell (AugmentedLSTMCell): AugmentedLSTMCell that is applied at every timestep.
    """

    def __init__(self, embed_dim: int, lstm_dim: int, go_forward: bool=True, recurrent_dropout_probability: float=0.0, use_highway: bool=True, use_input_projection_bias: bool=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.lstm_dim = lstm_dim
        self.go_forward = go_forward
        self.use_highway = use_highway
        self.recurrent_dropout_probability = recurrent_dropout_probability
        self.cell = AugmentedLSTMCell(self.embed_dim, self.lstm_dim, self.use_highway, use_input_projection_bias)
        log_class_usage(__class__)

    def get_dropout_mask(self, dropout_probability: float, tensor_for_masking: torch.Tensor) ->torch.Tensor:
        binary_mask = torch.rand(tensor_for_masking.size()) > dropout_probability
        dropout_mask = binary_mask.float().div(1.0 - dropout_probability)
        return dropout_mask

    def forward(self, inputs: PackedSequence, states: Optional[Tuple[torch.Tensor, torch.Tensor]]=None) ->Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Warning: DO NOT USE THIS LAYER DIRECTLY, INSTEAD USE the AugmentedLSTM class

        Given an input batch of sequential data such as word embeddings, produces
        a single layer unidirectional AugmentedLSTM representation of the sequential
        input and new state tensors.

        Args:
            inputs (PackedSequence): Input tensor of shape
                (bsize x seq_len x input_dim).
            states (Tuple[torch.Tensor, torch.Tensor]): Tuple of tensors containing
                the initial hidden state and the cell state of each element in
                the batch. Each of these tensors have a dimension of
                (1 x bsize x num_directions * nhid). Defaults to `None`.

        Returns:
            Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]:
                AgumentedLSTM representation of input and the
                state of the LSTM `t = seq_len`.
                Shape of representation is (bsize x seq_len x representation_dim).
                Shape of each state is (1 x bsize x nhid).

        """
        sequence_tensor, batch_lengths = pad_packed_sequence(inputs, batch_first=True)
        batch_size = sequence_tensor.size()[0]
        total_timesteps = sequence_tensor.size()[1]
        output_accumulator = sequence_tensor.new_zeros(batch_size, total_timesteps, self.lstm_dim)
        if states is None:
            full_batch_previous_memory = sequence_tensor.new_zeros(batch_size, self.lstm_dim)
            full_batch_previous_state = sequence_tensor.data.new_zeros(batch_size, self.lstm_dim)
        else:
            full_batch_previous_state = states[0].squeeze(0)
            full_batch_previous_memory = states[1].squeeze(0)
        current_length_index = batch_size - 1 if self.go_forward else 0
        if self.recurrent_dropout_probability > 0.0:
            dropout_mask = self.get_dropout_mask(self.recurrent_dropout_probability, full_batch_previous_memory)
        else:
            dropout_mask = None
        for timestep in range(total_timesteps):
            index = timestep if self.go_forward else total_timesteps - timestep - 1
            if self.go_forward:
                while batch_lengths[current_length_index] <= index:
                    current_length_index -= 1
            else:
                while current_length_index < len(batch_lengths) - 1 and batch_lengths[current_length_index + 1] > index:
                    current_length_index += 1
            previous_memory = full_batch_previous_memory[0:current_length_index + 1].clone()
            previous_state = full_batch_previous_state[0:current_length_index + 1].clone()
            timestep_input = sequence_tensor[0:current_length_index + 1, index]
            timestep_output, memory = self.cell(timestep_input, (previous_state, previous_memory), dropout_mask[0:current_length_index + 1] if dropout_mask is not None else None)
            full_batch_previous_memory = full_batch_previous_memory.data.clone()
            full_batch_previous_state = full_batch_previous_state.data.clone()
            full_batch_previous_memory[0:current_length_index + 1] = memory
            full_batch_previous_state[0:current_length_index + 1] = timestep_output
            output_accumulator[0:current_length_index + 1, index, :] = timestep_output
        output_accumulator = pack_padded_sequence(output_accumulator, batch_lengths, batch_first=True)
        final_state = full_batch_previous_state.unsqueeze(0), full_batch_previous_memory.unsqueeze(0)
        return output_accumulator, final_state


class ContextualWordConvolution(nn.Module):

    def __init__(self, in_channels: int, out_channels: int, kernel_sizes: List[int]):
        super().__init__()
        self.max_pool = nn.AdaptiveMaxPool1d(1)
        self.convs = nn.ModuleList([nn.Conv1d(in_channels, out_channels, k, padding=k - 1) for k in kernel_sizes])
        token_rep_size = len(kernel_sizes) * out_channels
        self.fc = nn.Linear(token_rep_size, token_rep_size)
        log_class_usage

    def forward(self, words: torch.Tensor):
        words = words.transpose(1, 2)
        conv_outs = [F.relu(conv(words)) for conv in self.convs]
        mp_outs = [self.max_pool(co).squeeze(2) for co in conv_outs]
        return self.fc(torch.cat(mp_outs, 1))


class Trim1d(nn.Module):
    """
    Trims a 1d convolutional output. Used to implement history-padding
    by removing excess padding from the right.

    """

    def __init__(self, trim):
        super(Trim1d, self).__init__()
        self.trim = trim

    def forward(self, x):
        return x[:, :, :-self.trim].contiguous()


class SeparableConv1d(nn.Module):
    """
    Implements a 1d depthwise separable convolutional layer. In regular convolutional
    layers, the input channels are mixed with each other to produce each output channel.
    Depthwise separable convolutions decompose this process into two smaller
    convolutions -- a depthwise and pointwise convolution.

    The depthwise convolution spatially convolves each input channel separately,
    then the pointwise convolution projects this result into a new channel space.
    This process reduces the number of FLOPS used to compute a convolution and also
    exhibits a regularization effect. The general behavior -- including the input
    parameters -- is equivalent to `nn.Conv1d`.

    `bottleneck` controls the behavior of the pointwise convolution. Instead of
    upsampling directly, we split the pointwise convolution into two pieces: the first
    convolution downsamples into a (sufficiently small) low dimension and the
    second convolution upsamples into the target (higher) dimension. Creating this
    bottleneck significantly cuts the number of parameters with minimal loss
    in performance.

    """

    def __init__(self, input_channels: int, output_channels: int, kernel_size: int, padding: int, dilation: int, bottleneck: int):
        super(SeparableConv1d, self).__init__()
        conv_layers = [nn.Conv1d(input_channels, input_channels, kernel_size, padding=padding, dilation=dilation, groups=input_channels)]
        if bottleneck > 0:
            conv_layers.extend([nn.Conv1d(input_channels, bottleneck, 1), nn.Conv1d(bottleneck, output_channels, 1)])
        else:
            conv_layers.append(nn.Conv1d(input_channels, output_channels, 1))
        self.conv = nn.Sequential(*conv_layers)

    def forward(self, x):
        return self.conv(x)


class OrderedNeuronLSTMLayer(Module):

    def __init__(self, embed_dim: int, lstm_dim: int, padding_value: float, dropout: float) ->None:
        super().__init__()
        self.lstm_dim = lstm_dim
        self.padding_value = padding_value
        self.dropout = nn.Dropout(dropout)
        total_size = embed_dim + lstm_dim
        self.f_gate = nn.Linear(total_size, lstm_dim)
        self.i_gate = nn.Linear(total_size, lstm_dim)
        self.o_gate = nn.Linear(total_size, lstm_dim)
        self.c_hat_gate = nn.Linear(total_size, lstm_dim)
        self.master_forget_no_cumax_gate = nn.Linear(total_size, lstm_dim)
        self.master_input_no_cumax_gate = nn.Linear(total_size, lstm_dim)
        log_class_usage(__class__)

    def forward(self, embedded_tokens: torch.Tensor, states: Tuple[torch.Tensor, torch.Tensor], seq_lengths: List[int]) ->Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        hidden, context = states
        batch_size = hidden.size(0)
        all_context = []
        all_hidden = []
        if self.dropout.p > 0.0:
            embedded_tokens = self.dropout(embedded_tokens)
        for batch in embedded_tokens:
            combined = torch.cat((batch, hidden), 1)
            ft = self.f_gate(combined).sigmoid()
            it = self.i_gate(combined).sigmoid()
            ot = self.o_gate(combined).sigmoid()
            c_hat = self.c_hat_gate(combined).tanh()
            master_forget_no_cumax = self.master_forget_no_cumax_gate(combined)
            master_forget = torch.cumsum(F.softmax(master_forget_no_cumax, dim=1), dim=1)
            master_input_no_cumax = self.master_input_no_cumax_gate(combined)
            master_input = torch.cumsum(F.softmax(master_input_no_cumax, dim=1), dim=1)
            wt = master_forget * master_input
            f_hat_t = ft * wt + (master_forget - wt)
            i_hat_t = it * wt + (master_input - wt)
            context = f_hat_t * context + i_hat_t * c_hat
            hidden = ot * context
            all_context.append(context)
            all_hidden.append(hidden)
        state_hidden = []
        state_context = []
        for i in range(batch_size):
            seq_length = seq_lengths[i]
            state_hidden.append(all_hidden[seq_length - 1][i])
            state_context.append(all_context[seq_length - 1][i])
        return torch.stack(all_hidden), (torch.stack(state_hidden), torch.stack(state_context))


class BoundaryPool(Module):


    class Config(ConfigBase):
        boundary_type: str = 'first'

    def __init__(self, config: Config, n_input: int) ->None:
        super().__init__(config)
        self.boundary_type = config.boundary_type
        log_class_usage(__class__)

    def forward(self, inputs: torch.Tensor, seq_lengths: torch.Tensor=None) ->torch.Tensor:
        max_len = inputs.size()[1]
        if self.boundary_type == 'first':
            return inputs[:, 0, :]
        elif self.boundary_type == 'last':
            assert max_len > 1
            return inputs[:, max_len - 1, :]
        elif self.boundary_type == 'firstlast':
            assert max_len > 1
            return torch.cat((inputs[:, 0, :], inputs[:, max_len - 1, :]), dim=1)
        else:
            raise Exception('Unknown configuration type {}'.format(self.boundary_type))


class SlotAttentionType(Enum):
    NO_ATTENTION = 'no_attention'
    CONCAT = 'concat'
    MULTIPLY = 'multiply'
    DOT = 'dot'


class SlotAttention(Module):


    class Config(ConfigBase):
        attn_dimension: int = 64
        attention_type: SlotAttentionType = SlotAttentionType.NO_ATTENTION

    def __init__(self, config: Config, n_input: int, batch_first: bool=True) ->None:
        super().__init__()
        self.batch_first = batch_first
        self.attention_type = config.attention_type
        if self.attention_type == SlotAttentionType.CONCAT:
            self.attention_add = nn.Sequential(nn.Linear(2 * n_input, config.attn_dimension, bias=False), nn.Tanh(), nn.Linear(config.attn_dimension, 1, bias=False))
        elif self.attention_type == SlotAttentionType.MULTIPLY:
            self.attention_mult = nn.Linear(n_input, n_input, bias=False)
        log_class_usage(__class__)

    def forward(self, inputs: torch.Tensor) ->torch.Tensor:
        if isinstance(inputs, PackedSequence):
            inputs, lengths = pad_packed_sequence(inputs, batch_first=self.batch_first)
        size = inputs.size()
        exp_inputs_2 = inputs.unsqueeze(1).expand(size[0], size[1], size[1], size[2])
        if self.attention_type == SlotAttentionType.CONCAT:
            exp_inputs_1 = inputs.unsqueeze(2).expand(size[0], size[1], size[1], size[2])
            catted = torch.cat((exp_inputs_1, exp_inputs_2), 3)
            attn_weights_add = F.softmax(self.attention_add(catted).squeeze(3), dim=2).unsqueeze(2)
            context_add = torch.matmul(attn_weights_add, exp_inputs_2).squeeze(2)
            output = torch.cat((inputs, context_add), 2)
        elif self.attention_type == SlotAttentionType.MULTIPLY or self.attention_type == SlotAttentionType.DOT:
            attended = inputs if self.attention_type == SlotAttentionType.DOT else self.attention_mult(inputs)
            attn_weights_mult = F.softmax(torch.matmul(inputs, torch.transpose(attended, 1, 2)), dim=2).unsqueeze(2)
            context_mult = torch.matmul(attn_weights_mult, exp_inputs_2).squeeze(2)
            output = torch.cat((inputs, context_mult), 2)
        else:
            output = inputs
        return output


def efficient_causal_attention_parallel(x, y, z):
    """
    efficient causal attention operation
    Args:
        x (Tensor): Tensor with shape `(batch, n, d1)`
        y (Tensor): Tensor with shape `(batch, n, d1)`
        z (Tensor): Tensor with shape '(batch, n, d2)`
    return:
    """
    bsz, n, d1 = x.size()
    sum_mat = torch.matmul(y.unsqueeze(3), z.unsqueeze(2))
    accum_mat = torch.cumsum(sum_mat, dim=1)
    res = torch.matmul(x.unsqueeze(2), accum_mat).squeeze(2)
    length_div = torch.arange(1, n + 1, device=x.device).unsqueeze(0).unsqueeze(2)
    res = res / length_div
    return res


def efficient_causal_attention_seq(x, y, z):
    """
    efficient causal attention operation
    Args:
        x (Tensor): Tensor with shape `(batch, n, d1)`
        y (Tensor): Tensor with shape `(batch, n, d1)`
        z (Tensor): Tensor with shape '(batch, n, d2)`
    return:
    """
    n = x.size(1)
    rets = []
    accum_mat = 0
    for i in range(n):
        xx = x[:, i:i + 1]
        yy = y[:, i:i + 1]
        zz = z[:, i:i + 1]
        accum_mat = accum_mat + torch.bmm(yy.transpose(1, 2), zz)
        rets.append(torch.bmm(xx, accum_mat).div(i + 1.0))
    return torch.cat(rets, dim=1)


def incremental_causal_attention(x, y, z, accum_mat, n):
    """
    efficient causal attention operation
    Args:
        x (Tensor): Tensor with shape `(batch, 1, d1)`
        y (Tensor): Tensor with shape `(batch, 1, d1)`
        z (Tensor): Tensor with shape `(batch, 1, d2)`
        accum_mat (Tensor): Tensor with shape `(batch, d1, d2)`
        n (Tensor): number of steps with shape `(batch, )`
    return:
    """
    bsz = n.size(0)
    accum_mat = accum_mat + torch.bmm(y.transpose(1, 2), z)
    out = torch.bmm(x, accum_mat).div(n.view(bsz, 1, 1))
    return out, accum_mat


class LunaSentenceEncoderLayer(nn.Module):
    """
    Implements a Luna Encoder Layer used in masked pre-trained language models
    and fine-tuned classication/regression mdoel
    """

    def __init__(self, embedding_dim: int=768, ffn_embedding_dim: int=3072, num_attention_heads: int=8, num_projected_attention_heads: int=8, dropout: float=0.1, attention_dropout: float=0.1, activation_dropout: float=0.1, activation_fn: str='relu', normalize_before: bool=False, tie_kv=True, export: bool=False, init_fn: Callable=None) ->None:
        super().__init__()
        if init_fn is not None:
            init_fn()
        self.embedding_dim = embedding_dim
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.activation_dropout_module = FairseqDropout(activation_dropout, module_name=self.__class__.__name__)
        self.activation_fn = utils.get_activation_fn(activation_fn)
        self.self_attn = self.build_self_attention(self.embedding_dim, num_attention_heads, num_projected_attention_heads, dropout=attention_dropout, tie_kv=tie_kv)
        self.normalize_before = normalize_before
        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)
        self.self_atten_proj_layer_norm = LayerNorm(self.embedding_dim, export=export)
        self.fc1 = self.build_fc1(self.embedding_dim, ffn_embedding_dim)
        self.fc2 = self.build_fc2(ffn_embedding_dim, self.embedding_dim)
        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)

    def build_fc1(self, input_dim, output_dim):
        return nn.Linear(input_dim, output_dim)

    def build_fc2(self, input_dim, output_dim):
        return nn.Linear(input_dim, output_dim)

    def build_self_attention(self, embed_dim, num_attention_heads, num_projected_attention_heads, dropout, tie_kv, q_noise, qn_block_size):
        return LunarMultiheadAttention(embed_dim, num_attention_heads, num_projected_attention_heads, dropout=dropout, self_attention=True, tie_kv=tie_kv, q_noise=q_noise, qn_block_size=qn_block_size)

    def forward(self, x: torch.Tensor, px: torch.Tensor, x_padding_mask: Optional[torch.Tensor]=None, px_padding_mask: Optional[torch.Tensor]=None):
        """
        LayerNorm is applied either before or after the self-attention/ffn
        modules similar to the original Transformer implementation.
        """
        residual = x
        presidual = px
        if self.normalize_before:
            x = self.self_attn_layer_norm(x)
            px = self.self_atten_proj_layer_norm(px)
        x, px, attn = self.self_attn(query=x, pquery=px, context=x, context_padding_mask=x_padding_mask, pcontext_padding_mask=px_padding_mask, need_weights=False)
        x = self.dropout_module(x)
        px = self.dropout_module(px)
        x = residual + x
        px = presidual + px
        if not self.normalize_before:
            x = self.self_attn_layer_norm(x)
            px = self.self_atten_proj_layer_norm(px)
        residual = x
        if self.normalize_before:
            x = self.final_layer_norm(x)
        x = self.activation_fn(self.fc1(x))
        x = self.activation_dropout_module(x)
        x = self.fc2(x)
        x = self.dropout_module(x)
        x = residual + x
        if not self.normalize_before:
            x = self.final_layer_norm(x)
        return x, px, attn


def get_sinusoidal_positional_embedding(length, embed_dim):
    half_dim = embed_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
    emb = torch.arange(length, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(length, -1)
    if embed_dim % 2 == 1:
        emb = torch.cat([emb, torch.zeros(length, 1)], dim=1)
    return emb


def init_bert_params(module):
    """
    Initialize the weights specific to the BERT Model.
    This overrides the default initializations depending on the specified arguments.
        1. If normal_init_linear_weights is set then weights of linear
           layer will be initialized using the normal distribution and
           bais will be set to the specified value.
        2. If normal_init_embed_weights is set then weights of embedding
           layer will be initialized using the normal distribution.
        3. If normal_init_proj_weights is set then weights of
           in_project_weight for MultiHeadAttention initialized using
           the normal distribution (to be validated).
    """
    if isinstance(module, LunaSentenceEncoder):
        module.projected_embeddings.data.normal_(mean=0.0, std=0.02)
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=0.02)
        if module.bias is not None:
            module.bias.data.zero_()
    if isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=0.02)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()
    if isinstance(module, LunarMultiheadAttention):
        module.pq_proj.weight.data.normal_(mean=0.0, std=0.02)
        module.q_proj.weight.data.normal_(mean=0.0, std=0.02)
        if module.pc_proj is not None:
            module.pc_proj.weight.data.normal_(mean=0.0, std=0.02)
            module.c_proj.weight.data.normal_(mean=0.0, std=0.02)
        else:
            module.pk_proj.weight.data.normal_(mean=0.0, std=0.02)
            module.pv_proj.weight.data.normal_(mean=0.0, std=0.02)
            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)
            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)


class LunaSentenceEncoder(nn.Module):
    """
    Implementation for a Bi-directional Luna based Sentence Encoder used
    in masked pre-trained language models.
    This first computes the token embedding using the token embedding matrix,
    position embeddings (if specified) and segment embeddings
    (if specified). After applying the specified number of
    TransformerEncoderLayers, it outputs all the internal states of the
    encoder as well as the final representation associated with the first
    token (usually CLS token).
    Input:
        - tokens: B x T matrix representing sentences
        - segment_labels: B x T matrix representing segment label for tokens
    Output:
        - a tuple of the following:
            - a list of internal model states used to compute the
              predictions where each tensor has shape T x B x C
            - sentence representation associated with first input token
              in format B x C.
    """

    def __init__(self, padding_idx: int, vocab_size: int, projection_length: int=128, num_encoder_layers: int=12, embedding_dim: int=768, ffn_embedding_dim: int=3072, num_attention_heads: int=12, num_projected_attention_heads: int=12, dropout: float=0.1, attention_dropout: float=0.1, activation_dropout: float=0.1, layerdrop: float=0.0, max_seq_len: int=512, num_segments: int=0, use_position_embeddings: bool=True, offset_positions_by_padding: bool=True, layernorm_embedding: bool=False, normalize_before: bool=False, dynamic_projection: bool=True, tie_kv=False, apply_bert_init: bool=False, activation_fn: str='gelu', learned_pos_embedding: bool=True, embed_scale: float=None, freeze_embeddings: bool=False, n_trans_layers_to_freeze: int=0, export: bool=False, traceable: bool=False) ->None:
        super().__init__()
        self.padding_idx = padding_idx
        self.vocab_size = vocab_size
        self.proj_len = projection_length
        self.dynamic_projection = dynamic_projection
        self.dropout_module = FairseqDropout(dropout, module_name=self.__class__.__name__)
        self.layerdrop = layerdrop
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim
        self.num_segments = num_segments
        self.use_position_embeddings = use_position_embeddings
        self.apply_bert_init = apply_bert_init
        self.learned_pos_embedding = learned_pos_embedding
        self.traceable = traceable
        self.tpu = False
        self.embed_tokens = self.build_embedding(self.vocab_size, self.embedding_dim, self.padding_idx)
        self.embed_scale = embed_scale
        if self.num_segments > 0:
            self.segment_embeddings = nn.Embedding(self.num_segments, self.embedding_dim, padding_idx=None)
            nn.init.normal_(self.segment_embeddings.weight, mean=0.0, std=self.embedding_dim ** -0.5)
        else:
            self.segment_embeddings = None
        self.embed_positions = PositionalEmbedding(self.max_seq_len, self.embedding_dim, padding_idx=self.padding_idx if offset_positions_by_padding else None, learned=self.learned_pos_embedding) if self.use_position_embeddings else None
        self.projected_embeddings = Parameter(torch.Tensor(self.proj_len, self.embedding_dim))
        nn.init.normal_(self.projected_embeddings, mean=0.0, std=self.embedding_dim ** -0.5)
        if self.use_position_embeddings and not self.learned_pos_embedding:
            projected_positions = get_sinusoidal_positional_embedding(self.proj_len, self.embedding_dim)
            if self.embed_scale is None:
                self.embed_scale = math.sqrt(self.embedding_dim)
        else:
            projected_positions = None
        self.register_buffer('projected_positions', projected_positions)
        if self.layerdrop > 0.0:
            self.layers = LayerDropModuleList(p=self.layerdrop)
        else:
            self.layers = nn.ModuleList([])
        self.layers.extend([self.build_luna_sentence_encoder_layer(embedding_dim=self.embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, num_projected_attention_heads=num_projected_attention_heads, dropout=self.dropout_module.p, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, normalize_before=normalize_before, tie_kv=tie_kv, export=export) for _ in range(num_encoder_layers)])
        assert not layernorm_embedding or not normalize_before
        if layernorm_embedding:
            self.emb_layer_norm = LayerNorm(self.embedding_dim, export=export)
            self.proj_emb_layer_norm = LayerNorm(self.embedding_dim, export=export)
        else:
            self.emb_layer_norm = None
            self.proj_emb_layer_norm = None
        if normalize_before:
            self.layer_norm = LayerNorm(self.embedding_dim, export=export)
            self.proj_layer_norm = LayerNorm(self.embedding_dim, export=export)
        else:
            self.layer_norm = None
            self.proj_layer_norm = None
        if self.apply_bert_init:
            self.apply(init_bert_params)

        def freeze_module_params(m):
            if m is not None:
                for p in m.parameters():
                    p.requires_grad = False
        if freeze_embeddings:
            self.projected_embeddings.requires_grad = False
            freeze_module_params(self.embed_tokens)
            freeze_module_params(self.segment_embeddings)
            freeze_module_params(self.embed_positions)
            freeze_module_params(self.emb_layer_norm)
            freeze_module_params(self.proj_emb_layer_norm)
        for layer in range(n_trans_layers_to_freeze):
            freeze_module_params(self.layers[layer])
        log_class_usage(__class__)

    def build_embedding(self, vocab_size, embedding_dim, padding_idx):
        embed_tokens = nn.Embedding(vocab_size, embedding_dim, padding_idx)
        nn.init.normal_(embed_tokens.weight, mean=0, std=embedding_dim ** -0.5)
        return embed_tokens

    def build_luna_sentence_encoder_layer(self, embedding_dim, ffn_embedding_dim, num_attention_heads, num_projected_attention_heads, dropout, attention_dropout, activation_dropout, activation_fn, normalize_before, tie_kv, export, q_noise, qn_block_size):
        return LunaSentenceEncoderLayer(embedding_dim=embedding_dim, ffn_embedding_dim=ffn_embedding_dim, num_attention_heads=num_attention_heads, num_projected_attention_heads=num_projected_attention_heads, dropout=dropout, attention_dropout=attention_dropout, activation_dropout=activation_dropout, activation_fn=activation_fn, normalize_before=normalize_before, tie_kv=tie_kv, export=export, q_noise=q_noise, qn_block_size=qn_block_size)

    def prepare_for_tpu_(self, **kwargs):
        self.tpu = True

    def forward(self, tokens: torch.Tensor, segment_labels: torch.Tensor=None, last_state_only: bool=False, positions: Optional[torch.Tensor]=None):
        x_padding_mask = tokens.eq(self.padding_idx)
        lengths = tokens.size(1) - x_padding_mask.sum(1)
        max_len = lengths.max() if self.dynamic_projection else self.proj_len
        x = self.embed_tokens(tokens)
        px = self.projected_embeddings[:max_len]
        if self.embed_scale is not None:
            x *= self.embed_scale
            px *= self.embed_scale
        if self.embed_positions is not None:
            x += self.embed_positions(tokens, positions=positions)
        if self.projected_positions is not None:
            px += self.projected_positions[:max_len]
        if self.segment_embeddings is not None and segment_labels is not None:
            x += self.segment_embeddings(segment_labels)
        if self.quant_noise is not None:
            x = self.quant_noise(x)
        if self.emb_layer_norm is not None:
            x = self.emb_layer_norm(x)
            px = self.proj_emb_layer_norm(px)
        bsz = x.size(0)
        len, dim = px.size()
        px = px.unsqueeze(0).expand(bsz, len, dim)
        if self.dynamic_projection:
            pidx = torch.arange(len).unsqueeze(0)
            px_padding_mask = pidx.ge(lengths.unsqueeze(1))
        else:
            px_padding_mask = None
        if not self.traceable and not self.tpu:
            if not x_padding_mask.any():
                x_padding_mask = None
            if px_padding_mask is not None and not px_padding_mask.any():
                px_padding_mask = None
        x = self.dropout_module(x)
        px = self.dropout_module(px)
        if x_padding_mask is not None:
            x = x * (1 - x_padding_mask.unsqueeze(-1).type_as(x))
        if px_padding_mask is not None:
            px = px * (1 - px_padding_mask.unsqueeze(-1).type_as(px))
        x = x.transpose(0, 1)
        px = px.transpose(0, 1)
        inner_states = []
        if not last_state_only:
            inner_states.append(x)
        for layer in self.layers:
            x, px, _ = layer(x, px, x_padding_mask=x_padding_mask, px_padding_mask=px_padding_mask)
            if not last_state_only:
                inner_states.append(x)
        if self.layer_norm is not None:
            x = self.layer_norm(x)
            px = self.proj_layer_norm(px)
        if last_state_only:
            inner_states = [x]
        return inner_states


class MultiheadLinearAttention(nn.Module):
    """
    This is a TorchScriptable implementation of MultiheadLinearAttention:
    https://arxiv.org/pdf/2006.04768.pdf. from fairseq for the purposes of
    creating a productionized Linformer model. It distills just
    the elements which are required to implement the RoBERTa use cases of
    MultiheadLinearAttention, and within that is restructured and rewritten to be able
    to be compiled by TorchScript for production use cases.

    The default constructor values match those required to import the public
    RoBERTa weights. Unless you are pretraining your own model, there's no need to
    change them.
    """

    def __init__(self, embed_dim: int, num_heads: int, scaling: float=0.125, dropout: float=0.1, compress_layer=None, bias: bool=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scaling = scaling
        self.dropout = nn.Dropout(dropout)
        self.kput_projection = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.vput_projection = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.qput_projection = nn.Linear(embed_dim, embed_dim, bias=bias)
        self.output_projection = nn.Linear(embed_dim, embed_dim)
        self.compress_k = compress_layer
        log_class_usage(__class__)

    def prune_multi_linear_heads(self, heads: List[int]):
        mask = torch.ones(self.num_heads, self.head_dim)
        for head in heads:
            mask[head] = 0
        mask = mask.view(-1).contiguous().eq(1)
        if torch.onnx.is_in_onnx_export():
            index = np.arange(len(mask), dtype=np.int64)
            index = torch.from_numpy(index)
        else:
            index = torch.arange(len(mask), dtype=torch.long, device=mask.device)
        self.kput_projection = self._prune_linear_layer(self.kput_projection, index[mask], dim=0)
        self.qput_projection = self._prune_linear_layer(self.qput_projection, index[mask], dim=0)
        self.vput_projection = self._prune_linear_layer(self.vput_projection, index[mask], dim=0)
        self.output_projection = self._prune_linear_layer(self.output_projection, index[mask], dim=1)
        self.num_heads = self.num_heads - len(heads)

    def _prune_linear_layer(self, layer: torch.nn.Linear, index: torch.Tensor, dim: int=0):
        """
        Prune a linear layer (a model parameters) to keep only entries in index.
        Return the pruned layer as a new layer with requires_grad=True.
        Used to remove heads.
        """
        index = index
        W = layer.weight.index_select(dim, index).clone().detach()
        if layer.bias is not None:
            if dim == 1:
                b = layer.bias.clone().detach()
            else:
                b = layer.bias[index].clone().detach()
        new_size = list(layer.weight.size())
        new_size[dim] = len(index)
        new_layer = torch.nn.Linear(new_size[1], new_size[0], bias=layer.bias is not None)
        new_layer.weight.requires_grad = False
        new_layer.weight.copy_(W.contiguous())
        new_layer.weight.requires_grad = True
        if layer.bias is not None:
            new_layer.bias.requires_grad = False
            new_layer.bias.copy_(b.contiguous())
            new_layer.bias.requires_grad = True
        return new_layer

    def get_compressed_projection(self, k_input: torch.Tensor, v_input: torch.Tensor, target_length: int) ->Tuple[torch.Tensor, torch.Tensor]:
        k_input = F.linear(k_input, self.compress_k.weight[:, 0:target_length]).permute(2, 0, 1).contiguous()
        v_input = F.linear(v_input, self.compress_k.weight[:, 0:target_length]).permute(2, 0, 1).contiguous()
        return k_input, v_input

    def forward(self, query, key_padding_mask):
        """Input shape: Time x Batch x Channel
        Timesteps can be masked by supplying a T x T mask in the
        `attn_mask` argument. Padding elements can be excluded from
        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:
        batch x source_length, where padding elements are indicated by 1s.
        """
        target_length, batch_size, embed_dim = query.size()
        mask_batch_size, source_length = key_padding_mask.size()
        assert embed_dim == self.embed_dim
        assert batch_size == mask_batch_size, 'query and key_padding_mask batch sizes differed'
        q = self.qput_projection(query)
        q *= self.scaling
        k_input = query.permute(1, 2, 0).contiguous()
        v_input = query.permute(1, 2, 0).contiguous()
        k_input, v_input = self.get_compressed_projection(k_input, v_input, target_length)
        k = self.kput_projection(k_input)
        v = self.vput_projection(v_input)
        batch_heads = batch_size * self.num_heads
        q = q.contiguous().view(-1, batch_heads, self.head_dim).transpose(0, 1)
        k = k.contiguous().view(-1, batch_heads, self.head_dim).transpose(0, 1)
        v = v.contiguous().view(-1, batch_heads, self.head_dim).transpose(0, 1)
        source_length = k.size(1)
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        assert list(attn_weights.shape) == [batch_heads, target_length, source_length]
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).type_as(attn_weights)
        attn_weights = self.dropout(attn_weights)
        attn = torch.bmm(attn_weights, v)
        assert list(attn.shape) == [batch_heads, target_length, self.head_dim]
        attn = attn.transpose(0, 1).contiguous().view(target_length, batch_size, self.head_dim * self.num_heads)
        attn = self.output_projection(attn)
        return attn


class QuantizedMultiheadLinearAttention(MultiheadLinearAttention):

    def __init__(self, embed_dim: int, num_heads: int, scaling: float=0.125, dropout: float=0.1, compress_layer=None, bias: bool=True):
        super().__init__(embed_dim=embed_dim, num_heads=num_heads, scaling=scaling, dropout=dropout, compress_layer=compress_layer, bias=bias)
        log_class_usage(__class__)

    def get_compressed_projection(self, k_input: torch.Tensor, v_input: torch.Tensor, target_length: int) ->Tuple[torch.Tensor, torch.Tensor]:
        assert self.compress_k.in_features >= target_length
        pad = 0, self.compress_k.in_features - target_length
        k_input = F.pad(k_input, pad)
        v_input = F.pad(v_input, pad)
        k_input = self.compress_k(k_input).permute(2, 0, 1).contiguous()
        v_input = self.compress_k(v_input).permute(2, 0, 1).contiguous()
        return k_input, v_input


class TransformerRepresentation(Module):
    """
    Representation consisting of stacked multi-head self-attention
    and position-wise feed-forward layers. Unlike `Transformer`, we assume
    inputs are already embedded, thus this representation can be used as
    a drop-in replacement for other temporal representations over
    text inputs (e.g., `BiLSTM` and `DeepCNNDeepCNNRepresentation`).
    """


    class Config(ConfigBase):
        num_layers: int = 3
        num_attention_heads: int = 4
        ffnn_embed_dim: int = 32
        dropout: float = 0.0
        scaling: Optional[float] = None

    def __init__(self, config: Config, embed_dim: int) ->None:
        super().__init__()
        self.layers = nn.ModuleList([self._create_transformer_layer(config, embed_dim) for _ in range(config.num_layers)])
        log_class_usage(__class__)

    def _create_transformer_layer(self, config: Config, embed_dim: int):
        return TransformerLayer(embedding_dim=embed_dim, attention=MultiheadSelfAttention(embed_dim=embed_dim, num_heads=config.num_attention_heads, scaling=config.scaling), residual_mlp=ResidualMLP(input_dim=embed_dim, hidden_dims=[config.ffnn_embed_dim], dropout=config.dropout))

    def forward(self, embedded_tokens: Tensor, padding_mask: Tensor) ->Tensor:
        """
        Forward inputs through the transformer layers.

        Args:
            embedded_tokens (B x T x H): Tokens previously encoded with token,
            positional, and segment embeddings.
            padding_mask (B x T): Boolean mask specifying token positions that
            self-attention should not operate on.

        Returns:
            last_state (B x T x H): Final transformer layer state.
        """
        last_state = embedded_tokens.transpose(0, 1)
        for layer in self.layers:
            last_state = layer(last_state, padding_mask)
        return last_state.transpose(0, 1)


def check_state_keys(state, keys_regex):
    """check if keys exists in state using full python paths"""
    regex = re.compile(keys_regex)
    for k, v in state.items():
        if regex.findall(k):
            return True
    return False


def merge_input_projection(state):
    """
    New checkpoints of fairseq multihead attention split in_projections into
    k,v,q projections. This function merge them back to to make it compatible.
    """
    items_to_add = {}
    keys_to_remove = []
    bias_suffix = ['q_proj.bias', 'k_proj.bias', 'v_proj.bias']
    weight_suffix = ['q_proj.weight', 'k_proj.weight', 'v_proj.weight']

    def override_state(k, suffix, new_suffix, idx):
        new_key = k[:-len(suffix)] + new_suffix
        dim = state[k].shape[0]
        if new_key not in items_to_add:
            items_to_add[new_key] = torch.zeros_like(state[k]).repeat(3, 1) if len(state[k].shape) > 1 else torch.zeros_like(state[k]).repeat(3)
        items_to_add[new_key][idx * dim:(idx + 1) * dim] = state[k]
        keys_to_remove.append(k)
    for k in state.keys():
        for idx, suffix in enumerate(weight_suffix):
            if k.endswith(suffix):
                override_state(k, suffix, 'in_proj_weight', idx)
        for idx, suffix in enumerate(bias_suffix):
            if k.endswith(suffix):
                override_state(k, suffix, 'in_proj_bias', idx)
    for k in keys_to_remove:
        del state[k]
    for key, value in items_to_add.items():
        state[key] = value
    return state


def remove_state_keys(state, keys_regex):
    """Remove keys from state that match a regex"""
    regex = re.compile(keys_regex)
    return {k: v for k, v in state.items() if not regex.findall(k)}


def rename_state_keys(state, keys_regex, replacement):
    """Rename keys from state that match a regex; replacement can use capture groups"""
    regex = re.compile(keys_regex)
    return {(k if not regex.findall(k) else regex.sub(replacement, k)): v for k, v in state.items()}


def rename_component_from_root(state, old_name, new_name):
    """Rename keys from state using full python paths"""
    return rename_state_keys(state, '^' + old_name.replace('.', '\\.') + '.?(.*)$', new_name + '.\\1')


def translate_roberta_state_dict(state_dict):
    """Translate the public RoBERTa weights to ones which match SentenceEncoder."""
    new_state = rename_component_from_root(state_dict, 'decoder.sentence_encoder', 'transformer')
    new_state = rename_component_from_root(new_state, 'encoder.sentence_encoder', 'transformer')
    new_state = rename_state_keys(new_state, 'embed_tokens', 'token_embedding')
    new_state = rename_state_keys(new_state, 'embed_positions', 'positional_embedding.embedding')
    new_state = rename_state_keys(new_state, 'emb_layer_norm', 'embedding_layer_norm')
    new_state = rename_state_keys(new_state, 'self_attn', 'attention')
    if check_state_keys(new_state, 'compress_k'):
        new_state = remove_state_keys(new_state, 'compress_layer')
    else:
        new_state = merge_input_projection(new_state)
    new_state = rename_state_keys(new_state, '_proj.(.*)', 'put_projection.\\1')
    new_state = rename_state_keys(new_state, 'fc1', 'residual_mlp.mlp.0')
    new_state = rename_state_keys(new_state, 'fc2', 'residual_mlp.mlp.3')
    new_state = remove_state_keys(new_state, '^sentence_')
    new_state = remove_state_keys(new_state, '_classification_head.')
    new_state = remove_state_keys(new_state, '^decoder\\.lm_head')
    new_state = remove_state_keys(new_state, '^encoder\\.lm_head')
    new_state = remove_state_keys(new_state, 'segment_embedding')
    return new_state


class PoolingMethod(Enum):
    """
    Pooling Methods are chosen from the "Feature-based Approachs" section in
    https://arxiv.org/pdf/1810.04805.pdf
    """
    AVG_CONCAT_LAST_4_LAYERS = 'avg_concat_last_4_layers'
    AVG_SECOND_TO_LAST_LAYER = 'avg_second_to_last_layer'
    AVG_LAST_LAYER = 'avg_last_layer'
    AVG_SUM_LAST_4_LAYERS = 'avg_sum_last_4_layers'
    CLS_TOKEN = 'cls_token'
    NO_POOL = 'no_pool'


def normalize_embeddings(embeddings: torch.Tensor):
    return torch.nn.functional.normalize(embeddings, eps=1e-06)


@torch.jit.script
def pad_length(len: int, padding_control: Optional[List[int]], max_len: int=-1) ->int:
    if not validate_padding_control(padding_control):
        raise NotImplementedError
    if padding_control is not None:
        for pad in padding_control:
            if pad >= len:
                len = pad
                break
    if max_len > 0:
        len = min(len, max_len)
    return len


BOS_WORD = '<s>'


EOS_WORD = '</s>'


PAD_WORD = '<pad>'


SPECIAL_WORD = '<special%i>'


SPECIAL_WORDS = 10


UNK_WORD = '<unk>'


logger = logging.getLogger(__name__)


class Dictionary(object):

    def __init__(self, id2word, word2id, counts):
        assert len(id2word) == len(word2id) == len(counts)
        self.id2word = id2word
        self.word2id = word2id
        self.counts = counts
        self.bos_index = word2id[BOS_WORD]
        self.eos_index = word2id[EOS_WORD]
        self.pad_index = word2id[PAD_WORD]
        self.unk_index = word2id[UNK_WORD]
        self.check_valid()

    def __len__(self):
        """
        Returns the number of words in the dictionary.
        """
        return len(self.id2word)

    def __getitem__(self, i):
        """
        Returns the word of the specified index.
        """
        return self.id2word[i]

    def __contains__(self, w):
        """
        Returns whether a word is in the dictionary.
        """
        return w in self.word2id

    def __eq__(self, y):
        """
        Compare this dictionary with another one.
        """
        self.check_valid()
        y.check_valid()
        if len(self.id2word) != len(y):
            return False
        return all(self.id2word[i] == y[i] for i in range(len(y)))

    def check_valid(self):
        """
        Check that the dictionary is valid.
        """
        assert self.bos_index == 0
        assert self.eos_index == 1
        assert self.pad_index == 2
        assert self.unk_index == 3
        assert all(self.id2word[4 + i] == SPECIAL_WORD % i for i in range(SPECIAL_WORDS))
        assert len(self.id2word) == len(self.word2id) == len(self.counts)
        assert set(self.word2id.keys()) == set(self.counts.keys())
        for i in range(len(self.id2word)):
            assert self.word2id[self.id2word[i]] == i
        last_count = 1e+18
        for i in range(4 + SPECIAL_WORDS, len(self.id2word) - 1):
            count = self.counts[self.id2word[i]]
            assert count <= last_count
            last_count = count

    def index(self, word, no_unk=False):
        """
        Returns the index of the specified word.
        """
        if no_unk:
            return self.word2id[word]
        else:
            return self.word2id.get(word, self.unk_index)

    def max_vocab(self, max_vocab):
        """
        Limit the vocabulary size.
        """
        assert max_vocab >= 1
        init_size = len(self)
        self.id2word = {k: v for k, v in self.id2word.items() if k < max_vocab}
        self.word2id = {v: k for k, v in self.id2word.items()}
        self.counts = {k: v for k, v in self.counts.items() if k in self.word2id}
        self.check_valid()
        logger.info('Maximum vocabulary size: %i. Dictionary size: %i -> %i (removed %i words).' % (max_vocab, init_size, len(self), init_size - len(self)))

    def min_count(self, min_count):
        """
        Threshold on the word frequency counts.
        """
        assert min_count >= 0
        init_size = len(self)
        self.id2word = {k: v for k, v in self.id2word.items() if self.counts[self.id2word[k]] >= min_count or k < 4 + SPECIAL_WORDS}
        self.word2id = {v: k for k, v in self.id2word.items()}
        self.counts = {k: v for k, v in self.counts.items() if k in self.word2id}
        self.check_valid()
        logger.info('Minimum frequency count: %i. Dictionary size: %i -> %i (removed %i words).' % (min_count, init_size, len(self), init_size - len(self)))

    @staticmethod
    def read_vocab(vocab_path):
        """
        Create a dictionary from a vocabulary file.
        """
        skipped = 0
        assert PathManager.isfile(vocab_path), vocab_path
        word2id = {BOS_WORD: 0, EOS_WORD: 1, PAD_WORD: 2, UNK_WORD: 3}
        for i in range(SPECIAL_WORDS):
            word2id[SPECIAL_WORD % i] = 4 + i
        counts = {k: (0) for k in word2id.keys()}
        f = PathManager.open(vocab_path, 'r', encoding='utf-8')
        for i, line in enumerate(f):
            if '\u2028' in line:
                skipped += 1
                continue
            line = line.rstrip().split()
            if len(line) != 2:
                skipped += 1
                continue
            assert len(line) == 2, (i, line)
            assert line[1].isdigit(), (i, line)
            if line[0] in word2id:
                skipped += 1
                None
                continue
            if not line[1].isdigit():
                skipped += 1
                None
                continue
            word2id[line[0]] = 4 + SPECIAL_WORDS + i - skipped
            counts[line[0]] = int(line[1])
        f.close()
        id2word = {v: k for k, v in word2id.items()}
        dico = Dictionary(id2word, word2id, counts)
        logger.info('Read %i words from the vocabulary file.' % len(dico))
        if skipped > 0:
            logger.warning('Skipped %i empty lines!' % skipped)
        return dico

    @staticmethod
    def index_data(path, bin_path, dico):
        """
        Index sentences with a dictionary.
        """
        if bin_path is not None and PathManager.isfile(bin_path):
            None
            data = torch.load(bin_path)
            assert dico == data['dico']
            return data
        positions = []
        sentences = []
        unk_words = {}
        f = PathManager.open(path, 'r', encoding='utf-8')
        for i, line in enumerate(f):
            if i % 1000000 == 0 and i > 0:
                None
            s = line.rstrip().split()
            if len(s) == 0:
                None
            count_unk = 0
            indexed = []
            for w in s:
                word_id = dico.index(w, no_unk=False)
                if 0 <= word_id < 4 + SPECIAL_WORDS and word_id != 3:
                    logger.warning('Found unexpected special word "%s" (%i)!!' % (w, word_id))
                    continue
                assert word_id >= 0
                indexed.append(word_id)
                if word_id == dico.unk_index:
                    unk_words[w] = unk_words.get(w, 0) + 1
                    count_unk += 1
            positions.append([len(sentences), len(sentences) + len(indexed)])
            sentences.extend(indexed)
            sentences.append(1)
        f.close()
        positions = np.int64(positions)
        if len(dico) < 1 << 16:
            sentences = np.uint16(sentences)
        elif len(dico) < 1 << 31:
            sentences = np.int32(sentences)
        else:
            raise Exception('Dictionary is too big.')
        assert sentences.min() >= 0
        data = {'dico': dico, 'positions': positions, 'sentences': sentences, 'unk_words': unk_words}
        if bin_path is not None:
            None
            torch.save(data, bin_path, pickle_protocol=4)
        return data


class ScriptTensorizer(torch.jit.ScriptModule):
    device: str
    seq_padding_control: Optional[List[int]]
    batch_padding_control: Optional[List[int]]

    def __init__(self):
        super().__init__()
        self.device = ''
        self.seq_padding_control = torch.jit.Attribute(None, Optional[List[int]])
        self.batch_padding_control = torch.jit.Attribute(None, Optional[List[int]])

    @torch.jit.script_method
    def set_device(self, device: str):
        self.device = device

    @torch.jit.export
    def set_padding_control(self, dimension: str, padding_control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        if not validate_padding_control(padding_control):
            raise RuntimeError('Malformed padding_control value')
        if dimension == 'sequence_length':
            self.seq_padding_control = padding_control
        elif dimension == 'batch_length':
            self.batch_padding_control = padding_control
        else:
            raise RuntimeError('Illegal padding dimension specified.')

    @torch.jit.script_method
    def tokenize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]]):
        """
        Process a single line of raw inputs into tokens, it supports
        two input formats:
            1) a single line of texts (single sentence or a pair)
            2) a single line of pre-processed tokens (single sentence or a pair)
        """
        raise NotImplementedError

    @torch.jit.script_method
    def numberize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]]):
        """
        Process a single line of raw inputs into numberized result, it supports
        two input formats:
            1) a single line of texts (single sentence or a pair)
            2) a single line of pre-processed tokens (single sentence or a pair)

        This function should handle the logic of calling tokenize(), add special
        tokens and vocab lookup.
        """
        raise NotImplementedError

    @torch.jit.script_method
    def tensorize(self, texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[List[str]]]]=None):
        """
        Process raw inputs into model input tensors, it supports two input
        formats:
            1) multiple rows of texts (single sentence or a pair)
            2) multiple rows of pre-processed tokens (single sentence or a pair)

        This function should handle the logic of calling numberize() and also
        padding the numberized result.
        """
        raise NotImplementedError

    @torch.jit.script_method
    def batch_size(self, texts: Optional[List[List[str]]], tokens: Optional[List[List[List[str]]]]) ->int:
        if texts is not None:
            return len(texts)
        elif tokens is not None:
            return len(tokens)
        else:
            raise RuntimeError('Empty input for both texts and tokens.')

    @torch.jit.script_method
    def row_size(self, texts_list: Optional[List[List[str]]]=None, tokens_list: Optional[List[List[List[str]]]]=None) ->int:
        if texts_list is not None:
            return len(texts_list[0])
        elif tokens_list is not None:
            return len(tokens_list[0])
        else:
            raise RuntimeError('Empty input for both texts and tokens.')

    @torch.jit.script_method
    def get_texts_by_index(self, texts: Optional[List[List[str]]], index: int) ->Optional[List[str]]:
        if texts is None:
            return None
        return texts[index]

    @torch.jit.script_method
    def get_tokens_by_index(self, tokens: Optional[List[List[List[str]]]], index: int) ->Optional[List[List[str]]]:
        if tokens is None:
            return None
        return tokens[index]


class ScriptBERTTensorizerBase(ScriptTensorizer):

    def __init__(self, tokenizer: torch.jit.ScriptModule, vocab: ScriptVocabulary, max_seq_len: int):
        super().__init__()
        self.tokenizer = tokenizer
        self.vocab = vocab
        self.vocab_lookup = VocabLookup(vocab)
        self.max_seq_len = torch.jit.Attribute(max_seq_len, int)

    @torch.jit.script_method
    def tokenize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]]) ->List[List[Tuple[str, int, int]]]:
        per_sentence_tokens: List[List[Tuple[str, int, int]]] = []
        if text_row is not None:
            for text in text_row:
                per_sentence_tokens.append(self.tokenizer.tokenize(text))
        elif token_row is not None:
            for sentence_raw_tokens in token_row:
                sentence_tokens: List[Tuple[str, int, int]] = []
                for raw_token in sentence_raw_tokens:
                    sentence_tokens.extend(self.tokenizer.tokenize(raw_token))
                per_sentence_tokens.append(sentence_tokens)
        return per_sentence_tokens

    @torch.jit.script_method
    def _lookup_tokens(self, tokens: List[Tuple[str, int, int]]) ->List[int]:
        raise NotImplementedError

    @torch.jit.script_method
    def _wrap_numberized_tokens(self, token_ids: List[int], idx: int) ->List[int]:
        return token_ids

    @torch.jit.script_method
    def numberize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]]) ->Tuple[List[int], List[int], int, List[int]]:
        token_ids: List[int] = []
        segment_labels: List[int] = []
        seq_len: int = 0
        positions: List[int] = []
        per_sentence_tokens: List[List[Tuple[str, int, int]]] = self.tokenize(text_row, token_row)
        for idx, per_sentence_token in enumerate(per_sentence_tokens):
            lookup_ids: List[int] = self._lookup_tokens(per_sentence_token)
            lookup_ids = self._wrap_numberized_tokens(lookup_ids, idx)
            token_ids.extend(lookup_ids)
            segment_labels.extend([idx] * len(lookup_ids))
        seq_len = len(token_ids)
        positions = [i for i in range(seq_len)]
        return token_ids, segment_labels, seq_len, positions

    @torch.jit.script_method
    def tensorize(self, texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[List[str]]]]=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        tokens_2d: List[List[int]] = []
        segment_labels_2d: List[List[int]] = []
        seq_len_2d: List[int] = []
        positions_2d: List[List[int]] = []
        for idx in range(self.batch_size(texts, tokens)):
            numberized: Tuple[List[int], List[int], int, List[int]] = self.numberize(self.get_texts_by_index(texts, idx), self.get_tokens_by_index(tokens, idx))
            tokens_2d.append(numberized[0])
            segment_labels_2d.append(numberized[1])
            seq_len_2d.append(numberized[2])
            positions_2d.append(numberized[3])
        tokens, pad_mask = pad_2d_mask(tokens_2d, pad_value=self.vocab.pad_idx)
        segment_labels = torch.tensor(pad_2d(segment_labels_2d, seq_lens=seq_len_2d, pad_idx=self.vocab.pad_idx), dtype=torch.long)
        positions = torch.tensor(pad_2d(positions_2d, seq_lens=seq_len_2d, pad_idx=self.vocab.pad_idx), dtype=torch.long)
        if self.device == '':
            return tokens, pad_mask, segment_labels, positions
        else:
            return tokens, pad_mask, segment_labels, positions


class ScriptRoBERTaTensorizer(ScriptBERTTensorizerBase):

    @torch.jit.script_method
    def _lookup_tokens(self, tokens: List[Tuple[str, int, int]]) ->List[int]:
        return self.vocab_lookup(tokens, bos_idx=self.vocab.bos_idx, eos_idx=self.vocab.eos_idx, use_eos_token_for_bos=False, max_seq_len=self.max_seq_len)[0]


@torch.jit.script
def reverse_tensor_list(int_list: List[torch.Tensor]) ->List[torch.Tensor]:
    res = int_list[:]
    res.reverse()
    return res


@torch.jit.script
def xaviervar(size: List[int], device: str):
    t = torch.empty(size, device=device)
    t = torch.nn.init.xavier_normal_(t)
    return t


class CompositionalNN(torch.jit.ScriptModule):
    """
    Combines a list / sequence of embeddings into one using a biLSTM
    """
    __constants__ = ['lstm_dim', 'linear_seq']

    def __init__(self, lstm_dim: int):
        super().__init__()
        self.lstm_dim = lstm_dim
        self.lstm_fwd = nn.LSTM(lstm_dim, lstm_dim, num_layers=1)
        self.lstm_rev = nn.LSTM(lstm_dim, lstm_dim, num_layers=1)
        self.linear_seq = nn.Sequential(nn.Linear(2 * lstm_dim, lstm_dim), nn.Tanh())

    @torch.jit.script_method
    def forward(self, x: List[torch.Tensor], device: str='cpu') ->torch.Tensor:
        """
        Embed the sequence. If the input corresponds to [IN:GL where am I at]:
        - x will contain the embeddings of [at I am where IN:GL] in that order.
        - Forward LSTM will embed the sequence [IN:GL where am I at].
        - Backward LSTM will embed the sequence [IN:GL at I am where].
        The final hidden states are concatenated and then projected.

        Args:
            x: Embeddings of the input tokens in *reversed* order
        Shapes:
            x: (1, lstm_dim) each
            return value: (1, lstm_dim)
        """
        lstm_hidden_fwd = xaviervar([1, 1, self.lstm_dim], device=device), xaviervar([1, 1, self.lstm_dim], device=device)
        lstm_hidden_rev = xaviervar([1, 1, self.lstm_dim], device=device), xaviervar([1, 1, self.lstm_dim], device=device)
        nonterminal_element = x[-1]
        reversed_rest = x[:-1]
        fwd_input = [nonterminal_element] + reverse_tensor_list(reversed_rest)
        rev_input = [nonterminal_element] + reversed_rest
        stacked_fwd = self.lstm_fwd(torch.stack(fwd_input), lstm_hidden_fwd)[0][0]
        stacked_rev = self.lstm_rev(torch.stack(rev_input), lstm_hidden_rev)[0][0]
        combined = torch.cat([stacked_fwd, stacked_rev], dim=1)
        subtree_embedding = self.linear_seq(combined)
        return subtree_embedding


class CompositionalSummationNN(torch.jit.ScriptModule):
    """
    Simpler version of CompositionalNN
    """
    __constants__ = ['lstm_dim', 'linear_seq']

    def __init__(self, lstm_dim: int):
        super().__init__()
        self.lstm_dim = lstm_dim
        self.linear_seq = nn.Sequential(nn.Linear(lstm_dim, lstm_dim), nn.Tanh())

    @torch.jit.script_method
    def forward(self, x: List[torch.Tensor], device: str='cpu') ->torch.Tensor:
        combined = torch.sum(torch.cat(x, dim=0), dim=0, keepdim=True)
        subtree_embedding = self.linear_seq(combined)
        return subtree_embedding


class Element:
    """
    Generic element representing a token / non-terminal / sub-tree on a stack.
    Used to compute valid actions in the RNNG parser.
    """

    def __init__(self, node: Any) ->None:
        self.node = node

    def __eq__(self, other) ->bool:
        return self.node == other.node

    def __repr__(self) ->str:
        return str(self.node)


class StackLSTM(Sized):
    """
    The Stack LSTM from Dyer et al: https://arxiv.org/abs/1505.08075
    """

    def __init__(self, lstm: nn.LSTM):
        """
        Shapes:
            initial_state: (lstm_layers, 1, lstm_hidden_dim) each
        """
        self.lstm = lstm
        initial_state = FloatTensor(lstm.num_layers, 1, lstm.hidden_size).fill_(0), FloatTensor(lstm.num_layers, 1, lstm.hidden_size).fill_(0)
        self.stack = [(initial_state, (self._lstm_output(initial_state), Element('Root')))]

    def _lstm_output(self, state: Tuple[torch.Tensor, torch.Tensor]) ->torch.Tensor:
        """
        Shapes:
            state: (lstm_layers, 1, lstm_hidden_dim) each
            return value: (1, lstm_hidden_dim)
        """
        return state[0][-1]

    def push(self, expression: torch.Tensor, element: Element) ->None:
        """
        Shapes:
            expression: (1, lstm_input_dim)
        """
        old_top_state = self.stack[-1][0]
        _, new_top_state = self.lstm(expression.unsqueeze(0), old_top_state)
        self.stack.append((new_top_state, (self._lstm_output(new_top_state), element)))

    def pop(self) ->Tuple[torch.Tensor, Element]:
        """
        Pops and returns tuple of output embedding (1, lstm_hidden_dim) and element
        """
        return self.stack.pop()[1]

    def embedding(self) ->torch.Tensor:
        """
        Shapes:
            return value: (1, lstm_hidden_dim)
        """
        assert len(self.stack) > 0, 'stack size must be greater than 0'
        top_state = self.stack[-1][0]
        return self._lstm_output(top_state)

    def element_from_top(self, index: int) ->Element:
        return self.stack[-(index + 1)][1][1]

    def __len__(self) ->int:
        return len(self.stack) - 1

    def __str__(self) ->str:
        return '->'.join([str(x[1][1]) for x in self.stack])

    def copy(self):
        other = StackLSTM(self.lstm)
        other.stack = list(self.stack)
        return other


class ParserState:
    """
    Maintains state of the Parser. Useful for beam search
    """

    def __init__(self, parser=None):
        if not parser:
            return
        self.buffer_stackrnn = StackLSTM(parser.buff_rnn)
        self.stack_stackrnn = StackLSTM(parser.stack_rnn)
        self.action_stackrnn = StackLSTM(parser.action_rnn)
        self.predicted_actions_idx = []
        self.action_scores = []
        self.num_open_NT = 0
        self.is_open_NT: List[bool] = []
        self.found_unsupported = False
        self.action_p = torch.Tensor()
        self.neg_prob = 0

    def finished(self):
        return len(self.stack_stackrnn) == 1 and len(self.buffer_stackrnn) == 0

    def copy(self):
        other = ParserState()
        other.buffer_stackrnn = self.buffer_stackrnn.copy()
        other.stack_stackrnn = self.stack_stackrnn.copy()
        other.action_stackrnn = self.action_stackrnn.copy()
        other.predicted_actions_idx = self.predicted_actions_idx.copy()
        other.action_scores = self.action_scores.copy()
        other.num_open_NT = self.num_open_NT
        other.is_open_NT = self.is_open_NT.copy()
        other.neg_prob = self.neg_prob
        other.found_unsupported = self.found_unsupported
        other.action_p = self.action_p.detach()
        return other

    def __gt__(self, other):
        return self.neg_prob > other.neg_prob

    def __eq__(self, other):
        return self.neg_prob == other.neg_prob


CLOSE = ']'


INTENT_PREFIX = 'IN:'


COMBINATION_INTENT_LABEL = INTENT_PREFIX + 'COMBINE'


SLOT_PREFIX = 'SL:'


COMBINATION_SLOT_LABEL = SLOT_PREFIX + 'COMBINE'


ESCAPE = '\\'


class Node_Info:
    """
    This class extracts the essential information for a mode, for use in rules.
    """

    def __init__(self, node):
        self.label = node.label
        self.tokens = node.list_tokens()
        self.parent_label = self.get_parent(node)
        self.children = []
        for a in node.children:
            if type(a) == Slot or type(a) == Intent:
                self.children.append(a.label)
        self.token_indices = node.get_token_indices()
        self.ancestors = [a.label for a in node.list_ancestors()]
        self.descendents = [d.label for d in node.list_nonTerminals()]
        self.prior_token = None
        prior = node.get_prior_token()
        if prior:
            self.prior_token = prior.label
        if type(node) == Token:
            self.label_type = 'TOKEN'
        elif type(node) == Intent:
            self.label_type = 'INTENT'
        elif type(node) == Slot:
            self.label_type = 'SLOT'
        self.same_span = self.get_same_span(node)

    def get_same_span(self, node):
        if node.parent:
            if set(node.parent.list_tokens()) == set(node.list_tokens()):
                return True
        return False

    def get_parent(self, node):
        if node.parent and type(node.parent) != Root:
            return node.parent.label
        return None

    def __str__(self):
        result = []
        result.append('Info:')
        result.append('Label: ' + self.label)
        result.append('Tokens: ' + ' '.join(self.tokens))
        result.append('Token Indicies: ' + ', '.join([str(i) for i in self.token_indices]))
        result.append('Prior Token: ' + str(self.prior_token))
        result.append('Parent: ' + str(self.parent_label))
        result.append('Children: ' + ', '.join(self.children))
        result.append('Ancestors: ' + ', '.join(self.ancestors))
        result.append('Descendents: ' + ', '.join(self.descendents))
        result.append('Label Type: ' + str(self.label_type))
        result.append('Same Span: ' + str(self.same_span))
        return '\n'.join(result)


OPEN = '['


class Token_Info:
    """
    This class extracts the essential information for a token for use in rules.
    """

    def __init__(self, node):
        self.token_word = node.label
        self.parent_label = self.get_parent(node)
        self.ancestors = [a.label for a in node.list_ancestors()]
        self.prior_token = None
        prior = node.get_prior_token()
        if prior:
            self.prior_token = prior.label
        self.next_token = None
        next_token = node.get_next_token()
        if next_token:
            self.next_token = next_token.label

    def get_parent(self, node):
        if node.parent and type(node.parent) != Root:
            return node.parent.label
        return None

    def __str__(self):
        result = []
        result.append('Token Info:')
        result.append('Token Word: ' + self.token_word)
        result.append('Previous Token: ' + str(self.prior_token))
        result.append('Next Token: ' + str(self.next_token))
        result.append('Parent: ' + str(self.parent_label))
        result.append('Ancestors: ' + ', '.join(self.ancestors))
        return '\n'.join(result)


def escape_brackets(string: str) ->str:
    return re.sub(f'([\\{OPEN}\\{CLOSE}\\{ESCAPE}])', f'\\{ESCAPE}\\1', string)


class Node:

    def __init__(self, label):
        self.label = label
        self.children = []
        self.parent = None

    def list_ancestors(self):
        ancestors = []
        if self.parent:
            if type(self.parent) != Root:
                ancestors.append(self.parent)
                ancestors += self.parent.list_ancestors()
        return ancestors

    def validate_node(self, *args):
        if self.children:
            for child in self.children:
                child.validate_node(*args)

    def list_tokens(self):
        tokens = []
        if self.children:
            for child in self.children:
                if type(child) == Token:
                    tokens.append(child.label)
                else:
                    tokens += child.list_tokens()
        return tokens

    def get_token_span(self):
        """
        0 indexed
        Like array slicing: For the first 3 tokens, returns 0, 3
        """
        indices = self.get_token_indices()
        if len(indices) > 0:
            return min(indices), max(indices) + 1
        else:
            return None

    def get_token_indices(self):
        indices = []
        if self.children:
            for child in self.children:
                if type(child) == Token:
                    indices.append(child.index)
                else:
                    indices += child.get_token_indices()
        return indices

    def list_nonTerminals(self):
        """
        Returns all Intent and Slot nodes subordinate to this node
        """
        non_terminals = []
        for child in self.children:
            if type(child) != Root and type(child) != Token:
                non_terminals.append(child)
                non_terminals += child.list_nonTerminals()
        return non_terminals

    def list_terminals(self):
        """
        Returns all Token nodes
        """
        terminals = []
        for child in self.children:
            if type(child) == Token:
                terminals.append(child)
            else:
                terminals += child.list_terminals()
        return terminals

    def get_info(self):
        if type(self) == Token:
            return Token_Info(self)
        return Node_Info(self)

    def flat_str(self):
        string = ''
        if type(self) == Intent or type(self) == Slot:
            string = OPEN
        if type(self) != Root:
            string += escape_brackets(str(self.label)) + ' '
        if self.children:
            for child in self.children:
                string += child.flat_str()
        if type(self) == Intent or type(self) == Slot:
            string += CLOSE + ' '
        return string

    def children_flat_str_spans(self):
        string = str(self.get_token_span()) + ':'
        if self.children:
            for child in self.children:
                string += child.flat_str()
        return string

    def __str__(self):
        string = self._recursive_str('', '')
        return string

    def _recursive_str(self, string, spacer):
        string = spacer + str(self.label) + '\n'
        spacer += '\t'
        if self.children:
            for child in self.children:
                string += child._recursive_str(string, spacer)
        return string

    def __eq__(self, other):
        return self.label == other.label and self.children == other.children


class Intent(Node):

    def __init__(self, label):
        super().__init__(label)

    def validate_node(self, *args):
        super().validate_node(*args)
        for child in self.children:
            if type(child) == Intent or type(child) == Root:
                raise TypeError('An intent child must be a slot or token: ' + self.label)

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return self.label == other.label and self.children == other.children


OOD_TOKEN = 'outOfDomain'


REDUCE = 'REDUCE'


SEQLOGICAL_LOTV_TOKEN = '0'


SHIFT = 'SHIFT'


class Annotation:

    def __init__(self, annotation_string: str, utterance: str='', brackets: str=OPEN + CLOSE, combination_labels: bool=True, add_dict_feat: bool=False, accept_flat_intents_slots: bool=False) ->None:
        super(Annotation, self).__init__()
        self.OPEN = brackets[0]
        self.CLOSE = brackets[1]
        self.combination_labels = combination_labels
        parts = annotation_string.rstrip().split('\t')
        if len(parts) == 5:
            [_, _, utterance, sparse_feat, self.seqlogical] = parts
        elif len(parts) == 1:
            [self.seqlogical] = parts
        else:
            raise ValueError('Cannot parse annotation_string')
        self.tree = Tree(self.build_tree(accept_flat_intents_slots), combination_labels, utterance)
        self.root: Root = self.tree.root

    def build_tree(self, accept_flat_intents_slots: bool=False):
        root = Root()
        node_stack: List[Any] = [root]
        curr_chars: List[str] = []
        token_count = 0
        expecting_label = False
        it = iter(self.seqlogical)
        while True:
            char = next(it, None)
            if char is None:
                break
            if char.isspace() or char in (OPEN, CLOSE):
                if curr_chars:
                    word = ''.join(curr_chars)
                    curr_chars = []
                    parent = node_stack[-1]
                    if expecting_label:
                        if word.startswith(INTENT_PREFIX):
                            node: Union[Intent, Slot, Token] = Intent(word)
                        elif word.startswith(SLOT_PREFIX):
                            node = Slot(word)
                        elif word == OOD_TOKEN:
                            node = Intent(word)
                        elif accept_flat_intents_slots:
                            if isinstance(parent, (Root, Slot)):
                                node = Intent(word)
                            elif isinstance(parent, Intent):
                                node = Slot(word)
                            else:
                                raise ValueError('The previous node in node_stack is not of type Root, Intent or Slot.')
                        else:
                            raise ValueError(f'Label {word} must start with IN: or SL:')
                        node_stack.append(node)
                        expecting_label = False
                    else:
                        if isinstance(parent, Root):
                            raise ValueError('Root cannot have a Token as child.')
                        node = Token(word, token_count)
                        token_count += 1
                    parent.children.append(node)
                    node.parent = parent
                if char in (OPEN, CLOSE):
                    if expecting_label:
                        raise ValueError("Invalid tree. No label found after '['.")
                    if char == OPEN:
                        expecting_label = True
                    else:
                        node_stack.pop()
            else:
                if char == ESCAPE:
                    char = next(it, None)
                    if char not in (OPEN, CLOSE, ESCAPE):
                        raise ValueError(f"Escape '{ESCAPE}' followed by none of '{OPEN}', '{CLOSE}', or '{ESCAPE}'.")
                curr_chars.append(char)
        if len(node_stack) != 1:
            raise ValueError('Invalid tree.')
        if len(root.children) > 1 and self.combination_labels:
            comb_intent = Intent(COMBINATION_INTENT_LABEL)
            node_stack.insert(1, comb_intent)
            for child in root.children:
                if type(child) == Intent:
                    comb_slot = Slot(COMBINATION_SLOT_LABEL)
                    comb_slot.parent = comb_intent
                    comb_slot.children.append(child)
                    comb_intent.children.append(comb_slot)
                    child.parent = comb_slot
                else:
                    child.parent = comb_intent
                    comb_intent.children.append(child)
            comb_intent.parent = root
            root.children = [comb_intent]
        return root

    def validate_alignment(self) ->bool:
        term_nodes = self.root.list_terminals()
        for term_node in term_nodes:
            label = term_node.label
            if label.startswith('{') and label.endswith('}'):
                if type(term_node.parent) != Slot or len(term_node.parent.children) != 1:
                    return False
        return True

    def __str__(self):
        """
        A tab-indented version of the tree.
        strip() removes an extra final newline added during recursion
        """
        return self.tree.__str__()

    def __eq__(self, other):
        return self.tree == other.tree


def is_intent_nonterminal(node_label: str) ->bool:
    return node_label.startswith(INTENT_PREFIX)


def is_slot_nonterminal(node_label: str) ->bool:
    return node_label.startswith(SLOT_PREFIX)


def is_unsupported(node_label: str) ->bool:
    return is_intent_nonterminal(node_label) and node_label.lower().find('unsupported', 0) > 0


def is_valid_nonterminal(node_label: str) ->bool:
    return node_label.startswith(INTENT_PREFIX) or node_label.startswith(SLOT_PREFIX)


class AnnotationNumberizer(Tensorizer):
    """
    Not really a Tensorizer (since it does not create tensors) but technically
    serves the same function. This class parses Annotations in the format below
    and extracts the actions (type List[List[int]])
    ::

        [IN:GET_ESTIMATED_DURATION How long will it take to [SL:METHOD_TRAVEL
        drive ] from [SL:SOURCE Chicago ] to [SL:DESTINATION Mississippi ] ]

    Extraction algorithm is handled by Annotation class. We only care about
    the list of actions, which before vocab index lookups would look like:
    ::

        [
            IN:GET_ESTIMATED_DURATION, SHIFT, SHIFT, SHIFT, SHIFT, SHIFT, SHIFT,
            SL:METHOD_TRAVEL, SHIFT, REDUCE,
            SHIFT,
            SL:SOURCE, SHIFT, REDUCE,
            SHIFT,
            SL:DESTINATION, SHIFT, REDUCE,
        ]

    """


    class Config(Tensorizer.Config):
        column: str = 'seqlogical'

    @classmethod
    def from_config(cls, config: Config):
        return cls(column=config.column, is_input=config.is_input)

    def __init__(self, column: str=Config.column, vocab=None, is_input: bool=Config.is_input):
        self.column = column
        self.vocab = vocab
        self.vocab_builder = None
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.column, str)]

    def initialize(self, vocab_builder=None, from_scratch=True):
        """Build vocabulary based on training corpus."""
        if self.vocab and from_scratch:
            return
        if not self.vocab_builder:
            self.vocab_builder = vocab_builder or VocabBuilder()
            self.vocab_builder.use_unk = False
            self.vocab_builder.use_pad = False
        try:
            while True:
                row = yield
                annotation = Annotation(row[self.column])
                actions = annotation.tree.to_actions()
                self.vocab_builder.add_all(actions)
        except GeneratorExit:
            self.vocab = self.vocab_builder.make_vocab()
            self.shift_idx = self.vocab.idx[SHIFT]
            self.reduce_idx = self.vocab.idx[REDUCE]

            def filterVocab(fn):
                return [token for nt, token in self.vocab.idx.items() if fn(nt)]
            self.ignore_subNTs_roots = filterVocab(is_unsupported)
            self.valid_NT_idxs = filterVocab(is_valid_nonterminal)
            self.valid_IN_idxs = filterVocab(is_intent_nonterminal)
            self.valid_SL_idxs = filterVocab(is_slot_nonterminal)

    def numberize(self, row):
        """Tokenize, look up in vocabulary."""
        annotation = Annotation(row[self.column])
        return self.vocab.lookup_all(annotation.tree.to_actions())

    def tensorize(self, batch):
        return batch


def create_src_lengths_mask(batch_size: int, src_lengths):
    """
    Generate boolean mask to prevent attention beyond the end of source

    Inputs:
      batch_size : int
      src_lengths : [batch_size] of sentence lengths

    Outputs:
      [batch_size, max_src_len]
    """
    max_srclen = src_lengths.max()
    src_indices = torch.arange(0, max_srclen).unsqueeze(0).type_as(src_lengths)
    src_indices = src_indices.expand(batch_size, max_srclen)
    src_lengths = src_lengths.unsqueeze(dim=1).expand(batch_size, max_srclen)
    return (src_indices < src_lengths).int().detach()


def masked_softmax(scores, src_lengths, src_length_masking: bool=True):
    """Apply source length masking then softmax.
    Input and output have shape bsz x src_len"""
    if src_length_masking:
        bsz, max_src_len = scores.size()
        src_mask = create_src_lengths_mask(bsz, src_lengths)
        scores = scores.masked_fill(src_mask == 0, -np.inf)
    return F.softmax(scores.float(), dim=-1).type_as(scores)


class DotAttention(nn.Module):

    def __init__(self, decoder_hidden_state_dim, context_dim, force_projection=False, src_length_masking=True):
        super().__init__()
        self.decoder_hidden_state_dim = decoder_hidden_state_dim
        self.context_dim = context_dim
        self.input_proj = None
        if force_projection or decoder_hidden_state_dim != context_dim:
            self.input_proj = nn.Linear(decoder_hidden_state_dim, context_dim, bias=True)
        self.src_length_masking = src_length_masking
        log_class_usage(__class__)

    def forward(self, decoder_state, source_hids, src_lengths):
        source_hids = source_hids.transpose(0, 1)
        if self.input_proj is not None:
            decoder_state = self.input_proj(decoder_state)
        attn_scores = torch.bmm(source_hids, decoder_state.unsqueeze(2)).squeeze(2)
        normalized_masked_attn_scores = masked_softmax(attn_scores, src_lengths, self.src_length_masking)
        attn_weighted_context = (source_hids * normalized_masked_attn_scores.unsqueeze(2)).contiguous().sum(1)
        return attn_weighted_context, normalized_masked_attn_scores.t()


class PyTextSeq2SeqModule(Module):
    instance_id: str = None

    def __init__(self):
        super().__init__()
        self.assign_id()
        log_class_usage(__class__)

    def assign_id(self):
        global global_counter
        self.instance_id = '.'.join([type(self).__name__, str(global_counter)])
        global_counter = global_counter + 1


def prepare_full_key(instance_id: str, key: str, secondary_key: Optional[str]=None):
    if secondary_key is not None:
        return instance_id + '.' + key + '.' + secondary_key
    else:
        return instance_id + '.' + key


class PyTextIncrementalDecoderComponent(PyTextSeq2SeqModule):

    def get_incremental_state(self, incremental_state: Dict[str, Tensor], key: str) ->Optional[Tensor]:
        """Helper for getting incremental state for an nn.Module."""
        full_key = prepare_full_key(self.instance_id, key)
        if incremental_state is None or full_key not in incremental_state:
            return None
        return incremental_state[full_key]

    def set_incremental_state(self, incremental_state: Dict[str, Tensor], key: str, value):
        """Helper for setting incremental state for an nn.Module."""
        if incremental_state is not None:
            full_key = prepare_full_key(self.instance_id, key)
            incremental_state[full_key] = value

    def reorder_incremental_state(self, incremental_state: Dict[str, Tensor], new_order: Tensor):
        pass


class MultiheadAttention(PyTextIncrementalDecoderComponent):
    """
    Refer Attention is All You Need for more details.

    This is a simplified implementation of multihead attention
    optimized for exporting using torchscript. Usage of nn.Linear() instead of
    F.Linear() helps to quantize the linear layers.

    Query represents the output from last decoder step. Key and Values are obtained from
    encoder. Attention weights are obtained from the dot product of query and key.
    Attention weights multiplied by the value gives output.
    """


    class Config(ConfigBase):
        dropout: float = 0.0
        kdim: Optional[int] = None
        vdim: Optional[int] = None
        bias: bool = True

    @classmethod
    def from_config(cls, config, embed_dim, num_heads):
        return cls(embed_dim, num_heads, **config._asdict())

    def __init__(self, embed_dim, num_heads, dropout, kdim=None, vdim=None, bias=True):
        super().__init__()
        self.embed_dim = embed_dim
        self.kdim = embed_dim if kdim is None else kdim
        self.vdim = embed_dim if vdim is None else vdim
        self.q_proj = Linear(embed_dim, embed_dim, bias=bias)
        self.k_proj = Linear(self.kdim, embed_dim, bias=bias)
        self.v_proj = Linear(self.vdim, embed_dim, bias=bias)
        self.num_heads = num_heads
        self.dropout = nn.Dropout(dropout)
        self.head_dim = embed_dim // num_heads
        assert self.head_dim * num_heads == self.embed_dim, 'embed_dim must be divisible by num_heads'
        self.scaling = self.head_dim ** -0.5
        self.out_proj = Linear(embed_dim, embed_dim, bias=bias)

    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor], need_weights: bool, incremental_state: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Tensor]]:
        target_len, bsz, embed_dim = query.size()
        src_len = key.size(0)
        assert embed_dim == self.embed_dim, str(embed_dim) + ' != ' + str(self.embed_dim)
        assert key is not None
        assert value is not None
        if incremental_state is not None:
            prev_key = self._get_input_buffer(incremental_state, 'prev_key')
        else:
            prev_key = None
        bsz_X_num_heads = bsz * self.num_heads
        q = self.q_proj(query)
        q *= self.scaling
        q = q.contiguous().view(target_len, bsz_X_num_heads, self.head_dim).transpose(0, 1)
        if prev_key is not None and incremental_state is not None:
            k = prev_key.view(bsz_X_num_heads, -1, self.head_dim)
            prev_value = self._get_input_buffer(incremental_state, 'prev_value')
            assert prev_value is not None
            v = prev_value.view(bsz_X_num_heads, -1, self.head_dim)
        else:
            k = self.k_proj(key)
            k = k.contiguous().view(-1, bsz_X_num_heads, self.head_dim).transpose(0, 1)
            v = self.v_proj(value)
            v = v.contiguous().view(-1, bsz_X_num_heads, self.head_dim).transpose(0, 1)
            if incremental_state is not None:
                self._set_input_buffer(incremental_state, 'prev_key', k.view(bsz, self.num_heads, -1, self.head_dim))
                self._set_input_buffer(incremental_state, 'prev_value', v.view(bsz, self.num_heads, -1, self.head_dim))
                key_padding_mask = self._get_input_buffer(incremental_state, 'prev_key_padding_mask')
                if key_padding_mask is not None:
                    self._set_input_buffer(incremental_state, 'prev_key_padding_mask', key_padding_mask)
        assert list(k.size()) == [bsz_X_num_heads, src_len, self.head_dim], f'key.size() :{k.size()} [ bsz_X_num_heads, src_len, self.head_dim] : [{bsz_X_num_heads}, {src_len}, {self.head_dim}]'
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        if key_padding_mask is not None:
            attn_weights = attn_weights.view(bsz, self.num_heads, target_len, src_len)
            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
            attn_weights = attn_weights.view(bsz_X_num_heads, target_len, src_len)
        assert list(attn_weights.size()) == [bsz_X_num_heads, target_len, src_len]
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_probs = self.dropout(attn_weights)
        attn = torch.bmm(attn_probs, v)
        assert list(attn.size()) == [bsz_X_num_heads, target_len, self.head_dim]
        attn = attn.transpose(0, 1).contiguous().view(target_len, bsz, embed_dim)
        attn = self.out_proj(attn)
        if need_weights:
            attn_weights = attn_weights.view(bsz, self.num_heads, target_len, src_len).transpose(1, 0)
            attn_weights_out = attn_weights.mean(dim=0)
        else:
            attn_weights_out = None
        return attn, attn_weights_out

    def reorder_incremental_state(self, incremental_state: Dict[str, Tensor], new_order: Tensor):
        """Reorder buffered internal state (for incremental generation)."""
        all_keys = ['prev_key', 'prev_value', 'prev_key_padding_mask']
        for key in all_keys:
            input_buffer = self._get_input_buffer(incremental_state, key)
            if input_buffer is not None:
                input_buffer = input_buffer.index_select(0, new_order)
                self._set_input_buffer(incremental_state, key, input_buffer)

    def _get_input_buffer(self, incremental_state: Dict[str, Tensor], key: str):
        return self.get_incremental_state(incremental_state, key)

    def _set_input_buffer(self, incremental_state: Dict[str, Tensor], key: str, value: Tensor):
        self.set_incremental_state(incremental_state, key, value)


class DecoupledMultiheadAttention(nn.Module):
    """
    Multiheaded Scaled Dot Product Attention. This function
    has the same exact signature as the one used in pytorch_translate
    with the added benefit of supporting torchscript
    """

    def __init__(self, embed_dim: int, context_dim: int, num_heads: int, dropout: float, unseen_mask=False, src_length_mask=True):
        super().__init__()
        assert embed_dim == context_dim
        d_model = embed_dim
        assert d_model % num_heads == 0
        if unseen_mask:
            raise NotImplementedError('Unseen mask not supported with sequential decoding')
        self._attn = MultiheadAttention(d_model, num_heads, dropout)
        self.use_src_length_mask = src_length_mask

    def forward(self, decoder_state: Tensor, source_hids: Tensor, src_len_mask: Optional[Tensor], squeeze: bool=True) ->Tuple[Tensor, Tensor]:
        """
        Computes MultiheadAttention with respect to either a vector
        or a tensor

        Inputs:
            decoder_state: (bsz x decoder_hidden_state_dim) or
                (bsz x T x decoder_hidden_state_dim)
            source_hids: srclen x bsz x context_dim
            src_lengths: bsz x 1, actual sequence lengths
            squeeze: Whether or not to squeeze on the time dimension.
                Even if decoder_state.dim() is 2 dimensional an
                explicit time step dimension will be unsqueezed.
        Outputs:
          [batch_size, max_src_len] if decoder_state.dim() == 2 & squeeze
            or
          [batch_size, 1, max_src_len] if decoder_state.dim() == 2 & !squeeze
            or
          [batch_size, T, max_src_len] if decoder_state.dim() == 3 & !squeeze
            or
          [batch_size, T, max_src_len] if decoder_state.dim() == 3 & squeeze & T != 1
            or
          [batch_size, max_src_len] if decoder_state.dim() == 3 & squeeze & T == 1
        """
        if decoder_state.dim() == 3:
            query = decoder_state
        elif decoder_state.dim() == 2:
            query = decoder_state.unsqueeze(1)
        else:
            raise ValueError('decoder state must be either 2 or 3 dimensional')
        query = query.transpose(0, 1)
        value = key = source_hids
        attn, attn_weights = self._attn.forward(query, key, value, key_padding_mask=src_len_mask, need_weights=True)
        if attn_weights is None:
            raise NotImplementedError('')
        attn_weights = attn_weights.transpose(0, 2)
        if squeeze:
            attn = attn.squeeze(0)
            attn_weights = attn_weights.squeeze(1)
            return attn, attn_weights
        return attn, attn_weights


class PlaceholderIdentity(nn.Module):


    class Config(ModuleConfig):
        pass

    def forward(self, x, incremental_state: Optional[Dict[str, Tensor]]=None):
        return x


class PlaceholderAttentionIdentity(nn.Module):

    def forward(self, query, key, value, need_weights: bool=None, key_padding_mask: Optional[Tensor]=None, incremental_state: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Optional[Tensor]]:
        optional_attention: Optional[Tensor] = None
        return query, optional_attention

    def reorder_incremental_state(self, incremental_state: Dict[str, Tensor], new_order: Tensor):
        pass


def unfold1d(x, kernel_size: int, padding_l: int, pad_value: float=0):
    """unfold T x B x C to T x B x C x K"""
    if kernel_size > 1:
        T, B, C = x.size()
        x = F.pad(x, (0, 0, 0, 0, padding_l, kernel_size - 1 - padding_l), value=pad_value)
        x = x.as_strided((T, B, C, kernel_size), (B * C, C, 1, B * C))
    else:
        x = x.unsqueeze(3)
    return x


class LightweightConv(PyTextIncrementalDecoderComponent):


    class Config(ConfigBase):
        num_heads: int = 2
        weight_softmax: bool = False
        bias: bool = True

    @classmethod
    def from_config(cls, config, input_size, kernel_size, convolution_type):
        return cls(input_size, kernel_size, convolution_type, **config._asdict())

    def __init__(self, input_size, kernel_size, convolution_type: str, num_heads, weight_softmax, bias):
        super().__init__()
        self.input_size = input_size
        self.kernel_size = kernel_size
        if convolution_type == 'non-causal':
            padding_l = kernel_size // 2 if kernel_size % 2 == 1 else ((kernel_size - 1) // 2, kernel_size // 2)
        elif convolution_type == 'causal':
            padding_l = kernel_size - 1
        else:
            raise Exception('Convolution type not supported')
        self.padding_l = padding_l
        self.num_heads = num_heads
        self.weight_softmax = weight_softmax
        self.weight = nn.Parameter(torch.Tensor(num_heads, 1, kernel_size))
        self.has_bias = bias
        if bias:
            self.bias = nn.Parameter(torch.Tensor(input_size).view(1, 1, -1))
        else:
            self.bias = nn.Parameter(torch.Tensor())
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.xavier_uniform_(self.weight)
        if self.has_bias:
            nn.init.constant_(self.bias, 0.0)

    def forward(self, x, incremental_state: Optional[Dict[str, Tensor]]=None):
        """Assuming the input, x, of the shape T x B x C and producing an output in the shape T x B x C
        args:
            x: Input of shape T x B x C, i.e. (timesteps, batch_size, input_size)
            incremental_state: A dict to keep the state
        """
        output = self._forward_unfolded(x, incremental_state)
        if self.has_bias:
            output = output + self.bias
        return output

    def _forward_unfolded(self, x, incremental_state: Optional[Dict[str, Tensor]]=None):
        """The conventional implementation of convolutions.
        Unfolding the input by having a window shifting to the right."""
        T, B, C = x.size()
        K, H = self.kernel_size, self.num_heads
        R = C // H
        assert R * H == C == self.input_size
        weight = self.weight.view(H, K)
        if incremental_state is not None:
            input_buffer = self._get_input_buffer(incremental_state)
            if input_buffer is not None:
                x_unfold = torch.cat([input_buffer, x.unsqueeze(3)], dim=3)
            else:
                x_unfold = x.unsqueeze(3).clone()
            if self.kernel_size > 1:
                self._set_input_buffer(incremental_state, x_unfold[:, :, :, -self.kernel_size + 1:])
            x_unfold = x_unfold.view(T * B * H, R, -1)
        else:
            x_unfold = unfold1d(x, self.kernel_size, self.padding_l, 0.0)
            x_unfold = x_unfold.view(T * B * H, R, K)
        if incremental_state is not None:
            weight = weight[:, -x_unfold.size(2):]
            K = weight.size(1)
        weight = weight.view(1, H, K).expand(T * B, H, K).contiguous().view(T * B * H, K, 1)
        output = torch.bmm(x_unfold, weight)
        output = output.view(T, B, C)
        return output

    def reorder_incremental_state(self, incremental_state: Dict[str, Tensor], new_order: Tensor):
        input_buffer = self._get_input_buffer(incremental_state)
        if input_buffer is not None:
            input_buffer = input_buffer.index_select(1, new_order)
            self._set_input_buffer(incremental_state, input_buffer)

    def _get_input_buffer(self, incremental_state: Dict[str, Tensor]):
        return self.get_incremental_state(incremental_state, 'input_buffer')

    def _set_input_buffer(self, incremental_state: Dict[str, Tensor], new_buffer: Tensor):
        return self.set_incremental_state(incremental_state, 'input_buffer', new_buffer)

    def extra_repr(self):
        s = '{}, kernel_size={}, padding_l={}, num_heads={}, weight_softmax={}, bias={}'.format(self.input_size, self.kernel_size, self.padding_l, self.num_heads, self.weight_softmax, self.has_bias)
        return s


class LightConvDecoderLayer(PyTextSeq2SeqModule):


    class Config(ConfigBase):
        attention_dropout: float = 0.0
        decoder_attention_heads: int = 1
        self_attention_heads: int = 1
        decoder_conv_dim: int = 128
        decoder_conv_type: Union[LightweightConv.Config, PlaceholderIdentity.Config] = LightweightConv.Config()
        attention_type: Union[MultiheadAttention.Config, None] = MultiheadAttention.Config()
        self_attention_type: Optional[MultiheadAttention.Config] = None
        decoder_embed_dim: int = 128
        decoder_ffn_embed_dim: int = 512
        decoder_glu: bool = True
        decoder_normalize_before: bool = False
        dropout: float = 0.1
        input_dropout: float = 0.1
        relu_dropout: float = 0.0
        need_attention: bool = True
        convolution_type: str = 'causal'

    @classmethod
    def from_config(cls, config, kernel_size):
        conv = create_module(config.decoder_conv_type, input_size=config.decoder_conv_dim, kernel_size=kernel_size, convolution_type=config.convolution_type)
        if config.attention_type is not None:
            attention = create_module(config.attention_type, config.decoder_embed_dim, config.decoder_attention_heads)
        else:
            attention = None
        if config.self_attention_type is not None:
            self_attention = create_module(config.self_attention_type, config.decoder_embed_dim, config.self_attention_heads)
        else:
            self_attention = None
        return cls(**config._asdict(), conv=conv, self_attention=self_attention, attention=attention)

    def __init__(self, attention_dropout, decoder_attention_heads, self_attention_heads, decoder_conv_dim, decoder_conv_type, attention_type, self_attention_type, decoder_embed_dim, decoder_ffn_embed_dim, decoder_glu, decoder_normalize_before, dropout, input_dropout, relu_dropout, need_attention, convolution_type, conv=None, self_attention=None, attention=None):
        super().__init__()
        self.embed_dim = decoder_embed_dim
        self.conv_dim = decoder_conv_dim
        if decoder_glu:
            self.linear1 = Linear(self.embed_dim, 2 * self.conv_dim)
            self.act = nn.GLU()
        else:
            self.linear1 = Linear(self.embed_dim, self.conv_dim)
            self.act = PlaceholderIdentity()
        self.conv = conv
        self.linear2 = Linear(self.conv_dim, self.embed_dim)
        self.dropout = dropout
        self.relu_dropout = relu_dropout
        self.input_dropout = input_dropout
        self.normalize_before = decoder_normalize_before
        self.conv_layer_norm = LayerNorm(self.embed_dim)
        if attention is None:
            self.no_encoder_attn = True
            self.encoder_attn = PlaceholderAttentionIdentity()
            self.encoder_attn_layer_norm = PlaceholderIdentity()
        else:
            self.no_encoder_attn = False
            self.encoder_attn = attention
            self.encoder_attn_layer_norm = LayerNorm(self.embed_dim)
        if self_attention is None:
            self.has_self_attn = False
            self.self_attn = PlaceholderAttentionIdentity()
        else:
            self.has_self_attn = True
            self.self_attn = self_attention
        self.fc1 = Linear(self.embed_dim, decoder_ffn_embed_dim)
        self.fc2 = Linear(decoder_ffn_embed_dim, self.embed_dim)
        self.final_layer_norm = LayerNorm(self.embed_dim)
        self.need_attn = need_attention

    def forward(self, x, encoder_out: Tensor, encoder_padding_mask: Optional[Tensor], decoder_padding_mask: Optional[Tensor], incremental_state: Optional[Dict[str, Tensor]]):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, src_len)` where padding elements are indicated by ``1``.

        Returns:
            encoded output of shape `(batch, src_len, embed_dim)`
        """
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.conv_layer_norm(x)
        if self.has_self_attn:
            x, _ = self.self_attn(x, key=x, value=x, key_padding_mask=decoder_padding_mask, need_weights=False, incremental_state=incremental_state)
            x = residual + x
            residual = x
        x = F.dropout(x, p=self.input_dropout, training=self.training)
        x = self.linear1(x)
        x = self.act(x)
        if decoder_padding_mask is not None:
            x = x.masked_fill(decoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)
        x = self.conv(x, incremental_state=incremental_state)
        x = self.linear2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.conv_layer_norm(x)
        attn: Optional[Tensor] = None
        if not self.no_encoder_attn:
            residual = x
            normalize = self.maybe_layer_norm(before=True)
            if normalize:
                x = self.encoder_attn_layer_norm(x)
            x, attn = self.encoder_attn(query=x, key=encoder_out, value=encoder_out, key_padding_mask=encoder_padding_mask, incremental_state=incremental_state, need_weights=not self.training and self.need_attn)
            x = F.dropout(x, p=self.dropout, training=self.training)
            x = residual + x
            normalize = self.maybe_layer_norm(after=True)
            if normalize:
                x = self.encoder_attn_layer_norm(x)
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.final_layer_norm(x)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.relu_dropout, training=self.training)
        x = self.fc2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.final_layer_norm(x)
        return x, attn

    def maybe_layer_norm(self, before: bool=False, after: bool=False):
        """This a utility function which helps to control the layer norm behavior
        `before` and `after` specific components using one variable in config.
        If self.normalize_before is set to True, output is true only when `before`
        is True
        """
        assert before ^ after, 'Incorrect usage'
        return after ^ self.normalize_before

    def reorder_incremental_state(self, incremental_state: Dict[str, Tensor], new_order: Tensor):
        self.self_attn.reorder_incremental_state(incremental_state, new_order)
        self.encoder_attn.reorder_incremental_state(incremental_state, new_order)
        self.conv.reorder_incremental_state(incremental_state, new_order)

    def extra_repr(self):
        return 'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'.format(self.dropout, self.relu_dropout, self.input_dropout, self.normalize_before)


class PostionalEmbedCombine(Enum):
    SUM = 'sum'
    CONCAT = 'concat'


class PostionalEmbedType(Enum):
    LEARNED = 'learned'
    SINUSOIDAL = 'sinusoidal'
    HYBRID = 'hybrid'


class ConvDecoderConfig(ConfigBase):
    dropout: float = 0.1
    decoder_embed_dim: int = 128
    decoder_input_dim: int = 128
    decoder_output_dim: int = 128
    max_target_positions: int = 128
    decoder_learned_pos: bool = False
    no_token_positional_embeddings: bool = False
    positional_embedding_type: PostionalEmbedType = PostionalEmbedType.LEARNED
    combine_pos_embed: PostionalEmbedCombine = PostionalEmbedCombine.CONCAT
    decoder_normalize_before: bool = False


def get_sinusoidal_embedding(num_embeddings: int, embedding_dim: int, padding_idx: int):
    """Build sinusoidal embeddings.

    This matches the implementation in tensor2tensor, but differs slightly
    from the description in Section 3.5 of "Attention Is All You Need".
    """
    half_dim = embedding_dim // 2
    emb = math.log(10000) / (half_dim - 1)
    emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
    emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)
    if embedding_dim % 2 == 1:
        emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)
    emb[padding_idx, :] = 0
    return emb


def make_positions(input, padding_idx: int):
    """Replace non-padding symbols with their position numbers.

    Position numbers begin at padding_idx+1. Padding symbols are ignored.
    """
    mask = input.ne(padding_idx)
    return torch.cumsum(mask, dim=1) * mask + padding_idx


class SinusoidalPositionalEmbedding(nn.Module):
    """This module produces sinusoidal positional embeddings of any length.

    Padding symbols are ignored.
    """

    def __init__(self, embedding_dim, padding_idx, init_size=124, learned_embed=False):
        super().__init__()
        self.embedding_dim = embedding_dim
        self.padding_idx = padding_idx
        if not learned_embed:
            self.sinusoidal_embedding_dim = embedding_dim
            self.learned_embed = learned_embed
            self.learned_embedding = PlaceholderIdentity()
        else:
            assert embedding_dim % 2 == 0
            self.sinusoidal_embedding_dim = embedding_dim // 2
            self.learned_embedding = nn.Embedding(init_size, embedding_dim // 2, padding_idx)
            self.learned_embed = learned_embed
        self.weights = nn.Parameter(get_sinusoidal_embedding(init_size, self.sinusoidal_embedding_dim, padding_idx))
        self.weights.requires_grad = False
        self.max_positions = int(100000.0)

    def forward(self, input, incremental_state: Optional[Dict[str, Tensor]]=None, timestep: Optional[int]=None):
        """Input is expected to be of size [bsz x seqlen]."""
        bsz, seq_len = input.size()
        max_pos = self.padding_idx + 1 + seq_len
        assert max_pos < self.weights.size(0), f'max_pos :{max_pos}, self.weights.size(0): {self.weights.size(0)}'
        if incremental_state is not None:
            pos = timestep if timestep is not None else seq_len
            assert pos != 0, 'Position cannot start from 0'
            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)
        positions = make_positions(input, self.padding_idx)
        sinusoidal_embedding = self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()
        if self.learned_embed:
            learned_embedding = self.learned_embedding(positions)
            embed_out = torch.cat([sinusoidal_embedding, learned_embedding], dim=-1)
        else:
            embed_out = sinusoidal_embedding
        return embed_out


def build_positional_embedding(positional_embedding_type: PostionalEmbedType, combine_pos_embed: PostionalEmbedCombine, max_target_positions: int, input_embed_dim: int, embed_dim: int, padding_idx: int, no_token_positional_embeddings: bool):
    if combine_pos_embed == PostionalEmbedCombine.SUM:
        pos_embed_dim = embed_dim
    elif combine_pos_embed == PostionalEmbedCombine.CONCAT:
        pos_embed_dim = embed_dim - input_embed_dim
    else:
        raise NotImplementedError
    if not no_token_positional_embeddings:
        if positional_embedding_type == PostionalEmbedType.LEARNED:
            return PositionalEmbedding(max_target_positions, pos_embed_dim, padding_idx)
        elif positional_embedding_type == PostionalEmbedType.SINUSOIDAL or positional_embedding_type == PostionalEmbedType.HYBRID:
            return SinusoidalPositionalEmbedding(pos_embed_dim, padding_idx, init_size=max_target_positions, learned_embed=positional_embedding_type == PostionalEmbedType.HYBRID)
        else:
            raise NotImplementedError('Positional embedding type not supported')
    else:
        return PlaceholderIdentity()


class LightConvDecoderBase(PyTextIncrementalDecoderComponent):


    class Config(ModuleConfig):
        decoder_config: ConvDecoderConfig = ConvDecoderConfig()
        layer_config: LightConvDecoderLayer.Config = LightConvDecoderLayer.Config()
        decoder_kernel_size_list: List[int] = [3, 7, 15]

    @classmethod
    def from_config(cls, config, tgt_dict, tgt_embedding):
        kernel_size_list = config.decoder_kernel_size_list
        layers = []
        for size in kernel_size_list:
            assert config.decoder_config.decoder_embed_dim == config.layer_config.decoder_embed_dim
            layers.append(create_module(config.layer_config, kernel_size=size))
        return cls(tgt_dict, tgt_embedding, layers, config.decoder_config)

    def __init__(self, target_dict, embed_tokens, layers, decoder_config):
        super().__init__()
        self.dropout = decoder_config.dropout
        input_embed_dim = embed_tokens.embedding_dim
        embed_dim = decoder_config.decoder_embed_dim
        output_embed_dim = decoder_config.decoder_output_dim
        padding_idx = target_dict.get_pad_index()
        self.max_target_positions = decoder_config.max_target_positions
        self.embed_tokens = embed_tokens
        self.embed_scale = math.sqrt(embed_dim)
        self.padding_idx = padding_idx
        self.no_token_positional_embeddings = decoder_config.no_token_positional_embeddings
        self.project_in_dim = Linear(input_embed_dim, embed_dim) if embed_dim != input_embed_dim else PlaceholderIdentity()
        self.embed_layer_norm = LayerNorm(embed_dim)
        self.combine_pos_embed = decoder_config.combine_pos_embed.value
        self.embed_positions = build_positional_embedding(positional_embedding_type=decoder_config.positional_embedding_type, combine_pos_embed=decoder_config.combine_pos_embed, max_target_positions=decoder_config.max_target_positions, input_embed_dim=input_embed_dim, embed_dim=embed_dim, padding_idx=padding_idx, no_token_positional_embeddings=decoder_config.no_token_positional_embeddings)
        self.layers = nn.ModuleList(layers)
        self.project_out_dim = Linear(embed_dim, output_embed_dim, bias=False) if embed_dim != output_embed_dim else PlaceholderIdentity()
        self.normalize = decoder_config.decoder_normalize_before
        if self.normalize:
            self.layer_norm = LayerNorm(embed_dim)
        else:
            self.layer_norm = PlaceholderIdentity()

    def forward_unprojected(self, prev_output_tokens: Tensor, encoder_out: Dict[str, Tensor], incremental_state: Optional[Dict[str, Tensor]]=None, timestep: Optional[int]=None) ->Tuple[Tensor, Dict[str, Tensor]]:
        output_dict: Dict[str, Tensor] = {}
        if incremental_state is not None:
            prev_output_tokens = prev_output_tokens[:, -1:]
        x = self.embed_scale * self.embed_tokens([[prev_output_tokens]])
        if not self.no_token_positional_embeddings:
            x = self.pos_embed(x, prev_output_tokens)
        else:
            x = self.project_in_dim(x)
        x = self.embed_layer_norm(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        output_dict['decoder_layer_0'] = x.clone()
        x = x.transpose(0, 1)
        last_layer_attn: Optional[Tensor] = None
        decoder_padding_mask = prev_output_tokens.eq(self.padding_idx)
        target_lengths = (~decoder_padding_mask).sum(dim=1)
        if not decoder_padding_mask.any():
            decoder_mask = None
        else:
            decoder_mask = decoder_padding_mask
        encoder = encoder_out['encoder_out']
        encoder_mask: Optional[Tensor] = None
        if 'encoder_mask' in encoder_out:
            encoder_mask = encoder_out['encoder_mask']
        for idx, layer in enumerate(self.layers):
            encoder = encoder_out['encoder_out']
            encoder_mask: Optional[Tensor] = None
            if 'encoder_mask' in encoder_out:
                encoder_mask = encoder_out['encoder_mask']
            x, last_layer_attn = layer(x, encoder, encoder_mask, decoder_mask, incremental_state)
            output_dict['decoder_layer_' + str(idx + 1)] = x.transpose(0, 1).clone()
        if self.normalize:
            x = self.layer_norm(x)
        x = x.transpose(0, 1)
        x = self.project_out_dim(x)
        if last_layer_attn is not None:
            output_dict['attn_scores'] = last_layer_attn
        output_dict['target_lengths'] = target_lengths
        output_dict['decoder_mask'] = decoder_padding_mask
        for key in encoder_out.keys():
            output_dict[key] = encoder_out[key]
        return x, output_dict

    def pos_embed(self, x, src_tokens):
        if self.combine_pos_embed == PostionalEmbedCombine.SUM.value:
            x = self.project_in_dim(x)
            return self._vanilla_transformer(x, src_tokens)
        elif self.combine_pos_embed == PostionalEmbedCombine.CONCAT.value:
            return self._concat_pos_embed(x, src_tokens)
        else:
            raise NotImplementedError('Method not supported')

    def _vanilla_transformer(self, x, src_tokens):
        x += self.embed_positions(src_tokens)
        return x

    def _concat_pos_embed(self, x, src_tokens):
        pos_embed = self.embed_positions(src_tokens)
        return torch.cat([x, pos_embed], dim=2)

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        if self.no_token_positional_embeddings:
            return self.max_target_positions
        return min(self.max_target_positions, self.embed_positions.max_positions)

    def reorder_incremental_state(self, incremental_state: Dict[str, Tensor], new_order: Tensor):
        for layer in self.layers:
            layer.reorder_incremental_state(incremental_state, new_order)

    def get_probs(self, decoder_out: Tuple[Tensor, Dict[str, Tensor]]) ->Tuple[Tensor, Tensor, Tensor]:
        return self.projection_layer.get_probs(decoder_out)


class DecoderWithLinearOutputProjection(PyTextSeq2SeqModule):
    """
    Common super class for decoder networks with output projection layers.
    """

    def __init__(self, out_vocab_size, out_embed_dim=512):
        super().__init__()
        self.linear_projection = nn.Linear(out_embed_dim, out_vocab_size)
        self.reset_parameters()
        log_class_usage(__class__)

    def reset_parameters(self):
        nn.init.uniform_(self.linear_projection.weight, -0.1, 0.1)
        nn.init.zeros_(self.linear_projection.bias)

    def forward(self, input_tokens, encoder_out: Dict[str, torch.Tensor], incremental_state: Optional[Dict[str, torch.Tensor]]=None, timestep: int=0) ->Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        x, features = self.forward_unprojected(input_tokens, encoder_out, incremental_state)
        logits = self.linear_projection(x)
        return logits, features

    def forward_unprojected(self, input_tokens, encoder_out, incremental_state=None):
        """Forward pass through the decoder without output projection."""
        raise NotImplementedError()


class LightConvDecoder(LightConvDecoderBase):

    def __init__(self, target_dict, embed_tokens, layers, decoder_config):
        super().__init__(target_dict, embed_tokens, layers, decoder_config)
        self.projection_layer = DecoderWithLinearOutputProjection(target_dict, target_dict, decoder_config.decoder_output_dim)

    def forward(self, prev_output_tokens: Tensor, encoder_out: Dict[str, Tensor], incremental_state: Optional[Dict[str, Tensor]]=None, timestep: Optional[int]=None) ->Tuple[Tensor, Dict[str, Tensor]]:
        hidden_decoder_output = self.forward_unprojected(prev_output_tokens, encoder_out, incremental_state, timestep)
        return self.projection_layer(encoder_out=encoder_out, decoder_out=hidden_decoder_output, incremental_state=incremental_state)

    def get_probs(self, decoder_out: Tuple[Tensor, Dict[str, Tensor]]) ->Tuple[Tensor, Tensor, Tensor]:
        return self.projection_layer.get_probs(decoder_out)


class DecoupledDecoderHead(nn.Module):
    fixed_generation_vocab_expanded = torch.jit.Final[Tensor]

    def __init__(self, src_dict, dst_dict, out_embed_dim=512, encoder_hidden_dim=None, pointer_attention_heads=1, fixed_generation_vocab=None, attention_dropout=0.2, model_output_logprob=True):
        super().__init__()
        self.linear_projection = nn.Linear(out_embed_dim, len(fixed_generation_vocab) if fixed_generation_vocab else len(dst_dict))
        self.num_embeddings = len(dst_dict)
        self.pointer_projection = nn.Linear(encoder_hidden_dim, out_embed_dim)
        self.pointer_prob_map = nn.Linear(out_embed_dim + out_embed_dim, 1)
        self.pointer_attention = DecoupledMultiheadAttention(out_embed_dim, out_embed_dim, pointer_attention_heads, src_length_mask=False, dropout=attention_dropout)
        self.fixed_vocab = not fixed_generation_vocab is None
        if self.fixed_vocab:
            assert isinstance(fixed_generation_vocab, list), 'List of indices is what is expected for fixed_generation_vocab'
            self.fixed_generation_vocab_expanded: Tensor = torch.tensor(fixed_generation_vocab, dtype=torch.long).unsqueeze(0).unsqueeze(0)
        else:
            self.fixed_generation_vocab_expanded: Tensor = torch.zeros([1])
        self.register_buffer('fixed_generation_vocab_expanded_buffer', self.fixed_generation_vocab_expanded)
        self.model_output_logprob = model_output_logprob

    def forward(self, encoder_out: Dict[str, Tensor], decoder_out: Tuple[Tensor, Dict[str, Tensor]], incremental_state: Optional[Dict[str, Tensor]]=None) ->Tuple[Tensor, Dict[str, Tensor]]:
        """
        Variables for Shape comments
        ----------------------------------
        B: Batch
        T_src: Length of source sequence
        T_trg: Length of target seuqence
        C: hidden dimension
        V_ont: Size of ontology vocabulary
        V_trg: Size of full target vocabulary
        """
        self.verify_encoder_out(encoder_out)
        encoder_outs = encoder_out['encoder_out']
        encoder_mask: Optional[Tensor] = None
        if 'encoder_mask' in encoder_out:
            encoder_mask = encoder_out['encoder_mask']
        src_tokens = self.get_pointer_src_tokens(encoder_out)
        x, output_dict = decoder_out
        output_dict['decoder_out'] = x
        logits = self.linear_projection(x)
        optional_fixed_logits = F.softmax(logits.float(), dim=2)
        if self.fixed_vocab:
            optional_fixed_logits_1 = torch.zeros((logits.size(0), logits.size(1), self.num_embeddings), device=logits.device, dtype=logits.dtype)
            fixed_expanded = self.fixed_generation_vocab_expanded_buffer.repeat(logits.size(0), logits.size(1), 1)
            optional_fixed_logits_1.scatter_add_(2, fixed_expanded, optional_fixed_logits)
            optional_fixed_logits = optional_fixed_logits_1
        encoder_outs = self.pointer_projection(encoder_outs)
        cur_src_attn, calc_src_attn_scores = self.pointer_attention(x, encoder_outs, encoder_mask, False)
        cur_src_attn = cur_src_attn
        calc_src_attn_scores = calc_src_attn_scores
        cur_src_attn = cur_src_attn.transpose(0, 1)
        calc_src_attn_scores = calc_src_attn_scores.transpose(0, 2)
        prob = torch.sigmoid(self.pointer_prob_map(torch.cat([cur_src_attn, x], dim=2)))
        vocab_attn_scores = torch.zeros(optional_fixed_logits.size(0), optional_fixed_logits.size(1), optional_fixed_logits.size(2), device=optional_fixed_logits.device, dtype=optional_fixed_logits.dtype)
        src_tokens_expanded = src_tokens.unsqueeze(1).repeat(1, logits.size(1), 1)
        vocab_attn_scores.scatter_add_(2, src_tokens_expanded, calc_src_attn_scores)
        explicit_copy_probs = prob * optional_fixed_logits + (1 - prob) * vocab_attn_scores
        if self.model_output_logprob:
            model_out = (explicit_copy_probs + 1e-07).log()
        else:
            model_out = explicit_copy_probs + 1e-07
        return model_out, output_dict

    @torch.jit.export
    def get_probs(self, decoder_out: Tuple[Tensor, Dict[str, Tensor]]) ->Tuple[Tensor, Tensor, Tensor]:
        model_out, output_dict = decoder_out
        if self.model_output_logprob:
            probs = model_out.exp()
            max_probs, idx = probs.max(dim=-1)
        else:
            max_probs, idx = model_out.max(dim=-1)
        return idx, max_probs, model_out

    def get_pointer_src_tokens(self, encoder_out: Dict[str, Tensor]) ->torch.Tensor:
        return encoder_out['src_tokens']

    def verify_encoder_out(self, encoder_out: Dict[str, Tensor]):
        verify_encoder_out(encoder_out, ['encoder_out', 'src_tokens'])


def extract_ontology_vocab(target_dictionary):
    fixed_generation_vocab = []
    for i, symbol in enumerate(target_dictionary._vocab):
        lower_symbol = symbol.lower()
        if lower_symbol[0] == '[' or lower_symbol == ']':
            fixed_generation_vocab.append(i)
    return fixed_generation_vocab


def log_and_overwrite(param_name: str, x, y) ->int:
    if x != y:
        logging.warning(f'Mismatch of {param_name} expected {y} got {x}')
    return y


class LightConvDecoupledDecoder(LightConvDecoderBase):


    class Config(ModuleConfig):
        decoder_config: ConvDecoderConfig = ConvDecoderConfig()
        layer_config: LightConvDecoderLayer.Config = LightConvDecoderLayer.Config()
        decoder_kernel_size_list: List[int] = [3, 7, 15]
        decoder_layers: int = 3
        decoupled_attention_heads: int = 1
        ontology_generation_only: bool = False
        model_output_logprob: bool = True

    def __init__(self, target_dict, embed_tokens, layers, decoder_config, ontology_generation_only, decoupled_attention_heads, model_output_logprob):
        super().__init__(target_dict, embed_tokens, layers, decoder_config)
        fixed_generation_vocab = None
        if ontology_generation_only:
            fixed_generation_vocab = extract_ontology_vocab(target_dict)
        self.projection_layer = DecoupledDecoderHead(target_dict, target_dict, out_embed_dim=decoder_config.decoder_output_dim, encoder_hidden_dim=decoder_config.decoder_input_dim, pointer_attention_heads=decoupled_attention_heads, fixed_generation_vocab=fixed_generation_vocab, model_output_logprob=model_output_logprob)

    def forward(self, prev_output_tokens: Tensor, encoder_out: Dict[str, Tensor], incremental_state: Optional[Dict[str, Tensor]]=None, timestep: Optional[int]=None) ->Tuple[Tensor, Dict[str, Tensor]]:
        hidden_decoder_output = self.forward_unprojected(prev_output_tokens, encoder_out, incremental_state, timestep)
        return self.projection_layer(encoder_out=encoder_out, decoder_out=hidden_decoder_output, incremental_state=incremental_state)

    @classmethod
    def from_config(cls, config, tgt_dict, tgt_embedding):
        kernel_size_list = config.decoder_kernel_size_list
        layers = []
        config.layer_config.decoder_embed_dim = log_and_overwrite(param_name='layer_config.decoder_embed_dim, decoder_config.decoder_embed_dim', x=config.layer_config.decoder_embed_dim, y=config.decoder_config.decoder_embed_dim)
        config.decoder_config.decoder_input_dim = log_and_overwrite(param_name='decoder_config.decoder_input_dim, decoder_config.decoder_embed_dim', x=config.decoder_config.decoder_input_dim, y=config.decoder_config.decoder_embed_dim)
        config.layer_config.decoder_conv_dim = log_and_overwrite(param_name='layer_config.decoder_conv_dim, decoder_config.decoder_embed_dim', x=config.layer_config.decoder_conv_dim, y=config.decoder_config.decoder_embed_dim)
        for size in kernel_size_list:
            assert config.decoder_config.decoder_embed_dim == config.layer_config.decoder_embed_dim
            layers.append(create_module(config.layer_config, kernel_size=size))
        return cls(tgt_dict, tgt_embedding, layers, config.decoder_config, config.ontology_generation_only, config.decoupled_attention_heads, config.model_output_logprob)


class LightConvEncoderLayer(PyTextSeq2SeqModule):


    class Config(ConfigBase):
        dropout: float = 0.1
        encoder_conv_dim: int = 128
        encoder_conv_type: Union[LightweightConv.Config, PlaceholderIdentity.Config] = LightweightConv.Config()
        self_attention_type: Optional[MultiheadAttention.Config] = None
        encoder_embed_dim: Optional[int] = None
        encoder_ffn_embed_dim: int = 512
        self_attention_heads: int = 2
        encoder_glu: bool = True
        encoder_normalize_before: bool = False
        input_dropout: float = 0.1
        relu_dropout: float = 0.0
        convolution_type: str = 'non-causal'

    @classmethod
    def from_config(cls, config, kernel_size):
        conv = create_module(config.encoder_conv_type, input_size=config.encoder_conv_dim, kernel_size=kernel_size, convolution_type='non-causal')
        if config.self_attention_type is not None:
            self_attention = create_module(config.self_attention_type, config.encoder_embed_dim, config.self_attention_heads)
        else:
            self_attention = None
        return cls(**config._asdict(), conv=conv, self_attention=self_attention)

    def __init__(self, dropout, encoder_conv_dim, encoder_conv_type, self_attention_type, encoder_embed_dim, encoder_ffn_embed_dim, self_attention_heads, encoder_glu, encoder_normalize_before, input_dropout, relu_dropout, convolution_type, conv=None, self_attention=None):
        super().__init__()
        self.embed_dim = encoder_embed_dim
        self.conv_dim = encoder_conv_dim
        if encoder_glu:
            self.linear1 = Linear(self.embed_dim, 2 * self.conv_dim)
            self.act = nn.GLU()
        else:
            self.linear1 = Linear(self.embed_dim, self.conv_dim)
            self.act = None
        self.conv = conv
        self.linear2 = Linear(self.conv_dim, self.embed_dim)
        self.dropout = dropout
        self.relu_dropout = relu_dropout
        self.input_dropout = input_dropout
        self.normalize_before = encoder_normalize_before
        self.fc1 = Linear(self.embed_dim, encoder_ffn_embed_dim)
        self.fc2 = Linear(encoder_ffn_embed_dim, self.embed_dim)
        self.layer_norm1 = LayerNorm(self.embed_dim)
        self.layer_norm2 = LayerNorm(self.embed_dim)
        if self_attention is None:
            self.has_self_attn = False
            self.self_attn = PlaceholderAttentionIdentity()
        else:
            self.has_self_attn = True
            self.self_attn = self_attention

    def forward(self, x, encoder_padding_mask: Optional[Tensor]=None):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, src_len)` where padding elements are indicated by ``1``.

        Returns:
            encoded output of shape `(batch, src_len, embed_dim)`
        """
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.layer_norm1(x)
        if self.has_self_attn:
            x, _ = self.self_attn(query=x, key=x, value=x, key_padding_mask=encoder_padding_mask, incremental_state=None, need_weights=False)
            x = residual + x
            residual = x
        x = F.dropout(x, p=self.input_dropout, training=self.training)
        x = self.linear1(x)
        x = self.act(x)
        if encoder_padding_mask is not None:
            x = x.masked_fill(encoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)
        x = self.conv(x)
        x = self.linear2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.layer_norm1(x)
        residual = x
        normalize = self.maybe_layer_norm(before=True)
        if normalize:
            x = self.layer_norm2(x)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, p=self.relu_dropout, training=self.training)
        x = self.fc2(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        x = residual + x
        normalize = self.maybe_layer_norm(after=True)
        if normalize:
            x = self.layer_norm2(x)
        return x

    def maybe_layer_norm(self, before: bool=False, after: bool=False):
        assert before ^ after, 'Incorrect arguments'
        return after ^ self.normalize_before

    def extra_repr(self):
        return 'dropout={}, relu_dropout={}, input_dropout={}, normalize_before={}'.format(self.dropout, self.relu_dropout, self.input_dropout, self.normalize_before)


class ConvEncoderConfig(ConfigBase):
    dropout: float = 0.1
    encoder_learned_pos: bool = False
    encoder_normalize_before: bool = False
    max_source_positions: int = 1024
    max_target_positions: int = 100
    no_token_positional_embeddings: bool = False
    positional_embedding_type: PostionalEmbedType = PostionalEmbedType.LEARNED
    combine_pos_embed: PostionalEmbedCombine = PostionalEmbedCombine.CONCAT
    encoder_embed_dim: int = 128
    embedding_dim: Optional[int] = 128


class NAREncoderUtility:

    def prepare_for_nar_inference(self, length_beam_size: int, encoder_out: Dict[str, Tensor]) ->Dict[str, Tensor]:
        """
        During masked NAR inference, multiple lengths are predicted for each
        item in the batch. Hence tiling has to be done in such a way that all new
        rows related to each item should be placed together. This is the assumption
        that we are following in the rest of the nar generation code.
        Eg:
        [row1, row2, row3] should be tiled as
        [row1, row1, row1, row2, row2, row2, row3, row3, row3]
        NOT [row1, row2, row3, row1, row2, row3, row1, row2, row3]
        """
        tiled_out = torch.jit.annotate(Dict[str, Tensor], {})
        bsz = encoder_out['encoder_out'].size(1)
        x = encoder_out['encoder_out']
        new_x = x.unsqueeze(2).repeat(1, 1, length_beam_size, 1).view(-1, bsz * length_beam_size, x.size(-1))
        tiled_out['encoder_out'] = new_x
        if 'encoder_mask' in encoder_out:
            new_encoder_mask = encoder_out['encoder_mask'].unsqueeze(1).repeat(1, length_beam_size, 1).view(bsz * length_beam_size, -1)
            tiled_out['encoder_mask'] = new_encoder_mask
        if 'src_tokens' in encoder_out:
            new_src_tokens = encoder_out['src_tokens'].unsqueeze(1).repeat(1, length_beam_size, 1).view(bsz * length_beam_size, -1)
            tiled_out['src_tokens'] = new_src_tokens
        if 'src_subword_begin_indices' in encoder_out:
            new_src_subword_begin_indices = encoder_out['src_subword_begin_indices'].unsqueeze(1).repeat(1, length_beam_size, 1).view(bsz * length_beam_size, -1)
            tiled_out['src_subword_begin_indices'] = new_src_subword_begin_indices
        if 'src_lengths' in encoder_out:
            new_src_lengths = encoder_out['src_lengths'].reshape(bsz, 1).unsqueeze(0).repeat(1, length_beam_size, 1).view(bsz * length_beam_size, -1)
            tiled_out['src_lengths'] = new_src_lengths
        if 'src_index_tokens' in encoder_out:
            new_src_tokens = encoder_out['src_index_tokens'].unsqueeze(1).repeat(1, length_beam_size, 1).view(bsz * length_beam_size, -1)
            tiled_out['src_index_tokens'] = new_src_tokens
        return tiled_out


class LightConvEncoder(PyTextSeq2SeqModule, NAREncoderUtility):


    class Config(ModuleConfig):
        encoder_config: ConvEncoderConfig = ConvEncoderConfig()
        layer_config: LightConvEncoderLayer.Config = LightConvEncoderLayer.Config()
        encoder_kernel_size_list: List[int] = [3, 7, 15]
        compression_dim: Optional[int] = 128

    @classmethod
    def from_config(cls, config, src_dict, src_embedding):
        kernel_size_list = config.encoder_kernel_size_list
        layers = []
        config.layer_config.encoder_embed_dim = config.encoder_config.encoder_embed_dim
        for size in kernel_size_list:
            assert config.encoder_config.encoder_embed_dim == config.layer_config.encoder_embed_dim
            layers.append(create_module(config.layer_config, kernel_size=size))
        return cls(src_dict, src_embedding, layers, config.encoder_config)

    def __init__(self, dictionary, embed_tokens, layers, encoder_config):
        super().__init__()
        self.dropout = encoder_config.dropout
        input_embed_dim = embed_tokens.embedding_dim
        self.padding_idx = dictionary.get_pad_index()
        self.max_source_positions = encoder_config.max_source_positions
        self.embed_scale = math.sqrt(input_embed_dim)
        self.no_token_positional_embeddings = encoder_config.no_token_positional_embeddings
        self.project_in_dim = Linear(input_embed_dim, encoder_config.encoder_embed_dim) if encoder_config.encoder_embed_dim != input_embed_dim else PlaceholderIdentity()
        self.embed_layer_norm = LayerNorm(encoder_config.encoder_embed_dim)
        self.combine_pos_embed = encoder_config.combine_pos_embed.value
        if encoder_config.combine_pos_embed == PostionalEmbedCombine.SUM:
            pos_embed_dim = encoder_config.encoder_embed_dim
        elif encoder_config.combine_pos_embed == PostionalEmbedCombine.CONCAT:
            pos_embed_dim = encoder_config.encoder_embed_dim - input_embed_dim
        else:
            raise NotImplementedError
        if not encoder_config.no_token_positional_embeddings:
            if encoder_config.positional_embedding_type == PostionalEmbedType.LEARNED:
                self.embed_positions = PositionalEmbedding(encoder_config.max_source_positions, pos_embed_dim, self.padding_idx)
            elif encoder_config.positional_embedding_type == PostionalEmbedType.SINUSOIDAL or encoder_config.positional_embedding_type == PostionalEmbedType.HYBRID:
                self.embed_positions = SinusoidalPositionalEmbedding(pos_embed_dim, self.padding_idx, init_size=encoder_config.max_source_positions, learned_embed=encoder_config.positional_embedding_type == PostionalEmbedType.HYBRID)
            else:
                raise NotImplementedError('Positional embedding type not supported')
        else:
            self.embed_positions = PlaceholderIdentity()
        self.layers = nn.ModuleList(layers)
        self.normalize = encoder_config.encoder_normalize_before
        if self.normalize:
            self.layer_norm = LayerNorm(encoder_config.encoder_embed_dim)
        else:
            self.layer_norm = PlaceholderIdentity()

    def forward(self, src_tokens: Tensor, src_embeddings: Tensor, src_lengths: Tensor) ->Dict[str, Tensor]:
        output_dict: Dict[str, Tensor] = {}
        x = self.embed_scale * src_embeddings
        if not self.no_token_positional_embeddings:
            x = self.pos_embed(x, src_tokens)
        else:
            x = self.project_in_dim(x)
        x = self.embed_layer_norm(x)
        x = F.dropout(x, p=self.dropout, training=self.training)
        output_dict['encoder_layer_0'] = x.clone()
        x = x.transpose(0, 1)
        encoder_padding_mask = src_tokens.eq(self.padding_idx)
        if not encoder_padding_mask.any():
            encoder_mask = None
        else:
            encoder_mask = encoder_padding_mask
        for idx, layer in enumerate(self.layers):
            x = layer(x, encoder_mask)
            output_dict['encoder_layer_' + str(idx + 1)] = x.transpose(0, 1).clone()
        if self.normalize:
            x = self.layer_norm(x)
        output_dict['src_tokens'] = src_tokens
        if src_lengths is not None:
            output_dict['src_lengths'] = src_lengths
        output_dict['encoder_out'] = x
        if encoder_mask is not None:
            output_dict['encoder_mask'] = encoder_mask
        return output_dict

    def reorder_encoder_out(self, encoder_out: Dict[str, Tensor], new_order: Tensor):
        encoder = encoder_out['encoder_out']
        encoder = encoder.index_select(1, new_order)
        output_dict = {'encoder_out': encoder}
        output_dict['src_tokens'] = encoder_out['src_tokens'].index_select(0, new_order)
        padding_mask = encoder_out.get('encoder_mask', None)
        if padding_mask is not None:
            padding_mask = padding_mask.index_select(0, new_order)
            output_dict['encoder_mask'] = padding_mask
        return output_dict

    def pos_embed(self, x, src_tokens):
        if self.combine_pos_embed == PostionalEmbedCombine.SUM.value:
            x = self.project_in_dim(x)
            return self._vanilla_transformer(x, src_tokens)
        elif self.combine_pos_embed == PostionalEmbedCombine.CONCAT.value:
            return self._concat_pos_embed(x, src_tokens)
        else:
            raise NotImplementedError('Method not supported')

    def _vanilla_transformer(self, x, src_tokens):
        x += self.embed_positions(src_tokens)
        return x

    def _concat_pos_embed(self, x, src_tokens):
        pos_embed = self.embed_positions(src_tokens)
        return torch.cat([x, pos_embed], dim=2)

    def max_positions(self):
        """Maximum input length supported by the encoder."""
        if self.no_token_positional_embeddings:
            return self.max_source_positions
        return min(self.max_source_positions, self.embed_positions.max_positions)

    def tile_encoder_out(self, tile_size: int, encoder_out: Dict[str, Tensor]) ->Dict[str, Tensor]:
        tiled_out = torch.jit.annotate(Dict[str, Tensor], {})
        x = encoder_out['encoder_out']
        new_x = x.repeat(1, tile_size, 1)
        tiled_out['encoder_out'] = new_x
        if 'encoder_mask' in encoder_out:
            new_encoder_mask = encoder_out['encoder_mask'].repeat(tile_size, 1)
            tiled_out['encoder_mask'] = new_encoder_mask
        if 'src_tokens' in encoder_out:
            tiled_out['src_tokens'] = encoder_out['src_tokens'].repeat(tile_size, 1)
        if 'src_lengths' in encoder_out:
            tiled_out['src_lengths'] = encoder_out['src_lengths'].repeat(tile_size, 1)
        return tiled_out

    def extra_repr(self):
        s = 'dropout={}, embed_scale={}, normalize={}'.format(self.dropout, self.embed_scale, self.normalize)
        return s


class CNNModel(PyTextSeq2SeqModule):


    class Config(ConfigBase):
        encoder: LightConvEncoder.Config = LightConvEncoder.Config()
        decoder: Union[LightConvDecoder.Config, LightConvDecoupledDecoder.Config] = LightConvDecoder.Config()

    @classmethod
    def from_config(cls, config: Config, src_dict, source_embedding, tgt_dict, target_embedding, dict_embedding=None):
        cls.validate_config(config)
        encoder = create_module(config.encoder, src_dict, source_embedding)
        decoder = create_module(config.decoder, tgt_dict, target_embedding)
        return cls(encoder, decoder, source_embedding)

    def __init__(self, encoder, decoder, source_embedding):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.source_embeddings = source_embedding

    def forward(self, src_tokens: Tensor, additional_features: List[List[Tensor]], src_lengths, prev_output_tokens, src_subword_begin_indices: Optional[Tensor]=None) ->Tuple[Tensor, Dict[str, Tensor]]:
        embeddings = self.source_embeddings([[src_tokens]] + additional_features)
        encoder_out = self.encoder(src_tokens, embeddings, src_lengths=src_lengths)
        decoder_out = self.decoder(prev_output_tokens, encoder_out=encoder_out)
        return decoder_out

    def get_normalized_probs(self, net_output, log_probs, sample=None):
        return self.decoder.get_normalized_probs(net_output, log_probs, sample)

    def max_decoder_positions(self):
        return max(self.encoder.max_positions(), self.decoder.max_positions())

    def get_embedding_module(self):
        return self.source_embeddings

    @classmethod
    def validate_config(cls, config):
        assert config.encoder.encoder_config.max_target_positions <= config.decoder.decoder_config.max_target_positions


class DecoupledCNNModel(CNNModel):


    class Config(CNNModel.Config):
        encoder: LightConvEncoder.Config = LightConvEncoder.Config()
        decoder: LightConvDecoupledDecoder.Config = LightConvDecoupledDecoder.Config()


class BeamRankingAlgorithm(Enum):
    LENGTH_CONDITIONED_RANK: str = 'LENGTH_CONDITIONED_RANK'
    LENGTH_CONDITIONED_RANK_MUL: str = 'LENGTH_CONDITIONED_RANK_MUL'
    AVERAGE_TOKEN_LPROB: str = 'AVERAGE_TOKEN_LPROB'
    TOKEN_LPROB: str = 'TOKEN_LPROB'
    LENGTH_CONDITIONED_AVERAGE_TOKEN_LPROB: str = 'LENGTH_CONDITIONED_AVERAGE_TOKEN_LPROB'
    LENGTH_CONDITIONED_AVERAGE_TOKEN_LPROB_MULTIPLIED: str = 'LENGTH_CONDITIONED_AVERAGE_TOKEN_LPROB_MULTIPLIED'
    LEN_ONLY: str = 'LEN_ONLY'


class EmbedQuantizeType(Enum):
    BIT_8 = '8bit'
    BIT_4 = '4bit'
    NONE = 'None'


def avg_token_lprob(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    avg_log_prob = token_lprob / target_lengths
    return avg_log_prob


def length(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    return length_lprob


def length_conditioned_avg_lprob_rank(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    avg_token_lprob_tensor = avg_token_lprob(token_lprob, length_lprob, target_lengths)
    return avg_token_lprob_tensor + length_lprob


def length_conditioned_avg_lprob_rank_mul(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    avg_token_lprob_tensor = avg_token_lprob(token_lprob, length_lprob, target_lengths)
    return avg_token_lprob_tensor + target_lengths * length_lprob


def length_conditioned_rank(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    return token_lprob + length_lprob


def length_conditioned_rank_mul(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    return token_lprob + target_lengths * length_lprob


def token_prob(token_lprob: torch.Tensor, length_lprob: torch.Tensor, target_lengths: torch.Tensor) ->torch.Tensor:
    return token_lprob


def get_beam_ranking_function(ranking_algorithm: BeamRankingAlgorithm):
    if ranking_algorithm == BeamRankingAlgorithm.LENGTH_CONDITIONED_RANK:
        return length_conditioned_rank
    elif ranking_algorithm == BeamRankingAlgorithm.LENGTH_CONDITIONED_RANK_MUL:
        return length_conditioned_rank_mul
    elif ranking_algorithm == BeamRankingAlgorithm.AVERAGE_TOKEN_LPROB:
        return avg_token_lprob
    elif ranking_algorithm == BeamRankingAlgorithm.LENGTH_CONDITIONED_AVERAGE_TOKEN_LPROB:
        return length_conditioned_avg_lprob_rank
    elif ranking_algorithm == BeamRankingAlgorithm.LENGTH_CONDITIONED_AVERAGE_TOKEN_LPROB_MULTIPLIED:
        return length_conditioned_avg_lprob_rank_mul
    elif ranking_algorithm == BeamRankingAlgorithm.TOKEN_LPROB:
        return token_prob
    elif ranking_algorithm == BeamRankingAlgorithm.LEN_ONLY:
        return length
    else:
        raise Exception(f'Unknown ranking algorithm {ranking_algorithm}')


def prepare_masked_target_for_lengths(beam: Tensor, mask_idx: int, pad_idx: int, length_beam_size: int=1) ->Tuple[Tensor, Tensor]:
    max_len = beam.max().item()
    bsz = beam.size(0)
    length_mask = torch.triu(torch.ones(max_len, max_len, device=beam.device, dtype=beam.dtype), diagonal=1)
    beam_indices = beam - 1
    length_mask = length_mask[beam_indices.reshape(-1)].reshape(bsz, length_beam_size, max_len)
    tgt_tokens = torch.zeros(bsz, length_beam_size, max_len, device=beam.device, dtype=beam.dtype).fill_(mask_idx)
    tgt_tokens = (1 - length_mask) * tgt_tokens + length_mask * pad_idx
    tgt_tokens = tgt_tokens.view(bsz * length_beam_size, max_len)
    return tgt_tokens, length_mask


class MaskedSequenceGenerator(Module):


    class Config(ConfigBase):
        beam_size: int = 3
        quantize: bool = True
        embed_quantize: EmbedQuantizeType = EmbedQuantizeType.NONE
        use_gold_length: bool = False
        force_eval_predictions: bool = True
        generate_predictions_every: int = 1
        beam_ranking_algorithm: BeamRankingAlgorithm = BeamRankingAlgorithm.LENGTH_CONDITIONED_RANK
        clip_target_length: bool = False
        targetlen_cap: int = 30
        targetlen_a: float = 0
        targetlen_b: float = 2
        targetlen_c: float = 2

    @classmethod
    def from_config(cls, config, model, length_prediction, trg_vocab, quantize=False, embed_quantize=False):
        return cls(config, model, length_prediction, trg_vocab, config.beam_size, config.use_gold_length, config.beam_ranking_algorithm, quantize, config.embed_quantize)

    def __init__(self, config, model, length_prediction_model, trg_vocab, beam_size, use_gold_length, beam_ranking_algorithm, quantize, embed_quantize):
        super().__init__()
        length_prediction_model = length_prediction_model.create_eval_module()
        self.model = model
        self.length_prediction_model = length_prediction_model
        self.quantize_generator(quantize=quantize, embed_quantize=embed_quantize)
        self.generator_setup(trg_vocab=trg_vocab, beam_size=beam_size, use_gold_length=use_gold_length, beam_ranking_algorithm=beam_ranking_algorithm, config=config)

    def generator_setup(self, trg_vocab, beam_size, use_gold_length, beam_ranking_algorithm, config):
        self.trg_vocab = ScriptVocabulary(list(trg_vocab), pad_idx=trg_vocab.get_pad_index(), bos_idx=trg_vocab.get_bos_index(-1), eos_idx=trg_vocab.get_eos_index(-1), mask_idx=trg_vocab.get_mask_index())
        self.length_beam_size = beam_size
        self.use_gold_length = use_gold_length
        self.beam_ranking_algorithm = get_beam_ranking_function(ranking_algorithm=beam_ranking_algorithm)
        self.clip_target_length = config.clip_target_length
        self.targetlen_cap = config.targetlen_cap
        self.targetlen_a = config.targetlen_a
        self.targetlen_b = config.targetlen_b
        self.targetlen_c = config.targetlen_c

    def quantize_generator(self, quantize=False, embed_quantize=EmbedQuantizeType.NONE):
        if quantize:
            qconfig_dict = {torch.nn.Linear: tq.per_channel_dynamic_qconfig}
            if embed_quantize != EmbedQuantizeType.NONE:
                if embed_quantize == EmbedQuantizeType.BIT_8:
                    qconfig_dict[torch.nn.Embedding] = float_qparams_weight_only_qconfig
                elif embed_quantize == EmbedQuantizeType.BIT_4:
                    raise NotImplementedError('4bit embedding quantization not yet supported')
                else:
                    raise NotImplementedError('Embedding Quantization should be either 8bit or 4bit')
            self.model = tq.quantize_dynamic(self.model, qconfig_dict, dtype=torch.qint8, inplace=False)
            self.length_prediction_model = tq.quantize_dynamic(self.length_prediction_model, {torch.nn.Linear: tq.per_channel_dynamic_qconfig}, dtype=torch.qint8, inplace=False)

    def get_encoder_out(self, src_tokens: Tensor, dict_feats: Optional[Tuple[Tensor, Tensor, Tensor]], contextual_embed: Optional[Tensor], char_feats: Optional[Tensor], src_subword_begin_indices: Optional[Tensor], src_lengths: Tensor, src_index_tokens: Optional[Tensor]=None) ->Dict[str, Tensor]:
        embedding_input = [[src_tokens]]
        if dict_feats is not None:
            embedding_input.append(list(dict_feats))
        if contextual_embed is not None:
            embedding_input.append([contextual_embed])
        if char_feats is not None:
            embedding_input.append([char_feats])
        embeddings = self.model.source_embeddings(embedding_input)
        encoder_out = self.model.encoder(src_tokens, embeddings, src_lengths=src_lengths)
        if src_index_tokens is not None:
            encoder_out['src_index_tokens'] = src_index_tokens
        return encoder_out

    def forward(self, src_tokens: Tensor, dict_feats: Optional[Tuple[Tensor, Tensor, Tensor]], contextual_embed: Optional[Tensor], char_feats: Optional[Tensor], src_lengths: Tensor, src_subword_begin_indices: Optional[Tensor]=None, target_lengths: Optional[Tensor]=None, beam_size: Optional[int]=None, src_index_tokens: Optional[Tensor]=None) ->Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
        encoder_out = self.get_encoder_out(src_tokens=src_tokens, dict_feats=dict_feats, contextual_embed=contextual_embed, char_feats=char_feats, src_subword_begin_indices=src_subword_begin_indices, src_lengths=src_lengths, src_index_tokens=src_index_tokens)
        encoder_mask: Optional[Tensor] = None
        if 'encoder_mask' in encoder_out:
            encoder_mask = encoder_out['encoder_mask']
        length_prediction_result: Dict[str, Tensor] = self.get_length_prediction(encoder_out=encoder_out, encoder_mask=encoder_mask, src_lengths=src_lengths, target_lengths=target_lengths, beam_size=beam_size)
        tgt_tokens = length_prediction_result['target_tokens']
        length_prob = length_prediction_result['length_probabilities']
        length_mask = length_prediction_result['length_mask']
        beam = length_prediction_result['beam']
        bsz = src_tokens.size(0)
        max_len = tgt_tokens.size(1)
        tiled_encoder_out = self.model.encoder.prepare_for_nar_inference(self.length_beam_size, encoder_out)
        tgt_tokens, token_probs, token_logits = self.generate(tiled_encoder_out=tiled_encoder_out, tgt_tokens=tgt_tokens)
        token_probs = token_probs.view(bsz, self.length_beam_size, max_len).log()
        lprobs = token_probs.sum(-1)
        hypotheses = tgt_tokens.view(bsz, self.length_beam_size, max_len)
        lprobs = lprobs.view(bsz, self.length_beam_size)
        tgt_lengths = (1 - length_mask).sum(-1)
        hyp_score = self.beam_ranking_algorithm(token_lprob=lprobs, length_lprob=length_prob, target_lengths=tgt_lengths)
        sorted_scores, indices = torch.sort(hyp_score, dim=-1, descending=True)
        all_indices = torch.arange(bsz).unsqueeze(-1)
        hypotheses = hypotheses[all_indices, indices]
        return hypotheses, beam, sorted_scores.exp(), token_probs, token_logits

    def generate(self, tiled_encoder_out: Dict[str, Tensor], tgt_tokens: torch.Tensor):
        pad_mask = tgt_tokens.eq(self.trg_vocab.pad_idx)
        tgt_tokens, token_probs, token_logits = self.generate_non_autoregressive(tiled_encoder_out, tgt_tokens)
        tgt_tokens[pad_mask] = torch.tensor(self.trg_vocab.pad_idx, device=tgt_tokens.device).long()
        token_probs[pad_mask] = torch.tensor(1.0, device=token_probs.device, dtype=token_probs.dtype)
        return tgt_tokens, token_probs, token_logits

    def get_length_prediction(self, encoder_out: Dict[str, Tensor], encoder_mask: Optional[Tensor], src_lengths: Tensor, target_lengths: Optional[Tensor]=None, beam_size: Optional[int]=None) ->Dict[str, Tensor]:
        predicted_tgt_length, _ = self.length_prediction_model(encoder_out['encoder_out'], encoder_mask)
        if beam_size is not None and beam_size != self.length_beam_size:
            self.length_beam_size = beam_size
        if self.clip_target_length:
            beam_vals, beam = predicted_tgt_length.topk(self.length_beam_size, dim=1)
            len_clips = self.get_clip_length(src_lengths)
            len_clips = len_clips.reshape(-1, 1).repeat(1, self.length_beam_size)
            acceptable_lens_mask = torch.le(beam, len_clips)
            beam = beam * acceptable_lens_mask + torch.logical_not(acceptable_lens_mask) * torch.ones_like(beam, dtype=beam.dtype, device=beam.device)
            beam_vals = beam_vals * acceptable_lens_mask + torch.logical_not(acceptable_lens_mask) * torch.full(beam_vals.size(), float('-inf'), dtype=beam_vals.dtype, device=beam_vals.device)
        else:
            beam_vals, beam = predicted_tgt_length.topk(self.length_beam_size, dim=1)
        beam[beam == 0] += 1
        length_prob = torch.gather(predicted_tgt_length, 1, beam)
        if self.use_gold_length:
            assert target_lengths is not None
            beam = target_lengths.reshape(-1, 1)
            self.length_beam_size = 1
            length_prob = torch.ones(beam.size(), device=target_lengths.device)
        tgt_tokens, length_mask = prepare_masked_target_for_lengths(beam, self.trg_vocab.mask_idx, self.trg_vocab.pad_idx, self.length_beam_size)
        return {'target_tokens': tgt_tokens, 'length_mask': length_mask, 'length_probabilities': length_prob, 'beam': beam}

    def get_clip_length(self, src_lengths: Tensor):
        predicted = torch.tensor(self.targetlen_a, dtype=src_lengths.dtype, device=src_lengths.device) * src_lengths * src_lengths + torch.tensor(self.targetlen_b, dtype=src_lengths.dtype, device=src_lengths.device) * src_lengths + torch.tensor(self.targetlen_c, dtype=src_lengths.dtype, device=src_lengths.device)
        capped = torch.min(predicted, torch.tensor(self.targetlen_cap, dtype=src_lengths.dtype, device=src_lengths.device))
        return capped

    @torch.jit.export
    def generate_hypo(self, tensors: Dict[str, Tensor]) ->Tuple[Tuple[Tensor, Tensor], Tensor]:
        """
        Generates hypotheses using beam search, also returning their scores

        Inputs:
            - tensors: dictionary containing needed tensors for generation

        Outputs:
            - (hypos, lens): tuple of Tensors
                - hypos: Tensor of shape (batch_size, beam_size, MAX) containing the generated tokens. MAX refers to the longest sequence in batch.
                - lens: Tensor of shape (batch_size, beam_size) containing generated sequence lengths
            - _hypo_scores: Tensor of shape (batch_size, beam_size) containing the scores for each generated sequence
        """
        actual_src_tokens = tensors['src_tokens']
        dict_feats: Optional[Tuple[Tensor, Tensor, Tensor]] = None
        contextual_embed: Optional[Tensor] = None
        char_feats: Optional[Tensor] = None
        if 'dict_tokens' in tensors:
            dict_feats = tensors['dict_tokens'], tensors['dict_weights'], tensors['dict_lengths']
        if 'contextual_embed' in tensors:
            contextual_embed = tensors['contextual_embed']
        if 'char_feats' in tensors:
            char_feats = tensors['char_feats']
        hypos, lens, hypo_scores, _, _ = self.forward(actual_src_tokens, dict_feats, contextual_embed, char_feats, tensors['src_lengths'], src_subword_begin_indices=tensors.get('src_subword_begin_indices'), target_lengths=tensors['target_lengths'], beam_size=self.length_beam_size, src_index_tokens=tensors.get('src_index_tokens'))
        return (hypos, lens), hypo_scores

    def generate_non_autoregressive(self, encoder_out: Dict[str, Tensor], tgt_tokens):
        decoder_out_tuple = self.model.decoder(tgt_tokens, encoder_out)
        tgt_tokens, token_probs, token_logits = self.model.decoder.get_probs(decoder_out_tuple)
        return tgt_tokens, token_probs, token_logits


def mean(rep: Tensor, padding_mask: Optional[Tensor]):
    rep_sum = rep.sum(dim=1)
    if padding_mask is not None:
        lengths = (~padding_mask).sum(dim=1).reshape(-1, 1)
    else:
        bsz, max_token_len, _embed_dim = rep.size()
        lengths = torch.full((bsz, 1), max_token_len, dtype=torch.long, device=rep.device)
    return rep_sum / lengths


def pool(pooling_type: str, words: Tensor, encoder_padding_mask: Optional[Tensor]):
    if pooling_type == 'mean':
        return mean(words, encoder_padding_mask)
    elif pooling_type == 'max':
        return words.max(dim=1)[0]
    elif pooling_type == 'none':
        return words
    else:
        raise NotImplementedError


class ConvLengthPredictionModule(Module):


    class Config(ModuleConfig):
        conv_dim: int = 128
        max_target_positions: int = 128
        length_dropout: float = 0.2
        kernel_sizes: List[int] = [3]
        glu: bool = True
        activation: Activation = Activation.GLU
        convolution_type: LightweightConv.Config = LightweightConv.Config()
        pooling_type: str = 'mean'

    def __init__(self, embed_dim: int, conv_dim: int, max_target_positions: int, length_dropout: float, glu: bool, activation, pooling_type, conv_layers):
        super().__init__()
        self.length_dropout = length_dropout
        self.conv_layers = nn.ModuleList(conv_layers)
        self.glu = glu
        if glu:
            self.linear1 = nn.Linear(embed_dim, 2 * conv_dim)
        else:
            self.linear1 = nn.Linear(embed_dim, conv_dim)
        self.linear2 = nn.Linear(conv_dim, embed_dim)
        self.activation = get_activation(activation, dim=2)
        self.pooling_type = pooling_type
        self.lengths_pred = nn.Linear(embed_dim, max_target_positions)

    def forward(self, x: Tensor, encoder_padding_mask: Optional[Tensor]=None):
        for conv in self.conv_layers:
            residual = x
            x = self.linear1(x)
            x = self.activation(x)
            if encoder_padding_mask is not None:
                x = x.masked_fill(encoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)
            x = conv(x)
            x = self.linear2(x)
            x = F.dropout(x, p=self.length_dropout, training=self.training)
            x = residual + x
        if encoder_padding_mask is not None:
            x = x.masked_fill(encoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)
        x = x.transpose(0, 1)
        x = F.relu(x)
        lengths_enc = pool(self.pooling_type, x, encoder_padding_mask)
        predicted_lengths_logits = self.lengths_pred(lengths_enc)
        predicted_lengths = F.log_softmax(predicted_lengths_logits, dim=-1)
        return predicted_lengths, predicted_lengths_logits

    def create_eval_module(self):
        return self

    @classmethod
    def from_config(cls, config: Config, embed_dim: int):
        conv_layers = []
        for kernel_size in config.kernel_sizes:
            conv_layers.append(create_module(config.convolution_type, input_size=config.conv_dim, kernel_size=kernel_size, convolution_type='non-causal'))
        return cls(embed_dim, config.conv_dim, config.max_target_positions, config.length_dropout, config.glu, config.activation, config.pooling_type, conv_layers)


class MaskedLengthPredictionModule(Module):


    class Config(ModuleConfig):
        length_hidden_dim: int = 128
        max_target_positions: int = 128
        length_dropout: float = 0.2

    def __init__(self, embed_dim: int, length_hidden_dim: int, max_target_positions: int, length_dropout: float):
        super().__init__()
        self.lengths_linear = nn.Linear(embed_dim, length_hidden_dim)
        self.lengths_pred = nn.Linear(length_hidden_dim, max_target_positions)
        self.length_dropout = length_dropout

    def forward(self, x: torch.Tensor, encoder_padding_mask: Optional[torch.Tensor]=None) ->Tuple[torch.Tensor, torch.Tensor]:
        if encoder_padding_mask is not None:
            x = x.masked_fill(encoder_padding_mask.transpose(0, 1).unsqueeze(2), 0)
        avg_enc = mean(x.transpose(0, 1), encoder_padding_mask)
        lengths_enc = self.lengths_linear(avg_enc)
        lengths_enc = F.relu(lengths_enc)
        lengths_enc = F.dropout(lengths_enc, p=self.length_dropout, training=self.training)
        predicted_lengths_logits = self.lengths_pred(lengths_enc)
        predicted_lengths = F.log_softmax(predicted_lengths_logits.float(), dim=-1)
        return predicted_lengths, predicted_lengths_logits

    def create_eval_module(self):
        return self

    @classmethod
    def from_config(cls, config: Config, embed_dim: int):
        return cls(embed_dim, config.length_hidden_dim, config.max_target_positions, config.length_dropout)


class Regularizer(Loss):
    """Generic regularization function to be added to a surrogate loss (e.g., cross-entropy)."""

    def __init__(self, config, ignore_index=1):
        self.ignore_index = ignore_index

    def __call__(self, logits, targets, reduce=True):
        raise NotImplementedError


class AdaptiveRegularizer(Regularizer):
    """
    Adaptive variant of `UniformRegularizer` which learns the mix-in noise distribution.

    Learning Better Structured Representations using Low-Rank Adaptive Label Smoothing
    (Ghoshal+ 2021; https://openreview.net/pdf?id=5NsEIflpbSv)
    """


    class Config(ConfigBase):
        eta: float = 0.1
        label_embedding_dim: int = 20
        label_embedding_dropout: float = 0.4

    def __init__(self, config, ignore_index=1):
        super().__init__(config, ignore_index)
        if config.eta < 0:
            raise ValueError('eta must be >= 0')
        if config.label_embedding_dropout < 0 or config.label_embedding_dropout >= 1:
            raise ValueError('label_embedding_dropout must be [0, 1)')
        self.eta = config.eta
        self.label_embedding_dim = config.label_embedding_dim
        self.label_embedding_dropout = config.label_embedding_dropout
        self.label_embedding = None

    def compute_adaptive_loss(self, logits, targets, label_embedding):
        """
        Using Equation 3 and 4, computes several terms of the adaptive penalty.
        Specifically, we implement adaptive smoothing (`smooth_term`) and
        an entropy constraint (`eta_term`).
        """
        if targets.dim() == logits.dim() - 1:
            targets = targets.unsqueeze(-1)
        U = torch.mm(torch.index_select(label_embedding, 0, targets.squeeze(-1)), label_embedding.T)
        V = F.softmax(U.float(), dim=-1)
        smooth_term = -torch.bmm(V.unsqueeze(1), logits.unsqueeze(2)).squeeze(2)
        eta_term = -self.eta * (-torch.bmm(U.unsqueeze(1), V.unsqueeze(2)).mean() + torch.logsumexp(U, axis=-1).mean())
        loss = smooth_term + eta_term
        return loss

    def __call__(self, logits, targets, reduce=True):
        mask = targets.ne(self.ignore_index)
        if self.label_embedding is None:
            num_labels = logits.shape[1]
            self.label_embedding = nn.Parameter(torch.ones(num_labels, self.label_embedding_dim), requires_grad=True)
        loss = self.compute_adaptive_loss(logits, targets, F.dropout(self.label_embedding, self.label_embedding_dropout))
        if reduce:
            return loss[mask].mean() if mask.any() else torch.tensor(0.0, device=logits.device)
        return loss


class EntropyRegularizer(Regularizer):
    """
    Entropy of the predicted distribution. Defined as:
        H[P(Y|X)] = - sum_i P(Y_i|X) * log P(Y_i|X)
    """

    def __call__(self, logits, targets, reduce=True):
        mask = targets.ne(self.ignore_index)
        loss = -torch.sum(logits * logits.exp(), dim=1)
        if reduce:
            return loss[mask].mean() if mask.any() else torch.tensor(0.0, device=logits.device)
        return loss


class UniformRegularizer(Regularizer):
    """
    Negative KL between the uniform and predicted distribution.
        Defined as:
            - KL(U || P(Y|X)) = - sum_i U_i * log (P(Y_i | X) / U_i)
                              = - sum_i U_i * log P(Y_i|X) + H[U]
                              = - (1/n) * sum_i log P(Y_i | X) + H[U]

    H[U] does not depend on X, thus it is omitted during optimization.
    """

    def __call__(self, logits, targets, reduce=True):
        mask = targets.ne(self.ignore_index)
        loss = -logits.mean(dim=1)
        if reduce:
            return loss[mask].mean() if mask.any() else torch.tensor(0.0, device=logits.device)
        return loss


class LabelSmoothingLoss(Loss):
    """Label loss with an optional regularizer for smoothing."""


    class Config(ConfigBase):
        beta: float = 0.1
        label_loss: Union[NLLLoss.Config, StructuredMarginLoss.Config, HingeLoss.Config] = NLLLoss.Config()
        smoothing_loss: Union[UniformRegularizer.Config, EntropyRegularizer.Config, AdaptiveRegularizer.Config] = UniformRegularizer.Config()

    def __init__(self, config, ignore_index=1):
        self.beta = config.beta
        self.label_loss_fn = create_loss(config.label_loss, ignore_index=ignore_index)
        self.smoothing_loss_fn = create_loss(config.smoothing_loss, ignore_index=ignore_index)
        self.ignore_index = ignore_index
        self.label_loss = 0
        self.smoothing_loss = 0

    def __call__(self, logits, targets, reduce=True):
        label_loss = self.label_loss_fn(logits, targets, reduce)
        if isinstance(self.label_loss_fn, StructuredLoss):
            logits = logits.reshape(-1, logits.size(-1))
            targets = targets.view(-1)
        smoothing_loss = self.smoothing_loss_fn(logits, targets, reduce)
        self.label_loss = label_loss
        self.smoothing_loss = smoothing_loss
        loss = label_loss
        if self.beta > 0:
            loss += self.beta * smoothing_loss
        return loss


def maybe_log_normalize(logits, logits_type, dim=-1):
    """Optionally log normalizes logits on the given dimension."""
    if logits_type == SourceType.LOGITS:
        return F.log_softmax(logits, dim)
    elif logits_type == SourceType.PROBS:
        return logits.log()
    elif logits_type == SourceType.LOG_PROBS:
        return logits
    else:
        raise NotImplementedError


class NARSequenceLoss(Loss):
    """Joint loss over labels and length of sequences for non-autoregressive modeling."""


    class Config(ConfigBase):
        beta: float = 0.1
        assert_valid_targets: bool = True
        label_type: SourceType = SourceType.LOG_PROBS
        length_type: SourceType = SourceType.LOG_PROBS
        label_loss: LabelSmoothingLoss.Config = LabelSmoothingLoss.Config()
        length_loss: LabelSmoothingLoss.Config = LabelSmoothingLoss.Config()
        disable_label_loss: bool = False

    def __init__(self, config, ignore_index=1):
        self.beta = config.beta
        self.assert_valid_targets = config.assert_valid_targets
        self.label_type = config.label_type
        self.length_type = config.length_type
        if isinstance(config.length_loss.label_loss, StructuredLoss):
            raise ValueError("StructuredLoss can't be used as a length loss")
        self.label_loss_fn = create_loss(config.label_loss, ignore_index=ignore_index)
        self.length_loss_fn = create_loss(config.length_loss, ignore_index=ignore_index)
        self.disable_label_loss = config.disable_label_loss

    def __call__(self, label_logits, label_targets, length_logits, length_targets, reduce=True):
        """
        label_logits: (B x T) x V_1
        label_targets: (B x T)
        length_logits: B x V_2
        length_targets: B
        """
        label_logits = maybe_log_normalize(logits=label_logits, logits_type=self.label_type, dim=-1)
        length_logits = maybe_log_normalize(logits=length_logits, logits_type=self.length_type, dim=-1)
        max_supported_dim = length_logits.size(1)
        length_targets = length_targets.unsqueeze(-1)
        if self.assert_valid_targets:
            if torch.any(length_targets >= max_supported_dim):
                total_violations = str(length_targets[length_targets >= max_supported_dim].flatten().tolist())
                raise RuntimeError(f'max_supported_dim: {max_supported_dim}, total violations: {total_violations}')
        else:
            length_targets[length_targets >= max_supported_dim] = max_supported_dim - 1
        label_loss = self.label_loss_fn(label_logits, label_targets, reduce)
        length_loss = self.length_loss_fn(length_logits, length_targets.squeeze(-1), reduce)
        if self.disable_label_loss:
            label_loss = label_loss * 0
        loss = label_loss + self.beta * length_loss
        return loss, {'label_loss': label_loss, 'length_loss': length_loss, 'label_label_loss': self.label_loss_fn.label_loss, 'label_smoothing_loss': self.label_loss_fn.smoothing_loss, 'length_label_loss': self.length_loss_fn.label_loss, 'length_smoothing_loss': self.length_loss_fn.smoothing_loss}


class SamplewiseLabelSmoothingLoss(LabelSmoothingLoss):
    """Label smoothing loss with sample-wise logging."""

    def __init__(self, config, ignore_index=-1):
        super().__init__(config, ignore_index)
        self.samplewise_label_loss = 0
        self.samplewise_smoothing_loss = 0

    def _reduce_mean(self, logits, targets, batch_size, label_loss, smoothing_loss, reduce=True):
        """
        Class-specific reduction function to extract sample-wise losses. Currently,
        passing in reduce="mean" averages over all samples without providing access
        to sample-wise losses.
        """
        orig_label_loss = label_loss.clone()
        orig_smoothing_loss = smoothing_loss.clone()
        mask = targets.ne(self.ignore_index)
        if mask.any():
            label_loss[~mask] = 0
            smoothing_loss[~mask] = 0
            lengths = torch.sum(mask.reshape(batch_size, -1), dim=1)
            samplewise_label_loss = torch.sum(label_loss.reshape(batch_size, -1), dim=-1) / lengths
            samplewise_smoothing_loss = torch.sum(smoothing_loss.reshape(batch_size, -1), dim=-1) / lengths
            samplewise_label_loss[torch.isnan(samplewise_label_loss)] = 0
            samplewise_smoothing_loss[torch.isnan(samplewise_smoothing_loss)] = 0
            label_loss = label_loss[mask]
            smoothing_loss = smoothing_loss[mask]
        else:
            samplewise_label_loss = torch.zeros(batch_size, device=logits.device)
            samplewise_smoothing_loss = torch.zeros(batch_size, device=logits.device)
            label_loss = torch.zeros(mask.shape, device=logits.shape)
            smoothing_loss = torch.zeros(mask.shape, device=logits.shape)
        label_loss = torch.mean(label_loss) if reduce else orig_label_loss
        smoothing_loss = torch.mean(smoothing_loss) if reduce else orig_smoothing_loss
        return samplewise_label_loss, samplewise_smoothing_loss, label_loss, smoothing_loss

    def __call__(self, logits, targets, reduce=True, batch_size=None):
        label_loss = self.label_loss_fn(logits, targets, reduce=False)
        smoothing_loss = self.smoothing_loss_fn(logits, targets, reduce=False)
        if batch_size is None:
            batch_size = logits.shape[0]
        samplewise_label_loss, samplewise_smoothing_loss, label_loss, smoothing_loss = self._reduce_mean(logits=logits, targets=targets, batch_size=batch_size, label_loss=label_loss, smoothing_loss=smoothing_loss, reduce=reduce)
        self.samplewise_label_loss = samplewise_label_loss
        self.samplewise_smoothing_loss = samplewise_smoothing_loss
        self.samplewise_total_loss = samplewise_label_loss + self.beta * samplewise_smoothing_loss if samplewise_label_loss is not None and samplewise_smoothing_loss is not None else None
        self.label_loss = label_loss
        self.smoothing_loss = smoothing_loss
        loss = label_loss + self.beta * smoothing_loss
        return loss


class NARSamplewiseSequenceLoss(NARSequenceLoss):
    """Non-autoregressive sequence loss with sample-wise logging."""


    class Config(NARSequenceLoss.Config):
        label_loss: SamplewiseLabelSmoothingLoss.Config = SamplewiseLabelSmoothingLoss.Config()
        length_loss: SamplewiseLabelSmoothingLoss.Config = SamplewiseLabelSmoothingLoss.Config()

    def __call__(self, label_logits, label_targets, length_logits, length_targets, reduce=True):
        """
        label_logits: (B x T) x V_1
        label_targets: (B x T)
        length_logits: B x V_2
        length_targets: B
        """
        label_logits = maybe_log_normalize(logits=label_logits, logits_type=self.label_type, dim=-1)
        length_logits = maybe_log_normalize(logits=length_logits, logits_type=self.length_type, dim=-1)
        max_length = int(torch.max(length_targets))
        batch_size = label_logits.shape[0] // max_length
        max_supported_dim = length_logits.size(1)
        length_targets = length_targets.unsqueeze(-1)
        if self.assert_valid_targets:
            if torch.any(length_targets >= max_supported_dim):
                total_violations = str(length_targets[length_targets >= max_supported_dim].flatten().tolist())
                raise RuntimeError(f'max_supported_dim: {max_supported_dim}, total violations: {total_violations}')
        else:
            length_targets[length_targets >= max_supported_dim] = max_supported_dim - 1
        label_loss = self.label_loss_fn(label_logits, label_targets, reduce, batch_size)
        length_loss = self.length_loss_fn(length_logits, length_targets.squeeze(-1), reduce)
        loss = label_loss + self.beta * length_loss
        samplewise_losses = {'samplewise_label_loss': self.label_loss_fn.samplewise_total_loss, 'samplewise_length_loss': self.length_loss_fn.samplewise_total_loss, 'samplewise_label_label_loss': self.label_loss_fn.samplewise_label_loss, 'samplewise_label_smoothing_loss': self.label_loss_fn.samplewise_smoothing_loss, 'samplewise_length_label_loss': self.length_loss_fn.samplewise_label_loss, 'samplewise_length_smoothing_loss': self.length_loss_fn.samplewise_smoothing_loss}
        return loss, {'label_loss': label_loss, 'length_loss': length_loss, 'label_label_loss': self.label_loss_fn.label_loss, 'label_smoothing_loss': self.label_loss_fn.smoothing_loss, 'length_label_loss': self.length_loss_fn.label_loss, 'length_smoothing_loss': self.length_loss_fn.smoothing_loss, **samplewise_losses}


class LearnedPositionalEmbedding(nn.Embedding):
    """
    This module learns positional embeddings up to a fixed maximum size.
    Padding ids are ignored by either offsetting based on padding_idx
    or by setting padding_idx to None and ensuring that the appropriate
    position ids are passed to the forward function.
    """

    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: int):
        super().__init__(num_embeddings, embedding_dim, padding_idx)
        if self.padding_idx is not None:
            self.max_positions = self.num_embeddings - self.padding_idx - 1
        else:
            self.max_positions = self.num_embeddings

    def forward(self, input: Tensor, incremental_state: Optional[Dict[str, Dict[str, Optional[Tensor]]]]=None, positions: Optional[Tensor]=None):
        """Input is expected to be of size [bsz x seqlen]."""
        assert positions is None or self.padding_idx is None, 'If positions is pre-computed then padding_idx should not be set.'
        if positions is None:
            if incremental_state is not None:
                positions = torch.zeros((1, 1), device=input.device, dtype=input.dtype).fill_(int(self.padding_idx + input.size(1)))
            else:
                positions = make_positions(input, self.padding_idx)
        return F.embedding(positions, self.weight, self.padding_idx, self.max_norm, self.norm_type, self.scale_grad_by_freq, self.sparse)


class RNNDecoderBase(PyTextIncrementalDecoderComponent):
    """
    RNN decoder with multihead attention. Attention is calculated using encoder
    output and output of decoder's first RNN layerself. Attention is applied
    after first RNN layer and concatenated to input of subsequent layers.
    """


    class Config(ConfigBase):
        encoder_hidden_dim: int = 512
        embed_dim: int = 512
        hidden_dim: int = 512
        out_embed_dim: int = 512
        cell_type: str = 'lstm'
        num_layers: int = 1
        dropout_in: float = 0.1
        dropout_out: float = 0.1
        attention_type: str = 'dot'
        attention_heads: int = 8
        first_layer_attention: bool = False
        averaging_encoder: bool = False

    @classmethod
    def from_config(cls, config, out_vocab_size, target_embedding):
        return cls(out_vocab_size, target_embedding, **config._asdict())

    def __init__(self, embed_tokens, encoder_hidden_dim, embed_dim, hidden_dim, out_embed_dim, cell_type, num_layers, dropout_in, dropout_out, attention_type, attention_heads, first_layer_attention, averaging_encoder):
        encoder_hidden_dim = max(1, encoder_hidden_dim)
        self.encoder_hidden_dim = encoder_hidden_dim
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.out_embed_dim = out_embed_dim
        self.dropout_in = dropout_in
        self.dropout_out = dropout_out
        self.attention_type = attention_type
        self.attention_heads = attention_heads
        self.first_layer_attention = first_layer_attention
        self.embed_tokens = embed_tokens
        self.hidden_dim = hidden_dim
        self.averaging_encoder = averaging_encoder
        if cell_type == 'lstm':
            cell_class = torch.nn.LSTMCell
        else:
            raise RuntimeError('Cell type not supported')
        self.change_hidden_dim = hidden_dim != encoder_hidden_dim
        if self.change_hidden_dim:
            hidden_init_fc_list = []
            cell_init_fc_list = []
            for _ in range(num_layers):
                hidden_init_fc_list.append(nn.Linear(encoder_hidden_dim, hidden_dim))
                cell_init_fc_list.append(nn.Linear(encoder_hidden_dim, hidden_dim))
            self.hidden_init_fc_list = nn.ModuleList(hidden_init_fc_list)
            self.cell_init_fc_list = nn.ModuleList(cell_init_fc_list)
        else:
            self.hidden_init_fc_list = nn.ModuleList([])
            self.cell_init_fc_list = nn.ModuleList([])
        if attention_type == 'dot':
            self.attention = DotAttention(decoder_hidden_state_dim=hidden_dim, context_dim=encoder_hidden_dim)
        else:
            raise RuntimeError(f'Attention type {attention_type} not supported')
        self.combined_output_and_context_dim = self.attention.context_dim + hidden_dim
        layers = []
        for layer in range(num_layers):
            if layer == 0:
                cell_input_dim = embed_dim
            else:
                cell_input_dim = hidden_dim
            if self.first_layer_attention or layer == 0:
                cell_input_dim += self.attention.context_dim
            layers.append(cell_class(input_size=cell_input_dim, hidden_size=hidden_dim))
        self.layers = nn.ModuleList(layers)
        self.num_layers = len(layers)
        if self.combined_output_and_context_dim != out_embed_dim:
            self.additional_fc = nn.Linear(self.combined_output_and_context_dim, out_embed_dim)
        else:
            self.additional_fc = PlaceholderIdentity()
        log_class_usage(__class__)

    def forward_unprojected(self, input_tokens, encoder_out: Dict[str, torch.Tensor], incremental_state: Optional[Dict[str, torch.Tensor]]=None) ->Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
        if incremental_state is not None and len(incremental_state) > 0:
            input_tokens = input_tokens[:, -1:]
        bsz, seqlen = input_tokens.size()
        encoder_outs = encoder_out['unpacked_output']
        src_lengths = encoder_out['src_lengths']
        x = self.embed_tokens([[input_tokens]])
        x = F.dropout(x, p=self.dropout_in, training=self.training)
        x = x.transpose(0, 1)
        cached_state = self._get_cached_state(incremental_state)
        if cached_state is not None:
            prev_hiddens, prev_cells, input_feed = cached_state
        else:
            if incremental_state is None:
                incremental_state = {}
            self._init_prev_states(encoder_out, incremental_state)
            init_state = self._get_cached_state(incremental_state)
            assert init_state is not None
            prev_hiddens, prev_cells, input_feed = init_state
        outs = []
        attn_scores_per_step: List[torch.Tensor] = []
        next_hiddens: List[torch.Tensor] = []
        next_cells: List[torch.Tensor] = []
        for j in range(seqlen):
            step_input = torch.cat((x[j, :, :], input_feed), dim=1)
            for i, rnn in enumerate(self.layers):
                hidden, cell = rnn(step_input, (prev_hiddens[i], prev_cells[i]))
                if self.first_layer_attention and i == 0:
                    input_feed, step_attn_scores = self.attention(hidden, encoder_outs, src_lengths)
                layer_output = F.dropout(hidden, p=self.dropout_out, training=self.training)
                step_input = layer_output
                if self.first_layer_attention:
                    step_input = torch.cat((step_input, input_feed), dim=1)
                next_hiddens.append(hidden)
                next_cells.append(cell)
            if not self.first_layer_attention:
                input_feed, step_attn_scores = self.attention(hidden, encoder_outs, src_lengths)
                attn_scores_per_step.append(step_attn_scores)
            combined_output_and_context = torch.cat((hidden, input_feed), dim=1)
            outs.append(combined_output_and_context)
            prev_hiddens = torch.stack(next_hiddens, 0)
            prev_cells = torch.stack(next_cells, 0)
            next_hiddens = []
            next_cells = []
        attn_scores = torch.stack(attn_scores_per_step, dim=1)
        attn_scores = attn_scores.transpose(0, 2)
        self._set_cached_state(incremental_state, (prev_hiddens, prev_cells, input_feed))
        x = torch.cat(outs, dim=0).view(seqlen, bsz, self.combined_output_and_context_dim)
        x = x.transpose(1, 0)
        x = self.additional_fc(x)
        x = F.dropout(x, p=self.dropout_out, training=self.training)
        return x, {'attn_scores': attn_scores, 'src_tokens': encoder_out['src_tokens'], 'src_lengths': encoder_out['src_lengths']}

    def reorder_incremental_state(self, incremental_state: Dict[str, torch.Tensor], new_order):
        """Reorder buffered internal state (for incremental generation)."""
        assert incremental_state is not None
        hiddens = self.get_incremental_state(incremental_state, 'cached_hiddens')
        assert hiddens is not None
        cells = self.get_incremental_state(incremental_state, 'cached_cells')
        assert cells is not None
        feeds = self.get_incremental_state(incremental_state, 'cached_feeds')
        assert feeds is not None
        self.set_incremental_state(incremental_state, 'cached_hiddens', hiddens.index_select(1, new_order))
        self.set_incremental_state(incremental_state, 'cached_cells', cells.index_select(1, new_order))
        self.set_incremental_state(incremental_state, 'cached_feeds', feeds.index_select(0, new_order))

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        return int(100000.0)

    def _init_prev_states(self, encoder_out: Dict[str, torch.Tensor], incremental_state: Dict[str, torch.Tensor]) ->None:
        encoder_output = encoder_out['unpacked_output']
        final_hiddens = encoder_out['final_hiddens']
        prev_cells = encoder_out['final_cells']
        if self.averaging_encoder:
            prev_hiddens = torch.stack([torch.mean(encoder_output, 0)] * self.num_layers, dim=0)
        else:
            prev_hiddens = final_hiddens
        if self.change_hidden_dim:
            transformed_hiddens: List[torch.Tensor] = []
            transformed_cells: List[torch.Tensor] = []
            i: int = 0
            for hidden_init_fc, cell_init_fc in zip(self.hidden_init_fc_list, self.cell_init_fc_list):
                transformed_hiddens.append(hidden_init_fc(prev_hiddens[i]))
                transformed_cells.append(cell_init_fc(prev_cells[i]))
                i += 1
            use_hiddens = torch.stack(transformed_hiddens, dim=0)
            use_cells = torch.stack(transformed_cells, dim=0)
        else:
            use_hiddens = prev_hiddens
            use_cells = prev_cells
        assert self.attention.context_dim
        initial_attn_context = torch.zeros(self.attention.context_dim, device=encoder_output.device)
        batch_size = encoder_output.size(1)
        self.set_incremental_state(incremental_state, 'cached_hiddens', use_hiddens)
        self.set_incremental_state(incremental_state, 'cached_cells', use_cells)
        self.set_incremental_state(incremental_state, 'cached_feeds', initial_attn_context.expand(batch_size, self.attention.context_dim))

    def get_normalized_probs(self, net_output, log_probs, sample):
        """Get normalized probabilities (or log probs) from a net's output."""
        logits = net_output[0]
        if log_probs:
            return fairseq_utils.log_softmax(logits, dim=-1)
        else:
            return fairseq_utils.softmax(logits, dim=-1)

    def _get_cached_state(self, incremental_state: Optional[Dict[str, torch.Tensor]]):
        if incremental_state is None or len(incremental_state) == 0:
            return None
        hiddens = self.get_incremental_state(incremental_state, 'cached_hiddens')
        assert hiddens is not None
        cells = self.get_incremental_state(incremental_state, 'cached_cells')
        assert cells is not None
        feeds = self.get_incremental_state(incremental_state, 'cached_feeds')
        assert feeds is not None
        return hiddens, cells, feeds

    def _set_cached_state(self, incremental_state: Optional[Dict[str, torch.Tensor]], state: Tuple[torch.Tensor, torch.Tensor, torch.Tensor]) ->None:
        if incremental_state is None:
            return
        hiddens, cells, feeds = state
        self.set_incremental_state(incremental_state, 'cached_hiddens', hiddens)
        self.set_incremental_state(incremental_state, 'cached_cells', cells)
        self.set_incremental_state(incremental_state, 'cached_feeds', feeds)


class RNNDecoder(RNNDecoderBase, DecoderWithLinearOutputProjection):

    def __init__(self, out_vocab_size, embed_tokens, encoder_hidden_dim, embed_dim, hidden_dim, out_embed_dim, cell_type, num_layers, dropout_in, dropout_out, attention_type, attention_heads, first_layer_attention, averaging_encoder):
        DecoderWithLinearOutputProjection.__init__(self, out_vocab_size, out_embed_dim=out_embed_dim)
        RNNDecoderBase.__init__(self, embed_tokens, encoder_hidden_dim, embed_dim, hidden_dim, out_embed_dim, cell_type, num_layers, dropout_in, dropout_out, attention_type, attention_heads, first_layer_attention, averaging_encoder)
        log_class_usage(__class__)


class LSTMSequenceEncoder(PyTextSeq2SeqModule):
    """RNN encoder using nn.LSTM for cuDNN support / ONNX exportability."""


    class Config(ConfigBase):
        embed_dim: int = 512
        hidden_dim: int = 512
        num_layers: int = 1
        dropout_in: float = 0.1
        dropout_out: float = 0.1
        bidirectional: bool = False

    def __init__(self, embed_dim, hidden_dim, num_layers, dropout_in, dropout_out, bidirectional):
        super().__init__()
        self.dropout_in = dropout_in
        self.dropout_out = dropout_out
        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional
        self.num_layers: int = num_layers
        self.word_dim = embed_dim
        self.bilstm = BiLSTM(num_layers=num_layers, bidirectional=bidirectional, embed_dim=embed_dim, hidden_dim=hidden_dim, dropout=dropout_out)
        log_class_usage(__class__)

    @classmethod
    def from_config(cls, config):
        return cls(**config._asdict())

    def forward(self, src_tokens: torch.Tensor, embeddings: torch.Tensor, src_lengths) ->Dict[str, torch.Tensor]:
        x = F.dropout(embeddings, p=self.dropout_in, training=self.training)
        x = x.transpose(0, 1)
        unpacked_output, final_hiddens, final_cells = self.bilstm(embeddings=x, lengths=src_lengths)
        return {'unpacked_output': unpacked_output, 'final_hiddens': final_hiddens, 'final_cells': final_cells, 'src_lengths': src_lengths, 'src_tokens': src_tokens, 'embeddings': embeddings}

    def max_positions(self):
        """Maximum output length supported by the decoder."""
        return int(100000.0)

    def tile_encoder_out(self, beam_size: int, encoder_out: Dict[str, torch.Tensor]) ->Dict[str, torch.Tensor]:
        tiled_encoder_out = encoder_out['unpacked_output'].expand(-1, beam_size, -1)
        hiddens = encoder_out['final_hiddens']
        tiled_hiddens: List[torch.Tensor] = []
        for i in range(self.num_layers):
            tiled_hiddens.append(hiddens[i].expand(beam_size, -1))
        cells = encoder_out['final_cells']
        tiled_cells: List[torch.Tensor] = []
        for i in range(self.num_layers):
            tiled_cells.append(cells[i].expand(beam_size, -1))
        return {'unpacked_output': tiled_encoder_out, 'final_hiddens': torch.stack(tiled_hiddens, dim=0), 'final_cells': torch.stack(tiled_cells, dim=0), 'src_lengths': encoder_out['src_lengths'], 'src_tokens': encoder_out['src_tokens']}


class RNNModel(PyTextSeq2SeqModule):


    class Config(ConfigBase):
        encoder: LSTMSequenceEncoder.Config = LSTMSequenceEncoder.Config()
        decoder: RNNDecoder.Config = RNNDecoder.Config()

    def __init__(self, encoder, decoder, source_embeddings):
        super().__init__()
        self.source_embeddings = source_embeddings
        self.encoder = encoder
        self.decoder = decoder
        log_class_usage(__class__)

    def forward(self, src_tokens: torch.Tensor, additional_features: List[List[torch.Tensor]], src_lengths, prev_output_tokens, incremental_state: Optional[Dict[str, torch.Tensor]]=None):
        embeddings = self.source_embeddings([[src_tokens]] + additional_features)
        encoder_out = self.encoder(src_tokens, embeddings, src_lengths=src_lengths)
        decoder_out = self.decoder(prev_output_tokens, encoder_out, incremental_state)
        return decoder_out

    @classmethod
    def from_config(cls, config: Config, source_vocab, source_embedding, target_vocab, target_embedding):
        out_vocab_size = len(target_vocab)
        encoder = create_module(config.encoder)
        decoder = create_module(config.decoder, out_vocab_size, target_embedding)
        return cls(encoder, decoder, source_embedding)

    def get_normalized_probs(self, net_output, log_probs, sample=None):
        return self.decoder.get_normalized_probs(net_output, log_probs, sample)

    def max_decoder_positions(self):
        return max(self.encoder.max_positions(), self.decoder.max_positions())


class ByteTokenTensorizer(Tensorizer):
    """Turn words into 2-dimensional tensors of int8 bytes. Words are padded to
    `max_byte_len`. Also computes sequence lengths (1-D tensor) and token lengths
    (2-D tensor). 0 is the pad byte.
    """
    NUM_BYTES = 256


    class Config(Tensorizer.Config):
        column: str = 'text'
        tokenizer: Tokenizer.Config = Tokenizer.Config()
        max_seq_len: Optional[int] = None
        max_byte_len: int = 15
        offset_for_non_padding: int = 0
        add_bos_token: bool = False
        add_eos_token: bool = False
        use_eos_token_for_bos: bool = False

    @classmethod
    def from_config(cls, config: Config):
        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
        return cls(text_column=config.column, tokenizer=tokenizer, max_seq_len=config.max_seq_len, max_byte_len=config.max_byte_len, offset_for_non_padding=config.offset_for_non_padding, add_bos_token=config.add_bos_token, add_eos_token=config.add_eos_token, use_eos_token_for_bos=config.use_eos_token_for_bos, is_input=config.is_input)

    def __init__(self, text_column, tokenizer=None, max_seq_len=Config.max_seq_len, max_byte_len=Config.max_byte_len, offset_for_non_padding=Config.offset_for_non_padding, add_bos_token=Config.add_bos_token, add_eos_token=Config.add_eos_token, use_eos_token_for_bos=Config.use_eos_token_for_bos, is_input=Config.is_input):
        self.text_column = text_column
        self.tokenizer = tokenizer or Tokenizer()
        self.max_seq_len = max_seq_len or 2 ** 30
        self.max_byte_len = max_byte_len
        self.offset_for_non_padding = offset_for_non_padding
        self.add_bos_token = add_bos_token
        self.add_eos_token = add_eos_token
        self.use_eos_token_for_bos = use_eos_token_for_bos
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.text_column, str)]

    def numberize(self, row):
        """Convert text to bytes, pad batch."""
        tokens = self.tokenizer.tokenize(row[self.text_column])[:self.max_seq_len - self.add_bos_token - self.add_eos_token]
        if self.add_bos_token:
            bos = EOS if self.use_eos_token_for_bos else BOS
            tokens = [Token(bos, -1, -1)] + tokens
        if self.add_eos_token:
            tokens.append(Token(EOS, -1, -1))
        if not tokens:
            tokens = [Token(PAD, -1, -1)]
        bytes = [self._numberize_token(token)[:self.max_byte_len] for token in tokens]
        token_lengths = len(tokens)
        byte_lengths = [len(token_bytes) for token_bytes in bytes]
        return bytes, token_lengths, byte_lengths

    def _numberize_token(self, token):
        return [(c + self.offset_for_non_padding) for c in token.value.encode()]

    def tensorize(self, batch, pad_token=0):
        bytes, token_lengths, byte_lengths = zip(*batch)
        pad_shape = len(batch), precision.pad_length(max(len(length) for length in byte_lengths)), self.max_byte_len
        return pad_and_tensorize(bytes, pad_shape=pad_shape, pad_token=pad_token), pad_and_tensorize(token_lengths), pad_and_tensorize(byte_lengths)

    def sort_key(self, row):
        return len(row[0])


class ContextualTokenEmbedding(EmbeddingBase):
    """Module for providing token embeddings from a pretrained model."""
    Config = ContextualTokenEmbeddingConfig

    @classmethod
    def from_config(cls, config: ContextualTokenEmbeddingConfig, *args, **kwargs):
        return cls(config.embed_dim, downsample_dim=config.downsample_dim)

    def __init__(self, embed_dim: int, downsample_dim: Optional[int]=None) ->None:
        super().__init__(embed_dim)
        self.input_embed_dim = embed_dim
        if downsample_dim:
            self.proj = Linear(embed_dim, downsample_dim)
            self.embedding_dim = downsample_dim
        else:
            self.proj = PlaceholderIdentity()

    def forward(self, embedding: torch.Tensor) ->torch.Tensor:
        embedding_shape = torch.onnx.operators.shape_as_tensor(embedding)
        if embedding_shape[1].item() % self.input_embed_dim != 0:
            raise ValueError(f'Input embedding_dim {embedding_shape[1]} is not a' + f' multiple of specified embedding_dim {self.input_embed_dim}')
        num_tokens = embedding_shape[1] // self.input_embed_dim
        new_embedding_shape = torch.cat((torch.tensor([-1], dtype=torch.long), num_tokens.view(1), torch.tensor([self.input_embed_dim], dtype=torch.long)))
        reshaped_embed = torch.onnx.operators.reshape_from_tensor_shape(embedding, new_embedding_shape)
        return self.proj(reshaped_embed)


Gazetteer = List[Dict[str, Dict[str, float]]]


class GazetteerTensorizer(Tensorizer):
    """
    Create 3 tensors for dict features.

    - idx: index of feature in token order.
    - weights: weight of feature in token order.
    - lens: number of features per token.

    For each input token, there will be the same number of `idx` and `weights` entries.
    (equal to the max number of features any token has in this row). The values
    in `lens` will tell how many of these features are actually used per token.

    Input format for the dict column is json and should be a list of dictionaries
    containing the "features" and their weight for each relevant "tokenIdx". Example:
    ::

        text: "Order coffee from Starbucks please"
        dict: [
            {"tokenIdx": 1, "features": {"drink/beverage": 0.8, "music/song": 0.2}},
            {"tokenIdx": 3, "features": {"store/coffee_shop": 1.0}}
        ]

    if we assume this vocab
    ::

        vocab = {
            UNK: 0, PAD: 1,
            "drink/beverage": 2, "music/song": 3, "store/coffee_shop": 4
        }

    this example will result in those tensors:
    ::

        idx =     [1,   1,   2,   3,   1,   1,   4,   1,   1,   1]
        weights = [0.0, 0.0, 0.8, 0.2, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]
        lens =    [1,        2,        1,        1,        1]

    """


    class Config(Tensorizer.Config):
        text_column: str = 'text'
        dict_column: str = 'dict'
        tokenizer: Tokenizer.Config = Tokenizer.Config()

    @classmethod
    def from_config(cls, config: Config):
        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
        return cls(config.text_column, config.dict_column, tokenizer, config.is_input)

    def __init__(self, text_column: str=Config.text_column, dict_column: str=Config.dict_column, tokenizer: Tokenizer=None, is_input: bool=Config.is_input):
        self.text_column = text_column
        self.dict_column = dict_column
        self.tokenizer = tokenizer or Tokenizer()
        self.vocab_builder = VocabBuilder()
        self.vocab = None
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.text_column, str), (self.dict_column, Gazetteer)]

    def initialize(self, from_scratch=True):
        """
        Look through the dataset for all dict features to create vocab.
        """
        if self.vocab and from_scratch:
            return
        try:
            while True:
                row = yield
                for token_dict in row[self.dict_column]:
                    self.vocab_builder.add_all(token_dict['features'])
        except GeneratorExit:
            self.vocab = self.vocab_builder.make_vocab()

    def numberize(self, row):
        """
        Numberize dict features. Fill in for tokens with no features with
        PAD and weight 0.0. All tokens need to have at least one entry.
        Tokens with more than one feature will have multiple idx and weight
        added in sequence.
        """
        num_tokens = len(self.tokenizer.tokenize(row[self.text_column]))
        num_labels = max(len(t['features']) for t in row[self.dict_column])
        res_idx = [self.vocab.get_pad_index()] * (num_labels * num_tokens)
        res_weights = [0.0] * (num_labels * num_tokens)
        res_lens = [1] * num_tokens
        for dict_feature in row[self.dict_column]:
            idx = dict_feature['tokenIdx']
            feats = dict_feature['features']
            pos = idx * num_labels
            res_lens[idx] = len(feats)
            for label, weight in feats.items():
                res_idx[pos] = self.vocab.lookup_all(label)
                res_weights[pos] = weight
                pos += 1
        return res_idx, res_weights, res_lens

    def tensorize(self, batch):
        feats, weights, lengths = zip(*batch)
        lengths_flattened = [li for l_list in lengths for li in l_list]
        seq_lens = [len(l_list) for l_list in lengths]
        max_ex_len = precision.pad_length(max(seq_lens))
        max_feat_len = max(lengths_flattened)
        all_lengths, all_feats, all_weights = [], [], []
        for i, seq_len in enumerate(seq_lens):
            ex_feats, ex_weights, ex_lengths = [], [], []
            feats_lengths, feats_vals, feats_weights = lengths[i], feats[i], weights[i]
            max_feat_len_example = max(feats_lengths)
            r_offset = 0
            for _ in feats_lengths:
                ex_feats.extend(feats_vals[r_offset:r_offset + max_feat_len_example])
                ex_feats.extend([self.vocab.get_pad_index()] * (max_feat_len - max_feat_len_example))
                ex_weights.extend(feats_weights[r_offset:r_offset + max_feat_len_example])
                ex_weights.extend([0.0] * (max_feat_len - max_feat_len_example))
                r_offset += max_feat_len_example
            ex_lengths.extend(feats_lengths)
            ex_padding = (max_ex_len - seq_len) * max_feat_len
            ex_feats.extend([self.vocab.get_pad_index()] * ex_padding)
            ex_weights.extend([0.0] * ex_padding)
            ex_lengths.extend([1] * (max_ex_len - seq_len))
            all_feats.append(ex_feats)
            all_weights.append(ex_weights)
            all_lengths.append(ex_lengths)
        return cuda.tensor(all_feats, torch.long), precision.maybe_half(cuda.tensor(all_weights, torch.float)), cuda.tensor(all_lengths, torch.long)


def GetTensor(tensor):
    if CUDA_ENABLED:
        return tensor
    else:
        return tensor


class BeamDecode(torch.nn.Module):
    """
    Decodes the output of Beam Search to get the top hypotheses
    """

    def __init__(self, eos_token_id, length_penalty, nbest, beam_size, stop_at_eos):
        super().__init__()
        self.eos_token_id: int = eos_token_id
        self.length_penalty: float = length_penalty
        self.nbest: int = nbest
        self.beam_size: int = beam_size
        self.stop_at_eos: bool = stop_at_eos

    @torch.no_grad()
    def forward(self, beam_tokens: Tensor, beam_scores: Tensor, token_weights: Tensor, beam_prev_indices: Tensor, num_steps: int) ->List[Tuple[Tensor, float, List[float], Tensor, Tensor]]:
        self._check_dimensions(beam_tokens, beam_scores, token_weights, beam_prev_indices, num_steps)
        end_states = self._get_all_end_states(beam_tokens, beam_scores, beam_prev_indices, num_steps)
        outputs = torch.jit.annotate(List[Tuple[Tensor, float, List[float], Tensor, Tensor]], [])
        for state_idx in range(len(end_states)):
            state = end_states[state_idx]
            hypothesis_score = float(state[0])
            beam_indices = self._get_output_steps_to_beam_indices(state, beam_prev_indices)
            beam_output = torch.jit.annotate(List[Tensor], [])
            token_level_scores = torch.jit.annotate(List[float], [])
            position = int(state[1])
            hyp_index = int(state[2])
            best_indices = torch.tensor([position, hyp_index])
            back_alignment_weights = []
            assert position + 1 == len(beam_indices)
            pos = 1
            prev_beam_index = -1
            while pos < len(beam_indices):
                beam_index = beam_indices[pos]
                beam_output.append(beam_tokens[pos][beam_index])
                if pos == 1:
                    token_level_scores.append(float(beam_scores[pos][beam_index]))
                else:
                    token_level_scores.append(float(beam_scores[pos][beam_index]) - float(beam_scores[pos - 1][prev_beam_index]))
                back_alignment_weights.append(token_weights[pos][beam_index].detach())
                prev_beam_index = beam_index
                pos += 1
            outputs.append((torch.stack(beam_output), hypothesis_score, token_level_scores, torch.stack(back_alignment_weights, dim=1), best_indices))
        return outputs

    def _get_output_steps_to_beam_indices(self, end_state: Tensor, beam_prev_indices: Tensor) ->List[int]:
        """
        Returns a mapping from each output position and the beam index that was
        picked from the beam search results.
        """
        present_position = int(end_state[1])
        beam_index = int(end_state[2])
        beam_indices = torch.jit.annotate(List[int], [])
        while present_position >= 0:
            beam_indices.insert(0, beam_index)
            beam_index = beam_prev_indices[present_position][beam_index]
            present_position = present_position - 1
        return beam_indices

    def _add_to_end_states(self, end_states: List[Tensor], min_score: float, state: Tensor, min_index: int) ->Tuple[List[Tensor], float, int]:
        """
        Maintains a list of atmost `nbest` highest end states
        """
        if len(end_states) < self.nbest:
            end_states.append(state)
            if state[0] <= min_score:
                min_score = state[0]
                min_index = len(end_states) - 1
        elif state[0] > min_score:
            end_states[min_index] = state
            min_index = -1
            min_score = 65504.0
            for idx in range(len(end_states)):
                s = end_states[idx]
                if s[0] <= min_score:
                    min_index = idx
                    min_score = s[0]
        return end_states, min_score, min_index

    def _get_all_end_states(self, beam_tokens: Tensor, beam_scores: Tensor, beam_prev_indices: Tensor, num_steps: int) ->Tensor:
        """
        Return all end states and hypothesis scores for those end states.
        """
        min_score = 65504.0
        min_index = -1
        end_states = torch.jit.annotate(List[Tensor], [])
        prev_hypo_is_finished = torch.zeros(self.beam_size).byte()
        position = 1
        while position <= num_steps:
            hypo_is_finished = torch.zeros(self.beam_size, dtype=torch.bool)
            for hyp_index in range(self.beam_size):
                prev_pos = beam_prev_indices[position][hyp_index]
                hypo_is_finished[hyp_index] = prev_hypo_is_finished[prev_pos]
                if hypo_is_finished[hyp_index] == 0:
                    if beam_tokens[position][hyp_index] == self.eos_token_id or position == num_steps:
                        if self.stop_at_eos:
                            hypo_is_finished[hyp_index] = 1
                        hypo_score = float(beam_scores[position][hyp_index])
                        if self.length_penalty != 0:
                            hypo_score = hypo_score / position ** self.length_penalty
                        end_states, min_score, min_index = self._add_to_end_states(end_states, min_score, torch.tensor([hypo_score, float(position), float(hyp_index)]), min_index)
            prev_hypo_is_finished = hypo_is_finished
            position = position + 1
        end_states = torch.stack(end_states)
        _, sorted_end_state_indices = end_states[:, 0].sort(dim=0, descending=True)
        end_states = end_states[sorted_end_state_indices, :]
        return end_states

    def _check_dimensions(self, beam_tokens: Tensor, beam_scores: Tensor, token_weights: Tensor, beam_prev_indices: Tensor, num_steps: int) ->None:
        assert beam_tokens.size(1) == self.beam_size, 'Dimension of beam_tokens : {} and beam size : {} are not consistent'.format(beam_tokens.size(), self.beam_size)
        assert beam_scores.size(1) == self.beam_size, 'Dimension of beam_scores : {} and beam size : {} are not consistent'.format(beam_scores.size(), self.beam_size)
        assert token_weights.size(1) == self.beam_size, 'Dimension of token_weights : {} and beam size : {} are not consistent'.format(token_weights.size(), self.beam_size)
        assert beam_prev_indices.size(1) == self.beam_size, 'Dimension of beam_prev_indices : {} and beam size : {} '
        """are not consistent""".format(beam_prev_indices.size(), self.beam_size)
        assert beam_tokens.size(0) <= num_steps + 1, 'Dimension of beam_tokens : {} and num_steps : {} are not consistent'.format(beam_tokens.size(), num_steps)
        assert beam_scores.size(0) <= num_steps + 1, 'Dimension of beam_scores : {} and num_steps : {} are not consistent'.format(beam_scores.size(), num_steps)
        assert token_weights.size(0) <= num_steps + 1, 'Dimension of token_weights : {} and num_steps : {} are not consistent'.format(token_weights.size(), num_steps)
        assert beam_prev_indices.size(0) <= num_steps + 1, 'Dimension of beam_prev_indices : {} and num_steps : {} are not consistent'.format(beam_prev_indices.size(), num_steps)


class DecoderBatchedStepEnsemble(nn.Module):
    """
    This method should have a common interface such that it can be called after
    the encoder as well as after the decoder
    """
    incremental_states: Dict[str, Dict[str, Tensor]]

    def __init__(self, models, beam_size, record_attention=False):
        super().__init__()
        self.models = nn.ModuleList(models)
        self.incremental_states = {}
        self.beam_size = beam_size
        self.record_attention = record_attention

    @torch.jit.export
    def reset_incremental_states(self):
        for idx, _model in enumerate(self.models):
            self.incremental_states[str(idx)] = {}

    def forward(self, prev_tokens: Tensor, prev_scores: Tensor, timestep: int, decoder_ips: List[Dict[str, Tensor]]) ->Tuple[Tensor, Tensor, Tensor, Tensor, List[Dict[str, Tensor]]]:
        """
        Decoder step inputs correspond one-to-one to encoder outputs.
        HOWEVER: after the first step, encoder outputs (i.e, the first
        len(self.models) elements of inputs) must be tiled k (beam size)
        times on the batch dimension (axis 1).
        """
        prev_tokens = prev_tokens.unsqueeze(1)
        log_probs_per_model = torch.jit.annotate(List[Tensor], [])
        attn_weights_per_model = torch.jit.annotate(List[Tensor], [])
        futures = torch.jit.annotate(List[Tuple[Tensor, Dict[str, Tensor]]], [])
        for idx, model in enumerate(self.models):
            decoder_ip = decoder_ips[idx]
            incremental_state = self.incremental_states[str(idx)]
            fut = model.decoder(prev_tokens, decoder_ip, incremental_state=incremental_state, timestep=timestep)
            futures.append(fut)
        for idx, _model in enumerate(self.models):
            fut = futures[idx]
            log_probs, features = fut
            log_probs_per_model.append(log_probs)
            if 'attn_scores' in features:
                attn_weights_per_model.append(features['attn_scores'])
        best_scores, best_tokens, prev_hypos, attention_weights = self.beam_search_aggregate_topk(log_probs_per_model, attn_weights_per_model, prev_scores, self.beam_size, self.record_attention)
        for model_state_ptr, model in enumerate(self.models):
            incremental_state = self.incremental_states[str(model_state_ptr)]
            model.decoder.reorder_incremental_state(incremental_state, prev_hypos)
        return best_tokens, best_scores, prev_hypos, attention_weights, decoder_ips

    def beam_search_aggregate_topk(self, log_probs_per_model: List[torch.Tensor], attn_weights_per_model: List[torch.Tensor], prev_scores: torch.Tensor, beam_size: int, record_attention: bool):
        average_log_probs = torch.mean(torch.cat(log_probs_per_model, dim=1), dim=1, keepdim=True)
        best_scores_k_by_k, best_tokens_k_by_k = torch.topk(average_log_probs.squeeze(1), k=beam_size)
        prev_scores_k_by_k = prev_scores.view(-1, 1).expand(-1, beam_size)
        total_scores_k_by_k = best_scores_k_by_k + prev_scores_k_by_k
        total_scores_flat = total_scores_k_by_k.view(-1)
        best_tokens_flat = best_tokens_k_by_k.view(-1)
        best_scores, best_indices = torch.topk(total_scores_flat, k=beam_size)
        best_tokens = best_tokens_flat.index_select(dim=0, index=best_indices).view(-1)
        prev_hypos = best_indices // beam_size
        if record_attention:
            average_attn_weights = torch.mean(torch.cat(attn_weights_per_model, dim=1), dim=1, keepdim=True)
            attention_weights = average_attn_weights.index_select(dim=0, index=prev_hypos)
            attention_weights = attention_weights.squeeze_(1)
        else:
            attention_weights = torch.zeros(beam_size, attn_weights_per_model[0].size(2))
        return best_scores, best_tokens, prev_hypos, attention_weights


class EncoderEnsemble(nn.Module):
    """
    This class will call the encoders from all the models in the ensemble.
    It will process the encoder output to prepare input for each decoder step
    input
    """

    def __init__(self, models, beam_size):
        super().__init__()
        self.models = nn.ModuleList(models)
        self.beam_size = beam_size

    def forward(self, src_tokens: Tensor, src_lengths: Tensor, dict_feat: Optional[Tuple[Tensor, Tensor, Tensor]]=None, contextual_token_embedding: Optional[Tensor]=None) ->List[Dict[str, Tensor]]:
        src_tokens_seq_first = src_tokens.t()
        futures = torch.jit.annotate(List[Dict[str, Tensor]], [])
        for model in self.models:
            embedding_input = [[src_tokens_seq_first]]
            if dict_feat is not None:
                embedding_input.append(list(dict_feat))
            if contextual_token_embedding is not None:
                embedding_input.append([contextual_token_embedding])
            embeddings = model.source_embeddings(embedding_input)
            futures.append(model.encoder(src_tokens_seq_first, embeddings, src_lengths))
        return self.prepare_decoderstep_ip(futures)

    def prepare_decoderstep_ip(self, futures: List[Dict[str, Tensor]]) ->List[Dict[str, Tensor]]:
        outputs = torch.jit.annotate(List[Dict[str, Tensor]], [])
        for idx, model in enumerate(self.models):
            encoder_out = futures[idx]
            tiled_encoder_out = model.encoder.tile_encoder_out(self.beam_size, encoder_out)
            outputs.append(tiled_encoder_out)
        return outputs


@torch.jit.script
def get_first_decoder_step_input(beam_size: int=5, eos_token_id: int=0, src_length: int=1) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
    prev_tokens = torch.full([beam_size], eos_token_id, dtype=torch.long)
    prev_scores = torch.full([beam_size], 1, dtype=torch.float)
    prev_hypos = torch.full([beam_size], 0, dtype=torch.long)
    attention_weights = torch.full([beam_size, src_length], 1, dtype=torch.float)
    return prev_tokens, prev_scores, prev_hypos, attention_weights


class BeamSearch(nn.Module):

    def __init__(self, model_list, tgt_dict_eos, beam_size: int=2, quantize: bool=False, record_attention: bool=False):
        super().__init__()
        self.models = model_list
        self.target_dict_eos = tgt_dict_eos
        self.beam_size = beam_size
        self.record_attention = record_attention
        encoder_ens = EncoderEnsemble(self.models, self.beam_size)
        if quantize:
            encoder_ens = tq.quantize_dynamic(encoder_ens, {torch.nn.Linear}, dtype=torch.qint8, inplace=False)
        self.encoder_ens = torch.jit.script(encoder_ens)
        decoder_ens = DecoderBatchedStepEnsemble(self.models, beam_size, record_attention=record_attention)
        if quantize:
            decoder_ens = tq.quantize_dynamic(decoder_ens, {torch.nn.Linear}, dtype=torch.qint8, inplace=False)
        self.decoder_ens = torch.jit.script(decoder_ens)

    def forward(self, src_tokens: torch.Tensor, src_lengths: torch.Tensor, num_steps: int, dict_feat: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]]=None, contextual_token_embedding: Optional[torch.Tensor]=None):
        self.decoder_ens.reset_incremental_states()
        decoder_ip = self.encoder_ens(src_tokens, src_lengths, dict_feat, contextual_token_embedding)
        prev_token, prev_scores, prev_hypos_indices, attention_weights = get_first_decoder_step_input(self.beam_size, self.target_dict_eos, src_lengths[0])
        all_tokens_list = [prev_token]
        all_scores_list = [prev_scores]
        all_prev_indices_list = [prev_hypos_indices]
        all_attentions_list: List[torch.Tensor] = []
        if self.record_attention:
            all_attentions_list.append(attention_weights)
        for i in range(num_steps):
            prev_token, prev_scores, prev_hypos_indices, attention_weights, decoder_ip = self.decoder_ens(prev_token, prev_scores, i + 1, decoder_ip)
            all_tokens_list.append(prev_token)
            all_scores_list.append(prev_scores)
            all_prev_indices_list.append(prev_hypos_indices)
            if self.record_attention:
                all_attentions_list.append(attention_weights)
        all_tokens = torch.stack(all_tokens_list)
        all_scores = torch.stack(all_scores_list)
        all_prev_indices = torch.stack(all_prev_indices_list)
        if self.record_attention:
            all_attn_weights = torch.stack(all_attentions_list)
        else:
            all_attn_weights = torch.zeros(num_steps + 1, self.beam_size, src_tokens.size(1))
        return all_tokens, all_scores, all_attn_weights, all_prev_indices


@torch.jit.script
def get_target_length(src_len: int, targetlen_cap: int, targetlen_a: float, targetlen_b: float, targetlen_c: float) ->int:
    target_length = int(min(targetlen_cap, src_len * targetlen_a * targetlen_a + src_len * targetlen_b + targetlen_c))
    assert target_length > 0, 'Target length cannot be less than 0 src_len:' + str(src_len) + ' target_length:' + str(target_length)
    return target_length


class ScriptedSequenceGenerator(Module):


    class Config(ConfigBase):
        beam_size: int = 2
        targetlen_cap: int = 100
        targetlen_a: float = 0
        targetlen_b: float = 2
        targetlen_c: float = 2
        quantize: bool = True
        length_penalty: float = 0.25
        nbest: int = 2
        stop_at_eos: bool = True
        record_attention: bool = False

    @classmethod
    def from_config(cls, config, models, trg_dict_eos):
        return cls(models, trg_dict_eos, config)

    def __init__(self, models, trg_dict_eos, config):
        super().__init__()
        self.targetlen_cap = config.targetlen_cap
        self.targetlen_a: float = float(config.targetlen_a)
        self.targetlen_b: float = float(config.targetlen_b)
        self.targetlen_c: float = float(config.targetlen_c)
        self.beam_search = BeamSearch(models, trg_dict_eos, beam_size=config.beam_size, quantize=config.quantize, record_attention=config.record_attention)
        self.beam_decode = BeamDecode(eos_token_id=trg_dict_eos, length_penalty=config.length_penalty, nbest=config.nbest, beam_size=config.beam_size, stop_at_eos=config.stop_at_eos)

    def forward(self, src_tokens: torch.Tensor, dict_feat: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]], contextual_token_embedding: Optional[torch.Tensor], src_lengths: torch.Tensor) ->List[Tuple[torch.Tensor, float, List[float], torch.Tensor, torch.Tensor]]:
        target_length = get_target_length(src_lengths.item(), self.targetlen_cap, self.targetlen_a, self.targetlen_b, self.targetlen_c)
        all_tokens, all_scores, all_weights, all_prev_indices = self.beam_search(src_tokens, src_lengths, target_length, dict_feat, contextual_token_embedding)
        return self.beam_decode(all_tokens, all_scores, all_weights, all_prev_indices, target_length)

    @torch.jit.export
    def generate_hypo(self, tensors: Dict[str, torch.Tensor]):
        actual_src_tokens = tensors['src_tokens'].t()
        dict_feat: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None
        if 'dict_tokens' in tensors:
            dict_feat = tensors['dict_tokens'], tensors['dict_weights'], tensors['dict_lengths']
        contextual_token_embedding: Optional[torch.Tensor] = None
        if 'contextual_token_embedding' in tensors:
            contextual_token_embedding = tensors['contextual_token_embedding']
        hypos_etc = self.forward(actual_src_tokens, dict_feat, contextual_token_embedding, tensors['src_lengths'])
        predictions = [[pred for pred, _, _, _, _ in hypos_etc]]
        scores = [[score for _, score, _, _, _ in hypos_etc]]
        return predictions, scores


@torch.jit.script
def get_single_unk_token(src_tokens: List[str], word_ids: List[int], copy_unk_token: bool, unk_idx: int):
    """Returns the string representation of the first UNK
    we get in our source utterance. We can then use this string instead of
    writing "<UNK>" in our decoding.
    """
    if copy_unk_token:
        for i, x in enumerate(word_ids):
            if x == unk_idx:
                return src_tokens[i]
    return None


class Seq2SeqJIT(torch.nn.Module):

    def __init__(self, src_dict, tgt_dict, sequence_generator, filter_eos_bos, copy_unk_token=False, dictfeat_dict=None):
        super().__init__()
        self.source_vocab = ScriptVocabulary(src_dict._vocab, src_dict.get_unk_index(), bos_idx=src_dict.get_bos_index(-1), eos_idx=src_dict.get_eos_index(-1))
        self.target_vocab = ScriptVocabulary(tgt_dict._vocab, tgt_dict.get_unk_index(), bos_idx=tgt_dict.get_bos_index(), eos_idx=tgt_dict.get_eos_index())
        if dictfeat_dict:
            self.dictfeat_vocab = ScriptVocabulary(dictfeat_dict._vocab, pad_idx=dictfeat_dict.idx[src_dict[src_dict.get_pad_index()]])
        else:
            self.dictfeat_vocab = ScriptVocabulary([])
        self.sequence_generator = sequence_generator
        self.copy_unk_token: bool = copy_unk_token
        self.unk_idx: int = self.source_vocab.unk_idx
        self.filter_eos_bos: bool = filter_eos_bos

    def prepare_generator_inputs(self, word_ids: List[int], dict_feat: Optional[Tuple[List[str], List[float], List[int]]]=None, contextual_token_embedding: Optional[List[float]]=None) ->Tuple[torch.Tensor, Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]], Optional[torch.Tensor], torch.Tensor]:
        src_len = len(word_ids)
        dict_tensors: Optional[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]] = None
        if dict_feat is not None:
            dict_tokens, dict_weights, dict_lengths = dict_feat
            dict_ids = self.dictfeat_vocab.lookup_indices_1d(dict_tokens)
            dict_tensors = torch.tensor([dict_ids]), torch.tensor([dict_weights], dtype=torch.float), torch.tensor([dict_lengths])
        contextual_embedding_tensor: Optional[torch.Tensor] = None
        if contextual_token_embedding is not None:
            assert len(contextual_token_embedding) % src_len == 0 and len(contextual_token_embedding) > 0, f'Incorrect size for contextual embeddings: {len(contextual_token_embedding)}, Expected a non-zero multiple of input token count {src_len} '
            contextual_embedding_tensor = torch.tensor([contextual_token_embedding], dtype=torch.float)
        return torch.tensor(word_ids).reshape(-1, 1), dict_tensors, contextual_embedding_tensor, torch.tensor([src_len])

    def forward(self, src_tokens: List[str], dict_feat: Optional[Tuple[List[str], List[float], List[int]]]=None, contextual_token_embedding: Optional[List[float]]=None) ->List[Tuple[List[str], float, List[float]]]:
        word_ids = self.source_vocab.lookup_indices_1d(src_tokens)
        single_unk_token: Optional[str] = get_single_unk_token(src_tokens, word_ids, self.copy_unk_token, self.unk_idx)
        words, dict_tensors, contextual_embedding_tensor, src_lengths = self.prepare_generator_inputs(word_ids, dict_feat, contextual_token_embedding)
        hypos_etc = self.sequence_generator(words, dict_tensors, contextual_embedding_tensor, src_lengths)
        hypos_list: List[Tuple[List[str], float, List[float]]] = []
        filter_token_list: List[int] = []
        if self.filter_eos_bos:
            filter_token_list = [self.target_vocab.bos_idx, self.target_vocab.eos_idx]
        for seq in hypos_etc:
            hyopthesis = seq[0]
            stringified = self.target_vocab.lookup_words_1d(hyopthesis, filter_token_list=filter_token_list, possible_unk_token=single_unk_token)
            hypos_list.append((stringified, seq[1], seq[2]))
        return hypos_list


class VectorNormalizer(torch.nn.Module):
    """Performs in-place normalization over all features of a dense feature
    vector by doing (x - mean)/stddev for each x in the feature vector.

    This is a ScriptModule so that the normalize function can be called at
    training time in the tensorizer, as well as at inference time by using it in
    your torchscript forward function. To use this in your tensorizer
    update_meta_data must be called once per row in your initialize function,
    and then calculate_feature_stats must be called upon the last time it runs.
    See usage in FloatListTensorizer for an example.

    Setting do_normalization=False will make the normalize function an identity
    function.
    """

    def __init__(self, dim: int, do_normalization: bool=True):
        super().__init__()
        self.num_rows = 0
        self.feature_sums = [0] * dim
        self.feature_squared_sums = [0] * dim
        self.do_normalization = do_normalization
        self.feature_avgs = [0.0] * dim
        self.feature_stddevs = [1.0] * dim

    def __getstate__(self):
        return {'num_rows': self.num_rows, 'feature_sums': self.feature_sums, 'feature_squared_sums': self.feature_squared_sums, 'do_normalization': self.do_normalization, 'feature_avgs': self.feature_avgs, 'feature_stddevs': self.feature_stddevs}

    def __setstate__(self, state):
        self.num_rows = state['num_rows']
        self.feature_sums = state['feature_sums']
        self.feature_squared_sums = state['feature_squared_sums']
        self.do_normalization = state['do_normalization']
        self.feature_avgs = state['feature_avgs']
        self.feature_stddevs = state['feature_stddevs']

    def forward(self):
        pass

    def update_meta_data(self, vec):
        if self.do_normalization:
            self.num_rows += 1
            for i in range(len(vec)):
                self.feature_sums[i] += vec[i]
                self.feature_squared_sums[i] += vec[i] ** 2

    def calculate_feature_stats(self):
        if self.do_normalization:
            self.feature_avgs = [(x / self.num_rows) for x in self.feature_sums]
            self.feature_stddevs = [((self.feature_squared_sums[i] / self.num_rows - self.feature_avgs[i] ** 2) ** 0.5) for i in range(len(self.feature_squared_sums))]

    def normalize(self, vec: List[List[float]]):
        if self.do_normalization:
            for i in range(len(vec)):
                for j in range(len(vec[i])):
                    vec[i][j] -= self.feature_avgs[j]
                    vec[i][j] /= self.feature_stddevs[j] if self.feature_stddevs[j] != 0 else 1.0
        return vec


def maybe_half(tensor):
    if FP16_ENABLED and tensor.type().split('.')[-1] == 'FloatTensor':
        return tensor.half()
    else:
        return tensor


class FloatListTensorizer(Tensorizer):
    """Numberize numeric labels."""


    class Config(Tensorizer.Config):
        column: str
        error_check: bool = False
        dim: Optional[int] = None
        normalize: bool = False

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.column, config.error_check, config.dim, config.normalize, config.is_input)

    def __init__(self, column: str, error_check: bool, dim: Optional[int], normalize: bool, is_input: bool=Config.is_input):
        self.column = column
        self.error_check = error_check
        self.dim = dim
        assert not normalize or self.dim is not None, 'Normalization requires dim'
        assert not self.error_check or self.dim is not None, 'Error check requires dim'
        if dim is None:
            dim = 0
        self.normalizer = VectorNormalizer(dim, normalize)
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.column, List[float])]

    def initialize(self):
        if not self.normalizer.do_normalization:
            self.normalizer.calculate_feature_stats()
            return
        try:
            while True:
                row = yield
                res = row[self.column]
                self.normalizer.update_meta_data(res)
        except GeneratorExit:
            self.normalizer.calculate_feature_stats()

    def numberize(self, row):
        dense = row[self.column]
        if self.error_check:
            assert len(dense) == self.dim, f"Dense feature didn't match expected dimension {self.dim}: {dense}"
        return self.normalizer.normalize([dense])[0]

    def tensorize(self, batch):
        pad_shape = (len(batch), self.dim) if self.dim else None
        return maybe_half(pad_and_tensorize(batch, dtype=torch.float, pad_shape=pad_shape))


class ExportType(Enum):
    RIGHT = 'RIGHT'
    LEFT = 'LEFT'
    NONE = 'NONE'


def init_params(module):
    """Initialize the RoBERTa weights for pre-training from scratch."""
    if isinstance(module, torch.nn.Linear):
        module.weight.data.normal_(mean=0.0, std=0.02)
        if module.bias is not None:
            module.bias.data.zero_()
    if isinstance(module, torch.nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=0.02)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()


class ExportConfig(ConfigBase):
    export_caffe2_path: Optional[str] = None
    export_onnx_path: str = '/tmp/model.onnx'
    export_torchscript_path: Optional[str] = None
    export_lite_path: Optional[str] = None
    torchscript_quantize: Optional[bool] = False
    accelerate: List[str] = []
    inference_interface: Optional[str] = None
    seq_padding_control: Optional[List[int]] = None
    batch_padding_control: Optional[List[int]] = None
    target: str = ''


def deprecation_warning(export_conf: ExportConfig):
    if export_conf.inference_interface is not None:
        None
        raise RuntimeError('export configuration not supported')
    elif export_conf.accelerate is not None or export_conf.seq_padding_control is not None or export_conf.batch_padding_control is not None:
        msg = ['***********  DEPRECATION WARNING  **********', 'Modules concurrently supporting untokenized', 'and tokenized inputs are being deprecated!', '', 'Preferably, use the corresponding Pytext{Type}Module', "hierarchy (sans 'Script') classes to offer models", 'including tokenization.', '*********************************************']
        for line in msg:
            None


class ScriptTriTowerModule(torch.jit.ScriptModule):

    def __init__(self):
        super().__init__()
        self.model_name: str = '*no name*'
        self.model_host = ['cpu']
        self.model_type = ['nlp']
        log_class_usage(self.__class__)

    def set_name(self, name: str):
        self.model_name = name

    @torch.jit.script_method
    def get_name(self) ->str:
        return self.model_name

    def set_host(self, hostlist: List[str]):
        self.model_host = hostlist

    @torch.jit.script_method
    def check_host(self, host: str) ->bool:
        return host in self.model_host

    def set_type(self, typelist: List[str]):
        self.model_type = typelist

    @torch.jit.script_method
    def check_type(self, type: str) ->bool:
        return type in self.model_type

    @torch.jit.script_method
    def set_device(self, device: str):
        self.right_tensorizer.set_device(device)
        self.middle_tensorizer.set_device(device)
        self.left_tensorizer.set_device(device)

    @torch.jit.script_method
    def set_padding_control(self, dimension: str, control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        self.right_tensorizer.set_padding_control(dimension, control)
        self.middle_tensorizer.set_padding_control(dimension, control)
        self.left_tensorizer.set_padding_control(dimension, control)

    def validate(self, export_conf: ExportConfig):
        deprecation_warning(export_conf)


@torch.jit.script
def squeeze_1d(inputs: Optional[List[str]]) ->Optional[List[List[str]]]:
    result: Optional[List[List[str]]] = None
    if inputs is not None:
        result = torch.jit.annotate(List[List[str]], [])
        for line in inputs:
            result.append([line])
    return result


@torch.jit.script
def resolve_texts(texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None) ->Optional[List[List[str]]]:
    if texts is not None:
        return squeeze_1d(texts)
    return multi_texts


@torch.jit.script
def squeeze_2d(inputs: Optional[List[List[str]]]) ->Optional[List[List[List[str]]]]:
    result: Optional[List[List[List[str]]]] = None
    if inputs is not None:
        result = torch.jit.annotate(List[List[List[str]]], [])
        for line in inputs:
            result.append([line])
    return result


class ScriptPyTextTriTowerModule(ScriptTriTowerModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, middle_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer):
        super().__init__()
        self.model = model
        self.output_layer = output_layer
        self.right_tensorizer = right_tensorizer
        self.middle_tensorizer = middle_tensorizer
        self.left_tensorizer = left_tensorizer

    @torch.jit.script_method
    def forward(self, right_texts: Optional[List[str]]=None, middle_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, middle_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None):
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        right_input_tensors = self.right_tensorizer(right_inputs)
        middle_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(middle_texts), tokens=squeeze_2d(middle_tokens), languages=squeeze_1d(languages))
        middle_input_tensors = self.middle_tensorizer(middle_inputs)
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        left_input_tensors = self.left_tensorizer(left_inputs)
        logits = self.model(right_input_tensors, middle_input_tensors, left_input_tensors)
        return self.output_layer(logits)


class ScriptPyTextTriTowerModuleWithDense(ScriptPyTextTriTowerModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, middle_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer, right_normalizer: VectorNormalizer, middle_normalizer: VectorNormalizer, left_normalizer: VectorNormalizer):
        super().__init__(model, output_layer, right_tensorizer, middle_tensorizer, left_tensorizer)
        self.right_normalizer = right_normalizer
        self.middle_normalizer = middle_normalizer
        self.left_normalizer = left_normalizer

    @torch.jit.script_method
    def forward(self, right_dense_feat: List[List[float]], middle_dense_feat: List[List[float]], left_dense_feat: List[List[float]], right_texts: Optional[List[str]]=None, middle_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, middle_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None):
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        right_input_tensors = self.right_tensorizer(right_inputs)
        middle_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(middle_texts), tokens=squeeze_2d(middle_tokens), languages=squeeze_1d(languages))
        middle_input_tensors = self.middle_tensorizer(middle_inputs)
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        left_input_tensors = self.left_tensorizer(left_inputs)
        right_dense_feat = self.right_normalizer.normalize(right_dense_feat)
        middle_dense_feat = self.middle_normalizer.normalize(middle_dense_feat)
        left_dense_feat = self.left_normalizer.normalize(left_dense_feat)
        right_dense_tensor = torch.tensor(right_dense_feat, dtype=torch.float)
        middle_dense_tensor = torch.tensor(middle_dense_feat, dtype=torch.float)
        left_dense_tensor = torch.tensor(left_dense_feat, dtype=torch.float)
        if self.right_tensorizer.device != '':
            right_dense_tensor = right_dense_tensor
        if self.middle_tensorizer.device != '':
            middle_dense_tensor = middle_dense_tensor
        if self.left_tensorizer.device != '':
            left_dense_tensor = left_dense_tensor
        logits = self.model(right_input_tensors, middle_input_tensors, left_input_tensors, right_dense_tensor, middle_dense_tensor, left_dense_tensor)
        return self.output_layer(logits)


class ScriptTwoTowerModule(torch.jit.ScriptModule):

    def __init__(self):
        super().__init__()
        self.model_name: str = '*no name*'
        self.model_host = ['cpu']
        self.model_type = ['nlp']
        log_class_usage(self.__class__)

    def set_name(self, name: str):
        self.model_name = name

    @torch.jit.script_method
    def get_name(self) ->str:
        return self.model_name

    def set_host(self, hostlist: List[str]):
        self.model_host = hostlist

    @torch.jit.script_method
    def check_host(self, host: str) ->bool:
        return host in self.model_host

    def set_type(self, typelist: List[str]):
        self.model_type = typelist

    @torch.jit.script_method
    def check_type(self, type: str) ->bool:
        return type in self.model_type

    @torch.jit.script_method
    def set_device(self, device: str):
        self.right_tensorizer.set_device(device)
        self.left_tensorizer.set_device(device)

    @torch.jit.script_method
    def set_padding_control(self, dimension: str, control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        self.right_tensorizer.set_padding_control(dimension, control)
        self.left_tensorizer.set_padding_control(dimension, control)

    def validate(self, export_conf: ExportConfig):
        deprecation_warning(export_conf)


class ScriptPyTextTwoTowerModule(ScriptTwoTowerModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer):
        super().__init__()
        self.model = model
        self.output_layer = output_layer
        self.right_tensorizer = right_tensorizer
        self.left_tensorizer = left_tensorizer

    @torch.jit.script_method
    def forward(self, right_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None):
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        left_input_tensors = self.left_tensorizer(left_inputs)
        logits = self.model(right_input_tensors, left_input_tensors)
        return self.output_layer(logits)


class ScriptPyTextTwoTowerModuleWithDense(ScriptPyTextTwoTowerModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer, right_normalizer: VectorNormalizer, left_normalizer: VectorNormalizer):
        super().__init__(model, output_layer, right_tensorizer, left_tensorizer)
        self.right_normalizer = right_normalizer
        self.left_normalizer = left_normalizer

    @torch.jit.script_method
    def forward(self, right_dense_feat: List[List[float]], left_dense_feat: List[List[float]], right_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None):
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        left_input_tensors = self.left_tensorizer(left_inputs)
        right_dense_feat = self.right_normalizer.normalize(right_dense_feat)
        left_dense_feat = self.left_normalizer.normalize(left_dense_feat)
        right_dense_tensor = torch.tensor(right_dense_feat, dtype=torch.float)
        left_dense_tensor = torch.tensor(left_dense_feat, dtype=torch.float)
        if self.right_tensorizer.device != '':
            right_dense_tensor = right_dense_tensor
        if self.left_tensorizer.device != '':
            left_dense_tensor = left_dense_tensor
        logits = self.model(right_input_tensors, left_input_tensors, right_dense_tensor, left_dense_tensor)
        return self.output_layer(logits)


class ScriptPyTextTwoTowerModuleWithDenseTower(ScriptPyTextTwoTowerModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer, right_normalizer: VectorNormalizer, left_normalizer: VectorNormalizer):
        super().__init__(model, output_layer, right_tensorizer, left_tensorizer)
        self.right_normalizer = right_normalizer
        self.left_normalizer = left_normalizer

    @torch.jit.script_method
    def forward(self, dense_feat: List[List[float]], right_dense_feat: List[List[float]], left_dense_feat: List[List[float]], right_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None):
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        left_input_tensors = self.left_tensorizer(left_inputs)
        right_dense_feat = self.right_normalizer.normalize(right_dense_feat)
        left_dense_feat = self.left_normalizer.normalize(left_dense_feat)
        right_dense_tensor = torch.tensor(right_dense_feat, dtype=torch.float)
        left_dense_tensor = torch.tensor(left_dense_feat, dtype=torch.float)
        if self.right_tensorizer.device != '':
            right_dense_tensor = right_dense_tensor
        if self.left_tensorizer.device != '':
            left_dense_tensor = left_dense_tensor
        dense_tensor = torch.tensor(dense_feat, dtype=torch.float)
        if self.left_tensorizer.device != '':
            dense_tensor = dense_tensor
        logits = self.model(right_input_tensors, left_input_tensors, right_dense_tensor, left_dense_tensor, dense_tensor)
        return self.output_layer(logits)


class NumericLabelTensorizer(Tensorizer):
    """Numberize numeric labels."""


    class Config(Tensorizer.Config):
        column: str = 'label'
        rescale_range: Optional[List[float]] = None
        is_input: bool = False

    @classmethod
    def from_config(cls, config: Config):
        return cls(config.column, config.rescale_range, config.is_input)

    def __init__(self, label_column: str=Config.column, rescale_range: Optional[List[float]]=Config.rescale_range, is_input: bool=Config.is_input):
        self.label_column = label_column
        if rescale_range is not None:
            assert len(rescale_range) == 2
            assert rescale_range[0] < rescale_range[1]
        self.rescale_range = rescale_range
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.label_column, str)]

    def numberize(self, row):
        """Numberize labels."""
        label = float(row[self.label_column])
        if self.rescale_range is not None:
            label -= self.rescale_range[0]
            label /= self.rescale_range[1] - self.rescale_range[0]
            assert 0 <= label <= 1
        return label

    def tensorize(self, batch):
        return pad_and_tensorize(batch, dtype=torch.float)


def create_conv_package(index: int, activation: Activation, in_channels: int, out_channels: int, kernel_size: int, causal: bool, dilated: bool, separable: bool, bottleneck: int, weight_norm: bool):
    """
    Creates a convolutional layer with the specified arguments.

    Args:
        index (int): Index of a convolutional layer in the stack.
        activation (Activation): Activation function.
        in_channels (int): Number of input channels.
        out_channels (int): Number of output channels.
        kernel_size (int): Size of 1d convolutional filter.
        causal (bool): Whether the convolution is causal or not. If set, it
        accounts for the temporal ordering of the inputs.
        dilated (bool): Whether the convolution is dilated or not. If set,
        the receptive field of the convolutional stack grows exponentially.
        separable (bool): Whether to use depthwise separable convolutions
        or not -- see `SeparableConv1d`.
        bottleneck (int): Bottleneck channel dimension for depthwise separable
        convolutions. See `SeparableConv1d` for an in-depth explanation.
        weight_norm (bool): Whether to add weight normalization to the
        regular convolutions or not.

    """
    if not separable and bottleneck > 0:
        raise RuntimeError('Bottleneck layers can only be used with separable convolutions')
    if separable and weight_norm:
        raise RuntimeError('Weight normalization is not supported for separable convolutions')

    def _compute_dilation(index, dilated):
        """
        If set, the dilation factor increases by a factor of two for each
        successive convolution to increase the receptive field exponentially.

        """
        if dilated:
            return 2 ** index
        return 1

    def _compute_padding(kernel_size, dilation, causal):
        """
        Non-causal convolutions are centered, so they will consume ((k - 1) // 2) * d
        padding on both the left and the right of the sequence. Causal convolutions
        are shifted to the left (to account for temporal ordering), so they will
        only consume padding from the left. Therefore, we pad this side with the
        full amount (k - 1) * d and remove the excess right-padding with `Trim1d`.

        """
        if causal:
            return (kernel_size - 1) * dilation
        return (kernel_size - 1) // 2 * dilation

    def _compute_out_channels(out_channels, activation):
        """
        Gated Linear Unit (GLU) activations train two groups of convolutions,
        then linearly combine their outputs through a gating mechanism. We
        double the number of `out_channels` to mimic these two groups.

        """
        if activation == Activation.GLU:
            return out_channels * 2
        return out_channels
    package = []
    dilation = _compute_dilation(index, dilated)
    padding = _compute_padding(kernel_size, dilation, causal)
    out_channels = _compute_out_channels(out_channels, activation)
    if separable:
        package.append(SeparableConv1d(in_channels, out_channels, kernel_size, padding, dilation, bottleneck))
    else:
        conv = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        if weight_norm:
            conv = nn.utils.weight_norm(conv)
        package.append(conv)
    if causal:
        package.append(Trim1d(padding))
    return package[0] if len(package) == 1 else nn.Sequential(*package)


class DoNothingTokenizer(Tokenizer):
    """
    Tokenizer that takes a list of strings and converts to a list of Tokens.
    Useful in cases where tokenizer is run before-hand
    """


    class Config(Component.Config):
        do_nothing: str = ''

    @classmethod
    def from_config(cls, config: Config):
        return cls()

    def __init__(self):
        super().__init__(None)

    def tokenize(self, tokens: Union[List[str], str]) ->List[Token]:
        if isinstance(tokens, str):
            try:
                tokens = json.loads(tokens)
                if isinstance(tokens, bool):
                    tokens = [str(tokens).lower()]
            except Exception:
                tokens = [tokens]
        tokens = [Token(token_text, -1, -1) for token_text in tokens if token_text]
        return tokens

    def torchscriptify(self):
        return ScriptDoNothingTokenizer()


class SlotLabelTensorizer(Tensorizer):
    """Numberize word/slot labels."""


    class Config(Tensorizer.Config):
        slot_column: str = 'slots'
        text_column: str = 'text'
        tokenizer: Tokenizer.Config = Tokenizer.Config()
        allow_unknown: bool = False
        is_input: bool = False

    @classmethod
    def from_config(cls, config: Config):
        tokenizer = create_component(ComponentType.TOKENIZER, config.tokenizer)
        return cls(config.slot_column, config.text_column, tokenizer, config.allow_unknown, config.is_input)

    def __init__(self, slot_column: str=Config.slot_column, text_column: str=Config.text_column, tokenizer: Tokenizer=None, allow_unknown: bool=Config.allow_unknown, is_input: bool=Config.is_input):
        self.slot_column = slot_column
        self.text_column = text_column
        self.allow_unknown = allow_unknown
        self.tokenizer = tokenizer or Tokenizer()
        self.pad_idx = Padding.DEFAULT_LABEL_PAD_IDX
        self.vocab_builder = VocabBuilder()
        self.vocab_builder.add(NO_LABEL)
        self.vocab_builder.use_pad = False
        self.vocab_builder.use_unk = self.allow_unknown
        self.vocab = None
        super().__init__(is_input)

    @property
    def column_schema(self):
        return [(self.text_column, str), (self.slot_column, List[Slot])]

    def initialize(self, from_scratch=True):
        """Look through the dataset for all labels and create a vocab map for them."""
        if self.vocab and from_scratch:
            return
        try:
            while True:
                row = yield
                slots = row[self.slot_column]
                self.vocab_builder.add_all(s.label for s in slots)
        except GeneratorExit:
            self.vocab = self.vocab_builder.make_vocab()

    def numberize(self, row):
        """
        Turn slot labels and text into a list of token labels with the same
        length as the number of tokens in the text.
        """
        slots = row[self.slot_column]
        text = row[self.text_column]
        tokens = self.tokenizer.tokenize(text)
        indexed_tokens = tokens
        labels = []
        current_slot = 0
        current_token = 0
        while current_token < len(tokens) and current_slot < len(slots):
            _, start, end = indexed_tokens[current_token]
            slot = slots[current_slot]
            if start > slot.end:
                current_slot += 1
            else:
                current_token += 1
                labels.append(slot.label if end > slot.start else NO_LABEL)
        labels += [NO_LABEL] * (len(tokens) - current_token)
        return self.vocab.lookup_all(labels)

    def tensorize(self, batch):
        return pad_and_tensorize(batch, dtype=torch.long)


@torch.jit.script
def make_sequence_lengths(batch: List[List[str]]) ->List[int]:
    seq_lens = torch.jit.annotate(List[int], [])
    for sentence in batch:
        seq_lens.append(len(sentence))
    return seq_lens


@torch.jit.script
def truncate_tokens(batch: List[List[str]], max_seq_len: int, pad_token: str) ->List[List[str]]:
    truncated: List[List[str]] = []
    for sentence in batch:
        if len(sentence) == 0:
            truncated.append([pad_token])
        elif max_seq_len > 0:
            truncated.append(sentence[0:max_seq_len])
        else:
            truncated.append(sentence)
    return truncated


class CharacterEmbedding(EmbeddingBase):
    """
    Wrapper for character aware CNN embeddings for tokens.
    Attributes:
        embedding (CNNCharacterEmbedding): Character embedding
    """
    Config = CharFeatConfig

    @classmethod
    def from_config(cls, config: CharFeatConfig, metadata: Optional[FieldMeta]=None, vocab_size: Optional[int]=None):
        """Factory method to construct an instance of CharacterEmbedding from
        the module's config object and the field's metadata object.

        Args:
            config (CharFeatConfig): Configuration object specifying all the
                parameters of CharacterEmbedding.
            metadata (FieldMeta): Object containing this field's metadata.

        Returns:
            type: An instance of CharacterEmbedding.

        """
        if vocab_size is None:
            vocab_size = metadata.vocab_size
        if config.highway_layers is not None:
            warnings.warn('Specifying highway_layers is deprecated, use ConnectionConfig instead.', DeprecationWarning)
            highway_layers = config.highway_layers
            resmlp_layers = 0
            resmlp_dropout = 0
        elif config.connection.connection_type == 'highway':
            highway_layers = config.connection.num_layers
            resmlp_layers = 0
            resmlp_dropout = 0
        elif config.connection.connection_type == 'resmlp':
            highway_layers = 0
            resmlp_layers = config.connection.num_layers
            resmlp_dropout = config.connection.dropout
        else:
            raise NotImplementedError("Connection type should be either 'highway' or 'resmlp'.")
        return cls(vocab_size, config.embed_dim, config.cnn.kernel_num, config.cnn.kernel_sizes, highway_layers, config.projection_dim, resmlp_layers, resmlp_dropout)

    def __init__(self, num_embeddings: int, embed_dim: int, out_channels: int, kernel_sizes: List[int], highway_layers: int, projection_dim: Optional[int], resmlp_layers: int=0, resmlp_dropout: float=0.1, *args, **kwargs) ->None:
        output_dim = CNNCharacterEmbedding.output_dim(num_kernels=len(kernel_sizes), out_channels=out_channels, projection_dim=projection_dim)
        super().__init__(output_dim)
        self.embedding = CNNCharacterEmbedding(num_embeddings=num_embeddings, embed_dim=embed_dim, out_channels=out_channels, kernel_sizes=kernel_sizes, highway_layers=highway_layers, projection_dim=projection_dim, resmlp_layers=resmlp_layers, resmlp_dropout=resmlp_dropout)
        log_class_usage(__class__)

    def forward(self, chars: torch.Tensor, precomputed_embeddings: Optional[torch.Tensor]=None) ->torch.Tensor:
        """See CNNCharacterEmbedding.forward() for details"""
        return self.embedding(chars, precomputed_embeddings=precomputed_embeddings)

    def load_state_dict(self, loaded_module: nn.Module) ->None:
        """Add backward compatibility to load CharEmbedding from older versions
        In older versions, state dict keys are like "char_embed.weight", "convs.0.weight", etc
        In newer versions, state dict keys are like "embedding.char_embed.weight", "embedding.convs.0.weight"
        """
        try:
            super().load_state_dict(loaded_module)
        except RuntimeError:
            self.embedding.load_state_dict(loaded_module)

    def get_embedding_weights(self):
        """
        Returns the embedding weights of size (num_embeddings, embed_dim).
        """
        return self.embedding.char_embed.weight


@torch.jit.script
def make_byte_inputs(batch: List[List[str]], max_byte_len: int, offset_for_non_padding: int=0) ->Tuple[torch.Tensor, torch.Tensor]:
    seq_lens = make_sequence_lengths(batch)
    max_num_tokens = max(seq_lens)
    bytes = torch.zeros(len(batch), max_num_tokens, max_byte_len, dtype=torch.long)
    for batch_index in range(len(batch)):
        sentence = batch[batch_index]
        for token_index in range(len(sentence)):
            token = sentence[token_index]
            for byte_index in range(min(len(token), max_byte_len)):
                s = token[byte_index]
                if s == '':
                    v = 256
                else:
                    v = ord(s)
                bytes[batch_index][token_index][byte_index] = v + offset_for_non_padding
    return bytes, torch.tensor(seq_lens)


def clip_list(input: Optional[List[str]], max_batch: int) ->Optional[List[str]]:
    if input is not None:
        return input[max_batch:]
    return None


def clip_listlist(input: Optional[List[List[str]]], max_batch: int) ->Optional[List[List[str]]]:
    if input is not None:
        return input[max_batch:]
    return None


def clip_listlist_float(input: Optional[List[List[float]]], max_batch: int) ->Optional[List[List[float]]]:
    if input is not None:
        return input[max_batch:]
    return None


def destructure_any_list(client_batch: List[int], result_any_list: List[Any]):
    res_list: List[List[Any]] = []
    start = 0
    for elems in client_batch:
        torch._assert(elems > 0, 'zero or negative range error')
        end = start + elems
        res_list.append(result_any_list[start:end])
        start = end
    return res_list


def destructure_tensor(client_batch: List[int], result_tensor: torch.Tensor) ->List[torch.Tensor]:
    start = 0
    res_list: List[torch.Tensor] = []
    for elems in client_batch:
        end = start + elems
        res_list.append(result_tensor.narrow(0, start, elems))
        start = end
    return res_list


def input_size(texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None) ->int:
    texts_: List[str] = []
    multi_texts_: List[List[str]] = []
    tokens_: List[List[str]] = []
    if texts is not None:
        texts_ = texts
    if multi_texts is not None:
        multi_texts_ = multi_texts
    if tokens is not None:
        tokens_ = tokens
    input_len = max([len(texts_), len(multi_texts_), len(tokens_)])
    return input_len


def limit_list(input: Optional[List[str]], max_batch: int) ->Optional[List[str]]:
    if input is not None:
        return input[:max_batch]
    return None


def limit_listlist(input: Optional[List[List[str]]], max_batch: int) ->Optional[List[List[str]]]:
    if input is not None:
        return input[:max_batch]
    return None


def limit_listlist_float(input: Optional[List[List[float]]], max_batch: int) ->Optional[List[List[float]]]:
    if input is not None:
        return input[:max_batch]
    return None


def max_tokens(per_sentence_tokens: List[List[Tuple[str, int, int]]]) ->int:
    """receive the tokenize output for a batch per_sentence_tokens,
    return the max token length of any sentence"""
    if len(per_sentence_tokens) == 0:
        return 0
    sentence_lengths = [len(sentence) for sentence in per_sentence_tokens]
    return max(sentence_lengths)


def nonify_listlist_float(input: List[List[float]]) ->Optional[List[List[float]]]:
    if len(input) == 0:
        return None
    return input


def validate_dense_feat(batch_element_dense_feat: Optional[List[List[float]]], length: int, uses_dense_feat: bool) ->List[List[float]]:
    if batch_element_dense_feat is not None:
        if not uses_dense_feat:
            raise RuntimeError('Dense feature (dense_feat) not allowed for this model type')
        if len(batch_element_dense_feat) != length:
            raise RuntimeError('Malformed request.  Length of client-batch dense_feat argument does not match length of text argument.')
        return batch_element_dense_feat
    elif uses_dense_feat:
        raise RuntimeError('Dense feature (dense_feat) is required for this model type, but not present (received dense_feat=None) in this request.')
    else:
        return []


def validate_make_prediction_batch_element(be: Tuple[Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]]]):
    if be[0] is not None:
        if be[1] is not None or be[2] is not None:
            raise RuntimeError('only one of texts, multi_texts, tokens can be not None.')
    elif be[1] is not None:
        if be[2] is not None:
            raise RuntimeError('only one of texts, multi_texts, tokens can be not None.')
    if be[3] is not None:
        raise RuntimeError('currently, languages != None is not supported.')


def zip_batch_any_list_list(zip_batch_list: List[int], result_list_1: List[List[Any]], result_list_2: List[List[Any]]) ->List[List[Any]]:
    res_list: List[List[Any]] = torch.jit.annotate(List[List[Any]], [])
    elem1 = 0
    elem2 = 0
    for zipper in zip_batch_list:
        if zipper > 0:
            res_list.append(result_list_1[elem1])
            elem1 = elem1 + 1
        else:
            res_list.append(result_list_1[elem2])
            elem2 = elem2 + 1
    return res_list


def zip_batch_tensor_list(zip_batch_list: List[int], result_list_1: List[torch.Tensor], result_list_2: List[torch.Tensor]) ->List[torch.Tensor]:
    res_list: List[torch.Tensor] = torch.jit.annotate(List[torch.Tensor], [])
    elem1 = 0
    elem2 = 0
    for zipper in zip_batch_list:
        if zipper > 0:
            res_list.append(result_list_1[elem1])
            elem1 = elem1 + 1
        else:
            res_list.append(result_list_1[elem2])
            elem2 = elem2 + 1
    return res_list


class ScriptPyTextEmbeddingModule(torch.jit.ScriptModule):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer):
        super().__init__()
        self.model = model
        self.tensorizer = tensorizer
        self.model_name: str = '*no name*'
        self.model_host = ['cpu']
        self.model_type = ['nlp']
        log_class_usage(self.__class__)

    def set_name(self, name: str):
        self.model_name = name

    @torch.jit.script_method
    def get_name(self) ->str:
        return self.model_name

    def set_host(self, hostlist: List[str]):
        self.model_host = hostlist

    @torch.jit.script_method
    def check_host(self, host: str) ->bool:
        return host in self.model_host

    def set_type(self, typelist: List[str]):
        self.model_type = typelist

    @torch.jit.script_method
    def check_type(self, type: str) ->bool:
        return type in self.model_type

    def validate(self, export_conf: ExportConfig):
        deprecation_warning(export_conf)

    @torch.jit.script_method
    def set_device(self, device: str):
        self.tensorizer.set_device(device)

    @torch.jit.script_method
    def get_max_seq_len(self) ->int:
        """
        This function returns the maximum sequence length for the model,
        if it is defined, otherwise raises a Runtime Error.
        """
        if hasattr(self.tensorizer, 'max_seq_len'):
            if self.tensorizer.max_seq_len is not None:
                return self.tensorizer.max_seq_len
        raise RuntimeError('max_seq_len not defined')

    @torch.jit.script_method
    def get_max_batch_len(self) ->int:
        """
        This function returns the maximum batch length for the model,
        if it is defined, otherwise -1.
        """
        if hasattr(self.tensorizer, 'batch_padding_control'):
            batch_padding_control = self.tensorizer.batch_padding_control
            if batch_padding_control is not None:
                return batch_padding_control[-1]
        return -1

    @torch.jit.script_method
    def set_padding_control(self, dimension: str, control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        self.tensorizer.set_padding_control(dimension, control)

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return False

    @torch.jit.script_method
    def forward_validate_dense_feat(self, dense_feat: Optional[List[List[float]]]) ->List[List[float]]:
        if self.uses_dense_feat():
            if dense_feat is None:
                raise RuntimeError('Dense feature (dense_feat) is required for this model type, but not present.')
            else:
                return dense_feat
        elif dense_feat is not None:
            raise RuntimeError('Dense feature (dense_feat) not allowed for this model type')
        else:
            return []

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput):
        input_tensors = self.tensorizer(inputs)
        return self.model(input_tensors).cpu()

    @torch.jit.script_method
    def forward_impl(self, texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, dense_feat: Optional[List[List[float]]]=None) ->torch.Tensor:
        self.forward_validate_dense_feat(dense_feat)
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, multi_texts), tokens=squeeze_2d(tokens), languages=squeeze_1d(languages))
        return self._forward(inputs)

    @torch.jit.script_method
    def forward(self, texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, dense_feat: Optional[List[List[float]]]=None):
        self.forward_validate_dense_feat(dense_feat)
        input_len = input_size(texts, multi_texts, tokens)
        max_batch = self.get_max_batch_len()
        if max_batch <= 0:
            max_batch = input_len
        result = self.forward_impl(limit_list(texts, max_batch), limit_listlist(multi_texts, max_batch), limit_listlist(tokens, max_batch), limit_list(languages, max_batch), limit_listlist_float(dense_feat, max_batch))
        if input_len > max_batch:
            texts = clip_list(texts, max_batch)
            multi_texts = clip_listlist(multi_texts, max_batch)
            tokens = clip_listlist(tokens, max_batch)
            languages = clip_list(languages, max_batch)
            dense_feat = clip_listlist_float(dense_feat, max_batch)
            while input_size(texts, multi_texts, tokens) > 0:
                result_extension = self.forward_impl(limit_list(texts, max_batch), limit_listlist(multi_texts, max_batch), limit_listlist(tokens, max_batch), limit_list(languages, max_batch), limit_listlist_float(dense_feat, max_batch))
                if isinstance(result, torch.Tensor):
                    result = torch.cat([result, result_extension], dim=0)
                else:
                    result.extend(result_extension)
                texts = clip_list(texts, max_batch)
                multi_texts = clip_listlist(multi_texts, max_batch)
                tokens = clip_listlist(tokens, max_batch)
                languages = clip_list(languages, max_batch)
                dense_feat = clip_listlist_float(dense_feat, max_batch)
        if isinstance(result, torch.Tensor):
            torch._assert(input_len == result.size()[0], 'Tensor output size must match input size')
        else:
            torch._assert(input_len == len(result), 'List output size must match input size')
        return result

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]]]]):
        batchsize = len(batch)
        client_batch_texts: List[int] = []
        client_batch_tokens: List[int] = []
        zip_batch_list: List[int] = []
        flat_texts: List[str] = []
        flat_tokens: List[List[str]] = []
        flat_dense_feat_texts: List[List[float]] = []
        flat_dense_feat_tokens: List[List[float]] = []
        for i in range(batchsize):
            validate_make_prediction_batch_element(batch[i])
            batch_element_texts = batch[i][0]
            batch_element_tokens = batch[i][2]
            batch_element_dense_feat = batch[i][4]
            if batch_element_texts is not None:
                flat_texts.extend(batch_element_texts)
                client_batch_texts.append(len(batch_element_texts))
                flat_dense_feat_texts.extend(validate_dense_feat(batch_element_dense_feat, len(batch_element_texts), self.uses_dense_feat()))
                zip_batch_list.append(1)
            elif batch_element_tokens is not None:
                flat_tokens.extend(batch_element_tokens)
                client_batch_tokens.append(len(batch_element_tokens))
                flat_dense_feat_tokens.extend(validate_dense_feat(batch_element_dense_feat, len(batch_element_tokens), self.uses_dense_feat()))
                zip_batch_list.append(-1)
            else:
                raise RuntimeError('Malformed request.')
        if len(flat_texts) == 0 and len(flat_tokens) == 0:
            raise RuntimeError('This is not good. Empty request batch.')
        if len(flat_texts) > 0 and len(flat_tokens) > 0:
            raise RuntimeError('Mixing tokens and texts not supported in this service.')
        elif len(flat_texts) > 0:
            flat_result_texts = self.forward(texts=flat_texts, multi_texts=None, tokens=None, languages=None, dense_feat=nonify_listlist_float(flat_dense_feat_texts))
            flat_result_tokens = flat_result_texts
        else:
            flat_result_tokens = self.forward(texts=None, multi_texts=None, tokens=flat_tokens, languages=None, dense_feat=nonify_listlist_float(flat_dense_feat_tokens))
            flat_result_texts = flat_result_tokens
        if isinstance(flat_result_tokens, torch.Tensor):
            return zip_batch_tensor_list(zip_batch_list, destructure_tensor(client_batch_texts, flat_result_texts), destructure_tensor(client_batch_tokens, flat_result_tokens))
        else:
            result_texts_any_list: List[Any] = torch.jit.annotate(List[Any], [])
            for v in flat_result_texts:
                result_texts_any_list.append(v)
            result_tokens_any_list: List[Any] = torch.jit.annotate(List[Any], [])
            for v in flat_result_tokens:
                result_tokens_any_list.append(v)
            return zip_batch_any_list_list(zip_batch_list, destructure_any_list(client_batch_texts, result_texts_any_list), destructure_any_list(client_batch_tokens, result_tokens_any_list))

    @torch.jit.script_method
    def make_batch(self, mega_batch: List[Tuple[Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], int]], goals: Dict[str, str]) ->List[List[Tuple[Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], int]]]:
        mega_batch_key_list = [(max_tokens(self.tensorizer.tokenize(x[0], x[2])), n) for n, x in enumerate(mega_batch)]
        sorted_mega_batch_key_list = sorted(mega_batch_key_list)
        sorted_mega_batch = [mega_batch[n] for key, n in sorted_mega_batch_key_list]
        max_bs = int(goals.get('batchsize', '4'))
        len_mb = len(mega_batch)
        num_batches = (len_mb + max_bs - 1) // max_bs
        batch_list: List[List[Tuple[Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], int]]] = []
        start = 0
        for _i in range(num_batches):
            end = min(start + max_bs, len_mb)
            batch_list.append(sorted_mega_batch[start:end])
            start = end
        return batch_list


class ScriptPyTextEmbeddingModuleIndex(ScriptPyTextEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, index: int=0):
        super().__init__(model, tensorizer)
        self.index: int = index
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return False

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput):
        input_tensors = self.tensorizer(inputs)
        return self.model(input_tensors)[self.index].cpu()


class ScriptPyTextModule(ScriptPyTextEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, tensorizer: ScriptTensorizer):
        super().__init__(model, tensorizer)
        self.output_layer = output_layer

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return False

    @torch.jit.script_method
    def forward_impl(self, texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, dense_feat: Optional[List[List[float]]]=None):
        self.forward_validate_dense_feat(dense_feat)
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, multi_texts), tokens=squeeze_2d(tokens), languages=squeeze_1d(languages))
        input_tensors = self.tensorizer(inputs)
        logits = self.model(input_tensors)
        return self.output_layer(logits)


class ScriptPyTextEmbeddingModuleWithDense(ScriptPyTextEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, normalizer: VectorNormalizer, concat_dense: bool=False):
        super().__init__(model, tensorizer)
        self.normalizer = normalizer
        self.concat_dense = torch.jit.Attribute(concat_dense, bool)
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return True

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput, dense_tensor: torch.Tensor):
        input_tensors = self.tensorizer(inputs)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        return self.model(input_tensors, dense_tensor).cpu()

    @torch.jit.script_method
    def forward_impl(self, texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, dense_feat: Optional[List[List[float]]]=None) ->torch.Tensor:
        dense_feat = self.forward_validate_dense_feat(dense_feat)
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, multi_texts), tokens=squeeze_2d(tokens), languages=squeeze_1d(languages))
        dense_feat = self.normalizer.normalize(dense_feat)
        dense_tensor = torch.tensor(dense_feat, dtype=torch.float)
        sentence_embedding = self._forward(inputs, dense_tensor)
        if self.concat_dense:
            return torch.cat([sentence_embedding, dense_tensor], 1)
        else:
            return sentence_embedding


class ScriptPyTextModuleWithDense(ScriptPyTextEmbeddingModuleWithDense):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, normalizer: VectorNormalizer):
        super().__init__(model, tensorizer, normalizer)
        self.output_layer = output_layer
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return True

    @torch.jit.script_method
    def forward_impl(self, texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, dense_feat: Optional[List[List[float]]]=None):
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, multi_texts), tokens=squeeze_2d(tokens), languages=squeeze_1d(languages))
        input_tensors = self.tensorizer(inputs)
        dense_feat = self.normalizer.normalize(self.forward_validate_dense_feat(dense_feat))
        dense_tensor = torch.tensor(dense_feat, dtype=torch.float)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        logits = self.model(input_tensors, dense_tensor)
        return self.output_layer(logits)


class ScriptPyTextEmbeddingModuleWithDenseIndex(ScriptPyTextEmbeddingModuleWithDense):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, normalizer: VectorNormalizer, index: int=0, concat_dense: bool=True):
        super().__init__(model, tensorizer, normalizer, concat_dense)
        self.index = torch.jit.Attribute(index, int)
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return True

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput, dense_tensor: torch.Tensor):
        input_tensors = self.tensorizer(inputs)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        return self.model(input_tensors, dense_tensor)[self.index].cpu()


class ScriptPyTextVariableSizeEmbeddingModule(ScriptPyTextEmbeddingModule):
    """
    Assumes model returns a tuple of representations and sequence lengths, then slices
    each example's representation according to length. Returns a list of tensors. The
    slicing is easier to do outside a traced model.
    """

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer):
        super().__init__(model, tensorizer)
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def uses_dense_feat(self) ->bool:
        return False

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput):
        input_tensors = self.tensorizer(inputs)
        reps, seq_lens = self.model(input_tensors)
        reps = reps.cpu()
        seq_lens = seq_lens.cpu()
        return [reps[i, :seq_lens[i]] for i in range(len(seq_lens))]

    @torch.jit.script_method
    def forward_impl(self, texts: Optional[List[str]]=None, multi_texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, dense_feat: Optional[List[List[float]]]=None) ->List[torch.Tensor]:
        self.forward_validate_dense_feat(dense_feat)
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, multi_texts), tokens=squeeze_2d(tokens), languages=squeeze_1d(languages))
        return self._forward(inputs)


class ScriptPyTextTwoTowerEmbeddingModule(ScriptTwoTowerModule):

    def __init__(self, model: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer):
        super().__init__()
        self.model = model
        self.right_tensorizer = right_tensorizer
        self.left_tensorizer = left_tensorizer
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, right_inputs: ScriptBatchInput, left_inputs: ScriptBatchInput):
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_input_tensors = self.left_tensorizer(left_inputs)
        return self.model(right_input_tensors, left_input_tensors).cpu()

    @torch.jit.script_method
    def forward(self, right_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None) ->torch.Tensor:
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        return self._forward(right_inputs, left_inputs)

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[Optional[List[str]], Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], Optional[List[List[float]]]]]) ->List[torch.Tensor]:
        argno = -1
        if argno == -1:
            raise RuntimeError('Argument number not specified during export.')
        batchsize = len(batch)
        TEXTS = 0
        client_batch: List[int] = []
        if argno == TEXTS:
            flat_right_texts: List[str] = []
            flat_left_texts: List[str] = []
            for i in range(batchsize):
                batch_right_element = batch[i][0]
                batch_left_element = batch[i][1]
                if batch_right_element is not None:
                    flat_right_texts.extend(batch_right_element)
                    client_batch.append(len(batch_right_element))
                else:
                    raise RuntimeError('Malformed request.')
                if batch_left_element is not None:
                    flat_left_texts.extend(batch_left_element)
                else:
                    raise RuntimeError('Malformed request.')
            flat_result: torch.Tensor = self.forward(right_texts=flat_right_texts, left_texts=flat_left_texts, right_tokens=None, left_tokens=None, languages=None, right_dense_feat=None, left_dense_feat=None)
        else:
            raise RuntimeError('Parameter type unsupported')
        return destructure_tensor(client_batch, flat_result)

    @torch.jit.script_method
    def make_batch(self, mega_batch: List[Tuple[Optional[List[str]], Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], Optional[List[List[float]]], int]], goals: Dict[str, str]) ->List[List[Tuple[Optional[List[str]], Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], Optional[List[List[float]]], int]]]:
        argno = -1
        if argno == -1:
            raise RuntimeError('Argument number not specified during export.')
        mega_batch_key_list = [(max_tokens(self.right_tensorizer.tokenize(x[0], x[2])), n) for n, x in enumerate(mega_batch)]
        sorted_mega_batch_key_list = sorted(mega_batch_key_list)
        sorted_mega_batch = [mega_batch[n] for key, n in sorted_mega_batch_key_list]
        max_bs = int(goals.get('batchsize', '4'))
        len_mb = len(mega_batch)
        num_batches = (len_mb + max_bs - 1) // max_bs
        batch_list: List[List[Tuple[Optional[List[str]], Optional[List[str]], Optional[List[List[str]]], Optional[List[List[str]]], Optional[List[str]], Optional[List[List[float]]], Optional[List[List[float]]], int]]] = []
        start = 0
        for _i in range(num_batches):
            end = min(start + max_bs, len_mb)
            batch_list.append(sorted_mega_batch[start:end])
            start = end
        return batch_list


class ScriptPyTextTwoTowerEmbeddingModuleWithDense(ScriptPyTextTwoTowerEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer, right_normalizer: VectorNormalizer, left_normalizer: VectorNormalizer):
        super().__init__(model, right_tensorizer, left_tensorizer)
        self.right_normalizer = right_normalizer
        self.left_normalizer = left_normalizer
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, right_inputs: ScriptBatchInput, left_inputs: ScriptBatchInput, right_dense_tensor: torch.Tensor, left_dense_tensor: torch.Tensor):
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_input_tensors = self.left_tensorizer(left_inputs)
        if self.right_tensorizer.device != '':
            right_dense_tensor = right_dense_tensor
        if self.left_tensorizer.device != '':
            left_dense_tensor = left_dense_tensor
        return self.model(right_input_tensors, left_input_tensors, right_dense_tensor, left_dense_tensor).cpu()

    @torch.jit.script_method
    def forward(self, right_texts: Optional[List[str]]=None, left_texts: Optional[List[str]]=None, right_tokens: Optional[List[List[str]]]=None, left_tokens: Optional[List[List[str]]]=None, languages: Optional[List[str]]=None, right_dense_feat: Optional[List[List[float]]]=None, left_dense_feat: Optional[List[List[float]]]=None) ->torch.Tensor:
        if right_dense_feat is None or left_dense_feat is None:
            raise RuntimeError('Expect dense feature.')
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(right_tokens), languages=squeeze_1d(languages))
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(left_tokens), languages=squeeze_1d(languages))
        right_dense_feat = self.right_normalizer.normalize(right_dense_feat)
        left_dense_feat = self.left_normalizer.normalize(left_dense_feat)
        right_dense_tensor = torch.tensor(right_dense_feat, dtype=torch.float)
        left_dense_tensor = torch.tensor(left_dense_feat, dtype=torch.float)
        sentence_embedding = self._forward(right_inputs, left_inputs, right_dense_tensor, left_dense_tensor)
        return sentence_embedding


def make_prediction_texts(batch: List[Tuple[List[str],]]) ->List[str]:
    batchsize = len(batch)
    if batchsize == 0:
        raise RuntimeError('Input batch must have at least 1 batch element')
    flat_texts: List[str] = []
    for i in range(batchsize):
        batch_element = batch[i][0]
        flat_texts.extend(batch_element)
    if len(flat_texts) == 0:
        raise RuntimeError('This is not good. Empty request batch.')
    return flat_texts


class PyTextEmbeddingModule(torch.jit.ScriptModule):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer):
        super().__init__()
        self.model = model
        self.tensorizer = tensorizer
        self.model_name: str = '*no name*'
        self.model_host = ['cpu']
        self.model_type = ['nlp']
        log_class_usage(self.__class__)

    def set_name(self, name: str):
        self.model_name = name

    @torch.jit.script_method
    def get_name(self) ->str:
        return self.model_name

    def set_host(self, hostlist: List[str]):
        self.model_host = hostlist

    @torch.jit.script_method
    def check_host(self, host: str) ->bool:
        return host in self.model_host

    def set_type(self, typelist: List[str]):
        self.model_type = typelist

    @torch.jit.script_method
    def check_type(self, type: str) ->bool:
        return type in self.model_type

    @torch.jit.script_method
    def set_device(self, device: str):
        self.tensorizer.set_device(device)

    @torch.jit.script_method
    def set_padding_control(self, dimension: str, control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        self.tensorizer.set_padding_control(dimension, control)

    @torch.jit.script_method
    def get_max_seq_len(self) ->int:
        """
        This function returns the maximum sequence length for the model,
        if it is defined, otherwise None.
        """
        if hasattr(self.tensorizer, 'max_seq_len'):
            if self.tensorizer.max_seq_len is not None:
                return self.tensorizer.max_seq_len
        raise RuntimeError('max_seq_len not defined')

    @torch.jit.script_method
    def get_max_batch_len(self) ->int:
        """
        This function returns the maximum batch length for the model,
        if it is defined, otherwise -1.
        """
        if hasattr(self.tensorizer, 'batch_padding_control'):
            batch_padding_control = self.tensorizer.batch_padding_control
            if batch_padding_control is not None:
                return batch_padding_control[-1]
        return -1

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput):
        input_tensors = self.tensorizer(inputs)
        return self.model(input_tensors).cpu()

    @torch.jit.script_method
    def forward_impl(self, texts: List[str]) ->torch.Tensor:
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, None), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        return self._forward(inputs)

    @torch.jit.script_method
    def forward(self, texts: List[str]) ->torch.Tensor:
        input_len = len(texts)
        max_batch = self.get_max_batch_len()
        if max_batch < 0:
            max_batch = input_len
        result = self.forward_impl(texts[:max_batch])
        if input_len > max_batch:
            texts = texts[max_batch:]
            while len(texts) > 0:
                result_extension = self.forward_impl(texts[:max_batch])
                if isinstance(result, torch.Tensor):
                    result = torch.cat([result, result_extension], dim=0)
                else:
                    result.extend(result_extension)
                texts = texts[max_batch:]
        return result

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[List[str],]]) ->List[torch.Tensor]:
        flat_result: torch.Tensor = self.forward(texts=make_prediction_texts(batch))
        return destructure_tensor([len(be[0]) for be in batch], flat_result)

    @torch.jit.script_method
    def make_batch(self, mega_batch: List[Tuple[List[str], int]], goals: Dict[str, str]) ->List[List[Tuple[List[str], int]]]:
        batchsize = len(mega_batch)
        if batchsize == 0:
            raise RuntimeError('Input batch must have at least 1 batch element')
        mega_batch_key_list = [(max_tokens(self.tensorizer.tokenize(x[0], None)), n) for n, x in enumerate(mega_batch)]
        sorted_mega_batch_key_list = sorted(mega_batch_key_list)
        sorted_mega_batch = [mega_batch[n] for _, n in sorted_mega_batch_key_list]
        max_bs = int(goals.get('batchsize', '4'))
        len_mb = len(mega_batch)
        num_batches = (len_mb + max_bs - 1) // max_bs
        batch_list: List[List[Tuple[List[str], int]]] = []
        start = 0
        for _i in range(num_batches):
            end = min(start + max_bs, len_mb)
            batch_list.append(sorted_mega_batch[start:end])
            start = end
        return batch_list


class PyTextLayerModule(PyTextEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, tensorizer: ScriptTensorizer):
        super().__init__(model, tensorizer)
        self.output_layer = output_layer

    @torch.jit.script_method
    def forward_impl(self, texts: List[str]):
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, None), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        input_tensors = self.tensorizer(inputs)
        logits = self.model(input_tensors)
        return self.output_layer(logits)


class PyTextEmbeddingModuleIndex(PyTextEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, index: int=0):
        super().__init__(model, tensorizer)
        self.index = torch.jit.Attribute(index, int)
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput):
        input_tensors = self.tensorizer(inputs)
        return self.model(input_tensors)[self.index].cpu()


def make_batch_texts_dense(tensorizer: ScriptTensorizer, mega_batch: List[Tuple[List[str], List[List[float]], int]], goals: Dict[str, str]) ->List[List[Tuple[List[str], List[List[float]], int]]]:
    batchsize = len(mega_batch)
    if batchsize == 0:
        raise RuntimeError('Input batch must have at least 1 batch element')
    mega_batch_key_list = [(max_tokens(tensorizer.tokenize(x[0], None)), n) for n, x in enumerate(mega_batch)]
    sorted_mega_batch_key_list = sorted(mega_batch_key_list)
    sorted_mega_batch = [mega_batch[n] for _, n in sorted_mega_batch_key_list]
    max_bs = int(goals.get('batchsize', '4'))
    len_mb = len(mega_batch)
    num_batches = (len_mb + max_bs - 1) // max_bs
    batch_list: List[List[Tuple[List[str], int]]] = []
    start = 0
    for _i in range(num_batches):
        end = min(start + max_bs, len_mb)
        batch_list.append(sorted_mega_batch[start:end])
        start = end
    return batch_list


def make_prediction_texts_dense(batch: List[Tuple[List[str], List[List[float]]]]) ->Tuple[List[str], List[List[float]]]:
    batchsize = len(batch)
    if batchsize == 0:
        raise RuntimeError('Input batch must have at least 1 batch element')
    flat_texts: List[str] = []
    flat_dense: List[List[float]] = []
    for i in range(batchsize):
        texts_element = batch[i][0]
        flat_texts.extend(texts_element)
        dense_element = batch[i][1]
        flat_dense.extend(dense_element)
        if len(texts_element) != len(dense_element):
            raise RuntimeError('This is not good. texts/dense client batch length mismatch')
    if len(flat_texts) == 0:
        raise RuntimeError('This is not good. Empty request batch.')
    return flat_texts, flat_dense


class PyTextEmbeddingModuleWithDense(PyTextEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, normalizer: VectorNormalizer, concat_dense: bool=False):
        super().__init__(model, tensorizer)
        self.normalizer = normalizer
        self.concat_dense: bool = concat_dense
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput, dense_tensor: torch.Tensor):
        input_tensors = self.tensorizer(inputs)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        return self.model(input_tensors, dense_tensor).cpu()

    @torch.jit.script_method
    def forward_impl(self, texts: List[str], dense_feat: List[List[float]]) ->torch.Tensor:
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, None), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        dense_feat = self.normalizer.normalize(dense_feat)
        dense_tensor = torch.tensor(dense_feat, dtype=torch.float)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        sentence_embedding = self._forward(inputs, dense_tensor)
        if self.concat_dense:
            return torch.cat([sentence_embedding, dense_tensor], 1)
        else:
            return sentence_embedding

    @torch.jit.script_method
    def forward(self, texts: List[str], dense_feat: List[List[float]]) ->torch.Tensor:
        input_len = len(texts)
        max_batch = self.get_max_batch_len()
        if max_batch < 0:
            max_batch = input_len
        result = self.forward_impl(texts[:max_batch], dense_feat[:max_batch])
        if input_len > max_batch:
            texts = texts[max_batch:]
            dense_feat = dense_feat[max_batch:]
            while len(texts) > 0:
                result_extension = self.forward_impl(texts[:max_batch], dense_feat[:max_batch])
                if isinstance(result, torch.Tensor):
                    result = torch.cat([result, result_extension], dim=0)
                else:
                    result.extend(result_extension)
                texts = texts[max_batch:]
                dense_feat = dense_feat[max_batch:]
        return result

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[List[str], List[List[float]]]]) ->List[torch.Tensor]:
        flat_texts, flat_dense = make_prediction_texts_dense(batch)
        flat_result: torch.Tensor = self.forward(texts=flat_texts, dense_feat=flat_dense)
        return destructure_tensor([len(be[0]) for be in batch], flat_result)

    @torch.jit.script_method
    def make_batch(self, mega_batch: List[Tuple[List[str], List[List[float]], int]], goals: Dict[str, str]) ->List[List[Tuple[List[str], List[List[float]], int]]]:
        return make_batch_texts_dense(self.tensorizer, mega_batch, goals)


class PyTextLayerModuleWithDense(PyTextEmbeddingModuleWithDense):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, normalizer: VectorNormalizer):
        super().__init__(model, tensorizer, normalizer)
        self.output_layer = output_layer
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def forward_impl(self, texts: List[str], dense_feat: List[List[float]]):
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, None), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        input_tensors = self.tensorizer(inputs)
        dense_feat = self.normalizer.normalize(dense_feat)
        dense_tensor = torch.tensor(dense_feat, dtype=torch.float)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        logits = self.model(input_tensors, dense_tensor)
        return self.output_layer(logits)


class PyTextEmbeddingModuleWithDenseIndex(PyTextEmbeddingModuleWithDense):

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer, normalizer: VectorNormalizer, index: int=0, concat_dense: bool=True):
        super().__init__(model, tensorizer, normalizer, concat_dense)
        self.index = torch.jit.Attribute(index, int)
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput, dense_tensor: torch.Tensor):
        input_tensors = self.tensorizer(inputs)
        if self.tensorizer.device != '':
            dense_tensor = dense_tensor
        return self.model(input_tensors, dense_tensor)[self.index].cpu()


def destructure_tensor_list(client_batch: List[int], result_tensor_list: List[torch.Tensor]) ->List[List[torch.Tensor]]:
    res_list: List[List[torch.Tensor]] = []
    start = 0
    for elems in client_batch:
        end = start + elems
        res_list.append(result_tensor_list[start:end])
        start = end
    return res_list


class PyTextVariableSizeEmbeddingModule(PyTextEmbeddingModule):
    """
    Assumes model returns a tuple of representations and sequence lengths, then slices
    each example's representation according to length. Returns a list of tensors. The
    slicing is easier to do outside a traced model.
    """

    def __init__(self, model: torch.jit.ScriptModule, tensorizer: ScriptTensorizer):
        super().__init__(model, tensorizer)
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, inputs: ScriptBatchInput):
        input_tensors = self.tensorizer(inputs)
        reps, seq_lens = self.model(input_tensors)
        reps = reps.cpu()
        seq_lens = seq_lens.cpu()
        return [reps[i, :seq_lens[i]] for i in range(len(seq_lens))]

    @torch.jit.script_method
    def forward_impl(self, texts: List[str]) ->List[torch.Tensor]:
        inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(texts, None), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        return self._forward(inputs)

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[List[str],]]) ->List[List[torch.Tensor]]:
        flat_result: List[torch.Tensor] = self.forward(texts=make_prediction_texts(batch))
        return destructure_tensor_list([len(be[0]) for be in batch], flat_result)


class PyTextTwoTowerEmbeddingModule(torch.jit.ScriptModule):

    def __init__(self, model: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer):
        super().__init__()
        self.model = model
        self.right_tensorizer = right_tensorizer
        self.left_tensorizer = left_tensorizer
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def set_device(self, device: str):
        self.right_tensorizer.set_device(device)
        self.left_tensorizer.set_device(device)

    @torch.jit.script_method
    def set_padding_control(self, dimension: str, control: Optional[List[int]]):
        """
        This functions will be called to set a padding style.
        None - No padding
        List: first element 0, round seq length to the smallest list element larger than inputs
        """
        self.right_tensorizer.set_padding_control(dimension, control)
        self.left_tensorizer.set_padding_control(dimension, control)

    @torch.jit.script_method
    def _forward(self, right_inputs: ScriptBatchInput, left_inputs: ScriptBatchInput):
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_input_tensors = self.left_tensorizer(left_inputs)
        return self.model(right_input_tensors, left_input_tensors).cpu()

    @torch.jit.script_method
    def forward(self, right_texts: List[str], left_texts: List[str]) ->torch.Tensor:
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        return self._forward(right_inputs, left_inputs)

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[List[str], List[str]]]) ->List[torch.Tensor]:
        batchsize = len(batch)
        flat_right_texts: List[str] = []
        flat_left_texts: List[str] = []
        for i in range(batchsize):
            batch_right_element = batch[i][0]
            batch_left_element = batch[i][1]
            flat_right_texts.extend(batch_right_element)
            flat_left_texts.extend(batch_left_element)
        flat_result: torch.Tensor = self.forward(right_texts=flat_right_texts, left_texts=flat_left_texts)
        return destructure_tensor([len(be[0]) for be in batch], flat_result)

    @torch.jit.script_method
    def make_batch(self, mega_batch: List[Tuple[List[str], List[str], int]], goals: Dict[str, str]) ->List[List[Tuple[List[str], List[str], int]]]:
        mega_batch_key_list = [(max_tokens(self.right_tensorizer.tokenize(x[0], None)), n) for n, x in enumerate(mega_batch)]
        sorted_mega_batch_key_list = sorted(mega_batch_key_list)
        sorted_mega_batch = [mega_batch[n] for key, n in sorted_mega_batch_key_list]
        max_bs = int(goals.get('batchsize', '4'))
        len_mb = len(mega_batch)
        num_batches = (len_mb + max_bs - 1) // max_bs
        batch_list: List[List[Tuple[List[str], List[str], int]]] = []
        start = 0
        for _i in range(num_batches):
            end = min(start + max_bs, len_mb)
            batch_list.append(sorted_mega_batch[start:end])
            start = end
        return batch_list


class PyTextTwoTowerLayerModule(PyTextTwoTowerEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer):
        super().__init__(model, right_tensorizer, left_tensorizer)
        self.output_layer = output_layer

    @torch.jit.script_method
    def forward(self, right_texts: List[str], left_texts: List[str]):
        logits = super().forward(right_texts, left_texts)
        return self.output_layer(logits)


class PyTextTwoTowerEmbeddingModuleWithDense(PyTextTwoTowerEmbeddingModule):

    def __init__(self, model: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer, right_normalizer: VectorNormalizer, left_normalizer: VectorNormalizer):
        super().__init__(model, right_tensorizer, left_tensorizer)
        self.right_normalizer = right_normalizer
        self.left_normalizer = left_normalizer
        log_class_usage(self.__class__)

    @torch.jit.script_method
    def _forward(self, right_inputs: ScriptBatchInput, left_inputs: ScriptBatchInput, right_dense_tensor: torch.Tensor, left_dense_tensor: torch.Tensor):
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_input_tensors = self.left_tensorizer(left_inputs)
        if self.right_tensorizer.device != '':
            right_dense_tensor = right_dense_tensor
        if self.left_tensorizer.device != '':
            left_dense_tensor = left_dense_tensor
        return self.model(right_input_tensors, left_input_tensors, right_dense_tensor, left_dense_tensor).cpu()

    @torch.jit.script_method
    def forward(self, right_texts: List[str], left_texts: List[str], right_dense_feat: List[List[float]], left_dense_feat: List[List[float]]) ->torch.Tensor:
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        right_dense_feat = self.right_normalizer.normalize(right_dense_feat)
        left_dense_feat = self.left_normalizer.normalize(left_dense_feat)
        right_dense_tensor = torch.tensor(right_dense_feat, dtype=torch.float)
        left_dense_tensor = torch.tensor(left_dense_feat, dtype=torch.float)
        sentence_embedding = self._forward(right_inputs, left_inputs, right_dense_tensor, left_dense_tensor)
        return sentence_embedding


class PyTextTwoTowerLayerModuleWithDense(PyTextTwoTowerLayerModule):

    def __init__(self, model: torch.jit.ScriptModule, output_layer: torch.jit.ScriptModule, right_tensorizer: ScriptTensorizer, left_tensorizer: ScriptTensorizer, right_normalizer: VectorNormalizer, left_normalizer: VectorNormalizer):
        super().__init__(model, output_layer, right_tensorizer, left_tensorizer)
        self.right_normalizer = right_normalizer
        self.left_normalizer = left_normalizer

    @torch.jit.script_method
    def forward(self, right_texts: List[str], left_texts: List[str], right_dense_feat: List[List[float]], left_dense_feat: List[List[float]]):
        right_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(right_texts), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        right_input_tensors = self.right_tensorizer(right_inputs)
        left_inputs: ScriptBatchInput = ScriptBatchInput(texts=resolve_texts(left_texts), tokens=squeeze_2d(None), languages=squeeze_1d(None))
        left_input_tensors = self.left_tensorizer(left_inputs)
        right_dense_feat = self.right_normalizer.normalize(right_dense_feat)
        left_dense_feat = self.left_normalizer.normalize(left_dense_feat)
        right_dense_tensor = torch.tensor(right_dense_feat, dtype=torch.float)
        left_dense_tensor = torch.tensor(left_dense_feat, dtype=torch.float)
        if self.right_tensorizer.device != '':
            right_dense_tensor = right_dense_tensor
        if self.left_tensorizer.device != '':
            left_dense_tensor = left_dense_tensor
        logits = self.model(right_input_tensors, left_input_tensors, right_dense_tensor, left_dense_tensor)
        return self.output_layer(logits)

    @torch.jit.script_method
    def make_prediction(self, batch: List[Tuple[List[str], List[str], List[List[float]], List[List[float]]]]) ->List[torch.Tensor]:
        batchsize = len(batch)
        flat_right_texts: List[str] = []
        flat_left_texts: List[str] = []
        flat_right_dense: List[List[float]] = []
        flat_left_dense: List[List[float]] = []
        for i in range(batchsize):
            batch_right_element = batch[i][0]
            batch_left_element = batch[i][1]
            batch_right_dense_element = batch[i][2]
            batch_left_dense_element = batch[i][3]
            flat_right_texts.extend(batch_right_element)
            flat_left_texts.extend(batch_left_element)
            flat_right_dense.extend(batch_right_dense_element)
            flat_left_dense.extend(batch_left_dense_element)
        flat_result: torch.Tensor = self.forward(right_texts=flat_right_texts, left_texts=flat_left_texts, right_dense_feat=flat_right_dense, left_dense_feat=flat_left_dense)
        return destructure_tensor([len(be[0]) for be in batch], flat_result)

    @torch.jit.script_method
    def make_batch(self, mega_batch: List[Tuple[List[str], List[str], List[List[float]], List[List[float]], int]], goals: Dict[str, str]) ->List[List[Tuple[List[str], List[str], List[List[float]], List[List[float]], int]]]:
        mega_batch_key_list = [(max_tokens(self.right_tensorizer.tokenize(x[0], None)), n) for n, x in enumerate(mega_batch)]
        sorted_mega_batch_key_list = sorted(mega_batch_key_list)
        sorted_mega_batch = [mega_batch[n] for key, n in sorted_mega_batch_key_list]
        max_bs = int(goals.get('batchsize', '4'))
        len_mb = len(mega_batch)
        num_batches = (len_mb + max_bs - 1) // max_bs
        batch_list: List[List[Tuple[List[str], List[str], List[List[float]], List[List[float]], int]]] = []
        start = 0
        for _i in range(num_batches):
            end = min(start + max_bs, len_mb)
            batch_list.append(sorted_mega_batch[start:end])
            start = end
        return batch_list


class ScriptInteger1DListTensorizer(torch.jit.ScriptModule):
    """
    TorchScript implementation of Integer1DListTensorizer in pytext/data/tensorizers.py
    """

    def __init__(self):
        super().__init__()
        self.pad_idx = 0

    @torch.jit.script_method
    def numberize(self, integerList: List[int]) ->Tuple[List[int], int]:
        return integerList, len(integerList)

    @torch.jit.script_method
    def tensorize(self, integerList: List[List[int]], seq_lens: List[int]) ->torch.Tensor:
        integerListTensor = torch.tensor(pad_2d(integerList, seq_lens=seq_lens, pad_idx=self.pad_idx), dtype=torch.long)
        return integerListTensor

    @torch.jit.ignore
    def torchscriptify(self):
        return torch.jit.script(self)


@torch.jit.script
def pad_2d_float(batch: List[List[float]], seq_lens: List[int], pad_val: float=0.0, max_len: int=-1) ->List[List[float]]:
    pad_to_length = max(seq_lens)
    if max_len > 0:
        pad_to_length = min(pad_to_length, max_len)
    for sentence in batch:
        padding = pad_to_length - len(sentence)
        if padding >= 0:
            for _ in range(padding):
                sentence.append(pad_val)
        else:
            for _ in range(-padding):
                sentence.pop()
    return batch


class ScriptFloat1DListTensorizer(torch.jit.ScriptModule):
    """
    TorchScript implementation of Float1DListTensorizer in pytext/data/tensorizers.py
    """

    def __init__(self):
        super().__init__()
        self.pad_val = 1.0

    @torch.jit.script_method
    def numberize(self, floatList: List[float]) ->Tuple[List[float], int]:
        return floatList, len(floatList)

    @torch.jit.script_method
    def tensorize(self, floatLists: List[List[float]], seq_lens: List[int]) ->torch.Tensor:
        floatListTensor = torch.tensor(pad_2d_float(floatLists, seq_lens=seq_lens, pad_val=self.pad_val), dtype=torch.float)
        return floatListTensor

    def torchscriptify(self):
        return torch.jit.script(self)


@torch.jit.script
def pad_3d_float(batch: List[List[List[float]]], seq_lens: List[int], pad_val: float=0.0) ->List[List[List[float]]]:
    outer_pad_to_length = max(seq_lens)
    inner_pad_to_length = -1
    for outer_list in batch:
        for inner_list in outer_list:
            inner_pad_to_length = max(inner_pad_to_length, len(inner_list))
    for outer_list in batch:
        for inner_list in outer_list:
            for _ in range(inner_pad_to_length - len(inner_list)):
                inner_list.append(pad_val)
        for _ in range(outer_pad_to_length - len(outer_list)):
            outer_list.append([pad_val] * inner_pad_to_length)
    return batch


class ScriptFloatListSeqTensorizer(torch.jit.ScriptModule):
    """
    TorchScript implementation of ScriptFloatListSeqTensorizer in pytext/data/tensorizers.py
    """

    def __init__(self, pad_token):
        super().__init__()
        self.pad_val = pad_token

    @torch.jit.script_method
    def numberize(self, floatList: List[List[float]]) ->Tuple[List[List[float]], int]:
        return floatList, len(floatList)

    @torch.jit.script_method
    def tensorize(self, floatLists: List[List[List[float]]], seq_lens: List[int]) ->Tuple[torch.Tensor, torch.Tensor]:
        floatListTensor = torch.tensor(pad_3d_float(floatLists, seq_lens=seq_lens, pad_val=self.pad_val), dtype=torch.float)
        seqLensTensor = torch.tensor(seq_lens, dtype=torch.long)
        return floatListTensor, seqLensTensor

    def torchscriptify(self):
        return torch.jit.script(self)


class ScriptXLMTensorizer(ScriptTensorizer):

    def __init__(self, tokenizer: torch.jit.ScriptModule, token_vocab: ScriptVocabulary, language_vocab: ScriptVocabulary, max_seq_len: int, default_language: str):
        super().__init__()
        self.tokenizer = tokenizer
        self.token_vocab = token_vocab
        self.language_vocab = language_vocab
        self.token_vocab_lookup = VocabLookup(token_vocab)
        self.language_vocab_lookup = VocabLookup(language_vocab)
        self.max_seq_len = torch.jit.Attribute(max_seq_len, int)
        self.default_language = torch.jit.Attribute(default_language, str)

    @torch.jit.script_method
    def tokenize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]], language_row: List[str]) ->Tuple[List[List[Tuple[str, int, int]]], List[List[Tuple[str, int, int]]]]:
        per_sentence_tokens: List[List[Tuple[str, int, int]]] = []
        per_sentence_languages: List[List[Tuple[str, int, int]]] = []
        if text_row is not None:
            """
            Tokenize every single text into a list of tokens.
            For example:
            text_row = ["hello world", "this is sentence"]
            per_sentence_tokens = [["hello", "world"], ["this", "is", "sentence"]]
            """
            for idx, text in enumerate(text_row):
                sentence_tokens: List[Tuple[str, int, int]] = self.tokenizer.tokenize(text)
                sentence_languages: List[Tuple[str, int, int]] = [(language_row[idx], token[1], token[2]) for token in sentence_tokens]
                per_sentence_tokens.append(sentence_tokens)
                per_sentence_languages.append(sentence_languages)
        elif token_row is not None:
            """
            Tokenize every single token into a sub tokens. (example: BPE)
            For example:
            token_row = [["hello", "world"], ["this", "is", "sentence"]]
            per_sentence_tokens = [
                ["he", "llo" "wo", "rld"], ["th", "is", "is", "sen", "tence"]
            ]
            """
            for idx, sentence_raw_tokens in enumerate(token_row):
                sentence_tokens: List[Tuple[str, int, int]] = []
                sentence_languages: List[Tuple[str, int, int]] = []
                for raw_token in sentence_raw_tokens:
                    sub_tokens: List[Tuple[str, int, int]] = self.tokenizer.tokenize(raw_token)
                    sub_languages: List[Tuple[str, int, int]] = [(language_row[idx], token[1], token[2]) for token in sub_tokens]
                    sentence_tokens.extend(sub_tokens)
                    sentence_languages.extend(sub_languages)
                per_sentence_tokens.append(sentence_tokens)
                per_sentence_languages.append(sentence_languages)
        return per_sentence_tokens, per_sentence_languages

    @torch.jit.script_method
    def _lookup_tokens(self, tokens: List[Tuple[str, int, int]], languages: List[Tuple[str, int, int]], max_seq_len: int) ->Tuple[List[int], List[int]]:
        token_ids: List[int] = self.token_vocab_lookup(tokens, bos_idx=self.token_vocab.eos_idx, eos_idx=self.token_vocab.eos_idx, use_eos_token_for_bos=True, max_seq_len=max_seq_len)[0]
        language_special_idx: int = self.language_vocab.idx.get(languages[0][0], self.language_vocab.unk_idx)
        language_ids = self.language_vocab_lookup(languages, bos_idx=language_special_idx, eos_idx=language_special_idx, use_eos_token_for_bos=True, max_seq_len=max_seq_len)[0]
        return token_ids, language_ids

    @torch.jit.script_method
    def numberize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]], language_row: List[str]) ->Tuple[List[int], List[int], int, List[int]]:
        per_sentence_tokens, per_sentence_languages = self.tokenize(text_row, token_row, language_row)
        token_ids: List[int] = []
        language_ids: List[int] = []
        max_seq_len: int = self.max_seq_len // len(per_sentence_tokens)
        for idx in range(len(per_sentence_tokens)):
            lookup_token_ids, lookup_language_ids = self._lookup_tokens(per_sentence_tokens[idx], per_sentence_languages[idx], max_seq_len)
            token_ids.extend(lookup_token_ids)
            language_ids.extend(lookup_language_ids)
        seq_len: int = len(token_ids)
        positions: List[int] = [i for i in range(seq_len)]
        return token_ids, language_ids, seq_len, positions

    @torch.jit.script_method
    def tensorize(self, texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[List[str]]]]=None, languages: Optional[List[List[str]]]=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        batch_size: int = self.batch_size(texts, tokens)
        row_size: int = self.row_size(texts, tokens)
        if languages is None:
            languages = [[self.default_language] * row_size] * batch_size
        tokens_2d: List[List[int]] = []
        languages_2d: List[List[int]] = []
        seq_len_2d: List[int] = []
        positions_2d: List[List[int]] = []
        for idx in range(batch_size):
            numberized: Tuple[List[int], List[int], int, List[int]] = self.numberize(self.get_texts_by_index(texts, idx), self.get_tokens_by_index(tokens, idx), languages[idx])
            tokens_2d.append(numberized[0])
            languages_2d.append(numberized[1])
            seq_len_2d.append(numberized[2])
            positions_2d.append(numberized[3])
        tokens, pad_mask = pad_2d_mask(tokens_2d, pad_value=self.token_vocab.pad_idx, seq_padding_control=self.seq_padding_control, max_seq_pad_len=self.max_seq_len, batch_padding_control=self.batch_padding_control)
        languages, _ = pad_2d_mask(languages_2d, pad_value=0, seq_padding_control=self.seq_padding_control, max_seq_pad_len=self.max_seq_len, batch_padding_control=self.batch_padding_control)
        positions, _ = pad_2d_mask(positions_2d, pad_value=0, seq_padding_control=self.seq_padding_control, max_seq_pad_len=self.max_seq_len, batch_padding_control=self.batch_padding_control)
        if self.device == '':
            return tokens, pad_mask, languages, positions
        else:
            return tokens, pad_mask, languages, positions


class MyTensorizer(ScriptTensorizer):

    def __init__(self, tokenizer: torch.jit.ScriptModule, vocab: ScriptVocabulary, max_seq_len: int=100):
        super().__init__()
        self.tokenizer = tokenizer
        self.vocab = vocab
        self.vocab_lookup = VocabLookup(vocab)
        self.max_seq_len = torch.jit.Attribute(max_seq_len, int)

    @torch.jit.script_method
    def _lookup_tokens(self, tokens: List[Tuple[str, int, int]]) ->List[int]:
        return self.vocab_lookup(tokens, bos_idx=None, eos_idx=self.vocab.eos_idx, use_eos_token_for_bos=False, max_seq_len=self.max_seq_len)[0]

    @torch.jit.script_method
    def _wrap_numberized_tokens(self, token_ids: List[int], idx: int) ->List[int]:
        if idx == 0:
            token_ids = [self.vocab.bos_idx] + token_ids
        return token_ids

    @torch.jit.script_method
    def tokenize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]]) ->List[List[Tuple[str, int, int]]]:
        """
        Process a single line of raw inputs into tokens, it supports
        two input formats:
            1) a single line of texts (single sentence or a pair)
            2) a single line of pre-processed tokens (single sentence or a pair)
        """
        per_sentence_tokens: List[List[Tuple[str, int, int]]] = []
        if text_row is not None:
            for text in text_row:
                per_sentence_tokens.append(self.tokenizer.tokenize(text))
        elif token_row is not None:
            for sentence_raw_tokens in token_row:
                sentence_tokens: List[Tuple[str, int, int]] = []
                for raw_token in sentence_raw_tokens:
                    sentence_tokens.extend(self.tokenizer.tokenize(raw_token))
                per_sentence_tokens.append(sentence_tokens)
        return per_sentence_tokens

    @torch.jit.script_method
    def numberize(self, text_row: Optional[List[str]], token_row: Optional[List[List[str]]]) ->Tuple[List[int], List[int], int, List[int]]:
        """
        Process a single line of raw inputs into numberized result, it supports
        two input formats:
            1) a single line of texts (single sentence or a pair)
            2) a single line of pre-processed tokens (single sentence or a pair)

        This function should handle the logic of calling tokenize(), add special
        tokens and vocab lookup.
        """
        token_ids: List[int] = []
        segment_labels: List[int] = []
        seq_len: int = 0
        positions: List[int] = []
        per_sentence_tokens: List[List[Tuple[str, int, int]]] = self.tokenize(text_row, token_row)
        for idx, per_sentence_token in enumerate(per_sentence_tokens):
            lookup_ids: List[int] = self._lookup_tokens(per_sentence_token)
            lookup_ids = self._wrap_numberized_tokens(lookup_ids, idx)
            token_ids.extend(lookup_ids)
            segment_labels.extend([idx] * len(lookup_ids))
        seq_len = len(token_ids)
        positions = list(range(seq_len))
        return token_ids, segment_labels, seq_len, positions

    @torch.jit.script_method
    def tensorize(self, texts: Optional[List[List[str]]]=None, tokens: Optional[List[List[List[str]]]]=None) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Process raw inputs into model input tensors, it supports two input
        formats:
            1) multiple rows of texts (single sentence or a pair)
            2) multiple rows of pre-processed tokens (single sentence or a pair)

        This function should handle the logic of calling numberize() and also
        padding the numberized result.
        """
        tokens_2d: List[List[int]] = []
        segment_labels_2d: List[List[int]] = []
        seq_len_2d: List[int] = []
        positions_2d: List[List[int]] = []
        for idx in range(self.batch_size(texts, tokens)):
            numberized: Tuple[List[int], List[int], int, List[int]] = self.numberize(self.get_texts_by_index(texts, idx), self.get_tokens_by_index(tokens, idx))
            tokens_2d.append(numberized[0])
            segment_labels_2d.append(numberized[1])
            seq_len_2d.append(numberized[2])
            positions_2d.append(numberized[3])
        tokens, pad_mask = pad_2d_mask(tokens_2d, pad_value=self.vocab.pad_idx)
        segment_labels = torch.tensor(pad_2d(segment_labels_2d, seq_lens=seq_len_2d, pad_idx=self.vocab.pad_idx), dtype=torch.long)
        positions = torch.tensor(pad_2d(positions_2d, seq_lens=seq_len_2d, pad_idx=self.vocab.pad_idx), dtype=torch.long)
        if self.device == '':
            return tokens, pad_mask, segment_labels, positions
        else:
            return tokens, pad_mask, segment_labels, positions

    @torch.jit.script_method
    def forward(self, inputs: ScriptBatchInput) ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        if inputs.texts is not None or inputs.tokens is not None:
            return self.tensorize(inputs.texts, inputs.tokens)
        else:
            raise RuntimeError('Empty input for both texts and tokens.')


@torch.jit.script
def utf8_chars(s: str) ->List[str]:
    """An implementation of UTF8 character iteration in TorchScript.
    There are no bitwise operations in torchscript, so we compare directly to
    integer values. There isn't a lot of validation, for instance if you pass
    in an improperly encoded string with an out-of-place continuation byte,
    or with a non-left-to-right byte order, you'll get unexpected results
    and likely throw. Torch itself takes in unicode strings and encodes them
    as UTF8, so that should be actively hard to do.

    The logic is simple: looking at the current start-of-character byte.
    If its high bit is 0, it's a 1-byte character. Otherwise, the number of
    bytes is the number of leading 1s in its binary representation, so
    find that number by comparing it directly to ints with the appropriate
    representation, then append that many bytes as a character and move past
    them to the next start byte.
    """
    chars = torch.jit.annotate(List[str], [])
    i = 0
    while i < len(s):
        byte = ord(s[i])
        if byte < 128:
            chars.append(s[i])
            i += 1
        else:
            if byte < 224:
                num_bytes = 2
            elif byte < 240:
                num_bytes = 3
            elif byte < 248:
                num_bytes = 4
            elif byte < 252:
                num_bytes = 5
            elif byte < 254:
                num_bytes = 6
            elif byte < 255:
                num_bytes = 7
            else:
                num_bytes = 8
            chars.append(s[i:i + num_bytes])
            i += num_bytes
    return chars


class Infer:
    """A value which can be inferred from a forward pass. Infer objects should
    be passed as arguments or keyword arguments to Lazy objects; see Lazy
    documentation for more details.
    """

    def __init__(self, resolve_fn):
        """resolve_fn is called by Lazy on the arguments of the first forward pass
        to the Lazy module, and the Infer object will be replaced in the call by the
        output of this function. It should have the same signature as the
        Lazy-wrapped Module's forward function."""
        self.resolve = resolve_fn

    @classmethod
    def dimension(cls, dim):
        """A helper for creating Infer arguments looking at specific dimensions."""
        return cls(lambda input: input.size()[dim])


class UninitializedLazyModuleError(Exception):
    """A lazy module was used improperly."""


class Lazy(nn.Module):
    """
    A module which is able to infer some of its parameters from the inputs to
    its first forward pass. Lazy wraps any other nn.Module, and arguments can be passed
    that will be used to construct that wrapped Module after the first forward pass.
    If any of these arguments are Infer objects, those arguments will be replaced by
    calling the callback of the Infer object on the forward pass input.

    For instance,
    >>> Lazy(nn.Linear, Infer(lambda input: input.size(-1)), 4)
    Lazy()

    takes its in_features dimension from the last dimension of the input to its forward
    pass. This can be simplified to

    >>> Lazy(nn.Linear, Infer.dimension(-1), 4)

    or a partial can be created, for instance

    >>> LazyLinear = Lazy.partial(nn.Linear, Infer.dimension(-1))
    >>> LazyLinear(4)
    Lazy()

    Finally, these Lazy objects explicitly forbid treating themselves normally;
    they must instead be replaced by calling `init_lazy_modules`
    on your model before training. For instance,

    >>> ll = lazy.Linear(4)
    >>> seq = nn.Sequential(ll)
    >>> seq
    Sequential(
        0: Lazy(),
    )
    >>> init_lazy_modules(seq, torch.rand(1, 2)
    Sequential(
        0: Linear(in_features=2, out_features=4, bias=True)
    )
    """

    def __init__(self, module_class, *args, **kwargs):
        super().__init__()
        self._module = None
        self._module_class = module_class
        self._args = args
        self._kwargs = kwargs

    @classmethod
    def partial(cls, module_class, *args, **kwargs):
        return functools.partial(cls, module_class, *args, **kwargs)

    @property
    def _parameters(self):
        raise UninitializedLazyModuleError('Must call init_lazy_modules before getting parameters')

    @_parameters.setter
    def _parameters(self, value):
        return None

    def __setattr__(self, name, value):
        return object.__setattr__(self, name, value)

    def forward(self, *args, **kwargs):
        if not self._module:
            constructor_args = [(arg if not isinstance(arg, Infer) else arg.resolve(*args, **kwargs)) for arg in self._args]
            constructor_kwargs = {key: (arg if not isinstance(arg, Infer) else arg.resolve(*args, **kwargs)) for key, arg in self._kwargs.items()}
            self._module = self._module_class(*constructor_args, **constructor_kwargs)
        return self._module(*args, **kwargs)

    def resolve(self):
        """Must make a call to forward before calling this function; returns the
        full nn.Module object constructed using inferred arguments/dimensions."""
        if not self._module:
            raise UninitializedLazyModuleError('Must call forward before calling resolve on a lazy module')
        return self._module


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (BiLSTM,
     lambda: ([], {'num_layers': 1, 'bidirectional': 4, 'embed_dim': 4, 'hidden_dim': 4, 'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4]), torch.ones([4], dtype=torch.int64)], {}),
     True),
    (ContextualTokenEmbedding,
     lambda: ([], {'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ContextualWordConvolution,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_sizes': [4, 4]}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (LastTimestepPool,
     lambda: ([], {'config': _mock_config(), 'n_input': 4}),
     lambda: ([torch.ones([4, 4, 4], dtype=torch.int64), torch.ones([4], dtype=torch.int64)], {}),
     True),
    (MaskedLengthPredictionModule,
     lambda: ([], {'embed_dim': 4, 'length_hidden_dim': 4, 'max_target_positions': 4, 'length_dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (MaxPool,
     lambda: ([], {'config': _mock_config(), 'n_input': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (MeanPool,
     lambda: ([], {'config': _mock_config(), 'n_input': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (MultiLabelClassificationScores,
     lambda: ([], {'scores': [_mock_layer()]}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (NoPool,
     lambda: ([], {'config': _mock_config(), 'n_input': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PlaceholderAttentionIdentity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (PlaceholderIdentity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SelfAttention,
     lambda: ([], {'config': _mock_config(dropout=0.5, attn_dimension=4), 'n_input': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (SeparableConv1d,
     lambda: ([], {'input_channels': 4, 'output_channels': 4, 'kernel_size': 4, 'padding': 4, 'dilation': 1, 'bottleneck': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (SinusoidalPositionalEmbedding,
     lambda: ([], {'embedding_dim': 4, 'padding_idx': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (SlotAttention,
     lambda: ([], {'config': _mock_config(attention_type=4), 'n_input': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (Trim1d,
     lambda: ([], {'trim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (VectorNormalizer,
     lambda: ([], {'dim': 4}),
     lambda: ([], {}),
     True),
]

class Test_facebookresearch_pytext(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

