import sys
_module = sys.modules[__name__]
del sys
chairssdhom_384x448 = _module
flyingchairs_320x448 = _module
flyingchairs_384x448 = _module
flyingchairs_raft_368x496 = _module
flyingchairsocc_bi_occ_384x448 = _module
flyingthings3d_raft_400x720 = _module
flyingthings3d_subset_384x768 = _module
flyingthings3d_subset_bi_with_occ_384x768 = _module
flyingthings3d_subset_chairssdhom_384x448 = _module
kitti2012_kitti2015_320x896 = _module
kitti2012_kitti2015_irr_320x896 = _module
kitti2012_kitti2015_test = _module
kitti2015_raft_288x960 = _module
kitti2015_raft_test = _module
sintel_384x768 = _module
sintel_cleanx100_sintel_fianlx100_flyingthings3d_raft_368x768 = _module
sintel_cleanx100_sintel_fianlx100_kitti2015x200_hd1kx5_flyingthings3d_raft_384x768 = _module
sintel_final_384x768 = _module
sintel_final_irr_with_occ_384x768 = _module
sintel_final_pwcnet_384x768 = _module
sintel_irr_with_occ_384x768 = _module
sintel_kitti2015_hd1k_320x768 = _module
sintel_kitti2015_hd1k_maskflownet_320x768 = _module
sintel_kitti_liteflownet2_320x768 = _module
sintel_liteflownet_384x768 = _module
sintel_pwcnet_384x768 = _module
default_runtime = _module
flownet2 = _module
flownet2cs = _module
flownet2css = _module
flownet2sd = _module
flownetc = _module
flownets = _module
gma = _module
irrpwc = _module
liteflownet = _module
liteflownet_pre_M2S2R2 = _module
liteflownet_pre_M3S3R3 = _module
liteflownet_pre_M4S4R4 = _module
liteflownet_pre_M5S5R5 = _module
liteflownet_pre_M6S6 = _module
liteflownet_pre_M6S6R6 = _module
liteflownet2 = _module
liteflownet2_pre_M3S3R3 = _module
liteflownet2_pre_M4S4R4 = _module
liteflownet2_pre_M5S5R5 = _module
liteflownet2_pre_M6S6 = _module
liteflownet2_pre_M6S6R6 = _module
maskflownet = _module
maskflownets = _module
pwcnet = _module
raft = _module
maskflownet_ft_1000k_schedule = _module
pwcnet_ft_300k_schedule = _module
pwcnet_plus_750k_schedule = _module
schedule_s_fine = _module
schedule_s_fine_half = _module
schedule_s_long = _module
schedule_s_short = _module
flownetc_8x1_sfine_flyingthings3d_subset_384x768 = _module
flownetc_8x1_sfine_sintel_384x448 = _module
flownetc_8x1_slong_flyingchairs_384x448 = _module
flownetc_kitti_test = _module
flownets_8x1_sfine_sintel_384x448 = _module
flownets_8x1_slong_flyingchairs_384x448 = _module
flownets_kitti_test = _module
flownet2_8x1_sfine_flyingthings3d_subset_384x768 = _module
flownet2_8x1_slong_flyingchairs_384x448 = _module
flownet2_kitti_test = _module
flownet2cs_8x1_sfine_flyingthings3d_subset_384x768 = _module
flownet2cs_8x1_slong_flyingchairs_384x448 = _module
flownet2cs_kitti_test = _module
flownet2css_8x1_sfine_flyingthings3d_subset_384x768 = _module
flownet2css_8x1_slong_flyingchairs_384x448 = _module
flownet2css_kitti_test = _module
flownet2sd_8x1_slong_chairssdhom_384x448 = _module
gma_8x2_120k_flyingchairs_368x496 = _module
gma_8x2_120k_flyingthings3d_400x720 = _module
gma_8x2_120k_flyingthings3d_sintel_368x768 = _module
gma_8x2_120k_mixed_368x768 = _module
gma_8x2_50k_kitti2015_288x960 = _module
irrpwc_8x1_sfine_half_flyingthings3d_subset_384x768 = _module
irrpwc_8x1_sshort_flyingchairsocc_384x448 = _module
irrpwc_ft_4x1_300k_kitti_320x896 = _module
irrpwc_ft_4x1_300k_sintel_384x768 = _module
irrpwc_ft_4x1_300k_sintel_final_384x768 = _module
liteflownet_8x1_500k_flyingthings3d_subset_384x768 = _module
liteflownet_ft_4x1_500k_kitti_320x896 = _module
liteflownet_ft_4x1_500k_sintel_384x768 = _module
liteflownet_pre_M2S2R2_8x1_flyingchairs_320x448 = _module
liteflownet_pre_M3S3R3_8x1_flyingchairs_320x448 = _module
liteflownet_pre_M4S4R4_8x1_flyingchairs_320x448 = _module
liteflownet_pre_M5S5R5_8x1_flyingchairs_320x448 = _module
liteflownet_pre_M6S6R6_8x1_flyingchairs_320x448 = _module
liteflownet_pre_M6S6_8x1_flyingchairs_320x448 = _module
liteflownet2_8x1_500k_flyingthing3d_subset_384x768 = _module
liteflownet2_ft_4x1_500k_kitti_320x896 = _module
liteflownet2_ft_4x1_600k_sintel_kitti_320x768 = _module
liteflownet2_pre_M3S3R3_8x1_flyingchairs_320x448 = _module
liteflownet2_pre_M4S4R4_8x1_flyingchairs_320x448 = _module
liteflownet2_pre_M5S5R5_8x1_flyingchairs_320x448 = _module
liteflownet2_pre_M6S6R6_8x1_flyingchairs_320x448 = _module
liteflownet2_pre_M6S6_8x1_flyingchairs_320x448 = _module
maskflownet_8x1_500k_flyingthings3d_subset_384x768 = _module
maskflownet_8x1_800k_flyingchairs_384x448 = _module
maskflownet_kitti_test = _module
maskflownets_8x1_sfine_flyingthings3d_subset_384x768 = _module
maskflownets_8x1_slong_flyingchairs_384x448 = _module
maskflownets_kitti_test = _module
pwcnet_8x1_sfine_flyingthings3d_subset_384x768 = _module
pwcnet_8x1_slong_flyingchairs_384x448 = _module
pwcnet_ft_4x1_300k_kitti_320x896 = _module
pwcnet_ft_4x1_300k_sintel_384x768 = _module
pwcnet_ft_4x1_300k_sintel_final_384x768 = _module
pwcnet_kitti_test = _module
pwcnet_plus_8x1_750k_sintel_kitti2015_hd1k_320x768 = _module
raft_8x2_100k_flyingchairs_368x496 = _module
raft_8x2_100k_flyingthings3d_400x720 = _module
raft_8x2_100k_flyingthings3d_sintel_368x768 = _module
raft_8x2_100k_mixed_368x768 = _module
raft_8x2_50k_kitti2015_288x960 = _module
raft_kitti_test = _module
image_demo = _module
video_demo = _module
conf = _module
mmflow = _module
apis = _module
inference = _module
test = _module
train = _module
core = _module
evaluation = _module
eval_hooks = _module
evaluation = _module
metrics = _module
hooks = _module
liteflownet_stage_loading = _module
multistagelr_updater = _module
utils = _module
dist_utils = _module
datasets = _module
base_dataset = _module
builder = _module
chairssdhom = _module
dataset_wrappers = _module
flyingchairs = _module
flyingchairsocc = _module
flyingthings3d = _module
flyingthings3d_subset = _module
hd1k = _module
kiiti2012 = _module
kitti2015 = _module
pipelines = _module
advanced_transform = _module
compose = _module
formatting = _module
loading = _module
transforms = _module
samplers = _module
distributed_sampler = _module
sintel = _module
flow_io = _module
image = _module
models = _module
builder = _module
decoders = _module
base_decoder = _module
context_net = _module
flownet_decoder = _module
gma_decoder = _module
irr_refine = _module
irrpwc_decoder = _module
liteflownet_decoder = _module
maskflownet_decoder = _module
pwcnet_decoder = _module
raft_decoder = _module
encoders = _module
flownet_encoder = _module
liteflownet_encoder = _module
pwcnet_encoder = _module
raft_encoder = _module
flow_estimators = _module
base = _module
flownet = _module
flownet2 = _module
irrpwc = _module
liteflownet = _module
maskflownet = _module
pwcnet = _module
raft = _module
losses = _module
census_loss = _module
multilevel_bce = _module
multilevel_charbonnier_loss = _module
multilevel_epe = _module
multilevel_flow_loss = _module
sequence_loss = _module
smooth_loss = _module
ssim = _module
attention1d = _module
basic_encoder = _module
correlation_block = _module
densenet = _module
estimators_link = _module
occlusion_estimation = _module
res_layer = _module
ops = _module
builder = _module
corr_lookup = _module
warp = _module
collect_env = _module
logger = _module
misc = _module
set_env = _module
version = _module
setup = _module
test_inference = _module
test_eval_hook = _module
test_evaluation = _module
test_metrics = _module
test_hooks = _module
test_advanced_transform = _module
test_dataset_builder = _module
test_dataset_wrappers = _module
test_datasets = _module
test_datasets_utils = _module
test_loading = _module
test_transform = _module
test_context_net = _module
test_flownet_decoder = _module
test_gma_decoder = _module
test_irr_decoder = _module
test_irr_refine = _module
test_liteflownet_decoder = _module
test_maskflownets_decoder = _module
test_pwcnet_decoder = _module
test_raft_decoder = _module
test_flownet_encoder = _module
test_liteflownet_encoder = _module
test_pwcnet_encoder = _module
test_raft_encoder = _module
test_flow_estimator = _module
test_losses = _module
test_attention = _module
test_basic_encoder = _module
test_corrblock = _module
test_densenet = _module
test_esimators_link = _module
test_occlusion_esimation = _module
test_res_layer = _module
test_corr_lookup = _module
test_warp = _module
test_misc = _module
test_set_env = _module
benchmark = _module
frame2video = _module
merge_imgs_flowmaps = _module
print_cfg = _module
prepare_flyingchairs = _module
test = _module
mmflow2torchserve = _module
mmflow_handler = _module
test_torchserve = _module
train = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchtext, torchvision, types, typing, uuid, warnings
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import copy


from typing import List


from typing import Optional


from typing import Sequence


from typing import Union


import numpy as np


import torch


from typing import Any


from typing import Dict


import torch.distributed as dist


import random


import warnings


from torch.utils.data import DataLoader


from collections import defaultdict


from abc import ABCMeta


from abc import abstractmethod


from torch.utils.data import Dataset


from functools import partial


from logging import Logger


from torch.utils.data.dataset import ConcatDataset as _ConcatDataset


from collections.abc import Sequence


import math


from typing import Iterator


from torch.utils.data import DistributedSampler as _DistributedSampler


from torch.utils.data import Sampler


import torch.nn as nn


from torch.nn import Module


from typing import Tuple


import torch.nn.functional as F


from collections import OrderedDict


from numpy import ndarray


from torch import Tensor


from torch import nn


from math import sqrt


import torch.utils.checkpoint as cp


import torchvision


import torch.multiprocessing as mp


import logging


from torch.utils.data import DistributedSampler


from torch.utils.data import RandomSampler


from torch.utils.data import SequentialSampler


from torch.nn.modules import AvgPool2d


import time


def Upsample(img, factor) ->torch.Tensor:
    """Upsampling function.

    Args:
        img (tensor(B, C, H, W)): Input image for upsampling.
        factor (int): Upsampling factor.

    Returns:
        Upsampled image.
    """
    if factor == 1:
        return img
    _, _, H, W = img.shape
    img = F.pad(img, [0, 1, 0, 1], mode='replicate')
    upsamp_img = F.interpolate(img, (H * factor + 1, W * factor + 1), mode='bilinear', align_corners=True)
    upsamp_img = upsamp_img[:, :, :-1, :-1]
    return upsamp_img


def binary_cross_entropy(pred: torch.Tensor, target: torch.Tensor, balance: bool, reduction: str) ->torch.Tensor:
    """Calculate (weighted) binary cross entropy between occlusion prediction
    and occlusion ground truth.

    The loss function is
    .. math::
      loss = w \\cdot o_pred\\ln{o_gt}+\\bar{w}(1-o_pred)\\ln{(1-o_gt)}

    where the weights is:
    .. math::
      w=\\frac{H \\cdot W}{\\sum{o_pred}+\\sum{o_gt}}

    and
    .. math::
      \\bar{w}=\\frac{H \\cdot W}{\\sum{(1-o_pred)}+\\sum{(1-o_gt)}}

    but from IRR released code, the weights were scaled by 0.5.

    Args:
        pred (Tensor): output predicted occ map from flow_estimator
            with shape(B, 1, H, W).
        target (Tensor): ground truth occ map with shape (B, 1, H, W).
        balance (bool): whether balance the class weights for irr models.
        reduction (str): the reduction to apply to the output:'none', 'mean',
            'sum'. 'none': no reduction will be applied and will return a
            full-size epe map, 'mean': the mean of the epe map is taken, 'sum':
            the epe map will be summed but averaged by batch_size.
            Default: 'sum'.

    Return:
        Tensor: value of pixel-wise binary cross entropy loss.
    """
    assert pred.shape == target.shape, f'pred shape {pred.shape} does not match target shape {target.shape}.'
    b = pred.shape[0]
    h, w = pred.shape[2:]
    pred = torch.sigmoid(pred)
    if balance:
        tp_weight = 0.5 * h * w / (torch.sum(target, dim=[1, 2, 3]) + torch.sum(pred, dim=[1, 2, 3]) + 1e-08)
        fn_weight = 0.5 * h * w / (torch.sum(1 - target, dim=[1, 2, 3]) + torch.sum(1 - pred, dim=[1, 2, 3]) + 1e-08)
    else:
        tp_weight, fn_weight = torch.ones(b), torch.ones(b)
    tp = -target * torch.log(pred + 1e-08) * tp_weight.view(b, 1, 1, 1)
    fn = -(1 - target) * torch.log(1 - pred + 1e-08) * fn_weight.view(b, 1, 1, 1)
    bce_map = tp + fn
    if reduction == 'none':
        return torch.squeeze(bce_map)
    elif reduction == 'mean':
        return torch.mean(bce_map)
    elif reduction == 'sum':
        return torch.sum(bce_map) / b


def multi_levels_binary_cross_entropy(preds_dict: Dict[str, Union[Sequence[torch.Tensor], torch.Tensor]], target: torch.Tensor, weights: Dict[str, float]=dict(level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005), balance: bool=False, reduction: str='sum') ->torch.Tensor:
    """Multi-level binary cross entropy function.

    Args:
        preds_dict (dict): multi-level output of predicted occlusion, and the
            contain in dict is a Tensor or list of Tensor with shape
            (B, 1, H_l, W_l), where l indicates the level.
        target (Tensor): ground truth of occlusion with shape (B, 1, H, W).
        weights (dict): manual rescaling weights given to the loss of occlusion
            at each level, and the keys in weights must correspond to predicted
            dict. Defaults to dict(
            level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005).
        balance (bool): whether balance true positive and false negative by
            predicted and ground truth labels. Defaults to False.
        reduction (str): the reduction to apply to the output:'none', 'mean',
            'sum'. 'none': no reduction will be applied and will return a
            full-size epe map, 'mean': the mean of the epe map is taken, 'sum':
            the epe map will be summed but averaged by batch_size.
            Default: 'sum'.

    Returns:
        Tensor: value of pixel-wise binary cross entropy loss.
    """
    assert preds_dict.keys() == weights.keys(), 'Error: Keys of prediction do not match keys of weights!'
    loss = 0
    for k in weights.keys():
        cur_pred = preds_dict[k] if isinstance(preds_dict[k], (tuple, list)) else [preds_dict[k]]
        num_preds = len(cur_pred)
        h, w = cur_pred[0].shape[2:]
        cur_weight = weights.get(k)
        cur_target = F.adaptive_avg_pool2d(target, [h, w])
        for pred in cur_pred:
            loss += binary_cross_entropy(pred, cur_target, balance, reduction) * cur_weight
    return loss / num_preds


class MultiLevelBCE(nn.Module):
    """Multi-level binary cross entropy.

    Args:
        weights (dict): manual rescaling weights given to the loss of occlusion
            at each level, and the keys in weights must correspond to predicted
            dict. Defaults to dict(
            level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005).
        balance (bool): whether balance true positive and false negative by
            predicted and ground truth labels. Defaults to False.
        reduction (str): the reduction to apply to the output:'none', 'mean',
            'sum'. 'none': no reduction will be applied and will return a
            full-size epe map, 'mean': the mean of the epe map is taken, 'sum':
            the epe map will be summed but averaged by batch_size.
            Default: 'sum'.
    """

    def __init__(self, weights: Dict[str, float]=dict(level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005), balance: bool=False, reduction: str='sum') ->None:
        super().__init__()
        assert isinstance(balance, bool)
        self.balance = balance
        assert isinstance(weights, dict)
        self.weights = weights
        assert reduction in ('mean', 'sum')
        self.reduction = reduction

    def forward(self, preds_dict: dict, target: torch.Tensor) ->torch.Tensor:
        """Forwar function for MultiLevelBCE.

        Args:
            preds_dict (dict): multi-level output of predicted occlusion, and
                the contain in dict is a Tensor or list of Tensor with shape
                (B, 1, H_l, W_l), where l indicates the level.
            target (Tensor): ground truth of occlusion with shape (B, 1, H, W).

        Returns:
            Tensor: value of pixel-wise binary cross entropy loss.
        """
        return multi_levels_binary_cross_entropy(preds_dict, target, weights=self.weights, balance=self.balance, reduction=self.reduction)

    def __repr__(self) ->str:
        repr_str = self.__class__.__name__
        repr_str += f"(balance={self.balance}, weights={self.weights}, reduction='{self.reduction}')"
        return repr_str


def charbonnier_loss(pred: torch.Tensor, target: torch.Tensor, q: float=0.2, eps: float=0.01) ->torch.Tensor:
    """Generalized Charbonnier loss function between output and ground truth.

    The loss function is
    .. math::
      loss = ((u-u_gt)^2+(v-v_gt)^2+eps)^q

    Generalized Charbonnier loss was used in LiteFlowNet when fine tuning,
    with eps=0.01 q=0.2.

    Args:
        pred (torch.Tensor): output flow map from flow_estimator
            shape(B, 2, H, W).
        target (torch.Tensor): ground truth flow map shape(B, 2, H, W).
        q (float): the exponent in charbonnier loss.
        eps (float): small constant to numerical stability when
            fine-tuning model. Defaults to 0.01.

    Returns:
        Tensor: loss map with the shape (B, H, W).
    """
    assert pred.shape == target.shape, f'pred shape {pred.shape} does not match target shape {target.shape}.'
    diff = torch.add(pred, -target)
    loss_map = (torch.sum(diff * diff, dim=1) + eps) ** q
    return loss_map


def multi_level_flow_loss(loss_function, preds_dict: Dict[str, Union[torch.Tensor, List[torch.Tensor]]], target: torch.Tensor, weights: Dict[str, float]=dict(level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005), valid: Optional[torch.Tensor]=None, flow_div: float=20.0, max_flow: float=float('inf'), resize_flow: str='downsample', reduction: str='sum', scale_as_level: bool=False, **kwargs) ->torch.Tensor:
    """Multi-level endpoint error loss function.

    Args:
        loss_function: pixel-wise loss function for optical flow map.
        preds_dict (dict): multi-level output of predicted optical flow, and
            the contain in dict is a Tensor or list of Tensor with shape
            (B, 1, H_l, W_l), where l indicates the level.
        target (Tensor): ground truth of optical flow with shape (B, 2, H, W).
        weights (dict): manual rescaling weights given to the loss of flow map
            at each level, and the keys in weights must correspond to predicted
            dict. Defaults to dict(
            level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005).
        valid (Tensor, optional): valid mask for optical flow.
            Defaults to None.
        flow_div (float): the divisor used to scale down ground truth.
            Defaults to 20.
        max_flow (float): maximum value of optical flow, if some pixel's flow
            of target is larger than it, this pixel is not valid. Default to
            float('inf').
        reduction (str): the reduction to apply to the output:'none', 'mean',
            'sum'. 'none': no reduction will be applied and will return a
            full-size epe map, 'mean': the mean of the epe map is taken, 'sum':
            the epe map will be summed but averaged by batch_size.
            Default: 'sum'.
        resize_flow (str): mode for reszing flow: 'downsample' and 'upsample',
            as multi-level predicted outputs don't match the ground truth.
            If set to 'downsample', it will downsample the ground truth, and
            if set to 'upsample' it will upsample the predicted flow, and
            'upsample' is used for sparse flow map as no generic interpolation
            mode can resize a ground truth of sparse flow correctly.
            Default to 'downsample'.
        scale_as_level (bool): Whether flow for each level is at its native
            spatial resolution. If `'scale_as_level'` is True, the ground
            truth is scaled at different levels, if it is False, the ground
            truth will not be scaled. Default to False.
        kwargs: arguments for loss_function.

    Returns:
        Tensor: end-point error loss.
    """
    assert isinstance(weights, dict)
    assert list(preds_dict.keys()).sort() == list(weights.keys()).sort(), 'Error: Keys of prediction do not match keys of weights!'
    mag = torch.sum(target ** 2, dim=1).sqrt()
    if valid is None:
        valid = torch.ones_like(target[:, 0, :, :])
    else:
        valid = (valid >= 0.5) & (mag < max_flow)
    target_div = target / flow_div
    c_org, h_org, w_org = target.shape[1:]
    assert c_org == 2, f'The channels ground truth must be 2, but got {c_org}'
    loss = 0
    for level in weights.keys():
        cur_pred = preds_dict[level] if isinstance(preds_dict[level], (tuple, list)) else [preds_dict[level]]
        num_preds = len(cur_pred)
        b, _, h, w = cur_pred[0].shape
        scale_factor = torch.Tensor([float(w / w_org), float(h / h_org)]) if scale_as_level else torch.Tensor([1.0, 1.0])
        cur_weight = weights.get(level)
        if resize_flow == 'downsample':
            cur_target = F.adaptive_avg_pool2d(target_div, [h, w])
            cur_valid = F.adaptive_max_pool2d(valid, [h, w])
        else:
            cur_target = target_div
            cur_valid = valid
        loss_map = torch.zeros_like(cur_target[:, 0, ...])
        for i_pred in cur_pred:
            if resize_flow == 'upsample':
                i_pred = F.interpolate(i_pred, size=cur_target.shape[2:], mode='bilinear', align_corners=False)
            cur_target = torch.einsum('b c h w, c -> b c h w', cur_target, scale_factor)
            loss_map += loss_function(i_pred, cur_target, **kwargs) * cur_valid
            if reduction == 'mean':
                loss += loss_map.sum() / (cur_valid.sum() + 1e-08) * cur_weight
            elif reduction == 'sum':
                loss += loss_map.sum() / b * cur_weight
    return loss / num_preds


class MultiLevelCharbonnierLoss(nn.Module):
    """Multi-level Generalized Charbonnier loss.

    Args:

        q (float): the exponent in charbonnier loss.
        eps (float): small constant to numerical stability when
            fine-tuning model. Defaults to 0.01.
        weights (dict): manual rescaling weights given to the loss of flow map
            at each level, and the keys in weights must correspond to predicted
            dict. Defaults to dict(
            level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005).
        flow_div (float): the divisor used to scale down ground truth.
            Defaults to 20.
        max_flow (float): maximum value of optical flow, if some pixel's flow
            of target is larger than it, this pixel is not valid. Default to
            float('inf').
        resize_flow (str): mode for reszing flow: 'downsample' and 'upsample',
            as multi-level predicted outputs don't match the ground truth.
            If set to 'downsample', it will downsample the ground truth, and
            if set to 'upsample' it will upsample the predicted flow, and
            'upsample' is used for sparse flow map as no generic interpolation
            mode can resize a ground truth of sparse flow correctly.
            Default to 'downsample'.
        scale_as_level (bool): Whether flow for each level is at its native
            spatial resolution. If `'scale_as_level'` is True, the ground
            truth is scaled at different levels, if it is False, the ground
            truth will not be scaled. Default to False.
        reduction (str): the reduction to apply to the output:'none', 'mean',
            'sum'. 'none': no reduction will be applied and will return a
            full-size epe map, 'mean': the mean of the epe map is taken, 'sum':
            the epe map will be summed but averaged by batch_size.
            Default: 'sum'.
    """

    def __init__(self, q: float=0.2, eps: float=0.01, flow_div: float=20.0, weights: Dict[str, float]=dict(level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005), max_flow: float=float('inf'), resize_flow: str='downsample', scale_as_level: bool=False, reduction: str='sum') ->None:
        super().__init__()
        assert isinstance(q, float) and q > 0.0
        self.q = q
        assert isinstance(eps, float) and eps > 0.0
        self.eps = eps
        assert flow_div > 0
        self.flow_div = flow_div
        assert isinstance(weights, dict)
        self.weights = weights
        assert max_flow > 0.0
        self.max_flow = max_flow
        assert resize_flow in ('downsample', 'upsample')
        self.resize_flow = resize_flow
        assert isinstance(scale_as_level, bool)
        self.scale_as_level = scale_as_level
        assert reduction in ('mean', 'sum')
        self.reduction = reduction

    def forward(self, pred: Dict[str, Union[torch.Tensor, List[torch.Tensor]]], target: torch.Tensor, valid: Optional[torch.Tensor]=None) ->torch.Tensor:
        """Forwar function for MultiLevelCharbonnierLoss.

        Args:
            preds_dict (dict): Multi-level output of predicted optical flow,
                and the contain in dict is a Tensor or list of Tensor with
                shape (B, 1, H_l, W_l), where l indicates the level.
            target (Tensor): Ground truth of optical flow with shape
                (B, 2, H, W).
            valid (Tensor, optional): Valid mask for optical flow.
                Defaults to None.

        Returns:
            Tensor: value of pixel-wise generalized Charbonnier loss.
        """
        return multi_level_flow_loss(charbonnier_loss, pred, target, weights=self.weights, valid=valid, flow_div=self.flow_div, max_flow=self.max_flow, resize_flow=self.resize_flow, scale_as_level=self.scale_as_level, reduction=self.reduction, q=self.q, eps=self.eps)

    def __repr__(self) ->str:
        repr_str = self.__class__.__name__
        repr_str += f"(resize_flow={self.resize_flow}, scale_as_level={self.scale_as_level}, flow_div={self.flow_div}, weights={self.weights}, q={self.q}, eps={self.eps}, reduction='{self.reduction}')"
        return repr_str


def endpoint_error(pred: torch.Tensor, target: torch.Tensor, p: int=2, q: Optional[float]=None, eps: Optional[float]=None) ->torch.Tensor:
    """Calculate end point errors between prediction and ground truth.

    If not define q, the loss function is
    .. math::
      loss = \\Vert \\mathbf{u}-\\mathbf{u_gt} \\Vert^p

    otherwise,
    .. math::
      loss = (\\Vert \\mathbf{u}-\\mathbf{u_gt} \\Vert^p+eps)^q

    For PWC-Net L2 norm loss: p=2, for the robust loss function p=1, q=0.4,
    eps=0.01.

    Args:
        pred (torch.Tensor): output flow map from flow_estimator
            shape(B, 2, H, W).
        target (torch.Tensor): ground truth flow map shape(B, 2, H, W).
        p (int): norm degree for loss. Options are 1 or 2. Defaults to 2.
        q (float, optional): used to give less penalty to outliers when
            fine-tuning model. Defaults to 0.4.
        eps (float, optional): a small constant to numerical stability when
            fine-tuning model. Defaults to 0.01.

    Returns:
        Tensor: end-point error map with the shape (B, H, W).
    """
    assert pred.shape == target.shape, f'pred shape {pred.shape} does not match target shape {target.shape}.'
    epe_map = torch.norm(pred - target, p, dim=1)
    if q is not None and eps is not None:
        epe_map = (epe_map + eps) ** q
    return epe_map


class MultiLevelEPE(nn.Module):
    """Multi-level end point error loss.

    Args:

        p (int): norm degree for loss. Options are 1 or 2. Defaults to 2.
        q (float): used to give less penalty to outliers when fine-tuning
            model. Defaults to None.
        eps (float): a small constant to numerical stability when fine-tuning
            model. Defaults to None.
        weights (dict): manual rescaling weights given to the loss of flow map
            at each level, and the keys in weights must correspond to predicted
            dict. Defaults to dict(
            level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005).
        flow_div (float): the divisor used to scale down ground truth.
            Defaults to 20.
        max_flow (float): maximum value of optical flow, if some pixel's flow
            of target is larger than it, this pixel is not valid. Default to
            float('inf').
        resize_flow (str): mode for reszing flow: 'downsample' and 'upsample',
            as multi-level predicted outputs don't match the ground truth.
            If set to 'downsample', it will downsample the ground truth, and
            if set to 'upsample' it will upsample the predicted flow, and
            'upsample' is used for sparse flow map as no generic interpolation
            mode can resize a ground truth of sparse flow correctly.
            Default to 'downsample'.
        scale_as_level (bool): Whether flow for each level is at its native
            spatial resolution. If `'scale_as_level'` is True, the ground
            truth is scaled at different levels, if it is False, the ground
            truth will not be scaled. Default to False.
        reduction (str): the reduction to apply to the output:'none', 'mean',
            'sum'. 'none': no reduction will be applied and will return a
            full-size epe map, 'mean': the mean of the epe map is taken, 'sum':
            the epe map will be summed but averaged by batch_size.
            Default: 'sum'.
    """

    def __init__(self, p: int=2, q: Optional[float]=None, eps: Optional[float]=None, weights: Dict[str, float]=dict(level6=0.32, level5=0.08, level4=0.02, level3=0.01, level2=0.005), flow_div: float=20.0, max_flow: float=float('inf'), resize_flow: str='downsample', scale_as_level: bool=False, reduction: str='sum') ->None:
        super().__init__()
        assert p == 1 or p == 2
        self.p = p
        self.q = q
        if self.q is not None:
            assert self.q > 0
        self.eps = eps
        if self.eps is not None:
            assert eps > 0
        assert flow_div > 0
        self.flow_div = flow_div
        assert isinstance(weights, dict)
        self.weights = weights
        assert max_flow > 0.0
        self.max_flow = max_flow
        assert resize_flow in ('downsample', 'upsample')
        self.resize_flow = resize_flow
        assert isinstance(scale_as_level, bool)
        self.scale_as_level = scale_as_level
        assert reduction in ('mean', 'sum')
        self.reduction = reduction

    def forward(self, preds_dict: Dict[str, Union[torch.Tensor, List[torch.Tensor]]], target: torch.Tensor, valid: Optional[torch.Tensor]=None) ->torch.Tensor:
        """Forwar function for MultiLevelEPE.

        Args:
            preds_dict (dict): Multi-level output of predicted optical flow,
                and the contain in dict is a Tensor or list of Tensor with
                shape (B, 1, H_l, W_l), where l indicates the level.
            target (Tensor): Ground truth of optical flow with shape
                (B, 2, H, W).
            valid (Tensor, optional): Valid mask for optical flow.
                Defaults to None.

        Returns:
            Tensor: value of pixel-wise end point error loss.
        """
        return multi_level_flow_loss(endpoint_error, preds_dict, target, weights=self.weights, valid=valid, flow_div=self.flow_div, max_flow=self.max_flow, resize_flow=self.resize_flow, scale_as_level=self.scale_as_level, reduction=self.reduction, p=self.p, q=self.q, eps=self.eps)

    def __repr__(self) ->str:
        repr_str = self.__class__.__name__
        repr_str += f"(resize_flow={self.resize_flow}, scale_as_level={self.scale_as_level}, flow_div={self.flow_div}, weights={self.weights}, p={self.p}, q={self.q}, eps={self.eps}, reduction='{self.reduction}')"
        return repr_str


def sequence_loss(preds, flow_gt, gamma, valid=None, max_flow=400):
    """Compute sequence loss between prediction and ground truth.

    Args:
        preds (list(torch.Tensor)): List of flow prediction from
            flow_estimator.
        flow_gt (torch.Tensor): Ground truth flow map.
        gamma (float): Scale factor gamma in loss calculation.
        valid (torch.Tensor, optional): Tensor Used to exclude invalid pixels.
            Default: None.
        max_flow (int, optional): Used to exclude extremely large
            displacements. Default: 400.

    Returns:
        flow_loss (float): Total sequence loss.
    """
    n_preds = len(preds)
    flow_loss = 0.0
    mag = torch.sum(flow_gt ** 2, dim=1).sqrt()
    if valid is None:
        valid = torch.ones(flow_gt[:, 0, :, :].shape)
    else:
        valid = (valid >= 0.5) & (mag < max_flow)
    for i, pred in enumerate(preds):
        i_weight = gamma ** (n_preds - i - 1)
        i_loss = (pred - flow_gt).abs()
        flow_loss += i_weight * (valid[:, None] * i_loss).mean()
    return flow_loss


class SequenceLoss(nn.Module):
    """Sequence Loss for RAFT.

    Args:
        gamma (float): The base of exponentially increasing weights. Default to
            0.8.
        max_flow (float): The maximum value of optical flow, if some pixel's
            flow of target is larger than it, this pixel is not valid.
                Default to 400.
    """

    def __init__(self, gamma: float=0.8, max_flow: float=400.0) ->None:
        super().__init__()
        self.gamma = gamma
        self.max_flow = max_flow

    def forward(self, flow_preds: Sequence[Tensor], flow_gt: Tensor, valid: Tensor=None) ->Tensor:
        """Forward function for MultiLevelEPE.

        Args:
            preds_dict Sequence[Tensor]: The list of predicted optical flow.
            target (Tensor): Ground truth of optical flow with shape
                (B, 2, H, W).
            valid (Tensor, optional): Valid mask for optical flow.
                Defaults to None.

        Returns:
            Tensor: value of pixel-wise end point error loss.
        """
        return sequence_loss(flow_preds, flow_gt, self.gamma, valid, self.max_flow)


class LinkOutput:
    """The link output between two estimators in FlowNet2."""

    def __init__(self) ->None:
        self.upsample_flow = None
        self.scaled_flow = None
        self.norm_scaled_flow = None
        self.warped_img2 = None
        self.diff = None
        self.brightness_err = None


def build_operators(cfg: dict) ->Module:
    """build opterator with config dict.

    Args:
        cfg (dict): The config dict of operator.

    Returns:
        Module: The built operator.
    """
    return build_from_cfg(cfg, OPERATORS)


class BasicLink(nn.Module):
    """Connect two separate flow estimators.

    BasicLink compute the following 5 values: upsampled flow prediction,
    normalized upsampled flow prediction, warped image, difference between the
    first image and warped image, brightness error.

    Args:
        scale_factor (int): Scale factor of upsampling. Default to 4.
        mode (str): Algorithm used for upsampling: 'nearest' , 'linear' ,
            'bilinear' , 'bicubic' , 'trilinear' , 'area'. Default: 'bilinear'.
        warp_cfg (dict): Config for warp operator. Default to
            dict(type='Warp', padding_mode='border', align_corners=True))
    """

    def __init__(self, scale_factor: int=4, mode: str='bilinear', warp_cfg: dict=dict(type='Warp', padding_mode='border', align_corners=True)):
        super(BasicLink, self).__init__()
        self.warp = build_operators(warp_cfg)
        self.upSample = nn.Upsample(scale_factor=scale_factor, mode=mode)

    def __call__(self, img1: Tensor, img2: Tensor, flow: Tensor, flow_div: float) ->LinkOutput:
        """Call function for BasicLink.

        Args:
            img1 (Tensor): The first input image.
            img2 (Tensor): The second input images.
            flow (Tensor): The estimated optical flow from the first image to
                the second image.
            flow_div (float): The divisor for scaling the value of optical
                flow.

        Returns:
            LinkOutput: The output for the next flow estimator.
        """
        upsample_flow = self.upSample(flow)
        scaled_flow = self.upSample(flow) * flow_div
        norm_scaled_flow = torch.norm(scaled_flow, p=2, dim=1, keepdim=True)
        warped_img2 = self.warp(img2, scaled_flow)
        diff = img1 - warped_img2
        bright_err = torch.norm(diff, p=2, dim=1, keepdim=True)
        output = LinkOutput()
        output.upsample_flow = upsample_flow
        output.scaled_flow = scaled_flow
        output.norm_scaled_flow = norm_scaled_flow
        output.warped_img2 = warped_img2
        output.diff = diff
        output.brightness_err = bright_err
        return output


class ResLayer(nn.Sequential):
    """ResLayer to build ResNet style backbone.

    Args:
        block (nn.Module): block used to build ResLayer.
        inplanes (int): inplanes of block.
        planes (int): planes of block.
        num_blocks (int): number of blocks.
        stride (int): stride of the first block. Default: 1
        avg_down (bool): Use AvgPool instead of stride conv when
            downsampling in the bottleneck. Default: False
        conv_cfg (dict): dictionary to construct and config conv layer.
            Default: None
        norm_cfg (dict): dictionary to construct and config norm layer.
            Default: dict(type='BN')
        multi_grid (int | None): Multi grid dilation rates of last
            stage. Default: None
        contract_dilation (bool): Whether contract first dilation of each layer
            Default: False
    """

    def __init__(self, block, inplanes, planes, num_blocks, stride=1, dilation=1, avg_down=False, conv_cfg=None, norm_cfg=dict(type='BN'), multi_grid=None, contract_dilation=False, **kwargs):
        self.block = block
        downsample = None
        if stride != 1 or inplanes != planes * block.expansion:
            downsample = []
            conv_stride = stride
            if avg_down:
                conv_stride = 1
                downsample.append(nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False))
            downsample.extend([build_conv_layer(conv_cfg, inplanes, planes * block.expansion, kernel_size=1, stride=conv_stride, bias=True), build_norm_layer(norm_cfg, planes * block.expansion)[1]])
            downsample = nn.Sequential(*downsample)
        layers = []
        if multi_grid is None:
            if dilation > 1 and contract_dilation:
                first_dilation = dilation // 2
            else:
                first_dilation = dilation
        else:
            first_dilation = multi_grid[0]
        layers.append(block(inplanes=inplanes, planes=planes, stride=stride, dilation=first_dilation, downsample=downsample, conv_cfg=conv_cfg, norm_cfg=norm_cfg, **kwargs))
        inplanes = planes * block.expansion
        for i in range(1, num_blocks):
            layers.append(block(inplanes=inplanes, planes=planes, stride=1, dilation=dilation if multi_grid is None else multi_grid[i], conv_cfg=conv_cfg, norm_cfg=norm_cfg, **kwargs))
        super(ResLayer, self).__init__(*layers)


def bilinear_sample(feat: Tensor, grid: Tensor, mode: str='bilinear', padding_mode: str='zeros', align_corners: bool=False, scale: bool=True) ->Tensor:
    """Computes the output using input feature values and pixel locations from
    grid.

    Args:
        feat (Tensor): The input feature.
        grid (Tensor): The coordinate grid or the scaled coordinate that has
            values in the range of [-1, 1].
        mode (str): Interpolation mode to calculate output values.
            Defaults to 'bilinear'.
        padding_mode (str): Padding mode for outside grid values.
            Defaults to 'zeros'.
        align_corners (bool): If set to True, the extrema (-1 and 1) are
            considered as referring to the center points of the input’s corner
            pixels. If set to False, they are instead considered as referring
            to the corner points of the input’s corner pixels, making the
            sampling more resolution agnostic. Default to False.
        scale (bool): Whether scale the grid values in the range of [-1, 1].
            Defaults to True.

    Returns:
        Tensor: The output tensor using input feature values and pixel
            locations from grid
    """
    H, W = feat.shape[-2:]
    if grid.shape[-1] != 2:
        grid = grid.permute(0, 2, 3, 1)
    if scale:
        grid[:, :, :, 0] = grid[:, :, :, 0] * 2.0 / max(W - 1, 1) - 1.0
        grid[:, :, :, 1] = grid[:, :, :, 1] * 2.0 / max(H - 1, 1) - 1.0
    return F.grid_sample(feat, grid, mode, padding_mode, align_corners)


def coords_grid(flow: Tensor) ->Tensor:
    """Generate shifted coordinate grid based based input flow.

    Args:
        flow (Tensor): Estimated optical flow.

    Returns:
        Tensor: The coordinate that shifted by input flow and scale in the
            range [-1, 1].
    """
    B, _, H, W = flow.shape
    xx = torch.arange(0, W, device=flow.device, requires_grad=False)
    yy = torch.arange(0, H, device=flow.device, requires_grad=False)
    coords = torch.meshgrid(yy, xx)
    coords = torch.stack(coords[::-1], dim=0).float()
    grid = coords[None].repeat(B, 1, 1, 1) + flow
    grid[:, 0, ...] = grid[:, 0, ...] * 2.0 / max(W - 1, 1) - 1.0
    grid[:, 1, ...] = grid[:, 1, ...] * 2.0 / max(H - 1, 1) - 1.0
    grid = grid.permute(0, 2, 3, 1)
    return grid


class CorrLookup(nn.Module):
    """Correlation lookup operator.

    This operator is used in `RAFT<https://arxiv.org/pdf/2003.12039.pdf>`_

    Args:
        radius (int): the radius of the local neighborhood of the pixels.
            Default to 4.
        mode (str): interpolation mode to calculate output values 'bilinear'
            | 'nearest' | 'bicubic'. Default: 'bilinear' Note: mode='bicubic'
            supports only 4-D input.
        padding_mode (str): padding mode for outside grid values 'zeros' |
            'border' | 'reflection'. Default: 'zeros'
        align_corners (bool): If set to True, the extrema (-1 and 1) are
            considered as referring to the center points of the input’s corner
            pixels. If set to False, they are instead considered as referring
            to the corner points of the input’s corner pixels, making the
            sampling more resolution agnostic. Default to True.
    """

    def __init__(self, radius: int=4, mode: str='bilinear', padding_mode: str='zeros', align_corners: bool=True) ->None:
        super().__init__()
        self.r = radius
        self.mode = mode
        self.padding_mode = padding_mode
        self.align_corners = align_corners

    def forward(self, corr_pyramid: Sequence[Tensor], flow: Tensor) ->Tensor:
        """Forward function of Correlation lookup.

        Args:
            corr_pyramid (Sequence[Tensor]): Correlation pyramid.
            flow (Tensor): Current estimated optical flow.

        Returns:
            Tensor: Feature map by indexing from the correlation pyramid.
        """
        B, _, H, W = flow.shape
        xx = torch.arange(0, W, device=flow.device)
        yy = torch.arange(0, H, device=flow.device)
        grid = coords_grid(B, xx, yy) + flow
        grid = grid.permute(0, 2, 3, 1)
        dx = torch.linspace(-self.r, self.r, 2 * self.r + 1, device=flow.device)
        dy = torch.linspace(-self.r, self.r, 2 * self.r + 1, device=flow.device)
        delta = torch.stack(torch.meshgrid(dy, dx), axis=-1)
        delta_lvl = delta.view(1, 2 * self.r + 1, 2 * self.r + 1, 2)
        out_corr_pyramid = []
        for i, corr in enumerate(corr_pyramid):
            centroid_lvl = grid.reshape(B * H * W, 1, 1, 2) / 2 ** i
            coords_lvl = centroid_lvl + delta_lvl
            corr = bilinear_sample(corr, coords_lvl, self.mode, self.padding_mode, self.align_corners)
            corr = corr.view(B, H, W, -1)
            out_corr_pyramid.append(corr)
        out = torch.cat(out_corr_pyramid, dim=-1)
        return out.permute(0, 3, 1, 2).contiguous().float()


class Warp(nn.Module):
    """Warping layer to warp feature using optical flow.

    Args:
        mode (str): interpolation mode to calculate output values. Options are
            'bilinear' and 'nearest'. Defaults to 'bilinear'.
        padding_mode (str): padding mode for outside grid values. Options are
            'zero', 'border' and 'reflection'. Defaults to 'zeros'.
        align_corners (bool): If set to True, the extrema (-1 and 1) are
            considered as referring to the center points of the input’s corner
            pixels. If set to False, they are instead considered as referring
            to the corner points of the input’s corner pixels, making the
            sampling more resolution agnostic. Default to False.
    """

    def __init__(self, mode: str='bilinear', padding_mode: str='zeros', align_corners: bool=False, use_mask: bool=True) ->None:
        super().__init__()
        self.mode = mode
        self.padding_mode = padding_mode
        self.align_corners = align_corners
        self.use_mask = use_mask

    def forward(self, feat: Tensor, flow: Tensor) ->Tensor:
        """Forward function for warp.

        Args:
            feat (Tensor): Input feature
            flow (Tensor): Input optical flow.

        Returns:
            Tensor: The output feature that was generated by warping input
                feature based input flow.
        """
        grid = coords_grid(flow)
        out = F.grid_sample(feat, grid, mode=self.mode, padding_mode=self.padding_mode, align_corners=self.align_corners)
        mask = torch.ones(feat.size(), device=feat.device, requires_grad=False)
        if self.use_mask:
            mask = F.grid_sample(mask, grid, mode=self.mode, padding_mode=self.padding_mode, align_corners=self.align_corners)
            mask = (mask > 0.9999).float()
        return out * mask

    def __repr__(self):
        s = self.__class__.__name__
        s += f'(mode={self.mode}, '
        s += f'padding_mode={self.padding_mode}, '
        s += f'align_corners={self.align_corners},'
        s += f'use_mask={self.use_mask})'
        return s


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (SequenceLoss,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (Warp,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 2, 4, 4])], {}),
     False),
]

class Test_open_mmlab_mmflow(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

